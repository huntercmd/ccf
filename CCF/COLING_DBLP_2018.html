 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="cssfile" rel="stylesheet" type="text/css" href="https://rawcdn.githack.com/huntercmd/blog/master/config/css/light.css">
<script src="https://rawcdn.githack.com/huntercmd/blog/d9beff1/config/css/skin.js"></script>
<script src="https://rawcdn.githack.com/huntercmd/blog/master/config/css/classie.js"></script>

<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#27th COLING 2018:Santa Fe, New Mexico, USA">27th COLING 2018:Santa Fe, New Mexico, USA</a><ul>
<li><a href="#Paper Num: 330 || Session Num: 0">Paper Num: 330 || Session Num: 0</a><ul>
<li><a href="#1. A New Approach to Animacy Detection.">1. A New Approach to Animacy Detection.</a></li>
<li><a href="#2. Zero Pronoun Resolution with Attention-based Neural Network.">2. Zero Pronoun Resolution with Attention-based Neural Network.</a></li>
<li><a href="#3. They Exist! Introducing Plural Mentions to Coreference Resolution and Entity Linking.">3. They Exist! Introducing Plural Mentions to Coreference Resolution and Entity Linking.</a></li>
<li><a href="#4. Triad-based Neural Network for Coreference Resolution.">4. Triad-based Neural Network for Coreference Resolution.</a></li>
<li><a href="#5. Unsupervised Morphology Learning with Statistical Paradigms.">5. Unsupervised Morphology Learning with Statistical Paradigms.</a></li>
<li><a href="#6. Challenges of language technologies for the indigenous languages of the Americas.">6. Challenges of language technologies for the indigenous languages of the Americas.</a></li>
<li><a href="#7. Low-resource Cross-lingual Event Type Detection via Distant Supervision with Minimal Effort.">7. Low-resource Cross-lingual Event Type Detection via Distant Supervision with Minimal Effort.</a></li>
<li><a href="#8. Neural Transition-based String Transduction for Limited-Resource Setting in Morphology.">8. Neural Transition-based String Transduction for Limited-Resource Setting in Morphology.</a></li>
<li><a href="#9. Distance-Free Modeling of Multi-Predicate Interactions in End-to-End Japanese Predicate-Argument Structure Analysis.">9. Distance-Free Modeling of Multi-Predicate Interactions in End-to-End Japanese Predicate-Argument Structure Analysis.</a></li>
<li><a href="#10. Sprucing up the trees - Error detection in treebanks.">10. Sprucing up the trees - Error detection in treebanks.</a></li>
<li><a href="#11. Two Local Models for Neural Constituent Parsing.">11. Two Local Models for Neural Constituent Parsing.</a></li>
<li><a href="#12. RNN Simulations of Grammaticality Judgments on Long-distance Dependencies.">12. RNN Simulations of Grammaticality Judgments on Long-distance Dependencies.</a></li>
<li><a href="#13. How Predictable is Your State? Leveraging Lexical and Contextual Information for Predicting Legislative Floor Action at the State Level.">13. How Predictable is Your State? Leveraging Lexical and Contextual Information for Predicting Legislative Floor Action at the State Level.</a></li>
<li><a href="#14. Learning to Search in Long Documents Using Document Structure.">14. Learning to Search in Long Documents Using Document Structure.</a></li>
<li><a href="#15. Incorporating Image Matching Into Knowledge Acquisition for Event-Oriented Relation Recognition.">15. Incorporating Image Matching Into Knowledge Acquisition for Event-Oriented Relation Recognition.</a></li>
<li><a href="#16. Representation Learning of Entities and Documents from Knowledge Base Descriptions.">16. Representation Learning of Entities and Documents from Knowledge Base Descriptions.</a></li>
<li><a href="#17. Simple Neologism Based Domain Independent Models to Predict Year of Authorship.">17. Simple Neologism Based Domain Independent Models to Predict Year of Authorship.</a></li>
<li><a href="#18. Neural Math Word Problem Solver with Reinforcement Learning.">18. Neural Math Word Problem Solver with Reinforcement Learning.</a></li>
<li><a href="#19. Personalizing Lexical Simplification.">19. Personalizing Lexical Simplification.</a></li>
<li><a href="#20. From Text to Lexicon: Bridging the Gap between Word Embeddings and Lexical Resources.">20. From Text to Lexicon: Bridging the Gap between Word Embeddings and Lexical Resources.</a></li>
<li><a href="#21. Lexi: A tool for adaptive, personalized text simplification.">21. Lexi: A tool for adaptive, personalized text simplification.</a></li>
<li><a href="#22. Identifying Emergent Research Trends by Key Authors and Phrases.">22. Identifying Emergent Research Trends by Key Authors and Phrases.</a></li>
<li><a href="#23. Embedding WordNet Knowledge for Textual Entailment.">23. Embedding WordNet Knowledge for Textual Entailment.</a></li>
<li><a href="#24. Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing in Knowledge Bases.">24. Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing in Knowledge Bases.</a></li>
<li><a href="#25. Joint Learning from Labeled and Unlabeled Data for Information Retrieval.">25. Joint Learning from Labeled and Unlabeled Data for Information Retrieval.</a></li>
<li><a href="#26. Modeling the Readability of German Targeting Adults and Children: An empirically broad analysis and its cross-corpus validation.">26. Modeling the Readability of German Targeting Adults and Children: An empirically broad analysis and its cross-corpus validation.</a></li>
<li><a href="#27. Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs.">27. Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs.</a></li>
<li><a href="#28. Par4Sim - Adaptive Paraphrasing for Text Simplification.">28. Par4Sim - Adaptive Paraphrasing for Text Simplification.</a></li>
<li><a href="#29. Topic or Style? Exploring the Most Useful Features for Authorship Attribution.">29. Topic or Style? Exploring the Most Useful Features for Authorship Attribution.</a></li>
<li><a href="#30. A Deep Dive into Word Sense Disambiguation with LSTM.">30. A Deep Dive into Word Sense Disambiguation with LSTM.</a></li>
<li><a href="#31. Enriching Word Embeddings with Domain Knowledge for Readability Assessment.">31. Enriching Word Embeddings with Domain Knowledge for Readability Assessment.</a></li>
<li><a href="#32. WikiRef: Wikilinks as a route to recommending appropriate references for scientific Wikipedia pages.">32. WikiRef: Wikilinks as a route to recommending appropriate references for scientific Wikipedia pages.</a></li>
<li><a href="#33. Authorship Identification for Literary Book Recommendations.">33. Authorship Identification for Literary Book Recommendations.</a></li>
<li><a href="#34. A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment in Portuguese.">34. A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment in Portuguese.</a></li>
<li><a href="#35. Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference.">35. Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference.</a></li>
<li><a href="#36. Cooperative Denoising for Distantly Supervised Relation Extraction.">36. Cooperative Denoising for Distantly Supervised Relation Extraction.</a></li>
<li><a href="#37. Adversarial Feature Adaptation for Cross-lingual Relation Classification.">37. Adversarial Feature Adaptation for Cross-lingual Relation Classification.</a></li>
<li><a href="#38. One-shot Learning for Question-Answering in Gaokao History Challenge.">38. One-shot Learning for Question-Answering in Gaokao History Challenge.</a></li>
<li><a href="#39. Dynamic Multi-Level Multi-Task Learning for Sentence Simplification.">39. Dynamic Multi-Level Multi-Task Learning for Sentence Simplification.</a></li>
<li><a href="#40. Interpretation of Implicit Conditions in Database Search Dialogues.">40. Interpretation of Implicit Conditions in Database Search Dialogues.</a></li>
<li><a href="#41. Few-Shot Charge Prediction with Discriminative Legal Attributes.">41. Few-Shot Charge Prediction with Discriminative Legal Attributes.</a></li>
<li><a href="#42. Can Taxonomy Help? Improving Semantic Question Matching using Question Taxonomy.">42. Can Taxonomy Help? Improving Semantic Question Matching using Question Taxonomy.</a></li>
<li><a href="#43. Natural Language Interface for Databases Using a Dual-Encoder Model.">43. Natural Language Interface for Databases Using a Dual-Encoder Model.</a></li>
<li><a href="#44. Employing Text Matching Network to Recognise Nuclearity in Chinese Discourse.">44. Employing Text Matching Network to Recognise Nuclearity in Chinese Discourse.</a></li>
<li><a href="#45. Joint Modeling of Structure Identification and Nuclearity Recognition in Macro Chinese Discourse Treebank.">45. Joint Modeling of Structure Identification and Nuclearity Recognition in Macro Chinese Discourse Treebank.</a></li>
<li><a href="#46. Implicit Discourse Relation Recognition using Neural Tensor Network with Interactive Attention and Sparse Learning.">46. Implicit Discourse Relation Recognition using Neural Tensor Network with Interactive Attention and Sparse Learning.</a></li>
<li><a href="#47. Transition-based Neural RST Parsing with Implicit Syntax Features.">47. Transition-based Neural RST Parsing with Implicit Syntax Features.</a></li>
<li><a href="#48. Deep Enhanced Representation for Implicit Discourse Relation Recognition.">48. Deep Enhanced Representation for Implicit Discourse Relation Recognition.</a></li>
<li><a href="#49. A Knowledge-Augmented Neural Network Model for Implicit Discourse Relation Classification.">49. A Knowledge-Augmented Neural Network Model for Implicit Discourse Relation Classification.</a></li>
<li><a href="#50. Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches.">50. Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches.</a></li>
<li><a href="#51. Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate Model.">51. Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate Model.</a></li>
<li><a href="#52. Improving Neural Machine Translation by Incorporating Hierarchical Subword Features.">52. Improving Neural Machine Translation by Incorporating Hierarchical Subword Features.</a></li>
<li><a href="#53. Design Challenges in Named Entity Transliteration.">53. Design Challenges in Named Entity Transliteration.</a></li>
<li><a href="#54. A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation.">54. A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation.</a></li>
<li><a href="#55. On Adversarial Examples for Character-Level Neural Machine Translation.">55. On Adversarial Examples for Character-Level Neural Machine Translation.</a></li>
<li><a href="#56. Systematic Study of Long Tail Phenomena in Entity Linking.">56. Systematic Study of Long Tail Phenomena in Entity Linking.</a></li>
<li><a href="#57. Neural Collective Entity Linking.">57. Neural Collective Entity Linking.</a></li>
<li><a href="#58. Exploiting Structure in Representation of Named Entities using Active Learning.">58. Exploiting Structure in Representation of Named Entities using Active Learning.</a></li>
<li><a href="#59. A Practical Incremental Learning Framework For Sparse Entity Extraction.">59. A Practical Incremental Learning Framework For Sparse Entity Extraction.</a></li>
<li><a href="#60. An Empirical Study on Fine-Grained Named Entity Recognition.">60. An Empirical Study on Fine-Grained Named Entity Recognition.</a></li>
<li><a href="#61. Does Higher Order LSTM Have Better Accuracy for Segmenting and Labeling Sequence Data?">61. Does Higher Order LSTM Have Better Accuracy for Segmenting and Labeling Sequence Data?</a></li>
<li><a href="#62. Ant Colony System for Multi-Document Summarization.">62. Ant Colony System for Multi-Document Summarization.</a></li>
<li><a href="#63. Multi-task dialog act and sentiment recognition on Mastodon.">63. Multi-task dialog act and sentiment recognition on Mastodon.</a></li>
<li><a href="#64. RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian.">64. RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian.</a></li>
<li><a href="#65. Self-Normalization Properties of Language Modeling.">65. Self-Normalization Properties of Language Modeling.</a></li>
<li><a href="#66. A Position-aware Bidirectional Attention Network for Aspect-level Sentiment Analysis.">66. A Position-aware Bidirectional Attention Network for Aspect-level Sentiment Analysis.</a></li>
<li><a href="#67. Dynamic Feature Selection with Attention in Incremental Parsing.">67. Dynamic Feature Selection with Attention in Incremental Parsing.</a></li>
<li><a href="#68. Vocabulary Tailored Summary Generation.">68. Vocabulary Tailored Summary Generation.</a></li>
<li><a href="#69. Reading Comprehension with Graph-based Temporal-Casual Reasoning.">69. Reading Comprehension with Graph-based Temporal-Casual Reasoning.</a></li>
<li><a href="#70. Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains.">70. Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains.</a></li>
<li><a href="#71. Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection">71. Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!</a> is All You Need!)</li>
<li><a href="#72. HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation.">72. HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation.</a></li>
<li><a href="#73. Multi-Perspective Context Aggregation for Semi-supervised Cloze-style Reading Comprehension.">73. Multi-Perspective Context Aggregation for Semi-supervised Cloze-style Reading Comprehension.</a></li>
<li><a href="#74. A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis.">74. A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis.</a></li>
<li><a href="#75. Open-Domain Event Detection using Distant Supervision.">75. Open-Domain Event Detection using Distant Supervision.</a></li>
<li><a href="#76. Semi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing.">76. Semi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing.</a></li>
<li><a href="#77. Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embeddings.">77. Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embeddings.</a></li>
<li><a href="#78. A review of Spanish corpora annotated with negation.">78. A review of Spanish corpora annotated with negation.</a></li>
<li><a href="#79. Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users, Aspects, and Overall Ratings.">79. Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users, Aspects, and Overall Ratings.</a></li>
<li><a href="#80. Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora.">80. Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora.</a></li>
<li><a href="#81. Learning Emotion-enriched Word Representations.">81. Learning Emotion-enriched Word Representations.</a></li>
<li><a href="#82. Evaluating the text quality, human likeness and tailoring component of PASS: A Dutch data-to-text system for soccer.">82. Evaluating the text quality, human likeness and tailoring component of PASS: A Dutch data-to-text system for soccer.</a></li>
<li><a href="#83. Answerable or Not: Devising a Dataset for Extending Machine Reading Comprehension.">83. Answerable or Not: Devising a Dataset for Extending Machine Reading Comprehension.</a></li>
<li><a href="#84. Style Obfuscation by Invariance.">84. Style Obfuscation by Invariance.</a></li>
<li><a href="#85. Encoding Sentiment Information into Word Vectors for Sentiment Analysis.">85. Encoding Sentiment Information into Word Vectors for Sentiment Analysis.</a></li>
<li><a href="#86. Multi-Task Neural Models for Translating Between Styles Within and Across Languages.">86. Multi-Task Neural Models for Translating Between Styles Within and Across Languages.</a></li>
<li><a href="#87. Towards a Language for Natural Language Treebank Transductions.">87. Towards a Language for Natural Language Treebank Transductions.</a></li>
<li><a href="#88. Generating Reasonable and Diversified Story Ending Using Sequence to Sequence Model with Adversarial Training.">88. Generating Reasonable and Diversified Story Ending Using Sequence to Sequence Model with Adversarial Training.</a></li>
<li><a href="#89. Point Precisely: Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism.">89. Point Precisely: Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism.</a></li>
<li><a href="#90. Enhancing General Sentiment Lexicons for Domain-Specific Use.">90. Enhancing General Sentiment Lexicons for Domain-Specific Use.</a></li>
<li><a href="#91. An Operation Network for Abstractive Sentence Compression.">91. An Operation Network for Abstractive Sentence Compression.</a></li>
<li><a href="#92. Enhanced Aspect Level Sentiment Classification with Auxiliary Memory.">92. Enhanced Aspect Level Sentiment Classification with Auxiliary Memory.</a></li>
<li><a href="#93. Author Profiling for Abuse Detection.">93. Author Profiling for Abuse Detection.</a></li>
<li><a href="#94. Automated Scoring: Beyond Natural Language Processing.">94. Automated Scoring: Beyond Natural Language Processing.</a></li>
<li><a href="#95. Aspect and Sentiment Aware Abstractive Review Summarization.">95. Aspect and Sentiment Aware Abstractive Review Summarization.</a></li>
<li><a href="#96. Effective Attention Modeling for Aspect-Level Sentiment Classification.">96. Effective Attention Modeling for Aspect-Level Sentiment Classification.</a></li>
<li><a href="#97. Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis.">97. Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis.</a></li>
<li><a href="#98. Multilevel Heuristics for Rationale-Based Entity Relation Classification in Sentences.">98. Multilevel Heuristics for Rationale-Based Entity Relation Classification in Sentences.</a></li>
<li><a href="#99. Adversarial Multi-lingual Neural Relation Extraction.">99. Adversarial Multi-lingual Neural Relation Extraction.</a></li>
<li><a href="#100. Neural Relation Classification with Text Descriptions.">100. Neural Relation Classification with Text Descriptions.</a></li>
<li><a href="#101. Abstract Meaning Representation for Multi-Document Summarization.">101. Abstract Meaning Representation for Multi-Document Summarization.</a></li>
<li><a href="#102. Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion.">102. Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion.</a></li>
<li><a href="#103. Adversarial Domain Adaptation for Variational Neural Language Generation in Dialogue Systems.">103. Adversarial Domain Adaptation for Variational Neural Language Generation in Dialogue Systems.</a></li>
<li><a href="#104. Ask No More: Deciding when to guess in referential visual dialogue.">104. Ask No More: Deciding when to guess in referential visual dialogue.</a></li>
<li><a href="#105. Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding.">105. Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding.</a></li>
<li><a href="#106. Dialogue-act-driven Conversation Model : An Experimental Study.">106. Dialogue-act-driven Conversation Model : An Experimental Study.</a></li>
<li><a href="#107. Structured Dialogue Policy with Graph Neural Networks.">107. Structured Dialogue Policy with Graph Neural Networks.</a></li>
<li><a href="#108. JTAV: Jointly Learning Social Media Content Representation by Fusing Textual, Acoustic, and Visual Features.">108. JTAV: Jointly Learning Social Media Content Representation by Fusing Textual, Acoustic, and Visual Features.</a></li>
<li><a href="#109. MEMD: A Diversity-Promoting Learning Framework for Short-Text Conversation.">109. MEMD: A Diversity-Promoting Learning Framework for Short-Text Conversation.</a></li>
<li><a href="#110. Refining Source Representations with Relation Networks for Neural Machine Translation.">110. Refining Source Representations with Relation Networks for Neural Machine Translation.</a></li>
<li><a href="#111. A Survey of Domain Adaptation for Neural Machine Translation.">111. A Survey of Domain Adaptation for Neural Machine Translation.</a></li>
<li><a href="#112. An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization.">112. An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization.</a></li>
<li><a href="#113. Fine-Grained Arabic Dialect Identification.">113. Fine-Grained Arabic Dialect Identification.</a></li>
<li><a href="#114. Who Feels What and Why? Annotation of a Literature Corpus with Semantic Roles of Emotions.">114. Who Feels What and Why? Annotation of a Literature Corpus with Semantic Roles of Emotions.</a></li>
<li><a href="#115. Local String Transduction as Sequence Labeling.">115. Local String Transduction as Sequence Labeling.</a></li>
<li><a href="#116. Deep Neural Networks at the Service of Multilingual Parallel Sentence Extraction.">116. Deep Neural Networks at the Service of Multilingual Parallel Sentence Extraction.</a></li>
<li><a href="#117. Diachronic word embeddings and semantic shifts: a survey.">117. Diachronic word embeddings and semantic shifts: a survey.</a></li>
<li><a href="#118. Interaction-Aware Topic Model for Microblog Conversations through Network Embedding and User Attention.">118. Interaction-Aware Topic Model for Microblog Conversations through Network Embedding and User Attention.</a></li>
<li><a href="#119. Cross-media User Profiling with Joint Textual and Social User Embedding.">119. Cross-media User Profiling with Joint Textual and Social User Embedding.</a></li>
<li><a href="#120. Incorporating Syntactic Uncertainty in Neural Machine Translation with a Forest-to-Sequence Model.">120. Incorporating Syntactic Uncertainty in Neural Machine Translation with a Forest-to-Sequence Model.</a></li>
<li><a href="#121. Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization.">121. Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization.</a></li>
<li><a href="#122. Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation.">122. Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation.</a></li>
<li><a href="#123. Fast and Accurate Reordering with ITG Transition RNN.">123. Fast and Accurate Reordering with ITG Transition RNN.</a></li>
<li><a href="#124. Neural Machine Translation with Decoding History Enhanced Attention.">124. Neural Machine Translation with Decoding History Enhanced Attention.</a></li>
<li><a href="#125. Transfer Learning for a Letter-Ngrams to Word Decoder in the Context of Historical Handwriting Recognition with Scarce Resources.">125. Transfer Learning for a Letter-Ngrams to Word Decoder in the Context of Historical Handwriting Recognition with Scarce Resources.</a></li>
<li><a href="#126. SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions.">126. SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions.</a></li>
<li><a href="#127. Crowdsourcing a Large Corpus of Clickbait on Twitter.">127. Crowdsourcing a Large Corpus of Clickbait on Twitter.</a></li>
<li><a href="#128. Cross-lingual Knowledge Projection Using Machine Translation and Target-side Knowledge Base Completion.">128. Cross-lingual Knowledge Projection Using Machine Translation and Target-side Knowledge Base Completion.</a></li>
<li><a href="#129. Assessing Quality Estimation Models for Sentence-Level Prediction.">129. Assessing Quality Estimation Models for Sentence-Level Prediction.</a></li>
<li><a href="#130. User-Level Race and Ethnicity Predictors from Twitter Text.">130. User-Level Race and Ethnicity Predictors from Twitter Text.</a></li>
<li><a href="#131. Multi-Source Multi-Class Fake News Detection.">131. Multi-Source Multi-Class Fake News Detection.</a></li>
<li><a href="#132. Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal Language Detection.">132. Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal Language Detection.</a></li>
<li><a href="#133. Twitter corpus of Resource-Scarce Languages for Sentiment Analysis and Multilingual Emoji Prediction.">133. Twitter corpus of Resource-Scarce Languages for Sentiment Analysis and Multilingual Emoji Prediction.</a></li>
<li><a href="#134. Towards identifying the optimal datasize for lexically-based Bayesian inference of linguistic phylogenies.">134. Towards identifying the optimal datasize for lexically-based Bayesian inference of linguistic phylogenies.</a></li>
<li><a href="#135. The Road to Success: Assessing the Fate of Linguistic Innovations in Online Communities.">135. The Road to Success: Assessing the Fate of Linguistic Innovations in Online Communities.</a></li>
<li><a href="#136. Ab Initio: Automatic Latin Proto-word Reconstruction.">136. Ab Initio: Automatic Latin Proto-word Reconstruction.</a></li>
<li><a href="#137. A Computational Model for the Linguistic Notion of Morphological Paradigm.">137. A Computational Model for the Linguistic Notion of Morphological Paradigm.</a></li>
<li><a href="#138. Relation Induction in Word Embeddings Revisited.">138. Relation Induction in Word Embeddings Revisited.</a></li>
<li><a href="#139. Contextual String Embeddings for Sequence Labeling.">139. Contextual String Embeddings for Sequence Labeling.</a></li>
<li><a href="#140. Learning Word Meta-Embeddings by Autoencoding.">140. Learning Word Meta-Embeddings by Autoencoding.</a></li>
<li><a href="#141. GenSense: A Generalized Sense Retrofitting Model.">141. GenSense: A Generalized Sense Retrofitting Model.</a></li>
<li><a href="#142. Variational Attention for Sequence-to-Sequence Models.">142. Variational Attention for Sequence-to-Sequence Models.</a></li>
<li><a href="#143. A New Concept of Deep Reinforcement Learning based Augmented General Tagging System.">143. A New Concept of Deep Reinforcement Learning based Augmented General Tagging System.</a></li>
<li><a href="#144. Learning from Measurements in Crowdsourcing Models: Inferring Ground Truth from Diverse Annotation Types.">144. Learning from Measurements in Crowdsourcing Models: Inferring Ground Truth from Diverse Annotation Types.</a></li>
<li><a href="#145. Reproducing and Regularizing the SCRN Model.">145. Reproducing and Regularizing the SCRN Model.</a></li>
<li><a href="#146. Structure-Infused Copy Mechanisms for Abstractive Summarization.">146. Structure-Infused Copy Mechanisms for Abstractive Summarization.</a></li>
<li><a href="#147. Measuring the Diversity of Automatic Image Descriptions.">147. Measuring the Diversity of Automatic Image Descriptions.</a></li>
<li><a href="#148. Extractive Headline Generation Based on Learning to Rank for Community Question Answering.">148. Extractive Headline Generation Based on Learning to Rank for Community Question Answering.</a></li>
<li><a href="#149. A Multi-Attention based Neural Network with External Knowledge for Story Ending Predicting Task.">149. A Multi-Attention based Neural Network with External Knowledge for Story Ending Predicting Task.</a></li>
<li><a href="#150. A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators.">150. A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators.</a></li>
<li><a href="#151. Embedding Words as Distributions with a Bayesian Skip-gram Model.">151. Embedding Words as Distributions with a Bayesian Skip-gram Model.</a></li>
<li><a href="#152. Assessing Composition in Sentence Vector Representations.">152. Assessing Composition in Sentence Vector Representations.</a></li>
<li><a href="#153. Subword-augmented Embedding for Cloze Reading Comprehension.">153. Subword-augmented Embedding for Cloze Reading Comprehension.</a></li>
<li><a href="#154. Enhancing Sentence Embedding with Generalized Pooling.">154. Enhancing Sentence Embedding with Generalized Pooling.</a></li>
<li><a href="#155. Treat us like the sequences we are: Prepositional Paraphrasing of Noun Compounds using LSTM.">155. Treat us like the sequences we are: Prepositional Paraphrasing of Noun Compounds using LSTM.</a></li>
<li><a href="#156. CASCADE: Contextual Sarcasm Detection in Online Discussion Forums.">156. CASCADE: Contextual Sarcasm Detection in Online Discussion Forums.</a></li>
<li><a href="#157. Recognizing Humour using Word Associations and Humour Anchor Extraction.">157. Recognizing Humour using Word Associations and Humour Anchor Extraction.</a></li>
<li><a href="#158. A Retrospective Analysis of the Fake News Challenge Stance-Detection Task.">158. A Retrospective Analysis of the Fake News Challenge Stance-Detection Task.</a></li>
<li><a href="#159. Exploiting Syntactic Structures for Humor Recognition.">159. Exploiting Syntactic Structures for Humor Recognition.</a></li>
<li><a href="#160. An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review Detection.">160. An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review Detection.</a></li>
<li><a href="#161. Robust Lexical Features for Improved Neural Network Named-Entity Recognition.">161. Robust Lexical Features for Improved Neural Network Named-Entity Recognition.</a></li>
<li><a href="#162. A Pseudo Label based Dataless Naive Bayes Algorithm for Text Classification with Seed Words.">162. A Pseudo Label based Dataless Naive Bayes Algorithm for Text Classification with Seed Words.</a></li>
<li><a href="#163. Visual Question Answering Dataset for Bilingual Image Understanding: A Study of Cross-Lingual Transfer Using Attention Maps.">163. Visual Question Answering Dataset for Bilingual Image Understanding: A Study of Cross-Lingual Transfer Using Attention Maps.</a></li>
<li><a href="#164. Style Detection for Free Verse Poetry from Text and Speech.">164. Style Detection for Free Verse Poetry from Text and Speech.</a></li>
<li><a href="#165. A Neural Question Answering Model Based on Semi-Structured Tables.">165. A Neural Question Answering Model Based on Semi-Structured Tables.</a></li>
<li><a href="#166. LCQMC: A Large-scale Chinese Question Matching Corpus.">166. LCQMC: A Large-scale Chinese Question Matching Corpus.</a></li>
<li><a href="#167. Genre Identification and the Compositional Effect of Genre in Literature.">167. Genre Identification and the Compositional Effect of Genre in Literature.</a></li>
<li><a href="#168. Transfer Learning for Entity Recognition of Novel Classes.">168. Transfer Learning for Entity Recognition of Novel Classes.</a></li>
<li><a href="#169. Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models.">169. Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models.</a></li>
<li><a href="#170. The APVA-TURBO Approach To Question Answering in Knowledge Base.">170. The APVA-TURBO Approach To Question Answering in Knowledge Base.</a></li>
<li><a href="#171. An Interpretable Reasoning Network for Multi-Relation Question Answering.">171. An Interpretable Reasoning Network for Multi-Relation Question Answering.</a></li>
<li><a href="#172. Task-oriented Word Embedding for Text Classification.">172. Task-oriented Word Embedding for Text Classification.</a></li>
<li><a href="#173. Adaptive Learning of Local Semantic and Global Structure Representations for Text Classification.">173. Adaptive Learning of Local Semantic and Global Structure Representations for Text Classification.</a></li>
<li><a href="#174. Lyrics Segmentation: Textual Macrostructure Detection using Convolutions.">174. Lyrics Segmentation: Textual Macrostructure Detection using Convolutions.</a></li>
<li><a href="#175. Learning What to Share: Leaky Multi-Task Network for Text Classification.">175. Learning What to Share: Leaky Multi-Task Network for Text Classification.</a></li>
<li><a href="#176. Towards an argumentative content search engine using weak supervision.">176. Towards an argumentative content search engine using weak supervision.</a></li>
<li><a href="#177. Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological Tags.">177. Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological Tags.</a></li>
<li><a href="#178. Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia.">178. Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia.</a></li>
<li><a href="#179. An Analysis of Annotated Corpora for Emotion Classification in Text.">179. An Analysis of Annotated Corpora for Emotion Classification in Text.</a></li>
<li><a href="#180. Investigating the Working of Text Classifiers.">180. Investigating the Working of Text Classifiers.</a></li>
<li><a href="#181. A Review on Deep Learning Techniques Applied to Answer Selection.">181. A Review on Deep Learning Techniques Applied to Answer Selection.</a></li>
<li><a href="#182. A Survey on Recent Advances in Named Entity Recognition from Deep Learning models.">182. A Survey on Recent Advances in Named Entity Recognition from Deep Learning models.</a></li>
<li><a href="#183. Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning.">183. Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning.</a></li>
<li><a href="#184. Joint Neural Entity Disambiguation with Output Space Search.">184. Joint Neural Entity Disambiguation with Output Space Search.</a></li>
<li><a href="#185. Learning to Progressively Recognize New Named Entities with Sequence to Sequence Models.">185. Learning to Progressively Recognize New Named Entities with Sequence to Sequence Models.</a></li>
<li><a href="#186. Responding E-commerce Product Questions via Exploiting QA Collections and Reviews.">186. Responding E-commerce Product Questions via Exploiting QA Collections and Reviews.</a></li>
<li><a href="#187. Aff2Vec: Affect-Enriched Distributional Word Representations.">187. Aff2Vec: Affect-Enriched Distributional Word Representations.</a></li>
<li><a href="#188. Aspect-based summarization of pros and cons in unstructured product reviews.">188. Aspect-based summarization of pros and cons in unstructured product reviews.</a></li>
<li><a href="#189. Learning Sentiment Composition from Sentiment Lexicons.">189. Learning Sentiment Composition from Sentiment Lexicons.</a></li>
<li><a href="#190. Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from Modern Hebrew.">190. Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from Modern Hebrew.</a></li>
<li><a href="#191. Scoring and Classifying Implicit Positive Interpretations: A Challenge of Class Imbalance.">191. Scoring and Classifying Implicit Positive Interpretations: A Challenge of Class Imbalance.</a></li>
<li><a href="#192. Exploratory Neural Relation Classification for Domain Knowledge Acquisition.">192. Exploratory Neural Relation Classification for Domain Knowledge Acquisition.</a></li>
<li><a href="#193. Who is Killed by Police: Introducing Supervised Attention for Hierarchical LSTMs.">193. Who is Killed by Police: Introducing Supervised Attention for Hierarchical LSTMs.</a></li>
<li><a href="#194. Open Information Extraction from Conjunctive Sentences.">194. Open Information Extraction from Conjunctive Sentences.</a></li>
<li><a href="#195. Graphene: Semantically-Linked Propositions in Open Information Extraction.">195. Graphene: Semantically-Linked Propositions in Open Information Extraction.</a></li>
<li><a href="#196. An Exploration of Three Lightly-supervised Representation Learning Approaches for Named Entity Classification.">196. An Exploration of Three Lightly-supervised Representation Learning Approaches for Named Entity Classification.</a></li>
<li><a href="#197. Multimodal Grounding for Language Processing.">197. Multimodal Grounding for Language Processing.</a></li>
<li><a href="#198. Stress Test Evaluation for Natural Language Inference.">198. Stress Test Evaluation for Natural Language Inference.</a></li>
<li><a href="#199. Grounded Textual Entailment.">199. Grounded Textual Entailment.</a></li>
<li><a href="#200. Recurrent One-Hop Predictions for Reasoning over Knowledge Graphs.">200. Recurrent One-Hop Predictions for Reasoning over Knowledge Graphs.</a></li>
<li><a href="#201. Hybrid Attention based Multimodal Network for Spoken Language Classification.">201. Hybrid Attention based Multimodal Network for Spoken Language Classification.</a></li>
<li><a href="#202. Exploring the Influence of Spelling Errors on Lexical Variation Measures.">202. Exploring the Influence of Spelling Errors on Lexical Variation Measures.</a></li>
<li><a href="#203. Stance Detection with Hierarchical Attention Network.">203. Stance Detection with Hierarchical Attention Network.</a></li>
<li><a href="#204. Correcting Chinese Word Usage Errors for Learning Chinese as a Second Language.">204. Correcting Chinese Word Usage Errors for Learning Chinese as a Second Language.</a></li>
<li><a href="#205. Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations.">205. Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations.</a></li>
<li><a href="#206. Context-Sensitive Generation of Open-Domain Conversational Responses.">206. Context-Sensitive Generation of Open-Domain Conversational Responses.</a></li>
<li><a href="#207. A LSTM Approach with Sub-Word Embeddings for Mongolian Phrase Break Prediction.">207. A LSTM Approach with Sub-Word Embeddings for Mongolian Phrase Break Prediction.</a></li>
<li><a href="#208. Synonymy in Bilingual Context: The CzEngClass Lexicon.">208. Synonymy in Bilingual Context: The CzEngClass Lexicon.</a></li>
<li><a href="#209. Convolutional Neural Network for Universal Sentence Embeddings.">209. Convolutional Neural Network for Universal Sentence Embeddings.</a></li>
<li><a href="#210. Rich Character-Level Information for Korean Morphological Analysis and Part-of-Speech Tagging.">210. Rich Character-Level Information for Korean Morphological Analysis and Part-of-Speech Tagging.</a></li>
<li><a href="#211. Why does PairDiff work? - A Mathematical Analysis of Bilinear Relational Compositional Operators for Analogy Detection.">211. Why does PairDiff work? - A Mathematical Analysis of Bilinear Relational Compositional Operators for Analogy Detection.</a></li>
<li><a href="#212. Real-time Change Point Detection using On-line Topic Models.">212. Real-time Change Point Detection using On-line Topic Models.</a></li>
<li><a href="#213. Automatically Creating a Lexicon of Verbal Polarity Shifters: Mono- and Cross-lingual Methods for German.">213. Automatically Creating a Lexicon of Verbal Polarity Shifters: Mono- and Cross-lingual Methods for German.</a></li>
<li><a href="#214. Part-of-Speech Tagging on an Endangered Language: a Parallel Griko-Italian Resource.">214. Part-of-Speech Tagging on an Endangered Language: a Parallel Griko-Italian Resource.</a></li>
<li><a href="#215. One vs. Many QA Matching with both Word-level and Sentence-level Attention Network.">215. One vs. Many QA Matching with both Word-level and Sentence-level Attention Network.</a></li>
<li><a href="#216. Learning to Generate Word Representations using Subword Information.">216. Learning to Generate Word Representations using Subword Information.</a></li>
<li><a href="#217. Urdu Word Segmentation using Conditional Random Fields (CRFs">217. Urdu Word Segmentation using Conditional Random Fields (CRFs).</a>.)</li>
<li><a href="#218. ReSyf: a French lexicon with ranked synonyms.">218. ReSyf: a French lexicon with ranked synonyms.</a></li>
<li><a href="#219. If you've seen some, you've seen them all: Identifying variants of multiword expressions.">219. If you've seen some, you've seen them all: Identifying variants of multiword expressions.</a></li>
<li><a href="#220. Learning Multilingual Topics from Incomparable Corpora.">220. Learning Multilingual Topics from Incomparable Corpora.</a></li>
<li><a href="#221. Using Word Embeddings for Unsupervised Acronym Disambiguation.">221. Using Word Embeddings for Unsupervised Acronym Disambiguation.</a></li>
<li><a href="#222. Indigenous language technologies in Canada: Assessment, challenges, and successes.">222. Indigenous language technologies in Canada: Assessment, challenges, and successes.</a></li>
<li><a href="#223. Pluralizing Nouns across Agglutinating Bantu Languages.">223. Pluralizing Nouns across Agglutinating Bantu Languages.</a></li>
<li><a href="#224. Automatically Extracting Qualia Relations for the Rich Event Ontology.">224. Automatically Extracting Qualia Relations for the Rich Event Ontology.</a></li>
<li><a href="#225. SeVeN: Augmenting Word Embeddings with Unsupervised Relation Vectors.">225. SeVeN: Augmenting Word Embeddings with Unsupervised Relation Vectors.</a></li>
<li><a href="#226. Evaluation of Unsupervised Compositional Representations.">226. Evaluation of Unsupervised Compositional Representations.</a></li>
<li><a href="#227. Using Formulaic Expressions in Writing Assistance Systems.">227. Using Formulaic Expressions in Writing Assistance Systems.</a></li>
<li><a href="#228. What's in Your Embedding, And How It Predicts Task Performance.">228. What's in Your Embedding, And How It Predicts Task Performance.</a></li>
<li><a href="#229. Word Sense Disambiguation Based on Word Similarity Calculation Using Word Vector Representation from a Knowledge-based Graph.">229. Word Sense Disambiguation Based on Word Similarity Calculation Using Word Vector Representation from a Knowledge-based Graph.</a></li>
<li><a href="#230. Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator.">230. Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator.</a></li>
<li><a href="#231. A Reassessment of Reference-Based Grammatical Error Correction Metrics.">231. A Reassessment of Reference-Based Grammatical Error Correction Metrics.</a></li>
<li><a href="#232. Information Aggregation via Dynamic Routing for Sequence Encoding.">232. Information Aggregation via Dynamic Routing for Sequence Encoding.</a></li>
<li><a href="#233. A Full End-to-End Semantic Role Labeler, Syntactic-agnostic Over Syntactic-aware?">233. A Full End-to-End Semantic Role Labeler, Syntactic-agnostic Over Syntactic-aware?</a></li>
<li><a href="#234. Authorship Attribution By Consensus Among Multiple Features.">234. Authorship Attribution By Consensus Among Multiple Features.</a></li>
<li><a href="#235. Modeling with Recurrent Neural Networks for Open Vocabulary Slots.">235. Modeling with Recurrent Neural Networks for Open Vocabulary Slots.</a></li>
<li><a href="#236. Challenges and Opportunities of Applying Natural Language Processing in Business Process Management.">236. Challenges and Opportunities of Applying Natural Language Processing in Business Process Management.</a></li>
<li><a href="#237. Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection.">237. Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection.</a></li>
<li><a href="#238. What represents "style" in authorship attribution?">238. What represents "style" in authorship attribution?</a></li>
<li><a href="#239. Learning Target-Specific Representations of Financial News Documents For Cumulative Abnormal Return Prediction.">239. Learning Target-Specific Representations of Financial News Documents For Cumulative Abnormal Return Prediction.</a></li>
<li><a href="#240. Model-Free Context-Aware Word Composition.">240. Model-Free Context-Aware Word Composition.</a></li>
<li><a href="#241. Learning Features from Co-occurrences: A Theoretical Analysis.">241. Learning Features from Co-occurrences: A Theoretical Analysis.</a></li>
<li><a href="#242. Towards a unified framework for bilingual terminology extraction of single-word and multi-word terms.">242. Towards a unified framework for bilingual terminology extraction of single-word and multi-word terms.</a></li>
<li><a href="#243. Neural Activation Semantic Models: Computational lexical semantic models of localized neural activations.">243. Neural Activation Semantic Models: Computational lexical semantic models of localized neural activations.</a></li>
<li><a href="#244. Folksonomication: Predicting Tags for Movies from Plot Synopses using Emotion Flow Encoded Neural Network.">244. Folksonomication: Predicting Tags for Movies from Plot Synopses using Emotion Flow Encoded Neural Network.</a></li>
<li><a href="#245. Emotion Representation Mapping for Automatic Lexicon Construction (Mostly">245. Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level.</a> Performs on Human Level.)</li>
<li><a href="#246. Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning.">246. Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning.</a></li>
<li><a href="#247. How emotional are you? Neural Architectures for Emotion Intensity Prediction in Microblogs.">247. How emotional are you? Neural Architectures for Emotion Intensity Prediction in Microblogs.</a></li>
<li><a href="#248. Expressively vulgar: The socio-dynamics of vulgarity and its effects on sentiment analysis in social media.">248. Expressively vulgar: The socio-dynamics of vulgarity and its effects on sentiment analysis in social media.</a></li>
<li><a href="#249. Clausal Modifiers in the Grammar Matrix.">249. Clausal Modifiers in the Grammar Matrix.</a></li>
<li><a href="#250. Sliced Recurrent Neural Networks.">250. Sliced Recurrent Neural Networks.</a></li>
<li><a href="#251. Multi-Task Learning for Sequence Tagging: An Empirical Study.">251. Multi-Task Learning for Sequence Tagging: An Empirical Study.</a></li>
<li><a href="#252. Using J-K-fold Cross Validation To Reduce Variance When Tuning NLP Models.">252. Using J-K-fold Cross Validation To Reduce Variance When Tuning NLP Models.</a></li>
<li><a href="#253. Incremental Natural Language Processing: Challenges, Strategies, and Evaluation.">253. Incremental Natural Language Processing: Challenges, Strategies, and Evaluation.</a></li>
<li><a href="#254. Gold Standard Annotations for Preposition and Verb Sense with Semantic Role Labels in Adult-Child Interactions.">254. Gold Standard Annotations for Preposition and Verb Sense with Semantic Role Labels in Adult-Child Interactions.</a></li>
<li><a href="#255. Multi-layer Representation Fusion for Neural Machine Translation.">255. Multi-layer Representation Fusion for Neural Machine Translation.</a></li>
<li><a href="#256. Toward Better Loanword Identification in Uyghur Using Cross-lingual Word Embeddings.">256. Toward Better Loanword Identification in Uyghur Using Cross-lingual Word Embeddings.</a></li>
<li><a href="#257. Adaptive Weighting for Neural Machine Translation.">257. Adaptive Weighting for Neural Machine Translation.</a></li>
<li><a href="#258. Generic refinement of expressive grammar formalisms with an application to discontinuous constituent parsing.">258. Generic refinement of expressive grammar formalisms with an application to discontinuous constituent parsing.</a></li>
<li><a href="#259. Double Path Networks for Sequence to Sequence Learning.">259. Double Path Networks for Sequence to Sequence Learning.</a></li>
<li><a href="#260. An Empirical Investigation of Error Types in Vietnamese Parsing.">260. An Empirical Investigation of Error Types in Vietnamese Parsing.</a></li>
<li><a href="#261. Learning with Noise-Contrastive Estimation: Easing training by learning to scale.">261. Learning with Noise-Contrastive Estimation: Easing training by learning to scale.</a></li>
<li><a href="#262. Parallel Corpora for bi-lingual English-Ethiopian Languages Statistical Machine Translation.">262. Parallel Corpora for bi-lingual English-Ethiopian Languages Statistical Machine Translation.</a></li>
<li><a href="#263. Multilingual Neural Machine Translation with Task-Specific Attention.">263. Multilingual Neural Machine Translation with Task-Specific Attention.</a></li>
<li><a href="#264. Combining Information-Weighted Sequence Alignment and Sound Correspondence Models for Improved Cognate Detection.">264. Combining Information-Weighted Sequence Alignment and Sound Correspondence Models for Improved Cognate Detection.</a></li>
<li><a href="#265. Tailoring Neural Architectures for Translating from Morphologically Rich Languages.">265. Tailoring Neural Architectures for Translating from Morphologically Rich Languages.</a></li>
<li><a href="#266. deepQuest: A Framework for Neural-based Quality Estimation.">266. deepQuest: A Framework for Neural-based Quality Estimation.</a></li>
<li><a href="#267. Butterfly Effects in Frame Semantic Parsing: impact of data processing on model ranking.">267. Butterfly Effects in Frame Semantic Parsing: impact of data processing on model ranking.</a></li>
<li><a href="#268. Sensitivity to Input Order: Evaluation of an Incremental and Memory-Limited Bayesian Cross-Situational Word Learning Model.">268. Sensitivity to Input Order: Evaluation of an Incremental and Memory-Limited Bayesian Cross-Situational Word Learning Model.</a></li>
<li><a href="#269. Sentence Weighting for Neural Machine Translation Domain Adaptation.">269. Sentence Weighting for Neural Machine Translation Domain Adaptation.</a></li>
<li><a href="#270. Quantifying training challenges of dependency parsers.">270. Quantifying training challenges of dependency parsers.</a></li>
<li><a href="#271. Seq2seq Dependency Parsing.">271. Seq2seq Dependency Parsing.</a></li>
<li><a href="#272. Revisiting the Hierarchical Multiscale LSTM.">272. Revisiting the Hierarchical Multiscale LSTM.</a></li>
<li><a href="#273. Character-Level Feature Extraction with Densely Connected Networks.">273. Character-Level Feature Extraction with Densely Connected Networks.</a></li>
<li><a href="#274. Neural Machine Translation Incorporating Named Entity.">274. Neural Machine Translation Incorporating Named Entity.</a></li>
<li><a href="#275. Semantic Parsing for Technical Support Questions.">275. Semantic Parsing for Technical Support Questions.</a></li>
<li><a href="#276. Deconvolution-Based Global Decoding for Neural Machine Translation.">276. Deconvolution-Based Global Decoding for Neural Machine Translation.</a></li>
<li><a href="#277. Pattern-revising Enhanced Simple Question Answering over Knowledge Bases.">277. Pattern-revising Enhanced Simple Question Answering over Knowledge Bases.</a></li>
<li><a href="#278. Integrating Question Classification and Deep Learning for improved Answer Selection.">278. Integrating Question Classification and Deep Learning for improved Answer Selection.</a></li>
<li><a href="#279. Knowledge as A Bridge: Improving Cross-domain Answer Selection with External Knowledge.">279. Knowledge as A Bridge: Improving Cross-domain Answer Selection with External Knowledge.</a></li>
<li><a href="#280. Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering.">280. Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering.</a></li>
<li><a href="#281. Rethinking the Agreement in Human Evaluation Tasks.">281. Rethinking the Agreement in Human Evaluation Tasks.</a></li>
<li><a href="#282. Dependent Gated Reading for Cloze-Style Question Answering.">282. Dependent Gated Reading for Cloze-Style Question Answering.</a></li>
<li><a href="#283. Automated Fact Checking: Task Formulations, Methods and Future Directions.">283. Automated Fact Checking: Task Formulations, Methods and Future Directions.</a></li>
<li><a href="#284. Can Rumour Stance Alone Predict Veracity?">284. Can Rumour Stance Alone Predict Veracity?</a></li>
<li><a href="#285. Attending Sentences to detect Satirical Fake News.">285. Attending Sentences to detect Satirical Fake News.</a></li>
<li><a href="#286. Predicting Stances from Social Media Posts using Factorization Machines.">286. Predicting Stances from Social Media Posts using Factorization Machines.</a></li>
<li><a href="#287. Automatic Detection of Fake News.">287. Automatic Detection of Fake News.</a></li>
<li><a href="#288. All-in-one: Multi-task Learning for Rumour Verification.">288. All-in-one: Multi-task Learning for Rumour Verification.</a></li>
<li><a href="#289. Open Information Extraction on Scientific Text: An Evaluation.">289. Open Information Extraction on Scientific Text: An Evaluation.</a></li>
<li><a href="#290. Simple Algorithms For Sentiment Analysis On Sentiment Rich, Data Poor Domains.">290. Simple Algorithms For Sentiment Analysis On Sentiment Rich, Data Poor Domains.</a></li>
<li><a href="#291. Word-Level Loss Extensions for Neural Temporal Relation Classification.">291. Word-Level Loss Extensions for Neural Temporal Relation Classification.</a></li>
<li><a href="#292. Personalized Text Retrieval for Learners of Chinese as a Foreign Language.">292. Personalized Text Retrieval for Learners of Chinese as a Foreign Language.</a></li>
<li><a href="#293. Punctuation as Native Language Interference.">293. Punctuation as Native Language Interference.</a></li>
<li><a href="#294. Investigating Productive and Receptive Knowledge: A Profile for Second Language Learning.">294. Investigating Productive and Receptive Knowledge: A Profile for Second Language Learning.</a></li>
<li><a href="#295. iParaphrasing: Extracting Visually Grounded Paraphrases via an Image.">295. iParaphrasing: Extracting Visually Grounded Paraphrases via an Image.</a></li>
<li><a href="#296. MCDTB: A Macro-level Chinese Discourse TreeBank.">296. MCDTB: A Macro-level Chinese Discourse TreeBank.</a></li>
<li><a href="#297. Corpus-based Content Construction.">297. Corpus-based Content Construction.</a></li>
<li><a href="#298. Bridging resolution: Task definition, corpus resources and rule-based experiments.">298. Bridging resolution: Task definition, corpus resources and rule-based experiments.</a></li>
<li><a href="#299. Semi-Supervised Disfluency Detection.">299. Semi-Supervised Disfluency Detection.</a></li>
<li><a href="#300. ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents.">300. ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents.</a></li>
<li><a href="#301. Arrows are the Verbs of Diagrams.">301. Arrows are the Verbs of Diagrams.</a></li>
<li><a href="#302. Improving Feature Extraction for Pathology Reports with Precise Negation Scope Detection.">302. Improving Feature Extraction for Pathology Reports with Precise Negation Scope Detection.</a></li>
<li><a href="#303. Bridge Video and Text with Cascade Syntactic Structure.">303. Bridge Video and Text with Cascade Syntactic Structure.</a></li>
<li><a href="#304. Multi-task and Multi-lingual Joint Learning of Neural Lexical Utterance Classification based on Partially-shared Modeling.">304. Multi-task and Multi-lingual Joint Learning of Neural Lexical Utterance Classification based on Partially-shared Modeling.</a></li>
<li><a href="#305. Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language.">305. Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language.</a></li>
<li><a href="#306. A Prospective-Performance Network to Alleviate Myopia in Beam Search for Response Generation.">306. A Prospective-Performance Network to Alleviate Myopia in Beam Search for Response Generation.</a></li>
<li><a href="#307. Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical Text.">307. Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical Text.</a></li>
<li><a href="#308. Addressee and Response Selection for Multilingual Conversation.">308. Addressee and Response Selection for Multilingual Conversation.</a></li>
<li><a href="#309. Graph Based Decoding for Event Sequencing and Coreference Resolution.">309. Graph Based Decoding for Event Sequencing and Coreference Resolution.</a></li>
<li><a href="#310. DIDEC: The Dutch Image Description and Eye-tracking Corpus.">310. DIDEC: The Dutch Image Description and Eye-tracking Corpus.</a></li>
<li><a href="#311. Narrative Schema Stability in News Text.">311. Narrative Schema Stability in News Text.</a></li>
<li><a href="#312. NIPS Conversational Intelligence Challenge 2017 Winner System: Skill-based Conversational Agent with Supervised Dialog Manager.">312. NIPS Conversational Intelligence Challenge 2017 Winner System: Skill-based Conversational Agent with Supervised Dialog Manager.</a></li>
<li><a href="#313. AMR Beyond the Sentence: the Multi-sentence AMR corpus.">313. AMR Beyond the Sentence: the Multi-sentence AMR corpus.</a></li>
<li><a href="#314. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model.">314. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model.</a></li>
<li><a href="#315. Learning Visually-Grounded Semantics from Contrastive Adversarial Samples.">315. Learning Visually-Grounded Semantics from Contrastive Adversarial Samples.</a></li>
<li><a href="#316. Structured Representation Learning for Online Debate Stance Prediction.">316. Structured Representation Learning for Online Debate Stance Prediction.</a></li>
<li><a href="#317. Modeling Multi-turn Conversation with Deep Utterance Aggregation.">317. Modeling Multi-turn Conversation with Deep Utterance Aggregation.</a></li>
<li><a href="#318. Argumentation Synthesis following Rhetorical Strategies.">318. Argumentation Synthesis following Rhetorical Strategies.</a></li>
<li><a href="#319. A Dataset for Building Code-Mixed Goal Oriented Conversation Systems.">319. A Dataset for Building Code-Mixed Goal Oriented Conversation Systems.</a></li>
<li><a href="#320. Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation.">320. Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation.</a></li>
<li><a href="#321. Incorporating Deep Visual Features into Multiobjective based Multi-view Search Results Clustering.">321. Incorporating Deep Visual Features into Multiobjective based Multi-view Search Results Clustering.</a></li>
<li><a href="#322. Integrating Tree Structures and Graph Structures with Neural Networks to Classify Discussion Discourse Acts.">322. Integrating Tree Structures and Graph Structures with Neural Networks to Classify Discussion Discourse Acts.</a></li>
<li><a href="#323. AnlamVer: Semantic Model Evaluation Dataset for Turkish - Word Similarity and Relatedness.">323. AnlamVer: Semantic Model Evaluation Dataset for Turkish - Word Similarity and Relatedness.</a></li>
<li><a href="#324. Arguments and Adjuncts in Universal Dependencies.">324. Arguments and Adjuncts in Universal Dependencies.</a></li>
<li><a href="#325. Distinguishing affixoid formations from compounds.">325. Distinguishing affixoid formations from compounds.</a></li>
<li><a href="#326. A Survey on Open Information Extraction.">326. A Survey on Open Information Extraction.</a></li>
<li><a href="#327. Design Challenges and Misconceptions in Neural Sequence Labeling.">327. Design Challenges and Misconceptions in Neural Sequence Labeling.</a></li>
<li><a href="#328. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering.">328. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering.</a></li>
<li><a href="#329. Authorless Topic Models: Biasing Models Away from Known Structure.">329. Authorless Topic Models: Biasing Models Away from Known Structure.</a></li>
<li><a href="#330. SGM: Sequence Generation Model for Multi-label Classification.">330. SGM: Sequence Generation Model for Multi-label Classification.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="27th COLING 2018:Santa Fe, New Mexico, USA">27th COLING 2018:Santa Fe, New Mexico, USA</h1>
<p><a href="https://www.aclweb.org/anthology/volumes/C18-1/">Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August 20-26, 2018.</a> Association for Computational Linguistics
<a href="https://dblp.uni-trier.de/db/conf/coling/coling2018.html">DBLP Link</a></p>
<h2 id="Paper Num: 330 || Session Num: 0">Paper Num: 330 || Session Num: 0</h2>
<h3 id="1. A New Approach to Animacy Detection.">1. A New Approach to Animacy Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1001/">Paper Link</a>    Pages:1-12</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jahan:Labiba">Labiba Jahan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chauhan:Geeticka">Geeticka Chauhan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Finlayson:Mark_A=">Mark A. Finlayson</a></p>
<p>Abstract:
Animacy is a necessary property for a referent to be an agent, and thus animacy detection is useful for a variety of natural language processing tasks, including word sense disambiguation, co-reference resolution, semantic role labeling, and others. Prior work treated animacy as a word-level property, and has developed statistical classifiers to classify words as either animate or inanimate. We discuss why this approach to the problem is ill-posed, and present a new approach based on classifying the animacy of co-reference chains. We show that simple voting approaches to inferring the animacy of a chain from its constituent words perform relatively poorly, and then present a hybrid system merging supervised machine learning (ML) and a small number of hand-built rules to compute the animacy of referring expressions and co-reference chains. This method achieves state of the art performance. The supervised ML component leverages features such as word embeddings over referring expressions, parts of speech, and grammatical and semantic roles. The rules take into consideration parts of speech and the hypernymy structure encoded in WordNet. The system achieves an F1 of 0.88 for classifying the animacy of referring expressions, which is comparable to state of the art results for classifying the animacy of words, and achieves an F1 of 0.75 for classifying the animacy of coreference chains themselves. We release our training and test dataset, which includes 142 texts (all narratives) comprising 156,154 words, 34,698 referring expressions, and 10,941 co-reference chains. We test the method on a subset of the OntoNotes dataset, showing using manual sampling that animacy classification is 90% +/- 2% accurate for coreference chains, and 92% +/- 1% for referring expressions. The data also contains 46 folktales, which present an interesting challenge because they often involve characters who are members of traditionally inanimate classes (e.g., stoves that walk, trees that talk). We show that our system is able to detect the animacy of these unusual referents with an F1 of 0.95.</p>
<p>Keywords:</p>
<h3 id="2. Zero Pronoun Resolution with Attention-based Neural Network.">2. Zero Pronoun Resolution with Attention-based Neural Network.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1002/">Paper Link</a>    Pages:13-23</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Qingyu">Qingyu Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0030:Yu">Yu Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0003:Weinan">Weinan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>Abstract:
Recent neural network methods for zero pronoun resolution explore multiple models for generating representation vectors for zero pronouns and their candidate antecedents. Typically, contextual information is utilized to encode the zero pronouns since they are simply gaps that contain no actual content. To better utilize contexts of the zero pronouns, we here introduce the self-attention mechanism for encoding zero pronouns. With the help of the multiple hops of attention, our model is able to focus on some informative parts of the associated texts and therefore produces an efficient way of encoding the zero pronouns. In addition, an attention-based recurrent neural network is proposed for encoding candidate antecedents by their contents. Experiment results are encouraging: our proposed attention-based model gains the best performance on the Chinese portion of the OntoNotes corpus, substantially surpasses existing Chinese zero pronoun resolution baseline systems.</p>
<p>Keywords:</p>
<h3 id="3. They Exist! Introducing Plural Mentions to Coreference Resolution and Entity Linking.">3. They Exist! Introducing Plural Mentions to Coreference Resolution and Entity Linking.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1003/">Paper Link</a>    Pages:24-34</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Ethan">Ethan Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Jinho_D=">Jinho D. Choi</a></p>
<p>Abstract:
This paper analyzes arguably the most challenging yet under-explored aspect of resolution tasks such as coreference resolution and entity linking, that is the resolution of plural mentions. Unlike singular mentions each of which represents one entity, plural mentions stand for multiple entities. To tackle this aspect, we take the character identification corpus from the SemEval 2018 shared task that consists of entity annotation for singular mentions, and expand it by adding annotation for plural mentions. We then introduce a novel coreference resolution algorithm that selectively creates clusters to handle both singular and plural mentions, and also a deep learning-based entity linking model that jointly handles both types of mentions through multi-task learning. Adjusted evaluation metrics are proposed for these tasks as well to handle the uniqueness of plural mentions. Our experiments show that the new coreference resolution and entity linking models significantly outperform traditional models designed only for singular mentions. To the best of our knowledge, this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks.</p>
<p>Keywords:</p>
<h3 id="4. Triad-based Neural Network for Coreference Resolution.">4. Triad-based Neural Network for Coreference Resolution.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1004/">Paper Link</a>    Pages:35-43</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Meng:Yuanliang">Yuanliang Meng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rumshisky:Anna">Anna Rumshisky</a></p>
<p>Abstract:
We propose a triad-based neural network system that generates affinity scores between entity mentions for coreference resolution. The system simultaneously accepts three mentions as input, taking mutual dependency and logical constraints of all three mentions into account, and thus makes more accurate predictions than the traditional pairwise approach. Depending on system choices, the affinity scores can be further used in clustering or mention ranking. Our experiments show that a standard hierarchical clustering using the scores produces state-of-art results with MUC and B 3 metrics on the English portion of CoNLL 2012 Shared Task. The model does not rely on many handcrafted features and is easy to train and use. The triads can also be easily extended to polyads of higher orders. To our knowledge, this is the first neural network system to model mutual dependency of more than two members at mention level.</p>
<p>Keywords:</p>
<h3 id="5. Unsupervised Morphology Learning with Statistical Paradigms.">5. Unsupervised Morphology Learning with Statistical Paradigms.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1005/">Paper Link</a>    Pages:44-54</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Hongzhi">Hongzhi Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marcus:Mitchell">Mitchell Marcus</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0001:Charles">Charles Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Ungar:Lyle_H=">Lyle H. Ungar</a></p>
<p>Abstract:
This paper describes an unsupervised model for morphological segmentation that exploits the notion of paradigms, which are sets of morphological categories (e.g., suffixes) that can be applied to a homogeneous set of words (e.g., nouns or verbs). Our algorithm identifies statistically reliable paradigms from the morphological segmentation result of a probabilistic model, and chooses reliable suffixes from them. The new suffixes can be fed back iteratively to improve the accuracy of the probabilistic model. Finally, the unreliable paradigms are subjected to pruning to eliminate unreliable morphological relations between words. The paradigm-based algorithm significantly improves segmentation accuracy. Our method achieves start-of-the-art results on experiments using the Morpho-Challenge data, including English, Turkish, and Finnish.</p>
<p>Keywords:</p>
<h3 id="6. Challenges of language technologies for the indigenous languages of the Americas.">6. Challenges of language technologies for the indigenous languages of the Americas.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1006/">Paper Link</a>    Pages:55-69</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mager:Manuel">Manuel Mager</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gutierrez=Vasques:Ximena">Ximena Gutierrez-Vasques</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sierra:Gerardo">Gerardo Sierra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meza=Ru=iacute=z:Iv=aacute=n_V=">Ivn V. Meza-Ruz</a></p>
<p>Abstract:
Indigenous languages of the American continent are highly diverse. However, they have received little attention from the technological perspective. In this paper, we review the research, the digital resources and the available NLP systems that focus on these languages. We present the main challenges and research questions that arise when distant languages and low-resource scenarios are faced. We would like to encourage NLP research in linguistically rich and diverse areas like the Americas.</p>
<p>Keywords:</p>
<h3 id="7. Low-resource Cross-lingual Event Type Detection via Distant Supervision with Minimal Effort.">7. Low-resource Cross-lingual Event Type Detection via Distant Supervision with Minimal Effort.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1007/">Paper Link</a>    Pages:70-82</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Muis:Aldrian_Obaja">Aldrian Obaja Muis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Otani:Naoki">Naoki Otani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vyas:Nidhi">Nidhi Vyas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Ruochen">Ruochen Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yiming">Yiming Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitamura:Teruko">Teruko Mitamura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>Abstract:
The use of machine learning for NLP generally requires resources for training. Tasks performed in a low-resource language usually rely on labeled data in another, typically resource-rich, language. However, there might not be enough labeled data even in a resource-rich language such as English. In such cases, one approach is to use a hand-crafted approach that utilizes only a small bilingual dictionary with minimal manual verification to create distantly supervised data. Another is to explore typical machine learning techniques, for example adversarial training of bilingual word representations. We find that in event-type detection taskthe task to classify [parts of] documents into a fixed set of labelsthey give about the same performance. We explore ways in which the two methods can be complementary and also see how to best utilize a limited budget for manual annotation to maximize performance gain.</p>
<p>Keywords:</p>
<h3 id="8. Neural Transition-based String Transduction for Limited-Resource Setting in Morphology.">8. Neural Transition-based String Transduction for Limited-Resource Setting in Morphology.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1008/">Paper Link</a>    Pages:83-93</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Makarov:Peter">Peter Makarov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Clematide:Simon">Simon Clematide</a></p>
<p>Abstract:
We present a neural transition-based model that uses a simple set of edit actions (copy, delete, insert) for morphological transduction tasks such as inflection generation, lemmatization, and reinflection. In a large-scale evaluation on four datasets and dozens of languages, our approach consistently outperforms state-of-the-art systems on low and medium training-set sizes and is competitive in the high-resource setting. Learning to apply a generic copy action enables our approach to generalize quickly from a few data points. We successfully leverage minimum risk training to compensate for the weaknesses of MLE parameter learning and neutralize the negative effects of training a pipeline with a separate character aligner.</p>
<p>Keywords:</p>
<h3 id="9. Distance-Free Modeling of Multi-Predicate Interactions in End-to-End Japanese Predicate-Argument Structure Analysis.">9. Distance-Free Modeling of Multi-Predicate Interactions in End-to-End Japanese Predicate-Argument Structure Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1009/">Paper Link</a>    Pages:94-106</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Matsubayashi:Yuichiroh">Yuichiroh Matsubayashi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Inui:Kentaro">Kentaro Inui</a></p>
<p>Abstract:
Capturing interactions among multiple predicate-argument structures (PASs) is a crucial issue in the task of analyzing PAS in Japanese. In this paper, we propose new Japanese PAS analysis models that integrate the label prediction information of arguments in multiple PASs by extending the input and last layers of a standard deep bidirectional recurrent neural network (bi-RNN) model. In these models, using the mechanisms of pooling and attention, we aim to directly capture the potential interactions among multiple PASs, without being disturbed by the word order and distance. Our experiments show that the proposed models improve the prediction accuracy specifically for cases where the predicate and argument are in an indirect dependency relation and achieve a new state of the art in the overall F1 on a standard benchmark corpus.</p>
<p>Keywords:</p>
<h3 id="10. Sprucing up the trees - Error detection in treebanks.">10. Sprucing up the trees - Error detection in treebanks.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1010/">Paper Link</a>    Pages:107-118</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rehbein:Ines">Ines Rehbein</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruppenhofer:Josef">Josef Ruppenhofer</a></p>
<p>Abstract:
We present a method for detecting annotation errors in manually and automatically annotated dependency parse trees, based on ensemble parsing in combination with Bayesian inference, guided by active learning. We evaluate our method in different scenarios: (i) for error detection in dependency treebanks and (ii) for improving parsing accuracy on in- and out-of-domain data.</p>
<p>Keywords:</p>
<h3 id="11. Two Local Models for Neural Constituent Parsing.">11. Two Local Models for Neural Constituent Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1011/">Paper Link</a>    Pages:119-132</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Teng:Zhiyang">Zhiyang Teng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a></p>
<p>Abstract:
Non-local features have been exploited by syntactic parsers for capturing dependencies between sub output structures. Such features have been a key to the success of state-of-the-art statistical parsers. With the rise of deep learning, however, it has been shown that local output decisions can give highly competitive accuracies, thanks to the power of dense neural input representations that embody global syntactic information. We investigate two conceptually simple local neural models for constituent parsing, which make local decisions to constituent spans and CFG rules, respectively. Consistent with previous findings along the line, our best model gives highly competitive results, achieving the labeled bracketing F1 scores of 92.4% on PTB and 87.3% on CTB 5.1.</p>
<p>Keywords:</p>
<h3 id="12. RNN Simulations of Grammaticality Judgments on Long-distance Dependencies.">12. RNN Simulations of Grammaticality Judgments on Long-distance Dependencies.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1012/">Paper Link</a>    Pages:133-144</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chowdhury:Shammur_Absar">Shammur Absar Chowdhury</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zamparelli:Roberto">Roberto Zamparelli</a></p>
<p>Abstract:
The paper explores the ability of LSTM networks trained on a language modeling task to detect linguistic structures which are ungrammatical due to extraction violations (extra arguments and subject-relative clause island violations), and considers its implications for the debate on language innatism. The results show that the current RNN model can correctly classify (un)grammatical sentences, in certain conditions, but it is sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of grammaticality, at least in the domain we tested.</p>
<p>Keywords:</p>
<h3 id="13. How Predictable is Your State? Leveraging Lexical and Contextual Information for Predicting Legislative Floor Action at the State Level.">13. How Predictable is Your State? Leveraging Lexical and Contextual Information for Predicting Legislative Floor Action at the State Level.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1013/">Paper Link</a>    Pages:145-160</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Eidelman:Vladimir">Vladimir Eidelman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kornilova:Anastassia">Anastassia Kornilova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Argyle:Daniel">Daniel Argyle</a></p>
<p>Abstract:
Modeling U.S. Congressional legislation and roll-call votes has received significant attention in previous literature, and while legislators across 50 state governments and D.C. propose over 100,000 bills each year, enacting over 30% of them on average, state level analysis has received relatively less attention due in part to the difficulty in obtaining the necessary data. Since each state legislature is guided by their own procedures, politics and issues, however, it is difficult to qualitatively asses the factors that affect the likelihood of a legislative initiative succeeding. We present several methods for modeling the likelihood of a bill receiving floor action across all 50 states and D.C. We utilize the lexical content of over 1 million bills, along with contextual legislature and legislator derived features to build our predictive models, allowing a comparison of what factors are important to the lawmaking process. Furthermore, we show that these signals hold complementary predictive power, together achieving an average improvement in accuracy of 18% over state specific baselines.</p>
<p>Keywords:</p>
<h3 id="14. Learning to Search in Long Documents Using Document Structure.">14. Learning to Search in Long Documents Using Document Structure.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1014/">Paper Link</a>    Pages:161-176</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Geva:Mor">Mor Geva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berant:Jonathan">Jonathan Berant</a></p>
<p>Abstract:
Reading comprehension models are based on recurrent neural networks that sequentially process the document tokens. As interest turns to answering more complex questions over longer documents, sequential reading of large portions of text becomes a substantial bottleneck. Inspired by how humans use document structure, we propose a novel framework for reading comprehension. We represent documents as trees, and model an agent that learns to interleave quick navigation through the document tree with more expensive answer extraction. To encourage exploration of the document tree, we propose a new algorithm, based on Deep Q-Network (DQN), which strategically samples tree nodes at training time. Empirically we find our algorithm improves question answering performance compared to DQN and a strong information-retrieval (IR) baseline, and that ensembling our model with the IR baseline results in further gains in performance.</p>
<p>Keywords:</p>
<h3 id="15. Incorporating Image Matching Into Knowledge Acquisition for Event-Oriented Relation Recognition.">15. Incorporating Image Matching Into Knowledge Acquisition for Event-Oriented Relation Recognition.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1015/">Paper Link</a>    Pages:177-189</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hong:Yu">Yu Hong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Yang">Yang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruan:Huibin">Huibin Ruan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zou:Bowei">Bowei Zou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Jianmin">Jianmin Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>Abstract:
Event relation recognition is a challenging language processing task. It is required to determine the relation class of a pair of query events, such as causality, under the condition that there isnt any reliable clue for use. We follow the traditional statistical approach in this paper, speculating the relation class of the target events based on the relation-class distributions on the similar events. There is minimal supervision used during the speculation process. In particular, we incorporate image processing into the acquisition of similar event instances, including the utilization of images for visually representing event scenes, and the use of the neural network based image matching for approximate calculation between events. We test our method on the ACE-R2 corpus and compared our model with the fully-supervised neural network models. Experimental results show that we achieve a comparable performance to CNN while slightly better than LSTM.</p>
<p>Keywords:</p>
<h3 id="16. Representation Learning of Entities and Documents from Knowledge Base Descriptions.">16. Representation Learning of Entities and Documents from Knowledge Base Descriptions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1016/">Paper Link</a>    Pages:190-201</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yamada:Ikuya">Ikuya Yamada</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shindo:Hiroyuki">Hiroyuki Shindo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Takefuji:Yoshiyasu">Yoshiyasu Takefuji</a></p>
<p>Abstract:
In this paper, we describe TextEnt, a neural network model that learns distributed representations of entities and documents directly from a knowledge base (KB). Given a document in a KB consisting of words and entity annotations, we train our model to predict the entity that the document describes and map the document and its target entity close to each other in a continuous vector space. Our model is trained using a large number of documents extracted from Wikipedia. The performance of the proposed model is evaluated using two tasks, namely fine-grained entity typing and multiclass text classification. The results demonstrate that our model achieves state-of-the-art performance on both tasks. The code and the trained representations are made available online for further academic research.</p>
<p>Keywords:</p>
<h3 id="17. Simple Neologism Based Domain Independent Models to Predict Year of Authorship.">17. Simple Neologism Based Domain Independent Models to Predict Year of Authorship.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1017/">Paper Link</a>    Pages:202-212</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kulkarni:Vivek">Vivek Kulkarni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tian:Yingtao">Yingtao Tian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dandiwala:Parth">Parth Dandiwala</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Skiena:Steven">Steven Skiena</a></p>
<p>Abstract:
We present domain independent models to date documents based only on neologism usage patterns. Our models capture patterns of neologism usage over time to date texts, provide insights into temporal locality of word usage over a span of 150 years, and generalize to various domains like News, Fiction, and Non-Fiction with competitive performance. Quite intriguingly, we show that by modeling only the distribution of usage counts over neologisms (the model being agnostic of the particular words themselves), we achieve competitive performance using several orders of magnitude fewer features (only 200 input features) compared to state of the art models some of which use 200K features.</p>
<p>Keywords:</p>
<h3 id="18. Neural Math Word Problem Solver with Reinforcement Learning.">18. Neural Math Word Problem Solver with Reinforcement Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1018/">Paper Link</a>    Pages:213-223</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Danqing">Danqing Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0022:Jing">Jing Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Chin=Yew">Chin-Yew Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Jian">Jian Yin</a></p>
<p>Abstract:
Sequence-to-sequence model has been applied to solve math word problems. The model takes math problem descriptions as input and generates equations as output. The advantage of sequence-to-sequence model requires no feature engineering and can generate equations that do not exist in training data. However, our experimental analysis reveals that this model suffers from two shortcomings: (1) generate spurious numbers; (2) generate numbers at wrong positions. In this paper, we propose incorporating copy and alignment mechanism to the sequence-to-sequence model (namely CASS) to address these shortcomings. To train our model, we apply reinforcement learning to directly optimize the solution accuracy. It overcomes the train-test discrepancy issue of maximum likelihood estimation, which uses the surrogate objective of maximizing equation likelihood during training while the evaluation metric is solution accuracy (non-differentiable) at test time. Furthermore, to explore the effectiveness of our neural model, we use our model output as a feature and incorporate it into the feature-based model. Experimental results show that (1) The copy and alignment mechanism is effective to address the two issues; (2) Reinforcement learning leads to better performance than maximum likelihood on this task; (3) Our neural model is complementary to the feature-based model and their combination significantly outperforms the state-of-the-art results.</p>
<p>Keywords:</p>
<h3 id="19. Personalizing Lexical Simplification.">19. Personalizing Lexical Simplification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1019/">Paper Link</a>    Pages:224-232</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee_0001:John">John Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yeung:Chak_Yan">Chak Yan Yeung</a></p>
<p>Abstract:
A lexical simplification (LS) system aims to substitute complex words with simple words in a text, while preserving its meaning and grammaticality. Despite individual users differences in vocabulary knowledge, current systems do not consider these variations; rather, they are trained to find one optimal substitution or ranked list of substitutions for all users. We evaluate the performance of a state-of-the-art LS system on individual learners of English at different proficiency levels, and measure the benefits of using complex word identification (CWI) models to personalize the system. Experimental results show that even a simple personalized CWI model, based on graded vocabulary lists, can help the system avoid some unnecessary simplifications and produce more readable output.</p>
<p>Keywords:</p>
<h3 id="20. From Text to Lexicon: Bridging the Gap between Word Embeddings and Lexical Resources.">20. From Text to Lexicon: Bridging the Gap between Word Embeddings and Lexical Resources.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1020/">Paper Link</a>    Pages:233-244</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kuznetsov:Ilia">Ilia Kuznetsov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
Distributional word representations (often referred to as word embeddings) are omnipresent in modern NLP. Early work has focused on building representations for word types, and recent studies show that lemmatization and part of speech (POS) disambiguation of targets in isolation improve the performance of word embeddings on a range of downstream tasks. However, the reasons behind these improvements, the qualitative effects of these operations and the combined performance of lemmatized and POS disambiguated targets are less studied. This work aims to close this gap and puts previous findings into a general perspective. We examine the effect of lemmatization and POS typing on word embedding performance in a novel resource-based evaluation scenario, as well as on standard similarity benchmarks. We show that these two operations have complimentary qualitative and vocabulary-level effects and are best used in combination. We find that the improvement is more pronounced for verbs and show how lemmatization and POS typing implicitly target some of the verb-specific issues. We claim that the observed improvement is a result of better conceptual alignment between word embeddings and lexical resources, stressing the need for conceptually plausible modeling of word embedding targets.</p>
<p>Keywords:</p>
<h3 id="21. Lexi: A tool for adaptive, personalized text simplification.">21. Lexi: A tool for adaptive, personalized text simplification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1021/">Paper Link</a>    Pages:245-258</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bingel:Joachim">Joachim Bingel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Paetzold:Gustavo">Gustavo Paetzold</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/S=oslash=gaard:Anders">Anders Sgaard</a></p>
<p>Abstract:
Most previous research in text simplification has aimed to develop generic solutions, assuming very homogeneous target audiences with consistent intra-group simplification needs. We argue that this assumption does not hold, and that instead we need to develop simplification systems that adapt to the individual needs of specific users. As a first step towards personalized simplification, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a browser extension, enabling easy access to the service for its users.</p>
<p>Keywords:</p>
<h3 id="22. Identifying Emergent Research Trends by Key Authors and Phrases.">22. Identifying Emergent Research Trends by Key Authors and Phrases.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1022/">Paper Link</a>    Pages:259-269</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Shenhao">Shenhao Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prasad:Animesh">Animesh Prasad</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kan:Min=Yen">Min-Yen Kan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sugiyama:Kazunari">Kazunari Sugiyama</a></p>
<p>Abstract:
Identifying emergent research trends is a key issue for both primary researchers as well as secondary research managers. Such processes can uncover the historical development of an area, and yield insight on developing topics. We propose an embedded trend detection framework for this task which incorporates our bijunctive hypothesis that important phrases are written by important authors within a field and vice versa. By ranking both author and phrase information in a multigraph, our method jointly determines key phrases and authoritative authors. We represent this intermediate output as phrasal embeddings, and feed this to a recurrent neural network (RNN) to compute trend scores that identify research trends. Over two large datasets of scientific articles, we demonstrate that our approach successfully detects past trends from the field, outperforming baselines based solely on text centrality or citation.</p>
<p>Keywords:</p>
<h3 id="23. Embedding WordNet Knowledge for Textual Entailment.">23. Embedding WordNet Knowledge for Textual Entailment.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1023/">Paper Link</a>    Pages:270-281</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lan:Yunshi">Yunshi Lan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang_0001:Jing">Jing Jiang</a></p>
<p>Abstract:
In this paper, we study how we can improve a deep learning approach to textual entailment by incorporating lexical entailment relations from WordNet. Our idea is to embed the lexical entailment knowledge contained in WordNet in specially-learned word vectors, which we call entailment vectors. We present a standard neural network model and a novel set-theoretic model to learn these entailment vectors from word pairs with known lexical entailment relations derived from WordNet. We further incorporate these entailment vectors into a decomposable attention model for textual entailment and evaluate the model on the SICK and the SNLI dataset. We find that using these special entailment word vectors, we can significantly improve the performance of textual entailment compared with a baseline that uses only standard word2vec vectors. The final performance of our model is close to or above the state of the art, but our method does not rely on any manually-crafted rules or extensive syntactic features.</p>
<p>Keywords:</p>
<h3 id="24. Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing in Knowledge Bases.">24. Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing in Knowledge Bases.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1024/">Paper Link</a>    Pages:282-292</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Hailong">Hailong Jin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hou:Lei">Lei Hou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Juanzi">Juanzi Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Tiansi">Tiansi Dong</a></p>
<p>Abstract:
Fine-grained entity typing aims at identifying the semantic type of an entity in KB. Type information is very important in knowledge bases, but are unfortunately incomplete even in some large knowledge bases. Limitations of existing methods are either ignoring the structure and type information in KB or requiring large scale annotated corpus. To address these issues, we propose an attributed and predictive entity embedding method, which can fully utilize various kinds of information comprehensively. Extensive experiments on two real DBpedia datasets show that our proposed method significantly outperforms 8 state-of-the-art methods, with 4.0% and 5.2% improvement in Mi-F1 and Ma-F1, respectively.</p>
<p>Keywords:</p>
<h3 id="25. Joint Learning from Labeled and Unlabeled Data for Information Retrieval.">25. Joint Learning from Labeled and Unlabeled Data for Information Retrieval.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1025/">Paper Link</a>    Pages:293-302</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Bo">Bo Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Ping">Ping Cheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jia:Le">Le Jia</a></p>
<p>Abstract:
Recently, a significant number of studies have focused on neural information retrieval (IR) models. One category of works use unlabeled data to train general word embeddings based on term proximity, which can be integrated into traditional IR models. The other category employs labeled data (e.g. click-through data) to train end-to-end neural IR models consisting of layers for target-specific representation learning. The latter idea accounts better for the IR task and is favored by recent research works, which is the one we will follow in this paper. We hypothesize that general semantics learned from unlabeled data can complement task-specific representation learned from labeled data of limited quality, and that a combination of the two is favorable. To this end, we propose a learning framework which can benefit from both labeled and more abundant unlabeled data for representation learning in the context of IR. Through a joint learning fashion in a single neural framework, the learned representation is optimized to minimize both the supervised loss on query-document matching and the unsupervised loss on text reconstruction. Standard retrieval experiments on TREC collections indicate that the joint learning methodology leads to significant better performance of retrieval over several strong baselines for IR.</p>
<p>Keywords:</p>
<h3 id="26. Modeling the Readability of German Targeting Adults and Children: An empirically broad analysis and its cross-corpus validation.">26. Modeling the Readability of German Targeting Adults and Children: An empirically broad analysis and its cross-corpus validation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1026/">Paper Link</a>    Pages:303-317</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wei=szlig=:Zarah">Zarah Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meurers:Detmar">Detmar Meurers</a></p>
<p>Abstract:
We analyze two novel data sets of German educational media texts targeting adults and children. The analysis is based on 400 automatically extracted measures of linguistic complexity from a wide range of linguistic domains. We show that both data sets exhibit broad linguistic adaptation to the target audience, which generalizes across both data sets. Our most successful binary classification model for German readability robustly shows high accuracy between 89.4%98.9% for both data sets. To our knowledge, this comprehensive German readability model is the first for which robust cross-corpus performance has been shown. The research also contributes resources for German readability assessment that are externally validated as successful for different target audiences: we compiled a new corpus of German news broadcast subtitles, the Tagesschau/Logo corpus, and crawled a GEO/GEOlino corpus substantially enlarging the data compiled by Hancke et al. 2012.</p>
<p>Keywords:</p>
<h3 id="27. Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs.">27. Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1027/">Paper Link</a>    Pages:318-330</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stajner:Sanja">Sanja Stajner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hulpus:Ioana">Ioana Hulpus</a></p>
<p>Abstract:
Complexity of texts is usually assessed only at the lexical and syntactic levels. Although it is known that conceptual complexity plays a significant role in text understanding, no attempts have been made at assessing it automatically. We propose to automatically estimate the conceptual complexity of texts by exploiting a number of graph-based measures on a large knowledge base. By using a high-quality language learners corpus for English, we show that graph-based measures of individual text concepts, as well as the way they relate to each other in the knowledge graph, have a high discriminative power when distinguishing between two versions of the same text. Furthermore, when used as features in a binary classification task aiming to choose the simpler of two versions of the same text, our measures achieve high performance even in a default setup.</p>
<p>Keywords:</p>
<h3 id="28. Par4Sim - Adaptive Paraphrasing for Text Simplification.">28. Par4Sim - Adaptive Paraphrasing for Text Simplification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1028/">Paper Link</a>    Pages:331-342</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yimam:Seid_Muhie">Seid Muhie Yimam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Biemann:Chris">Chris Biemann</a></p>
<p>Abstract:
Learning from a real-world data stream and continuously updating the model without explicit supervision is a new challenge for NLP applications with machine learning components. In this work, we have developed an adaptive learning system for text simplification, which improves the underlying learning-to-rank model from usage data, i.e. how users have employed the system for the task of simplification. Our experimental result shows that, over a period of time, the performance of the embedded paraphrase ranking model increases steadily improving from a score of 62.88% up to 75.70% based on the NDCG@10 evaluation metrics. To our knowledge, this is the first study where an NLP component is adaptively improved through usage.</p>
<p>Keywords:</p>
<h3 id="29. Topic or Style? Exploring the Most Useful Features for Authorship Attribution.">29. Topic or Style? Exploring the Most Useful Features for Authorship Attribution.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1029/">Paper Link</a>    Pages:343-353</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sari:Yunita">Yunita Sari</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stevenson:Mark">Mark Stevenson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vlachos:Andreas">Andreas Vlachos</a></p>
<p>Abstract:
Approaches to authorship attribution, the task of identifying the author of a document, are based on analysis of individuals writing style and/or preferred topics. Although the problem has been widely explored, no previous studies have analysed the relationship between dataset characteristics and effectiveness of different types of features. This study carries out an analysis of four widely used datasets to explore how different types of features affect authorship attribution accuracy under varying conditions. The results of the analysis are applied to authorship attribution models based on both discrete and continuous representations. We apply the conclusions from our analysis to an extension of an existing approach to authorship attribution and outperform the prior state-of-the-art on two out of the four datasets used.</p>
<p>Keywords:</p>
<h3 id="30. A Deep Dive into Word Sense Disambiguation with LSTM.">30. A Deep Dive into Word Sense Disambiguation with LSTM.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1030/">Paper Link</a>    Pages:354-365</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Le:Minh">Minh Le</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Postma:Marten">Marten Postma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Urbani:Jacopo">Jacopo Urbani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vossen:Piek">Piek Vossen</a></p>
<p>Abstract:
LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by Yuan et al. (2016) returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by Yuan et al. (2016). Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation. All code and trained models are made freely available.</p>
<p>Keywords:</p>
<h3 id="31. Enriching Word Embeddings with Domain Knowledge for Readability Assessment.">31. Enriching Word Embeddings with Domain Knowledge for Readability Assessment.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1031/">Paper Link</a>    Pages:366-378</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Zhiwei">Zhiwei Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Qing">Qing Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Yafeng">Yafeng Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Daoxu">Daoxu Chen</a></p>
<p>Abstract:
In this paper, we present a method which learns the word embedding for readability assessment. For the existing word embedding models, they typically focus on the syntactic or semantic relations of words, while ignoring the reading difficulty, thus they may not be suitable for readability assessment. Hence, we provide the knowledge-enriched word embedding (KEWE), which encodes the knowledge on reading difficulty into the representation of words. Specifically, we extract the knowledge on word-level difficulty from three perspectives to construct a knowledge graph, and develop two word embedding models to incorporate the difficulty context derived from the knowledge graph to define the loss functions. Experiments are designed to apply KEWE for readability assessment on both English and Chinese datasets, and the results demonstrate both effectiveness and potential of KEWE.</p>
<p>Keywords:</p>
<h3 id="32. WikiRef: Wikilinks as a route to recommending appropriate references for scientific Wikipedia pages.">32. WikiRef: Wikilinks as a route to recommending appropriate references for scientific Wikipedia pages.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1032/">Paper Link</a>    Pages:379-389</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jana:Abhik">Abhik Jana</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kanojiya:Pranjal">Pranjal Kanojiya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goyal:Pawan">Pawan Goyal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mukherjee_0001:Animesh">Animesh Mukherjee</a></p>
<p>Abstract:
The exponential increase in the usage of Wikipedia as a key source of scientific knowledge among the researchers is making it absolutely necessary to metamorphose this knowledge repository into an integral and self-contained source of information for direct utilization. Unfortunately, the references which support the content of each Wikipedia entity page, are far from complete. Why are the reference section ill-formed for most Wikipedia pages? Is this section edited as frequently as the other sections of a page? Can there be appropriate surrogates that can automatically enhance the reference section? In this paper, we propose a novel two step approach  WikiRef  that (i) leverages the wikilinks present in a scientific Wikipedia target page and, thereby, (ii) recommends highly relevant references to be included in that target page appropriately and automatically borrowed from the reference section of the wikilinks. In the first step, we build a classifier to ascertain whether a wikilink is a potential source of reference or not. In the following step, we recommend references to the target page from the reference section of the wikilinks that are classified as potential sources of references in the first step. We perform an extensive evaluation of our approach on datasets from two different domains  Computer Science and Physics. For Computer Science we achieve a notably good performance with a precision@1 of 0.44 for reference recommendation as opposed to 0.38 obtained from the most competitive baseline. For the Physics dataset, we obtain a similar performance boost of 10% with respect to the most competitive baseline.</p>
<p>Keywords:</p>
<h3 id="33. Authorship Identification for Literary Book Recommendations.">33. Authorship Identification for Literary Book Recommendations.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1033/">Paper Link</a>    Pages:390-400</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Alharthi:Haifa">Haifa Alharthi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Inkpen:Diana">Diana Inkpen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Szpakowicz:Stan">Stan Szpakowicz</a></p>
<p>Abstract:
Book recommender systems can help promote the practice of reading for pleasure, which has been declining in recent years. One factor that influences reading preferences is writing style. We propose a system that recommends books after learning their authors style. To our knowledge, this is the first work that applies the information learned by an author-identification model to book recommendations. We evaluated the system according to a top-k recommendation scenario. Our system gives better accuracy when compared with many state-of-the-art methods. We also conducted a qualitative analysis by checking if similar books/authors were annotated similarly by experts.</p>
<p>Keywords:</p>
<h3 id="34. A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment in Portuguese.">34. A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment in Portuguese.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1034/">Paper Link</a>    Pages:401-413</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Leal:Sidney_Evaldo">Sidney Evaldo Leal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duran:Magali_Sanches">Magali Sanches Duran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Alu=iacute=sio:Sandra_Maria">Sandra Maria Alusio</a></p>
<p>Abstract:
Effective textual communication depends on readers being proficient enough to comprehend texts, and texts being clear enough to be understood by the intended audience, in a reading task. When the meaning of textual information and instructions is not well conveyed, many losses and damages may occur. Among the solutions to alleviate this problem is the automatic evaluation of sentence readability, task which has been receiving a lot of attention due to its large applicability. However, a shortage of resources, such as corpora for training and evaluation, hinders the full development of this task. In this paper, we generate a nontrivial sentence corpus in Portuguese. We evaluate three scenarios for building it, taking advantage of a parallel corpus of simplification, in which each sentence triplet is aligned and has simplification operations annotated, being ideal for justifying possible mistakes of future methods. The best scenario of our corpus PorSimplesSent is composed of 4,888 pairs, which is bigger than a similar corpus for English; all the three versions of it are publicly available. We created four baselines for PorSimplesSent and made available a pairwise ranking method, using 17 linguistic and psycholinguistic features, which correctly identifies the ranking of sentence pairs with an accuracy of 74.2%.</p>
<p>Keywords:</p>
<h3 id="35. Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference.">35. Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1035/">Paper Link</a>    Pages:414-425</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Du:Qianlong">Qianlong Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Keh=Yih">Keh-Yih Su</a></p>
<p>Abstract:
This paper proposes to perform natural language inference with Word-Pair-Dependency-Triplets. Most previous DNN-based approaches either ignore syntactic dependency among words, or directly use tree-LSTM to generate sentence representation with irrelevant information. To overcome the problems mentioned above, we adopt Word-Pair-Dependency-Triplets to improve alignment and inference judgment. To be specific, instead of comparing each triplet from one passage with the merged information of another passage, we first propose to perform comparison directly between the triplets of the given passage-pair to make the judgement more interpretable. Experimental results show that the performance of our approach is better than most of the approaches that use tree structures, and is comparable to other state-of-the-art approaches.</p>
<p>Keywords:</p>
<h3 id="36. Cooperative Denoising for Distantly Supervised Relation Extraction.">36. Cooperative Denoising for Distantly Supervised Relation Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1036/">Paper Link</a>    Pages:426-436</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lei:Kai">Kai Lei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Daoyuan">Daoyuan Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yaliang">Yaliang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Du:Nan">Nan Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0007:Min">Min Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fan_0001:Wei">Wei Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Ying">Ying Shen</a></p>
<p>Abstract:
Distantly supervised relation extraction greatly reduces human efforts in extracting relational facts from unstructured texts. However, it suffers from noisy labeling problem, which can degrade its performance. Meanwhile, the useful information expressed in knowledge graph is still underutilized in the state-of-the-art methods for distantly supervised relation extraction. In the light of these challenges, we propose CORD, a novelCOopeRativeDenoising framework, which consists two base networks leveraging text corpus and knowledge graph respectively, and a cooperative module involving their mutual learning by the adaptive bi-directional knowledge distillation and dynamic ensemble with noisy-varying instances. Experimental results on a real-world dataset demonstrate that the proposed method reduces the noisy labels and achieves substantial improvement over the state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="37. Adversarial Feature Adaptation for Cross-lingual Relation Classification.">37. Adversarial Feature Adaptation for Cross-lingual Relation Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1037/">Paper Link</a>    Pages:437-448</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zou:Bowei">Bowei Zou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Zengzhuang">Zengzhuang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hong:Yu">Yu Hong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>Abstract:
Relation Classification aims to classify the semantic relationship between two marked entities in a given sentence. It plays a vital role in a variety of natural language processing applications. Most existing methods focus on exploiting mono-lingual data, e.g., in English, due to the lack of annotated data in other languages. In this paper, we come up with a feature adaptation approach for cross-lingual relation classification, which employs a generative adversarial network (GAN) to transfer feature representations from one language with rich annotated data to another language with scarce annotated data. Such a feature adaptation approach enables feature imitation via the competition between a relation classification network and a rival discriminator. Experimental results on the ACE 2005 multilingual training corpus, treating English as the source language and Chinese the target, demonstrate the effectiveness of our proposed approach, yielding an improvement of 5.7% over the state-of-the-art.</p>
<p>Keywords:</p>
<h3 id="38. One-shot Learning for Question-Answering in Gaokao History Challenge.">38. One-shot Learning for Question-Answering in Gaokao History Challenge.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1038/">Paper Link</a>    Pages:449-461</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Zhuosheng">Zhuosheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a></p>
<p>Abstract:
Answering questions from university admission exams (Gaokao in Chinese) is a challenging AI task since it requires effective representation to capture complicated semantic relations between questions and answers. In this work, we propose a hybrid neural model for deep question-answering task from history examinations. Our model employs a cooperative gated neural network to retrieve answers with the assistance of extra labels given by a neural turing machine labeler. Empirical study shows that the labeler works well with only a small training dataset and the gated mechanism is good at fetching the semantic representation of lengthy answers. Experiments on question answering demonstrate the proposed model obtains substantial performance gains over various neural model baselines in terms of multiple evaluation metrics.</p>
<p>Keywords:</p>
<h3 id="39. Dynamic Multi-Level Multi-Task Learning for Sentence Simplification.">39. Dynamic Multi-Level Multi-Task Learning for Sentence Simplification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1039/">Paper Link</a>    Pages:462-476</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Han">Han Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pasunuru:Ramakanth">Ramakanth Pasunuru</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a></p>
<p>Abstract:
Sentence simplification aims to improve readability and understandability, based on several operations such as splitting, deletion, and paraphrasing. However, a valid simplified sentence should also be logically entailed by its input sentence. In this work, we first present a strong pointer-copy mechanism based sequence-to-sequence sentence simplification model, and then improve its entailment and paraphrasing capabilities via multi-task learning with related auxiliary tasks of entailment and paraphrase generation. Moreover, we propose a novel multi-level layered soft sharing approach where each auxiliary task shares different (higher versus lower) level layers of the sentence simplification model, depending on the tasks semantic versus lexico-syntactic nature. We also introduce a novel multi-armed bandit based training approach that dynamically learns how to effectively switch across tasks during multi-task learning. Experiments on multiple popular datasets demonstrate that our model outperforms competitive simplification systems in SARI and FKGL automatic metrics, and human evaluation. Further, we present several ablation analyses on alternative layer sharing methods, soft versus hard sharing, dynamic multi-armed bandit sampling approaches, and our models learned entailment and paraphrasing skills.</p>
<p>Keywords:</p>
<h3 id="40. Interpretation of Implicit Conditions in Database Search Dialogues.">40. Interpretation of Implicit Conditions in Database Search Dialogues.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1040/">Paper Link</a>    Pages:477-486</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fukunaga:Shun=ya">Shun-ya Fukunaga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nishikawa:Hitoshi">Hitoshi Nishikawa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tokunaga:Takenobu">Takenobu Tokunaga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yokono:Hikaru">Hikaru Yokono</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Takahashi:Tetsuro">Tetsuro Takahashi</a></p>
<p>Abstract:
Targeting the database search dialogue, we propose to utilise information in the user utterances that do not directly mention the database (DB) field of the backend database system but are useful for constructing database queries. We call this kind of information implicit conditions. Interpreting the implicit conditions enables the dialogue system more natural and efficient in communicating with humans. We formalised the interpretation of the implicit conditions as classifying user utterances into the related DB field while identifying the evidence for that classification at the same time. Introducing this new task is one of the contributions of this paper. We implemented two models for this task: an SVM-based model and an RCNN-based model. Through the evaluation using a corpus of simulated dialogues between a real estate agent and a customer, we found that the SVM-based model showed better performance than the RCNN-based model.</p>
<p>Keywords:</p>
<h3 id="41. Few-Shot Charge Prediction with Discriminative Legal Attributes.">41. Few-Shot Charge Prediction with Discriminative Legal Attributes.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1041/">Paper Link</a>    Pages:487-498</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Zikun">Zikun Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiang">Xiang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Cunchao">Cunchao Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a></p>
<p>Abstract:
Automatic charge prediction aims to predict the final charges according to the fact descriptions in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge prediction perform adequately on those high-frequency charges but are not yet capable of predicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs, whose fact descriptions are fairly similar to each other. To address these issues, we introduce several discriminative attributes of charges as the internal mapping between fact descriptions and charges. These attributes provide additional information for few-shot charges, as well as effective signals for distinguishing confusing charges. More specifically, we propose an attribute-attentive charge prediction model to infer the attributes and charges simultaneously. Experimental results on real-work datasets demonstrate that our proposed model achieves significant and consistent improvements than other state-of-the-art baselines. Specifically, our model outperforms other baselines by more than 50% in the few-shot scenario. Our codes and datasets can be obtained from <a href="https://github.com/thunlp/attribute_charge">https://github.com/thunlp/attribute_charge</a>.</p>
<p>Keywords:</p>
<h3 id="42. Can Taxonomy Help? Improving Semantic Question Matching using Question Taxonomy.">42. Can Taxonomy Help? Improving Semantic Question Matching using Question Taxonomy.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1042/">Paper Link</a>    Pages:499-513</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Deepak">Deepak Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pujari:Rajkumar">Rajkumar Pujari</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Ekbal:Asif">Asif Ekbal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Maitra:Anutosh">Anutosh Maitra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jain:Tom_Geo">Tom Geo Jain</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sengupta:Shubhashis">Shubhashis Sengupta</a></p>
<p>Abstract:
In this paper, we propose a hybrid technique for semantic question matching. It uses a proposed two-layered taxonomy for English questions by augmenting state-of-the-art deep learning models with question classes obtained from a deep learning based question classifier. Experiments performed on three open-domain datasets demonstrate the effectiveness of our proposed approach. We achieve state-of-the-art results on partial ordering question ranking (POQR) benchmark dataset. Our empirical analysis shows that coupling standard distributional features (provided by the question encoder) with knowledge from taxonomy is more effective than either deep learning or taxonomy-based knowledge alone.</p>
<p>Keywords:</p>
<h3 id="43. Natural Language Interface for Databases Using a Dual-Encoder Model.">43. Natural Language Interface for Databases Using a Dual-Encoder Model.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1043/">Paper Link</a>    Pages:514-524</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hosu:Ionel=Alexandru">Ionel-Alexandru Hosu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Iacob:Radu_Cristian_Alexandru">Radu Cristian Alexandru Iacob</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brad:Florin">Florin Brad</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruseti:Stefan">Stefan Ruseti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rebedea:Traian">Traian Rebedea</a></p>
<p>Abstract:
We propose a sketch-based two-step neural model for generating structured queries (SQL) based on a users request in natural language. The sketch is obtained by using placeholders for specific entities in the SQL query, such as column names, table names, aliases and variables, in a process similar to semantic parsing. The first step is to apply a sequence-to-sequence (SEQ2SEQ) model to determine the most probable SQL sketch based on the request in natural language. Then, a second network designed as a dual-encoder SEQ2SEQ model using both the text query and the previously obtained sketch is employed to generate the final SQL query. Our approach shows improvements over previous approaches on two recent large datasets (WikiSQL and SENLIDB) suitable for data-driven solutions for natural language interfaces for databases.</p>
<p>Keywords:</p>
<h3 id="44. Employing Text Matching Network to Recognise Nuclearity in Chinese Discourse.">44. Employing Text Matching Network to Recognise Nuclearity in Chinese Discourse.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1044/">Paper Link</a>    Pages:525-535</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Sheng">Sheng Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Peifeng">Peifeng Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Qiaoming">Qiaoming Zhu</a></p>
<p>Abstract:
The task of nuclearity recognition in Chinese discourse remains challenging due to the demand for more deep semantic information. In this paper, we propose a novel text matching network (TMN) that encodes the discourse units and the paragraphs by combining Bi-LSTM and CNN to capture both global dependency information and local n-gram information. Moreover, it introduces three components of text matching, the Cosine, Bilinear and Single Layer Network, to incorporate various similarities and interactions among the discourse units. Experimental results on the Chinese Discourse TreeBank show that our proposed TMN model significantly outperforms various strong baselines in both micro-F1 and macro-F1.</p>
<p>Keywords:</p>
<h3 id="45. Joint Modeling of Structure Identification and Nuclearity Recognition in Macro Chinese Discourse Treebank.">45. Joint Modeling of Structure Identification and Nuclearity Recognition in Macro Chinese Discourse Treebank.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1045/">Paper Link</a>    Pages:536-546</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chu:Xiaomin">Xiaomin Chu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Feng">Feng Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Yi">Yi Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Qiaoming">Qiaoming Zhu</a></p>
<p>Abstract:
Discourse parsing is a challenging task and plays a critical role in discourse analysis. This paper focus on the macro level discourse structure analysis, which has been less studied in the previous researches. We explore a macro discourse structure presentation schema to present the macro level discourse structure, and propose a corresponding corpus, named Macro Chinese Discourse Treebank. On these bases, we concentrate on two tasks of macro discourse structure analysis, including structure identification and nuclearity recognition. In order to reduce the error transmission between the associated tasks, we adopt a joint model of the two tasks, and an Integer Linear Programming approach is proposed to achieve global optimization with various kinds of constraints.</p>
<p>Keywords:</p>
<h3 id="46. Implicit Discourse Relation Recognition using Neural Tensor Network with Interactive Attention and Sparse Learning.">46. Implicit Discourse Relation Recognition using Neural Tensor Network with Interactive Attention and Sparse Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1046/">Paper Link</a>    Pages:547-558</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Fengyu">Fengyu Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Ruifang">Ruifang He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Di">Di Jin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dang:Jianwu">Jianwu Dang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Longbiao">Longbiao Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiangang">Xiangang Li</a></p>
<p>Abstract:
Implicit discourse relation recognition aims to understand and annotate the latent relations between two discourse arguments, such as temporal, comparison, etc. Most previous methods encode two discourse arguments separately, the ones considering pair specific clues ignore the bidirectional interactions between two arguments and the sparsity of pair patterns. In this paper, we propose a novel neural Tensor network framework with Interactive Attention and Sparse Learning (TIASL) for implicit discourse relation recognition. (1) We mine the most correlated word pairs from two discourse arguments to model pair specific clues, and integrate them as interactive attention into argument representations produced by the bidirectional long short-term memory network. Meanwhile, (2) the neural tensor network with sparse constraint is proposed to explore the deeper and the more important pair patterns so as to fully recognize discourse relations. The experimental results on PDTB show that our proposed TIASL framework is effective.</p>
<p>Keywords:</p>
<h3 id="47. Transition-based Neural RST Parsing with Implicit Syntax Features.">47. Transition-based Neural RST Parsing with Implicit Syntax Features.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1047/">Paper Link</a>    Pages:559-570</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Nan">Nan Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Meishan">Meishan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Guohong">Guohong Fu</a></p>
<p>Abstract:
Syntax has been a useful source of information for statistical RST discourse parsing. Under the neural setting, a common approach integrates syntax by a recursive neural network (RNN), requiring discrete output trees produced by a supervised syntax parser. In this paper, we propose an implicit syntax feature extraction approach, using hidden-layer vectors extracted from a neural syntax parser. In addition, we propose a simple transition-based model as the baseline, further enhancing it with dynamic oracle. Experiments on the standard dataset show that our baseline model with dynamic oracle is highly competitive. When implicit syntax features are integrated, we are able to obtain further improvements, better than using explicit Tree-RNN.</p>
<p>Keywords:</p>
<h3 id="48. Deep Enhanced Representation for Implicit Discourse Relation Recognition.">48. Deep Enhanced Representation for Implicit Discourse Relation Recognition.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1048/">Paper Link</a>    Pages:571-583</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bai:Hongxiao">Hongxiao Bai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a></p>
<p>Abstract:
Implicit discourse relation recognition is a challenging task as the relation prediction without explicit connectives in discourse parsing needs understanding of text spans and cannot be easily derived from surface features from the input sentence pairs. Thus, properly representing the text is very crucial to this task. In this paper, we propose a model augmented with different grained text representations, including character, subword, word, sentence, and sentence pair levels. The proposed deeper model is evaluated on the benchmark treebank and achieves state-of-the-art accuracy with greater than 48% in 11-way and F1 score greater than 50% in 4-way classifications for the first time according to our best knowledge.</p>
<p>Keywords:</p>
<h3 id="49. A Knowledge-Augmented Neural Network Model for Implicit Discourse Relation Classification.">49. A Knowledge-Augmented Neural Network Model for Implicit Discourse Relation Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1049/">Paper Link</a>    Pages:584-595</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kishimoto:Yudai">Yudai Kishimoto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Murawaki:Yugo">Yugo Murawaki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kurohashi:Sadao">Sadao Kurohashi</a></p>
<p>Abstract:
Identifying discourse relations that are not overtly marked with discourse connectives remains a challenging problem. The absence of explicit clues indicates a need for the combination of world knowledge and weak contextual clues, which can hardly be learned from a small amount of manually annotated data. In this paper, we address this problem by augmenting the input text with external knowledge and context and by adopting a neural network model that can effectively handle the augmented text. Experiments show that external knowledge did improve the classification accuracy. Contextual information provided no significant gain for implicit discourse relations, but it did for explicit ones.</p>
<p>Keywords:</p>
<h3 id="50. Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches.">50. Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1050/">Paper Link</a>    Pages:596-606</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kuang:Shaohui">Shaohui Kuang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Weihua">Weihua Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>Abstract:
Sentences in a well-formed text are connected to each other via various links to form the cohesive structure of the text. Current neural machine translation (NMT) systems translate a text in a conventional sentence-by-sentence fashion, ignoring such cross-sentence links and dependencies. This may lead to generate an incoherent target text for a coherent source text. In order to handle this issue, we propose a cache-based approach to modeling coherence for neural machine translation by capturing contextual information either from recently translated sentences or the entire document. Particularly, we explore two types of caches: a dynamic cache, which stores words from the best translation hypotheses of preceding sentences, and a topic cache, which maintains a set of target-side topical words that are semantically related to the document to be translated. On this basis, we build a new layer to score target words in these two caches with a cache-based neural model. Here the estimated probabilities from the cache-based neural model are combined with NMT probabilities into the final word prediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines.</p>
<p>Keywords:</p>
<h3 id="51. Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate Model.">51. Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate Model.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1051/">Paper Link</a>    Pages:607-617</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kuang:Shaohui">Shaohui Kuang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a></p>
<p>Abstract:
Neural machine translation (NMT) systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring inter-sentence information. This may make the translation of a sentence ambiguous or even inconsistent with the translations of neighboring sentences. In order to handle this issue, we propose an inter-sentence gate model that uses the same encoder to encode two adjacent sentences and controls the amount of information flowing from the preceding sentence to the translation of the current sentence with an inter-sentence gate. In this way, our proposed model can capture the connection between sentences and fuse recency from neighboring sentences into neural machine translation. On several NIST Chinese-English translation tasks, our experiments demonstrate that the proposed inter-sentence gate model achieves substantial improvements over the baseline.</p>
<p>Keywords:</p>
<h3 id="52. Improving Neural Machine Translation by Incorporating Hierarchical Subword Features.">52. Improving Neural Machine Translation by Incorporating Hierarchical Subword Features.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1052/">Paper Link</a>    Pages:618-629</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Morishita:Makoto">Makoto Morishita</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Suzuki:Jun">Jun Suzuki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nagata:Masaaki">Masaaki Nagata</a></p>
<p>Abstract:
This paper focuses on subword-based Neural Machine Translation (NMT). We hypothesize that in the NMT model, the appropriate subword units for the following three modules (layers) can differ: (1) the encoder embedding layer, (2) the decoder embedding layer, and (3) the decoder output layer. We find the subword based on Sennrich et al. (2016) has a feature that a large vocabulary is a superset of a small vocabulary and modify the NMT model enables the incorporation of several different subword units in a single embedding layer. We refer these small subword features as hierarchical subword features. To empirically investigate our assumption, we compare the performance of several different subword units and hierarchical subword features for both the encoder and decoder embedding layers. We confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets.</p>
<p>Keywords:</p>
<h3 id="53. Design Challenges in Named Entity Transliteration.">53. Design Challenges in Named Entity Transliteration.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1053/">Paper Link</a>    Pages:630-640</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Merhav:Yuval">Yuval Merhav</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Ash:Stephen">Stephen Ash</a></p>
<p>Abstract:
We analyze some of the fundamental design challenges that impact the development of a multilingual state-of-the-art named entity transliteration system, including curating bilingual named entity datasets and evaluation of multiple transliteration methods. We empirically evaluate the transliteration task using the traditional weighted finite state transducer (WFST) approach against two neural approaches: the encoder-decoder recurrent neural network method and the recent, non-sequential Transformer method. In order to improve availability of bilingual named entity transliteration datasets, we release personal name bilingual dictionaries mined from Wikidata for English to Russian, Hebrew, Arabic, and Japanese Katakana. Our code and dictionaries are publicly available.</p>
<p>Keywords:</p>
<h3 id="54. A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation.">54. A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1054/">Paper Link</a>    Pages:641-652</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lakew:Surafel_Melaku">Surafel Melaku Lakew</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cettolo:Mauro">Mauro Cettolo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Federico:Marcello">Marcello Federico</a></p>
<p>Abstract:
Recently, neural machine translation (NMT) has been extended to multilinguality, that is to handle more than one translation direction with a single system. Multilingual NMT showed competitive performance against pure bilingual systems. Notably, in low-resource settings, it proved to work effectively and efficiently, thanks to shared representation space that is forced across languages and induces a sort of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot inference across language pairs never seen at training time. Despite the increasing interest in this framework, an in-depth analysis of what a multilingual NMT model is capable of and what it is not is still missing. Motivated by this, our work (i) provides a quantitative and comparative analysis of the translations produced by bilingual, multilingual and zero-shot systems; (ii) investigates the translation quality of two of the currently dominant neural architectures in MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively explores how the closeness between languages influences the zero-shot translation. Our analysis leverages multiple professional post-edits of automatic translations by several different systems and focuses both on automatic standard metrics (BLEU and TER) and on widely used error categories, which are lexical, morphology, and word order errors.</p>
<p>Keywords:</p>
<h3 id="55. On Adversarial Examples for Character-Level Neural Machine Translation.">55. On Adversarial Examples for Character-Level Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1055/">Paper Link</a>    Pages:653-663</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Ebrahimi:Javid">Javid Ebrahimi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lowd:Daniel">Daniel Lowd</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dou:Dejing">Dejing Dou</a></p>
<p>Abstract:
Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through black-box adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the models robustness significantly.</p>
<p>Keywords:</p>
<h3 id="56. Systematic Study of Long Tail Phenomena in Entity Linking.">56. Systematic Study of Long Tail Phenomena in Entity Linking.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1056/">Paper Link</a>    Pages:664-674</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/i/Ilievski:Filip">Filip Ilievski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vossen:Piek">Piek Vossen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schlobach:Stefan">Stefan Schlobach</a></p>
<p>Abstract:
State-of-the-art entity linkers achieve high accuracy scores with probabilistic methods. However, these scores should be considered in relation to the properties of the datasets they are evaluated on. Until now, there has not been a systematic investigation of the properties of entity linking datasets and their impact on system performance. In this paper we report on a series of hypotheses regarding the long tail phenomena in entity linking datasets, their interaction, and their impact on system performance. Our systematic study of these hypotheses shows that evaluation datasets mainly capture head entities and only incidentally cover data from the tail, thus encouraging systems to overfit to popular/frequent and non-ambiguous cases. We find the most difficult cases of entity linking among the infrequent candidates of ambiguous forms. With our findings, we hope to inspire future designs of both entity linking systems and evaluation datasets. To support this goal, we provide a list of recommended actions for better inclusion of tail cases.</p>
<p>Keywords:</p>
<h3 id="57. Neural Collective Entity Linking.">57. Neural Collective Entity Linking.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1057/">Paper Link</a>    Pages:675-686</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cao_0002:Yixin">Yixin Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hou:Lei">Lei Hou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Juanzi">Juanzi Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a></p>
<p>Abstract:
Entity Linking aims to link entity mentions in texts to knowledge bases, and neural models have achieved recent success in this task. However, most existing methods rely on local contexts to resolve entities independently, which may usually fail due to the data sparsity of local information. To address this issue, we propose a novel neural model for collective entity linking, named as NCEL. NCEL apply Graph Convolutional Network to integrate both local contextual features and global coherence information for entity linking. To improve the computation efficiency, we approximately perform graph convolution on a subgraph of adjacent entity mentions instead of those in the entire text. We further introduce an attention scheme to improve the robustness of NCEL to data noise and train the model on Wikipedia hyperlinks to avoid overfitting and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the linking performance as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.</p>
<p>Keywords:</p>
<h3 id="58. Exploiting Structure in Representation of Named Entities using Active Learning.">58. Exploiting Structure in Representation of Named Entities using Active Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1058/">Paper Link</a>    Pages:687-699</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bhutani:Nikita">Nikita Bhutani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qian_0002:Kun">Kun Qian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0001:Yunyao">Yunyao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jagadish:H=_V=">H. V. Jagadish</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hern=aacute=ndez:Mauricio_A=">Mauricio A. Hernndez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vasa:Mitesh">Mitesh Vasa</a></p>
<p>Abstract:
Fundamental to several knowledge-centric applications is the need to identify named entities from their textual mentions. However, entities lack a unique representation and their mentions can differ greatly. These variations arise in complex ways that cannot be captured using textual similarity metrics. However, entities have underlying structures, typically shared by entities of the same entity type, that can help reason over their name variations. Discovering, learning and manipulating these structures typically requires high manual effort in the form of large amounts of labeled training data and handwritten transformation programs. In this work, we propose an active-learning based framework that drastically reduces the labeled data required to learn the structures of entities. We show that programs for mapping entity mentions to their structures can be automatically generated using human-comprehensible labels. Our experiments show that our framework consistently outperforms both handwritten programs and supervised learning models. We also demonstrate the utility of our framework in relation extraction and entity resolution tasks.</p>
<p>Keywords:</p>
<h3 id="59. A Practical Incremental Learning Framework For Sparse Entity Extraction.">59. A Practical Incremental Learning Framework For Sparse Entity Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1059/">Paper Link</a>    Pages:700-710</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Al=Olimat:Hussein">Hussein Al-Olimat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gustafson:Steven">Steven Gustafson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mackay:Jason">Jason Mackay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thirunarayan:Krishnaprasad">Krishnaprasad Thirunarayan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sheth:Amit_P=">Amit P. Sheth</a></p>
<p>Abstract:
This work addresses challenges arising from extracting entities from textual data, including the high cost of data annotation, model accuracy, selecting appropriate evaluation criteria, and the overall quality of annotation. We present a framework that integrates Entity Set Expansion (ESE) and Active Learning (AL) to reduce the annotation cost of sparse data and provide an online evaluation method as feedback. This incremental and interactive learning framework allows for rapid annotation and subsequent extraction of sparse data while maintaining high accuracy. We evaluate our framework on three publicly available datasets and show that it drastically reduces the cost of sparse entity annotation by an average of 85% and 45% to reach 0.9 and 1.0 F-Scores respectively. Moreover, the method exhibited robust performance across all datasets.</p>
<p>Keywords:</p>
<h3 id="60. An Empirical Study on Fine-Grained Named Entity Recognition.">60. An Empirical Study on Fine-Grained Named Entity Recognition.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1060/">Paper Link</a>    Pages:711-722</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mai:Khai">Khai Mai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pham:Thai=Hoang">Thai-Hoang Pham</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Minh_Trung">Minh Trung Nguyen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duc:Nguyen_Tuan">Nguyen Tuan Duc</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bollegala:Danushka">Danushka Bollegala</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sasano:Ryohei">Ryohei Sasano</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sekine:Satoshi">Satoshi Sekine</a></p>
<p>Abstract:
Named entity recognition (NER) has attracted a substantial amount of research. Recently, several neural network-based models have been proposed and achieved high performance. However, there is little research on fine-grained NER (FG-NER), in which hundreds of named entity categories must be recognized, especially for non-English languages. It is still an open question whether there is a model that is robust across various settings or the proper model varies depending on the language, the number of named entity categories, and the size of training datasets. This paper first presents an empirical comparison of FG-NER models for English and Japanese and demonstrates that LSTM+CNN+CRF (Ma and Hovy, 2016), one of the state-of-the-art methods for English NER, also works well for English FG-NER but does not work well for Japanese, a language that has a large number of character types. To tackle this problem, we propose a method to improve the neural network-based Japanese FG-NER performance by removing the CNN layer and utilizing dictionary and category embeddings. Experiment results show that the proposed method improves Japanese FG-NER F-score from 66.76% to 75.18%.</p>
<p>Keywords:</p>
<h3 id="61. Does Higher Order LSTM Have Better Accuracy for Segmenting and Labeling Sequence Data?">61. Does Higher Order LSTM Have Better Accuracy for Segmenting and Labeling Sequence Data?</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1061/">Paper Link</a>    Pages:723-733</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0050:Yi">Yi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Shuming">Shuming Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yang">Yang Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xuancheng">Xuancheng Ren</a></p>
<p>Abstract:
Existing neural models usually predict the tag of the current token independent of the neighboring tags. The popular LSTM-CRF model considers the tag dependencies between every two consecutive tags. However, it is hard for existing neural models to take longer distance dependencies between tags into consideration. The scalability is mainly limited by the complex model structures and the cost of dynamic programming during training. In our work, we first design a new model called high order LSTM to predict multiple tags for the current token which contains not only the current tag but also the previous several tags. We call the number of tags in one prediction as order. Then we propose a new method called Multi-Order BiLSTM (MO-BiLSTM) which combines low order and high order LSTMs together. MO-BiLSTM keeps the scalability to high order models with a pruning technique. We evaluate MO-BiLSTM on all-phrase chunking and NER datasets. Experiment results show that MO-BiLSTM achieves the state-of-the-art result in chunking and highly competitive results in two NER datasets.</p>
<p>Keywords:</p>
<h3 id="62. Ant Colony System for Multi-Document Summarization.">62. Ant Colony System for Multi-Document Summarization.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1062/">Paper Link</a>    Pages:734-744</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Al=Saleh:Asma_Bader">Asma Bader Al-Saleh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Menai:Mohamed_El_Bachir">Mohamed El Bachir Menai</a></p>
<p>Abstract:
This paper proposes an extractive multi-document summarization approach based on an ant colony system to optimize the information coverage of summary sentences. The implemented system was evaluated on both English and Arabic versions of the corpus of the Text Analysis Conference 2011 MultiLing Pilot by using ROUGE metrics. The evaluation results are promising in comparison to those of the participating systems. Indeed, our system achieved the best scores based on several ROUGE metrics.</p>
<p>Keywords:</p>
<h3 id="63. Multi-task dialog act and sentiment recognition on Mastodon.">63. Multi-task dialog act and sentiment recognition on Mastodon.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1063/">Paper Link</a>    Pages:745-754</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cerisara:Christophe">Christophe Cerisara</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jafaritazehjani:Somayeh">Somayeh Jafaritazehjani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Oluokun:Adedayo">Adedayo Oluokun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Le:Hoa_T=">Hoa T. Le</a></p>
<p>Abstract:
Because of license restrictions, it often becomes impossible to strictly reproduce most research results on Twitter data already a few months after the creation of the corpus. This situation worsened gradually as time passes and tweets become inaccessible. This is a critical issue for reproducible and accountable research on social media. We partly solve this challenge by annotating a new Twitter-like corpus from an alternative large social medium with licenses that are compatible with reproducible experiments: Mastodon. We manually annotate both dialogues and sentiments on this corpus, and train a multi-task hierarchical recurrent network on joint sentiment and dialog act recognition. We experimentally demonstrate that transfer learning may be efficiently achieved between both tasks, and further analyze some specific correlations between sentiments and dialogues on social media. Both the annotated corpus and deep network are released with an open-source license.</p>
<p>Keywords:</p>
<h3 id="64. RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian.">64. RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1064/">Paper Link</a>    Pages:755-763</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rogers:Anna">Anna Rogers</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Romanov:Alexey">Alexey Romanov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rumshisky:Anna">Anna Rumshisky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Volkova:Svitlana">Svitlana Volkova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gronas:Mikhail">Mikhail Gronas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gribov:Alex">Alex Gribov</a></p>
<p>Abstract:
This paper presents RuSentiment, a new dataset for sentiment analysis of social media posts in Russian, and a new set of comprehensive annotation guidelines that are extensible to other languages. RuSentiment is currently the largest in its class for Russian, with 31,185 posts annotated with Fleiss kappa of 0.58 (3 annotations per post). To diversify the dataset, 6,950 posts were pre-selected with an active learning-style strategy. We report baseline classification results, and we also release the best-performing embeddings trained on 3.2B tokens of Russian VKontakte posts.</p>
<p>Keywords:</p>
<h3 id="65. Self-Normalization Properties of Language Modeling.">65. Self-Normalization Properties of Language Modeling.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1065/">Paper Link</a>    Pages:764-773</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Goldberger:Jacob">Jacob Goldberger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Melamud:Oren">Oren Melamud</a></p>
<p>Abstract:
Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function. In the context of language modeling, this property is particularly appealing as it may significantly reduce run-times due to large word vocabularies. In this study, we provide a comprehensive investigation of language modeling self-normalization. First, we theoretically analyze the inherent self-normalization properties of Noise Contrastive Estimation (NCE) language models. Then, we compare them empirically to softmax-based approaches, which are self-normalized using explicit regularization, and suggest a hybrid model with compelling properties. Finally, we uncover a surprising negative correlation between self-normalization and perplexity across the board, as well as some regularity in the observed errors, which may potentially be used for improving self-normalization algorithms in the future.</p>
<p>Keywords:</p>
<h3 id="66. A Position-aware Bidirectional Attention Network for Aspect-level Sentiment Analysis.">66. A Position-aware Bidirectional Attention Network for Aspect-level Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1066/">Paper Link</a>    Pages:774-784</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Shuqin">Shuqin Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Lipeng">Lipeng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hou:Yuexian">Yuexian Hou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yin">Yin Song</a></p>
<p>Abstract:
Aspect-level sentiment analysis aims to distinguish the sentiment polarity of each specific aspect term in a given sentence. Both industry and academia have realized the importance of the relationship between aspect term and sentence, and made attempts to model the relationship by designing a series of attention models. However, most existing methods usually neglect the fact that the position information is also crucial for identifying the sentiment polarity of the aspect term. When an aspect term occurs in a sentence, its neighboring words should be given more attention than other words with long distance. Therefore, we propose a position-aware bidirectional attention network (PBAN) based on bidirectional GRU. PBAN not only concentrates on the position information of aspect terms, but also mutually models the relation between aspect term and sentence by employing bidirectional attention mechanism. The experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our proposed PBAN model.</p>
<p>Keywords:</p>
<h3 id="67. Dynamic Feature Selection with Attention in Incremental Parsing.">67. Dynamic Feature Selection with Attention in Incremental Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1067/">Paper Link</a>    Pages:785-794</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kohita:Ryosuke">Ryosuke Kohita</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Noji:Hiroshi">Hiroshi Noji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Matsumoto_0001:Yuji">Yuji Matsumoto</a></p>
<p>Abstract:
One main challenge for incremental transition-based parsers, when future inputs are invisible, is to extract good features from a limited local context. In this work, we present a simple technique to maximally utilize the local features with an attention mechanism, which works as context- dependent dynamic feature selection. Our model learns, for example, which tokens should a parser focus on, to decide the next action. Our multilingual experiment shows its effectiveness across many languages. We also present an experiment with augmented test dataset and demon- strate it helps to understand the models behavior on locally ambiguous points.</p>
<p>Keywords:</p>
<h3 id="68. Vocabulary Tailored Summary Generation.">68. Vocabulary Tailored Summary Generation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1068/">Paper Link</a>    Pages:795-805</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Krishna:Kundan">Kundan Krishna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Murhekar:Aniket">Aniket Murhekar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sharma:Saumitra">Saumitra Sharma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Srinivasan:Balaji_Vasan">Balaji Vasan Srinivasan</a></p>
<p>Abstract:
Neural sequence-to-sequence models have been successfully extended for summary generation.However, existing frameworks generate a single summary for a given input and do not tune the summaries towards any additional constraints/preferences. Such a tunable framework is desirable to account for linguistic preferences of the specific audience who will consume the summary. In this paper, we propose a neural framework to generate summaries constrained to a vocabulary-defined linguistic preferences of a target audience. The proposed method accounts for the generation context by tuning the summary words at the time of generation. Our evaluations indicate that the proposed approach tunes summaries to the target vocabulary while still maintaining a superior summary quality against a state-of-the-art word embedding based lexical substitution algorithm, suggesting the feasibility of the proposed approach. We demonstrate two applications of the proposed approach - to generate understandable summaries with simpler words, and readable summaries with shorter words.</p>
<p>Keywords:</p>
<h3 id="69. Reading Comprehension with Graph-based Temporal-Casual Reasoning.">69. Reading Comprehension with Graph-based Temporal-Casual Reasoning.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1069/">Paper Link</a>    Pages:806-817</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Yawei">Yawei Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng_0001:Gong">Gong Cheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qu:Yuzhong">Yuzhong Qu</a></p>
<p>Abstract:
Complex questions in reading comprehension tasks require integrating information from multiple sentences. In this work, to answer such questions involving temporal and causal relations, we generate event graphs from text based on dependencies, and rank answers by aligning event graphs. In particular, the alignments are constrained by graph-based reasoning to ensure temporal and causal agreement. Our focused approach self-adaptively complements existing solutions; it is automatically triggered only when applicable. Experiments on RACE and MCTest show that state-of-the-art methods are notably improved by using our approach as an add-on.</p>
<p>Keywords:</p>
<h3 id="70. Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains.">70. Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1070/">Paper Link</a>    Pages:818-830</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Barnes:Jeremy">Jeremy Barnes</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klinger:Roman">Roman Klinger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Walde:Sabine_Schulte_im">Sabine Schulte im Walde</a></p>
<p>Abstract:
Domain adaptation for sentiment analysis is challenging due to the fact that supervised classifiers are very sensitive to changes in domain. The two most prominent approaches to this problem are structural correspondence learning and autoencoders. However, they either require long training times or suffer greatly on highly divergent domains. Inspired by recent advances in cross-lingual sentiment analysis, we provide a novel perspective and cast the domain adaptation problem as an embedding projection task. Our model takes as input two mono-domain embedding spaces and learns to project them to a bi-domain space, which is jointly optimized to (1) project across domains and to (2) predict sentiment. We perform domain adaptation experiments on 20 source-target domain pairs for sentiment classification and report novel state-of-the-art results on 11 domain pairs, including the Amazon domain adaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that our model performs comparably to state-of-the-art approaches on domains that are similar, while performing significantly better on highly divergent domains. Our code is available at <a href="https://github.com/jbarnesspain/domain_blse">https://github.com/jbarnesspain/domain_blse</a></p>
<p>Keywords:</p>
<h3 id="71. Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!">71. Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1071/">Paper Link</a>    Pages:831-844</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Eger:Steffen">Steffen Eger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Daxenberger:Johannes">Johannes Daxenberger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stab:Christian">Christian Stab</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
Argumentation mining (AM) requires the identification of complex discourse structures and has lately been applied with success monolingually. In this work, we show that the existing resources are, however, not adequate for assessing cross-lingual AM, due to their heterogeneity or lack of complexity. We therefore create suitable parallel corpora by (human and machine) translating a popular AM dataset consisting of persuasive student essays into German, French, Spanish, and Chinese. We then compare (i) annotation projection and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at <a href="http://github.com/UKPLab/coling2018-xling_argument_mining">http://github.com/UKPLab/coling2018-xling_argument_mining</a>.</p>
<p>Keywords:</p>
<h3 id="72. HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation.">72. HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1072/">Paper Link</a>    Pages:845-856</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Sixing">Sixing Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Dawei">Dawei Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0015:Ying">Ying Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Xing">Xing Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Zhonghai">Zhonghai Wu</a></p>
<p>Abstract:
Recent years have witnessed a surge of interest on response generation for neural conversation systems. Most existing models are implemented by following the Encoder-Decoder framework and operate sentences of conversations at word-level. The word-level model is suffering from the Unknown Words Issue and the Preference Issue, which seriously impact the quality of generated responses, for example, generated responses may become irrelevant or too general (i.e. safe responses). To address these issues, this paper proposes a hybrid-level Encoder-Decoder model (HL-EncDec), which not only utilizes the word-level features but also character-level features. We conduct several experiments to evaluate HL-EncDec on a Chinese corpus, experimental results show our model significantly outperforms other non-word-level models in automatic metrics and human annotations and is able to generate more informative responses. We also conduct experiments with a small-scale English dataset to show the generalization ability.</p>
<p>Keywords:</p>
<h3 id="73. Multi-Perspective Context Aggregation for Semi-supervised Cloze-style Reading Comprehension.">73. Multi-Perspective Context Aggregation for Semi-supervised Cloze-style Reading Comprehension.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1073/">Paper Link</a>    Pages:857-867</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Liang">Liang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Sujian">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Wei">Wei Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Kewei">Kewei Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Meng">Meng Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jia:Ruoyu">Ruoyu Jia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jingming">Jingming Liu</a></p>
<p>Abstract:
Cloze-style reading comprehension has been a popular task for measuring the progress of natural language understanding in recent years. In this paper, we design a novel multi-perspective framework, which can be seen as the joint training of heterogeneous experts and aggregate context information from different perspectives. Each perspective is modeled by a simple aggregation module. The outputs of multiple aggregation modules are fed into a one-timestep pointer network to get the final answer. At the same time, to tackle the problem of insufficient labeled data, we propose an efficient sampling mechanism to automatically generate more training examples by matching the distribution of candidates between labeled and unlabeled data. We conduct our experiments on a recently released cloze-test dataset CLOTH (Xie et al., 2017), which consists of nearly 100k questions designed by professional teachers. Results show that our method achieves new state-of-the-art performance over previous strong baselines.</p>
<p>Keywords:</p>
<h3 id="74. A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis.">74. A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1074/">Paper Link</a>    Pages:868-877</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zou:Yicheng">Yicheng Zou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gui:Tao">Tao Gui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Qi">Qi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>Abstract:
Attention mechanisms have been leveraged for sentiment classification tasks because not all words have the same importance. However, most existing attention models did not take full advantage of sentiment lexicons, which provide rich sentiment information and play a critical role in sentiment analysis. To achieve the above target, in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods.</p>
<p>Keywords:</p>
<h3 id="75. Open-Domain Event Detection using Distant Supervision.">75. Open-Domain Event Detection using Distant Supervision.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1075/">Paper Link</a>    Pages:878-891</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Araki:Jun">Jun Araki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitamura:Teruko">Teruko Mitamura</a></p>
<p>Abstract:
This paper introduces open-domain event detection, a new event detection paradigm to address issues of prior work on restricted domains and event annotation. The goal is to detect all kinds of events regardless of domains. Given the absence of training data, we propose a distant supervision method that is able to generate high-quality training data. Using a manually annotated event corpus as gold standard, our experiments show that despite no direct supervision, the model outperforms supervised models. This result indicates that the distant supervision enables robust event detection in various domains, while obviating the need for human annotation of events.</p>
<p>Keywords:</p>
<h3 id="76. Semi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing.">76. Semi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1076/">Paper Link</a>    Pages:892-904</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Bo">Bo Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/An:Bo">Bo An</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xianpei">Xianpei Han</a></p>
<p>Abstract:
Semantic parsers critically rely on accurate and high-coverage lexicons. However, traditional semantic parsers usually utilize annotated logical forms to learn the lexicon, which often suffer from the lexicon coverage problem. In this paper, we propose a graph-based semi-supervised learning framework that makes use of large text corpora and lexical resources. This framework first constructs a graph with a phrase similarity model learned by utilizing many text corpora and lexical resources. Next, graph propagation algorithm identifies the label distribution of unlabeled phrases from labeled ones. We evaluate our approach on two benchmarks: Webquestions and Free917. The results show that, in both datasets, our method achieves substantial improvement when comparing to the base system that does not utilize the learned lexicon, and gains competitive results when comparing to state-of-the-art systems.</p>
<p>Keywords:</p>
<h3 id="77. Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embeddings.">77. Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embeddings.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1077/">Paper Link</a>    Pages:905-914</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/ShafieiBavani:Elaheh">Elaheh ShafieiBavani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Ebrahimi:Mohammad">Mohammad Ebrahimi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wong:Raymond_K=">Raymond K. Wong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0001:Fang">Fang Chen</a></p>
<p>Abstract:
We present a new summary evaluation approach that does not require human model summaries. Our approach exploits the compositional capabilities of corpus-based and lexical resource-based word embeddings to develop the features reflecting coverage, diversity, informativeness, and coherence of summaries. The features are then used to train a learning model for predicting the summary content quality in the absence of gold models. We evaluate the proposed metric in replicating the human assigned scores for summarization systems and summaries on data from query-focused and update summarization tasks in TAC 2008 and 2009. The results show that our feature combination provides reliable estimates of summary content quality when model summaries are not available.</p>
<p>Keywords:</p>
<h3 id="78. A review of Spanish corpora annotated with negation.">78. A review of Spanish corpora annotated with negation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1078/">Paper Link</a>    Pages:915-924</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zafra:Salud_Mar=iacute=a_Jim=eacute=nez">Salud Mara Jimnez Zafra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morante:Roser">Roser Morante</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mart=iacute=n=Valdivia:Maite">Maite Martn-Valdivia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/L=oacute=pez:Luis_Alfonso_Ure=ntilde=a">Luis Alfonso Urea Lpez</a></p>
<p>Abstract:
The availability of corpora annotated with negation information is essential to develop negation processing systems in any language. However, there is a lack of these corpora even for languages like English, and when there are corpora available they are small and the annotations are not always compatible across corpora. In this paper we review the existing corpora annotated with negation in Spanish with the purpose of first, gathering the information to make it available for other researchers and, second, analyzing how compatible are the corpora and how has the linguistic phenomenon been addressed. Our final aim is to develop a supervised negation processing system for Spanish, for which we need training and test data. Our analysis shows that it will not be possible to merge the small corpora existing for Spanish due to lack of compatibility in the annotations.</p>
<p>Keywords:</p>
<h3 id="79. Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users, Aspects, and Overall Ratings.">79. Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users, Aspects, and Overall Ratings.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1079/">Paper Link</a>    Pages:925-936</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Junjie">Junjie Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Haitong">Haitong Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>Abstract:
Document-level multi-aspect sentiment classification aims to predict users sentiment polarities for different aspects of a product in a review. Existing approaches mainly focus on text information. However, the authors (i.e. users) and overall ratings of reviews are ignored, both of which are proved to be significant on interpreting the sentiments of different aspects in this paper. Therefore, we propose a model called Hierarchical User Aspect Rating Network (HUARN) to consider user preference and overall ratings jointly. Specifically, HUARN adopts a hierarchical architecture to encode word, sentence, and document level information. Then, user attention and aspect attention are introduced into building sentence and document level representation. The document representation is combined with user and overall rating information to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances.</p>
<p>Keywords:</p>
<h3 id="80. Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora.">80. Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1080/">Paper Link</a>    Pages:937-949</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hazem:Amir">Amir Hazem</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morin:Emmanuel">Emmanuel Morin</a></p>
<p>Abstract:
Recent evaluations on bilingual lexicon extraction from specialized comparable corpora have shown contrasted performance while using word embedding models. This can be partially explained by the lack of large specialized comparable corpora to build efficient representations. Within this context, we try to answer the following questions: First, (i) among the state-of-the-art embedding models, whether trained on specialized corpora or pre-trained on large general data sets, which one is the most appropriate model for bilingual terminology extraction? Second (ii) is it worth it to combine multiple embeddings trained on different data sets? For that purpose, we propose the first systematic evaluation of different word embedding models for bilingual terminology extraction from specialized comparable corpora. We emphasize how the character-based embedding model outperforms other models on the quality of the extracted bilingual lexicons. Further more, we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets. Our approach leads to higher performance than the best individual embedding model.</p>
<p>Keywords:</p>
<h3 id="81. Learning Emotion-enriched Word Representations.">81. Learning Emotion-enriched Word Representations.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1081/">Paper Link</a>    Pages:950-961</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Agrawal:Ameeta">Ameeta Agrawal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/An:Aijun">Aijun An</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Papagelis:Manos">Manos Papagelis</a></p>
<p>Abstract:
Most word representation learning methods are based on the distributional hypothesis in linguistics, according to which words that are used and occur in the same contexts tend to possess similar meanings. As a consequence, emotionally dissimilar words, such as happy and sad occurring in similar contexts would purport more similar meaning than emotionally similar words, such as happy and joy. This complication leads to rather undesirable outcome in predictive tasks that relate to affect (emotional state), such as emotion classification and emotion similarity. In order to address this limitation, we propose a novel method of obtaining emotion-enriched word representations, which projects emotionally similar words into neighboring spaces and emotionally dissimilar ones far apart. The proposed approach leverages distant supervision to automatically obtain a large training dataset of text documents and two recurrent neural network architectures for learning the emotion-enriched representations. Through extensive evaluation on two tasks, including emotion classification and emotion similarity, we demonstrate that the proposed representations outperform several competitive general-purpose and affective word representations.</p>
<p>Keywords:</p>
<h3 id="82. Evaluating the text quality, human likeness and tailoring component of PASS: A Dutch data-to-text system for soccer.">82. Evaluating the text quality, human likeness and tailoring component of PASS: A Dutch data-to-text system for soccer.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1082/">Paper Link</a>    Pages:962-972</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Chris_van_der">Chris van der Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Verduijn:Bart">Bart Verduijn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Krahmer:Emiel">Emiel Krahmer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wubben:Sander">Sander Wubben</a></p>
<p>Abstract:
We present an evaluation of PASS, a data-to-text system that generates Dutch soccer reports from match statistics which are automatically tailored towards fans of one club or the other. The evaluation in this paper consists of two studies. An intrinsic human-based evaluation of the systems output is described in the first study. In this study it was found that compared to human-written texts, computer-generated texts were rated slightly lower on style-related text components (fluency and clarity) and slightly higher in terms of the correctness of given information. Furthermore, results from the first study showed that tailoring was accurately recognized in most cases, and that participants struggled with correctly identifying whether a text was written by a human or computer. The second study investigated if tailoring affects perceived text quality, for which no results were garnered. This lack of results might be due to negative preconceptions about computer-generated texts which were found in the first study.</p>
<p>Keywords:</p>
<h3 id="83. Answerable or Not: Devising a Dataset for Extending Machine Reading Comprehension.">83. Answerable or Not: Devising a Dataset for Extending Machine Reading Comprehension.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1083/">Paper Link</a>    Pages:973-983</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nakanishi:Mao">Mao Nakanishi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kobayashi:Tetsunori">Tetsunori Kobayashi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hayashi:Yoshihiko">Yoshihiko Hayashi</a></p>
<p>Abstract:
Machine-reading comprehension (MRC) has recently attracted attention in the fields of natural language processing and machine learning. One of the problematic presumptions with current MRC technologies is that each question is assumed to be answerable by looking at a given text passage. However, to realize human-like language comprehension ability, a machine should also be able to distinguish not-answerable questions (NAQs) from answerable questions. To develop this functionality, a dataset incorporating hard-to-detect NAQs is vital; however, its manual construction would be expensive. This paper proposes a dataset creation method that alters an existing MRC dataset, the Stanford Question Answering Dataset, and describes the resulting dataset. The value of this dataset is likely to increase if each NAQ in the dataset is properly classified with the difficulty of identifying it as an NAQ. This difficulty level would allow researchers to evaluate a machines NAQ detection performance more precisely. Therefore, we propose a method for automatically assigning difficulty level labels, which measures the similarity between a question and the target text passage. Our NAQ detection experiments demonstrate that the resulting dataset, having difficulty level annotations, is valid and potentially useful in the development of advanced MRC models.</p>
<p>Keywords:</p>
<h3 id="84. Style Obfuscation by Invariance.">84. Style Obfuscation by Invariance.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1084/">Paper Link</a>    Pages:984-996</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Emmery:Chris">Chris Emmery</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Ar=eacute=valo:Enrique_Manjavacas">Enrique Manjavacas Arvalo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chrupala:Grzegorz">Grzegorz Chrupala</a></p>
<p>Abstract:
The task of obfuscating writing style using sequence models has previously been investigated under the framework of obfuscation-by-transfer, where the input text is explicitly rewritten in another style. A side effect of this framework are the frequent major alterations to the semantic content of the input. In this work, we propose obfuscation-by-invariance, and investigate to what extent models trained to be explicitly style-invariant preserve semantics. We evaluate our architectures in parallel and non-parallel settings, and compare automatic and human evaluations on the obfuscated sentences. Our experiments show that the performance of a style classifier can be reduced to chance level, while the output is evaluated to be of equal quality to models applying style-transfer. Additionally, human evaluation indicates a trade-off between the level of obfuscation and the observed quality of the output in terms of meaning preservation and grammaticality.</p>
<p>Keywords:</p>
<h3 id="85. Encoding Sentiment Information into Word Vectors for Sentiment Analysis.">85. Encoding Sentiment Information into Word Vectors for Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1085/">Paper Link</a>    Pages:997-1007</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Ye:Zhe">Zhe Ye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Fang">Fang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldwin:Timothy">Timothy Baldwin</a></p>
<p>Abstract:
General-purpose pre-trained word embeddings have become a mainstay of natural language processing, and more recently, methods have been proposed to encode external knowledge into word embeddings to benefit specific downstream tasks. The goal of this paper is to encode sentiment knowledge into pre-trained word vectors to improve the performance of sentiment analysis. Our proposed method is based on a convolutional neural network (CNN) and an external sentiment lexicon. Experiments on four popular sentiment analysis datasets show that this method improves the accuracy of sentiment analysis compared to a number of benchmark methods.</p>
<p>Keywords:</p>
<h3 id="86. Multi-Task Neural Models for Translating Between Styles Within and Across Languages.">86. Multi-Task Neural Models for Translating Between Styles Within and Across Languages.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1086/">Paper Link</a>    Pages:1008-1021</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Niu:Xing">Xing Niu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rao:Sudha">Sudha Rao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carpuat:Marine">Marine Carpuat</a></p>
<p>Abstract:
Generating natural language requires conveying content in an appropriate style. We explore two related tasks on generating text of varying formality: monolingual formality transfer and formality-sensitive machine translation. We propose to solve these tasks jointly using multi-task learning, and show that our models achieve state-of-the-art performance for formality transfer and are able to perform formality-sensitive translation without being explicitly trained on style-annotated translation examples.</p>
<p>Keywords:</p>
<h3 id="87. Towards a Language for Natural Language Treebank Transductions.">87. Towards a Language for Natural Language Treebank Transductions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1087/">Paper Link</a>    Pages:1022-1032</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Prolo:Carlos_A=">Carlos A. Prolo</a></p>
<p>Abstract:
This paper describes a transduction language suitable for natural language treebank transformations and motivates its application to tasks that have been used and described in the literature. The language, which is the basis for a tree transduction tool allows for clean, precise and concise description of what has been very confusingly, ambiguously, and incompletely textually described in the literature also allowing easy non-hard-coded implementation. We also aim at getting feedback from the NLP community to eventually converge to a de facto standard for such transduction language.</p>
<p>Keywords:</p>
<h3 id="88. Generating Reasonable and Diversified Story Ending Using Sequence to Sequence Model with Adversarial Training.">88. Generating Reasonable and Diversified Story Ending Using Sequence to Sequence Model with Adversarial Training.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1088/">Paper Link</a>    Pages:1033-1043</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zhongyang">Zhongyang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Xiao">Xiao Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>Abstract:
Story generation is a challenging problem in artificial intelligence (AI) and has received a lot of interests in the natural language processing (NLP) community. Most previous work tried to solve this problem using Sequence to Sequence (Seq2Seq) model trained with Maximum Likelihood Estimation (MLE). However, the pure MLE training objective much limits the power of Seq2Seq model in generating high-quality storys. In this paper, we propose using adversarial training augmented Seq2Seq model to generate reasonable and diversified story endings given a story context. Our model includes a generator that defines the policy of generating a story ending, and a discriminator that labels story endings as human-generated or machine-generated. Carefully designed human and automatic evaluation metrics demonstrate that our adversarial training augmented Seq2Seq model can generate more reasonable and diversified story endings compared to purely MLE-trained Seq2Seq model. Moreover, our model achieves better performance on the task of Story Cloze Test with an accuracy of 62.6% compared with state-of-the-art baseline methods.</p>
<p>Keywords:</p>
<h3 id="89. Point Precisely: Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism.">89. Point Precisely: Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1089/">Paper Link</a>    Pages:1044-1055</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Liunian">Liunian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>Abstract:
The task of data-to-text generation aims to generate descriptive texts conditioned on a number of database records, and recent neural models have shown significant progress on this task. The attention based encoder-decoder models with copy mechanism have achieved state-of-the-art results on a few data-to-text datasets. However, such models still face the problem of putting incorrect data records in the generated texts, especially on some more challenging datasets like RotoWire. In this paper, we propose a two-stage approach with a delayed copy mechanism to improve the precision of data records in the generated texts. Our approach first adopts an encoder-decoder model to generate a template text with data slots to be filled and then leverages a proposed delayed copy mechanism to fill in the slots with proper data records. Our delayed copy mechanism can take into account all the information of the input data records and the full generated template text by using double attention, position-aware attention and a pairwise ranking loss. The two models in the two stages are trained separately. Evaluation results on the RotoWire dataset verify the efficacy of our proposed approach to generate better templates and copy data records more precisely.</p>
<p>Keywords:</p>
<h3 id="90. Enhancing General Sentiment Lexicons for Domain-Specific Use.">90. Enhancing General Sentiment Lexicons for Domain-Specific Use.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1090/">Paper Link</a>    Pages:1056-1064</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kreutz:Tim">Tim Kreutz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Daelemans:Walter">Walter Daelemans</a></p>
<p>Abstract:
Lexicon based methods for sentiment analysis rely on high quality polarity lexicons. In recent years, automatic methods for inducing lexicons have increased the viability of lexicon based methods for polarity classification. SentProp is a framework for inducing domain-specific polarities from word embeddings. We elaborate on SentProp by evaluating its use for enhancing DuOMan, a general-purpose lexicon, for use in the political domain. By adding only top sentiment bearing words from the vocabulary and applying small polarity shifts in the general-purpose lexicon, we increase accuracy in an in-domain classification task. The enhanced lexicon performs worse than the original lexicon in an out-domain task, showing that the words we added and the polarity shifts we applied are domain-specific and do not translate well to an out-domain setting.</p>
<p>Keywords:</p>
<h3 id="91. An Operation Network for Abstractive Sentence Compression.">91. An Operation Network for Abstractive Sentence Compression.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1091/">Paper Link</a>    Pages:1065-1076</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Naitong">Naitong Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0002:Jie">Jie Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Minlie">Minlie Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu_0001:Xiaoyan">Xiaoyan Zhu</a></p>
<p>Abstract:
Sentence compression condenses a sentence while preserving its most important contents. Delete-based models have the strong ability to delete undesired words, while generate-based models are able to reorder or rephrase the words, which are more coherent to human sentence compression. In this paper, we propose Operation Network, a neural network approach for abstractive sentence compression, which combines the advantages of both delete-based and generate-based sentence compression models. The central idea of Operation Network is to model the sentence compression process as an editing procedure. First, unnecessary words are deleted from the source sentence, then new words are either generated from a large vocabulary or copied directly from the source sentence. A compressed sentence can be obtained by a series of such edit operations (delete, copy and generate). Experiments show that Operation Network outperforms state-of-the-art baselines.</p>
<p>Keywords:</p>
<h3 id="92. Enhanced Aspect Level Sentiment Classification with Auxiliary Memory.">92. Enhanced Aspect Level Sentiment Classification with Auxiliary Memory.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1092/">Paper Link</a>    Pages:1077-1087</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Peisong">Peisong Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qian:Tieyun">Tieyun Qian</a></p>
<p>Abstract:
In aspect level sentiment classification, there are two common tasks: to identify the sentiment of an aspect (category) or a term. As specific instances of aspects, terms explicitly occur in sentences. It is beneficial for models to focus on nearby context words. In contrast, as high level semantic concepts of terms, aspects usually have more generalizable representations. However, conventional methods cannot utilize the information of aspects and terms at the same time, because few datasets are annotated with both aspects and terms. In this paper, we propose a novel deep memory network with auxiliary memory to address this problem. In our model, a main memory is used to capture the important context words for sentiment classification. In addition, we build an auxiliary memory to implicitly convert aspects and terms to each other, and feed both of them to the main memory. With the interaction between two memories, the features of aspects and terms can be learnt simultaneously. We compare our model with the state-of-the-art methods on four datasets from different domains. The experimental results demonstrate the effectiveness of our model.</p>
<p>Keywords:</p>
<h3 id="93. Author Profiling for Abuse Detection.">93. Author Profiling for Abuse Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1093/">Paper Link</a>    Pages:1088-1098</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mishra:Pushkar">Pushkar Mishra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tredici:Marco_Del">Marco Del Tredici</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yannakoudakis:Helen">Helen Yannakoudakis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shutova:Ekaterina">Ekaterina Shutova</a></p>
<p>Abstract:
The rapid growth of social media in recent years has fed into some highly undesirable phenomena such as proliferation of hateful and offensive language on the Internet. Previous research suggests that such abusive content tends to come from users who share a set of common stereotypes and form communities around them. The current state-of-the-art approaches to abuse detection are oblivious to user and community information and rely entirely on textual (i.e., lexical and semantic) cues. In this paper, we propose a novel approach to this problem that incorporates community-based profiling features of Twitter users. Experimenting with a dataset of 16k tweets, we show that our methods significantly outperform the current state of the art in abuse detection. Further, we conduct a qualitative analysis of model characteristics. We release our code, pre-trained models and all the resources used in the public domain.</p>
<p>Keywords:</p>
<h3 id="94. Automated Scoring: Beyond Natural Language Processing.">94. Automated Scoring: Beyond Natural Language Processing.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1094/">Paper Link</a>    Pages:1099-1109</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Madnani:Nitin">Nitin Madnani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cahill:Aoife">Aoife Cahill</a></p>
<p>Abstract:
In this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. Automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.</p>
<p>Keywords:</p>
<h3 id="95. Aspect and Sentiment Aware Abstractive Review Summarization.">95. Aspect and Sentiment Aware Abstractive Review Summarization.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1095/">Paper Link</a>    Pages:1110-1120</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0007:Min">Min Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qu:Qiang">Qiang Qu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Ying">Ying Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qiao">Qiao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Wei">Wei Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Jia">Jia Zhu</a></p>
<p>Abstract:
Review text has been widely studied in traditional tasks such as sentiment analysis and aspect extraction. However, to date, no work is towards the abstractive review summarization that is essential for business organizations and individual consumers to make informed decisions. This work takes the lead to study the aspect/sentiment-aware abstractive review summarization by exploring multi-factor attentions. Specifically, we propose an interactive attention mechanism to interactively learns the representations of context words, sentiment words and aspect words within the reviews, acted as an encoder. The learned sentiment and aspect representations are incorporated into the decoder to generate aspect/sentiment-aware review summaries via an attention fusion network. In addition, the abstractive summarizer is jointly trained with the text categorization task, which helps learn a category-specific text encoder, locating salient aspect information and exploring the variations of style and wording of content with respect to different text categories. The experimental results on a real-life dataset demonstrate that our model achieves impressive results compared to other strong competitors.</p>
<p>Keywords:</p>
<h3 id="96. Effective Attention Modeling for Aspect-Level Sentiment Classification.">96. Effective Attention Modeling for Aspect-Level Sentiment Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1096/">Paper Link</a>    Pages:1121-1131</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/He:Ruidan">Ruidan He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Wee_Sun">Wee Sun Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ng:Hwee_Tou">Hwee Tou Ng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dahlmeier:Daniel">Daniel Dahlmeier</a></p>
<p>Abstract:
Aspect-level sentiment classification aims to determine the sentiment polarity of a review sentence towards an opinion target. A sentence could contain multiple sentiment-target pairs; thus the main challenge of this task is to separate different opinion contexts for different targets. To this end, attention mechanism has played an important role in previous state-of-the-art neural models. The mechanism is able to capture the importance of each context word towards a target by modeling their semantic associations. We build upon this line of research and propose two novel approaches for improving the effectiveness of attention. First, we propose a method for target representation that better captures the semantic meaning of the opinion target. Second, we introduce an attention model that incorporates syntactic information into the attention mechanism. We experiment on attention-based LSTM (Long Short-Term Memory) models using the datasets from SemEval 2014, 2015, and 2016. The experimental results show that the conventional attention-based LSTM can be substantially improved by incorporating the two approaches.</p>
<p>Keywords:</p>
<h3 id="97. Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis.">97. Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1097/">Paper Link</a>    Pages:1132-1144</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Moore_0001:Andrew">Andrew Moore</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rayson:Paul">Paul Rayson</a></p>
<p>Abstract:
Lack of repeatability and generalisability are two significant threats to continuing scientific development in Natural Language Processing. Language models and learning methods are so complex that scientific conference papers no longer contain enough space for the technical depth required for replication or reproduction. Taking Target Dependent Sentiment Analysis as a case study, we show how recent work in the field has not consistently released code, or described settings for learning methods in enough detail, and lacks comparability and generalisability in train, test or validation data. To investigate generalisability and to enable state of the art comparative evaluations, we carry out the first reproduction studies of three groups of complementary methods and perform the first large-scale mass evaluation on six different English datasets. Reflecting on our experiences, we recommend that future replication or reproduction experiments should always consider a variety of datasets alongside documenting and releasing their methods and published code in order to minimise the barriers to both repeatability and generalisability. We have released our code with a model zoo on GitHub with Jupyter Notebooks to aid understanding and full documentation, and we recommend that others do the same with their papers at submission time through an anonymised GitHub account.</p>
<p>Keywords:</p>
<h3 id="98. Multilevel Heuristics for Rationale-Based Entity Relation Classification in Sentences.">98. Multilevel Heuristics for Rationale-Based Entity Relation Classification in Sentences.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1098/">Paper Link</a>    Pages:1145-1155</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hsu:Shiou_Tian">Shiou Tian Hsu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chaudhary:Mandar_S=">Mandar S. Chaudhary</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Samatova:Nagiza_F=">Nagiza F. Samatova</a></p>
<p>Abstract:
Rationale-based models provide a unique way to provide justifiable results for relation classification models by identifying rationales (key words and phrases that a person can use to justify the relation in the sentence) during the process. However, existing generative networks used to extract rationales come with a trade-off between extracting diversified rationales and achieving good classification results. In this paper, we propose a multilevel heuristic approach to regulate rationale extraction to avoid extracting monotonous rationales without compromising classification performance. In our model, rationale selection is regularized by a semi-supervised process and features from different levels: word, syntax, sentence, and corpus. We evaluate our approach on the SemEval 2010 dataset that includes 19 relation classes and the quality of extracted rationales with our manually-labeled rationales. Experiments show a significant improvement in classification performance and a 20% gain in rationale interpretability compared to state-of-the-art approaches.</p>
<p>Keywords:</p>
<h3 id="99. Adversarial Multi-lingual Neural Relation Extraction.">99. Adversarial Multi-lingual Neural Relation Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1099/">Paper Link</a>    Pages:1156-1166</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Xiaozhi">Xiaozhi Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xu">Xu Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Yankai">Yankai Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a></p>
<p>Abstract:
Multi-lingual relation extraction aims to find unknown relational facts from text in various languages. Existing models cannot well capture the consistency and diversity of relation patterns in different languages. To address these issues, we propose an adversarial multi-lingual neural relation extraction (AMNRE) model, which builds both consistent and individual representations for each sentence to consider the consistency and diversity among languages. Further, we adopt an adversarial training strategy to ensure those consistent sentence representations could effectively extract the language-consistent relation patterns. The experimental results on real-world datasets demonstrate that our AMNRE model significantly outperforms the state-of-the-art models. The source code of this paper can be obtained from <a href="https://github.com/thunlp/AMNRE">https://github.com/thunlp/AMNRE</a>.</p>
<p>Keywords:</p>
<h3 id="100. Neural Relation Classification with Text Descriptions.">100. Neural Relation Classification with Text Descriptions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1100/">Paper Link</a>    Pages:1167-1177</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Feiliang">Feiliang Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Di">Di Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Zhihui">Zhihui Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yongcheng">Yongcheng Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Rongsheng">Rongsheng Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yongkang">Yongkang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Xiaobo">Xiaobo Liang</a></p>
<p>Abstract:
Relation classification is an important task in natural language processing fields. State-of-the-art methods usually concentrate on building deep neural networks based classification models on the training data in which the relations of the labeled entity pairs are given. However, these methods usually suffer from the data sparsity issue greatly. On the other hand, we notice that it is very easily to obtain some concise text descriptions for almost all of the entities in a relation classification task. The text descriptions can provide helpful supplementary information for relation classification. But they are ignored by most of existing methods. In this paper, we propose DesRC, a new neural relation classification method which integrates entities text descriptions into deep neural networks models. We design a two-level attention mechanism to select the most useful information from the intra-sentence aspect and the cross-sentence aspect. Besides, the adversarial training method is also used to further improve the classification per-formance. Finally, we evaluate the proposed method on the SemEval 2010 dataset. Extensive experiments show that our method achieves much better experimental results than other state-of-the-art relation classification methods.</p>
<p>Keywords:</p>
<h3 id="101. Abstract Meaning Representation for Multi-Document Summarization.">101. Abstract Meaning Representation for Multi-Document Summarization.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1101/">Paper Link</a>    Pages:1178-1190</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liao:Kexin">Kexin Liao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lebanoff:Logan">Logan Lebanoff</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0004:Fei">Fei Liu</a></p>
<p>Abstract:
Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.</p>
<p>Keywords:</p>
<h3 id="102. Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion.">102. Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1102/">Paper Link</a>    Pages:1191-1204</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nayeem:Mir_Tafseer">Mir Tafseer Nayeem</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fuad:Tanvir_Ahmed">Tanvir Ahmed Fuad</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chali:Yllias">Yllias Chali</a></p>
<p>Abstract:
In this work, we aim at developing an unsupervised abstractive summarization system in the multi-document setting. We design a paraphrastic sentence fusion model which jointly performs sentence fusion and paraphrasing using skip-gram word embedding model at the sentence level. Our model improves the information coverage and at the same time abstractiveness of the generated sentences. We conduct our experiments on the human-generated multi-sentence compression datasets and evaluate our system on several newly proposed Machine Translation (MT) evaluation metrics. Furthermore, we apply our sentence level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. We also propose an optimal solution for the classical summary length limit problem which was not addressed in the past research. For the document level summary, we conduct experiments on the datasets of two different domains (e.g., news article and user reviews) which are well suited for multi-document abstractive summarization. Our experiments demonstrate that the methods bring significant improvements over the state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="103. Adversarial Domain Adaptation for Variational Neural Language Generation in Dialogue Systems.">103. Adversarial Domain Adaptation for Variational Neural Language Generation in Dialogue Systems.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1103/">Paper Link</a>    Pages:1205-1217</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tran:Van=Khanh">Van-Khanh Tran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Le=Minh">Le-Minh Nguyen</a></p>
<p>Abstract:
Domain Adaptation arises when we aim at learning from source domain a model that can perform acceptably well on a different target domain. It is especially crucial for Natural Language Generation (NLG) in Spoken Dialogue Systems when there are sufficient annotated data in the source domain, but there is a limited labeled data in the target domain. How to effectively utilize as much of existing abilities from source domains is a crucial issue in domain adaptation. In this paper, we propose an adversarial training procedure to train a Variational encoder-decoder based language generator via multiple adaptation steps. In this procedure, a model is first trained on a source domain data and then fine-tuned on a small set of target domain utterances under the guidance of two proposed critics. Experimental results show that the proposed method can effectively leverage the existing knowledge in the source domain to adapt to another related domain by using only a small amount of in-domain data.</p>
<p>Keywords:</p>
<h3 id="104. Ask No More: Deciding when to guess in referential visual dialogue.">104. Ask No More: Deciding when to guess in referential visual dialogue.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1104/">Paper Link</a>    Pages:1218-1233</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shekhar:Ravi">Ravi Shekhar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baumg=auml=rtner:Tim">Tim Baumgrtner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Venkatesh:Aashish">Aashish Venkatesh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bruni:Elia">Elia Bruni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bernardi:Raffaella">Raffaella Bernardi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fern=aacute=ndez:Raquel">Raquel Fernndez</a></p>
<p>Abstract:
Our goal is to explore how the abilities brought in by a dialogue manager can be included in end-to-end visually grounded conversational agents. We make initial steps towards this general goal by augmenting a task-oriented visual dialogue model with a decision-making component that decides whether to ask a follow-up question to identify a target referent in an image, or to stop the conversation to make a guess. Our analyses show that adding a decision making component produces dialogues that are less repetitive and that include fewer unnecessary questions, thus potentially leading to more efficient and less unnatural interactions.</p>
<p>Keywords:</p>
<h3 id="105. Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding.">105. Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1105/">Paper Link</a>    Pages:1234-1245</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hou:Yutai">Yutai Hou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yijia">Yijia Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Che:Wanxiang">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>Abstract:
In this paper, we study the problem of data augmentation for language understanding in task-oriented dialogue system. In contrast to previous work which augments an utterance without considering its relation with other utterances, we propose a sequence-to-sequence generation based data augmentation framework that leverages one utterances same semantic alternatives in the training data. A novel diversity rank is incorporated into the utterance representation to make the model produce diverse utterances and these diversely augmented utterances help to improve the language understanding module. Experimental results on the Airline Travel Information System dataset and a newly created semantic frame annotation on Stanford Multi-turn, Multi-domain Dialogue Dataset show that our framework achieves significant improvements of 6.38 and 10.04 F-scores respectively when only a training set of hundreds utterances is represented. Case studies also confirm that our method generates diverse utterances.</p>
<p>Keywords:</p>
<h3 id="106. Dialogue-act-driven Conversation Model : An Experimental Study.">106. Dialogue-act-driven Conversation Model : An Experimental Study.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1106/">Paper Link</a>    Pages:1246-1256</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kumar:Harshit">Harshit Kumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Agarwal:Arvind">Arvind Agarwal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Joshi:Sachindra">Sachindra Joshi</a></p>
<p>Abstract:
The utility of additional semantic information for the task of next utterance selection in an automated dialogue system is the focus of study in this paper. In particular, we show that additional information available in the form of dialogue acts when used along with context given in the form of dialogue history improves the performance irrespective of the underlying model being generative or discriminative. In order to show the model agnostic behavior of dialogue acts, we experiment with several well-known models such as sequence-to-sequence encoder-decoder model, hierarchical encoder-decoder model, and Siamese-based models with and without hierarchy; and show that in all models, incorporating dialogue acts improves the performance by a significant margin. We, furthermore, propose a novel way of encoding dialogue act information, and use it along with hierarchical encoder to build a model that can use the sequential dialogue act information in a natural way. Our proposed model achieves an MRR of about 84.8% for the task of next utterance selection on a newly introduced Daily Dialogue dataset, and outperform the baseline models. We also provide a detailed analysis of results including key insights that explain the improvement in MRR because of dialog act information.</p>
<p>Keywords:</p>
<h3 id="107. Structured Dialogue Policy with Graph Neural Networks.">107. Structured Dialogue Policy with Graph Neural Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1107/">Paper Link</a>    Pages:1257-1268</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0001:Lu">Lu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Bowen">Bowen Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Long:Sishan">Sishan Long</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Kai">Kai Yu</a></p>
<p>Abstract:
Recently, deep reinforcement learning (DRL) has been used for dialogue policy optimization. However, many DRL-based policies are not sample-efficient. Most recent advances focus on improving DRL optimization algorithms to address this issue. Here, we take an alternative route of designing neural network structure that is better suited for DRL-based dialogue management. The proposed structured deep reinforcement learning is based on graph neural networks (GNN), which consists of some sub-networks, each one for a node on a directed graph. The graph is defined according to the domain ontology and each node can be considered as a sub-agent. During decision making, these sub-agents have internal message exchange between neighbors on the graph. We also propose an approach to jointly optimize the graph structure as well as the parameters of GNN. Experiments show that structured DRL significantly outperforms previous state-of-the-art approaches in almost all of the 18 tasks of the PyDial benchmark.</p>
<p>Keywords:</p>
<h3 id="108. JTAV: Jointly Learning Social Media Content Representation by Fusing Textual, Acoustic, and Visual Features.">108. JTAV: Jointly Learning Social Media Content Representation by Fusing Textual, Acoustic, and Visual Features.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1108/">Paper Link</a>    Pages:1269-1280</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Hongru">Hongru Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Haozheng">Haozheng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0023:Jun">Jun Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/You:Shaodi">Shaodi You</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Zhe">Zhe Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Jin=Mao">Jin-Mao Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Zhenglu">Zhenglu Yang</a></p>
<p>Abstract:
Learning social media content is the basis of many real-world applications, including information retrieval and recommendation systems, among others. In contrast with previous works that focus mainly on single modal or bi-modal learning, we propose to learn social media content by fusing jointly textual, acoustic, and visual information (JTAV). Effective strategies are proposed to extract fine-grained features of each modality, that is, attBiGRU and DCRNN. We also introduce cross-modal fusion and attentive pooling techniques to integrate multi-modal information comprehensively. Extensive experimental evaluation conducted on real-world datasets demonstrate our proposed model outperforms the state-of-the-art approaches by a large margin.</p>
<p>Keywords:</p>
<h3 id="109. MEMD: A Diversity-Promoting Learning Framework for Short-Text Conversation.">109. MEMD: A Diversity-Promoting Learning Framework for Short-Text Conversation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1109/">Paper Link</a>    Pages:1281-1291</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zou:Meng">Meng Zou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xihan">Xihan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Haokun">Haokun Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deng:Zhi=Hong">Zhi-Hong Deng</a></p>
<p>Abstract:
Neural encoder-decoder models have been widely applied to conversational response generation, which is a research hot spot in recent years. However, conventional neural encoder-decoder models tend to generate commonplace responses like I dont know regardless of what the input is. In this paper, we analyze this problem from a new perspective: latent vectors. Based on it, we propose an easy-to-extend learning framework named MEMD (Multi-Encoder to Multi-Decoder), in which an auxiliary encoder and an auxiliary decoder are introduced to provide necessary training guidance without resorting to extra data or complicating networks inner structure. Experimental results demonstrate that our method effectively improve the quality of generated responses according to automatic metrics and human evaluations, yielding more diverse and smooth replies.</p>
<p>Keywords:</p>
<h3 id="110. Refining Source Representations with Relation Networks for Neural Machine Translation.">110. Refining Source Representations with Relation Networks for Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1110/">Paper Link</a>    Pages:1292-1303</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Wen">Wen Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Jiawei">Jiawei Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng_0004:Yang">Yang Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qun">Qun Liu</a></p>
<p>Abstract:
Although neural machine translation with the encoder-decoder framework has achieved great success recently, it still suffers drawbacks of forgetting distant information, which is an inherent disadvantage of recurrent neural network structure, and disregarding relationship between source words during encoding step. Whereas in practice, the former information and relationship are often useful in current step. We target on solving these problems and thus introduce relation networks to learn better representations of the source. The relation networks are able to facilitate memorization capability of recurrent neural network via associating source words with each other, this would also help retain their relationships. Then the source representations and all the relations are fed into the attention component together while decoding, with the main encoder-decoder framework unchanged. Experiments on several datasets show that our method can improve the translation performance significantly over the conventional encoder-decoder model and even outperform the approach involving supervised syntactic knowledge.</p>
<p>Keywords:</p>
<h3 id="111. A Survey of Domain Adaptation for Neural Machine Translation.">111. A Survey of Domain Adaptation for Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1111/">Paper Link</a>    Pages:1304-1319</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chu:Chenhui">Chenhui Chu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Rui">Rui Wang</a></p>
<p>Abstract:
Neural machine translation (NMT) is a deep learning based approach for machine translation, which yields the state-of-the-art translation performance in scenarios where large-scale parallel corpora are available. Although the high-quality and domain-specific translation is crucial in the real world, domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly in such scenarios. Domain adaptation that leverages both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domain-specific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT.</p>
<p>Keywords:</p>
<h3 id="112. An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization.">112. An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1112/">Paper Link</a>    Pages:1320-1331</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Gongbo">Gongbo Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cap:Fabienne">Fabienne Cap</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pettersson:Eva">Eva Pettersson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nivre:Joakim">Joakim Nivre</a></p>
<p>Abstract:
In this paper, we apply different NMT models to the problem of historical spelling normalization for five languages: English, German, Hungarian, Icelandic, and Swedish. The NMT models are at different levels, have different attention mechanisms, and different neural network architectures. Our results show that NMT models are much better than SMT models in terms of character error rate. The vanilla RNNs are competitive to GRUs/LSTMs in historical spelling normalization. Transformer models perform better only when provided with more training data. We also find that subword-level models with a small subword vocabulary are better than character-level models. In addition, we propose a hybrid method which further improves the performance of historical spelling normalization.</p>
<p>Keywords:</p>
<h3 id="113. Fine-Grained Arabic Dialect Identification.">113. Fine-Grained Arabic Dialect Identification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1113/">Paper Link</a>    Pages:1332-1344</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Salameh:Mohammad">Mohammad Salameh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bouamor:Houda">Houda Bouamor</a></p>
<p>Abstract:
Previous work on the problem of Arabic Dialect Identification typically targeted coarse-grained five dialect classes plus Standard Arabic (6-way classification). This paper presents the first results on a fine-grained dialect classification task covering 25 specific cities from across the Arab World, in addition to Standard Arabic  a very challenging task. We build several classification systems and explore a large space of features. Our results show that we can identify the exact city of a speaker at an accuracy of 67.9% for sentences with an average length of 7 words (a 9% relative error reduction over the state-of-the-art technique for Arabic dialect identification) and reach more than 90% when we consider 16 words. We also report on additional insights from a data analysis of similarity and difference across Arabic dialects.</p>
<p>Keywords:</p>
<h3 id="114. Who Feels What and Why? Annotation of a Literature Corpus with Semantic Roles of Emotions.">114. Who Feels What and Why? Annotation of a Literature Corpus with Semantic Roles of Emotions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1114/">Paper Link</a>    Pages:1345-1359</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Evgeny">Evgeny Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klinger:Roman">Roman Klinger</a></p>
<p>Abstract:
Most approaches to emotion analysis in fictional texts focus on detecting the emotion expressed in text. We argue that this is a simplification which leads to an overgeneralized interpretation of the results, as it does not take into account who experiences an emotion and why. Emotions play a crucial role in the interaction between characters and the events they are involved in. Until today, no specific corpora that capture such an interaction were available for literature. We aim at filling this gap and present a publicly available corpus based on Project Gutenberg, REMAN (Relational EMotion ANnotation), manually annotated for spans which correspond to emotion trigger phrases and entities/events in the roles of experiencers, targets, and causes of the emotion. We provide baseline results for the automatic prediction of these relational structures and show that emotion lexicons are not able to encompass the high variability of emotion expressions and demonstrate that statistical models benefit from joint modeling of emotions with its roles in all subtasks. The corpus that we provide enables future research on the recognition of emotions and associated entities in text. It supports qualitative literary studies and digital humanities. The corpus is available at <a href="http://www.ims.uni-stuttgart.de/data/reman">http://www.ims.uni-stuttgart.de/data/reman</a> .</p>
<p>Keywords:</p>
<h3 id="115. Local String Transduction as Sequence Labeling.">115. Local String Transduction as Sequence Labeling.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1115/">Paper Link</a>    Pages:1360-1371</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ribeiro:Joana">Joana Ribeiro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Narayan:Shashi">Shashi Narayan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carreras:Xavier">Xavier Carreras</a></p>
<p>Abstract:
We show that the general problem of string transduction can be reduced to the problem of sequence labeling. While character deletion and insertions are allowed in string transduction, they do not exist in sequence labeling. We show how to overcome this difference. Our approach can be used with any sequence labeling algorithm and it works best for problems in which string transduction imposes a strong notion of locality (no long range dependencies). We experiment with spelling correction for social media, OCR correction, and morphological inflection, and we see that it behaves better than seq2seq models and yields state-of-the-art results in several cases.</p>
<p>Keywords:</p>
<h3 id="116. Deep Neural Networks at the Service of Multilingual Parallel Sentence Extraction.">116. Deep Neural Networks at the Service of Multilingual Parallel Sentence Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1116/">Paper Link</a>    Pages:1372-1383</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Aghaebrahimian:Ahmad">Ahmad Aghaebrahimian</a></p>
<p>Abstract:
Wikipedia provides an invaluable source of parallel multilingual data, which are in high demand for various sorts of linguistic inquiry, including both theoretical and practical studies. We introduce a novel end-to-end neural model for large-scale parallel data harvesting from Wikipedia. Our model is language-independent, robust, and highly scalable. We use our system for collecting parallel German-English, French-English and Persian-English sentences. Human evaluations at the end show the strong performance of this model in collecting high-quality parallel data. We also propose a statistical framework which extends the results of our human evaluation to other language pairs. Our model also obtained a state-of-the-art result on the German-English dataset of BUCC 2017 shared task on parallel sentence extraction from comparable corpora.</p>
<p>Keywords:</p>
<h3 id="117. Diachronic word embeddings and semantic shifts: a survey.">117. Diachronic word embeddings and semantic shifts: a survey.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1117/">Paper Link</a>    Pages:1384-1397</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kutuzov:Andrey">Andrey Kutuzov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/=/=Oslash=vrelid:Lilja">Lilja vrelid</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Szymanski:Terrence">Terrence Szymanski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Velldal:Erik">Erik Velldal</a></p>
<p>Abstract:
Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.</p>
<p>Keywords:</p>
<h3 id="118. Interaction-Aware Topic Model for Microblog Conversations through Network Embedding and User Attention.">118. Interaction-Aware Topic Model for Microblog Conversations through Network Embedding and User Attention.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1118/">Paper Link</a>    Pages:1398-1409</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/He:Ruifang">Ruifang He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xuefei">Xuefei Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Di">Di Jin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Longbiao">Longbiao Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dang:Jianwu">Jianwu Dang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiangang">Xiangang Li</a></p>
<p>Abstract:
Traditional topic models are insufficient for topic extraction in social media. The existing methods only consider text information or simultaneously model the posts and the static characteristics of social media. They ignore that one discusses diverse topics when dynamically interacting with different people. Moreover, people who talk about the same topic have different effects on the topic. In this paper, we propose an Interaction-Aware Topic Model (IATM) for microblog conversations by integrating network embedding and user attention. A conversation network linking users based on reposting and replying relationship is constructed to mine the dynamic user behaviours. We model dynamic interactions and user attention so as to learn interaction-aware edge embeddings with social context. Then they are incorporated into neural variational inference for generating the more consistent topics. The experiments on three real-world datasets show that our proposed model is effective.</p>
<p>Keywords:</p>
<h3 id="119. Cross-media User Profiling with Joint Textual and Social User Embedding.">119. Cross-media User Profiling with Joint Textual and Social User Embedding.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1119/">Paper Link</a>    Pages:1410-1420</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Jingjing">Jingjing Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Shoushan">Shoushan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:MingQi">MingQi Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Hanqian">Hanqian Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>Abstract:
In realistic scenarios, a user profiling model (e.g., gender classification or age regression) learned from one social media might perform rather poorly when tested on another social media due to the different data distributions in the two media. In this paper, we address cross-media user profiling by bridging the knowledge between the source and target media with a uniform user embedding learning approach. In our approach, we first construct a cross-media user-word network to capture the relationship among users through the textual information and a modified cross-media user-user network to capture the relationship among users through the social information. Then, we learn user embedding by jointly learning the heterogeneous network composed of above two networks. Finally, we train a classification (or regression) model with the obtained user embeddings as input to perform user profiling. Empirical studies demonstrate the effectiveness of the proposed approach to two cross-media user profiling tasks, i.e., cross-media gender classification and cross-media age regression.</p>
<p>Keywords:</p>
<h3 id="120. Incorporating Syntactic Uncertainty in Neural Machine Translation with a Forest-to-Sequence Model.">120. Incorporating Syntactic Uncertainty in Neural Machine Translation with a Forest-to-Sequence Model.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1120/">Paper Link</a>    Pages:1421-1429</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zaremoodi:Poorya">Poorya Zaremoodi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haffari:Gholamreza">Gholamreza Haffari</a></p>
<p>Abstract:
Incorporating syntactic information in Neural Machine Translation (NMT) can lead to better reorderings, particularly useful when the language pairs are syntactically highly divergent or when the training bitext is not large. Previous work on using syntactic information, provided by top-1 parse trees generated by (inevitably error-prone) parsers, has been promising. In this paper, we propose a forest-to-sequence NMT model to make use of exponentially many parse trees of the source sentence to compensate for the parser errors. Our method represents the collection of parse trees as a packed forest, and learns a neural transducer to translate from the input forest to the target sentence. Experiments on English to German, Chinese and Farsi translation tasks show the superiority of our approach over the sequence-to-sequence and tree-to-sequence neural translation models.</p>
<p>Keywords:</p>
<h3 id="121. Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization.">121. Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1121/">Paper Link</a>    Pages:1430-1441</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Haoran">Haoran Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Junnan">Junnan Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>Abstract:
In this paper, we investigate the sentence summarization task that produces a summary from a source sentence. Neural sequence-to-sequence models have gained considerable success for this task, while most existing approaches only focus on improving the informativeness of the summary, which ignore the correctness, i.e., the summary should not contain unrelated information with respect to the source sentence. We argue that correctness is an essential requirement for summarization systems. Considering a correct summary is semantically entailed by the source sentence, we incorporate entailment knowledge into abstractive summarization models. We propose an entailment-aware encoder under multi-task framework (i.e., summarization generation and entailment recognition) and an entailment-aware decoder by entailment Reward Augmented Maximum Likelihood (RAML) training. Experiment results demonstrate that our models significantly outperform baselines from the aspects of informativeness and correctness.</p>
<p>Keywords:</p>
<h3 id="122. Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation.">122. Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1122/">Paper Link</a>    Pages:1442-1453</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gr=eacute=goire:Francis">Francis Grgoire</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Langlais:Philippe">Philippe Langlais</a></p>
<p>Abstract:
Parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. We propose a bidirectional recurrent neural network based approach to extract parallel sentences from collections of multilingual texts. Our experiments with noisy parallel corpora show that we can achieve promising results against a competitive baseline by removing the need of specific feature engineering or additional external resources. To justify the utility of our approach, we extract sentence pairs from Wikipedia articles to train machine translation systems and show significant improvements in translation performance.</p>
<p>Keywords:</p>
<h3 id="123. Fast and Accurate Reordering with ITG Transition RNN.">123. Fast and Accurate Reordering with ITG Transition RNN.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1123/">Paper Link</a>    Pages:1454-1463</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Hao">Hao Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ng:Axel_H=">Axel H. Ng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sproat:Richard">Richard Sproat</a></p>
<p>Abstract:
Attention-based sequence-to-sequence neural network models learn to jointly align and translate. The quadratic-time attention mechanism is powerful as it is capable of handling arbitrary long-distance reordering, but computationally expensive. In this paper, towards making neural translation both accurate and efficient, we follow the traditional pre-reordering approach to decouple reordering from translation. We add a reordering RNN that shares the input encoder with the decoder. The RNNs are trained jointly with a multi-task loss function and applied sequentially at inference time. The task of the reordering model is to predict the permutation of the input words following the target language word order. After reordering, the attention in the decoder becomes more peaked and monotonic. For reordering, we adopt the Inversion Transduction Grammars (ITG) and propose a transition system to parse input to trees for reordering. We harness the ITG transition system with RNN. With the modeling power of RNN, we achieve superior reordering accuracy without any feature engineering. In experiments, we apply the model to the task of text normalization. Compared to a strong baseline of attention-based RNN, our ITG RNN re-ordering model can reach the same reordering accuracy with only 1/10 of the training data and is 2.5x faster in decoding.</p>
<p>Keywords:</p>
<h3 id="124. Neural Machine Translation with Decoding History Enhanced Attention.">124. Neural Machine Translation with Decoding History Enhanced Attention.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1124/">Paper Link</a>    Pages:1464-1473</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Mingxuan">Mingxuan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Jun">Jun Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Zhixing">Zhixing Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Jinsong">Jinsong Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bian:Chao">Chao Bian</a></p>
<p>Abstract:
Neural machine translation with source-side attention have achieved remarkable performance. however, there has been little work exploring to attend to the target-side which can potentially enhance the memory capbility of NMT. We reformulate a Decoding History Enhanced Attention mechanism (DHEA) to render NMT model better at selecting both source-side and target-side information. DHA enables dynamic control of the ratios at which source and target contexts contribute to the generation of target words, offering a way to weakly induce structure relations among both source and target tokens. It also allows training errors to be directly back-propagated through short-cut connections and effectively alleviates the gradient vanishing problem. The empirical study on Chinese-English translation shows that our model with proper configuration can improve by 0:9 BLEU upon Transformer and the best reported results in the dataset. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.</p>
<p>Keywords:</p>
<h3 id="125. Transfer Learning for a Letter-Ngrams to Word Decoder in the Context of Historical Handwriting Recognition with Scarce Resources.">125. Transfer Learning for a Letter-Ngrams to Word Decoder in the Context of Historical Handwriting Recognition with Scarce Resources.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1125/">Paper Link</a>    Pages:1474-1484</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Granet:Adeline">Adeline Granet</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morin:Emmanuel">Emmanuel Morin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mouch=egrave=re:Harold">Harold Mouchre</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Quiniou:Solen">Solen Quiniou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Viard=Gaudin:Christian">Christian Viard-Gaudin</a></p>
<p>Abstract:
Lack of data can be an issue when beginning a new study on historical handwritten documents. In order to deal with this, we present the character-based decoder part of a multilingual approach based on transductive transfer learning for a historical handwriting recognition task on Italian Comedy Registers. The decoder must build a sequence of characters that corresponds to a word from a vector of letter-ngrams. As learning data, we created a new dataset from untapped resources that covers the same domain and period of our Italian Comedy data, as well as resources from common domains, periods, or languages. We obtain a 97.42% Character Recognition Rate and a 86.57% Word Recognition Rate on our Italian Comedy data, despite a lexical coverage of 67% between the Italian Comedy data and the training data. These results show that an efficient system can be obtained by a carefully selecting the datasets used for the transfer learning.</p>
<p>Keywords:</p>
<h3 id="126. SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions.">126. SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1126/">Paper Link</a>    Pages:1485-1497</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cohan:Arman">Arman Cohan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Desmet:Bart">Bart Desmet</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yates:Andrew">Andrew Yates</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Soldaini:Luca">Luca Soldaini</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/MacAvaney:Sean">Sean MacAvaney</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goharian:Nazli">Nazli Goharian</a></p>
<p>Abstract:
Mental health is a significant and growing public health concern. As language usage can be leveraged to obtain crucial insights into mental health conditions, there is a need for large-scale, labeled, mental health-related datasets of users who have been diagnosed with one or more of such conditions. In this paper, we investigate the creation of high-precision patterns to identify self-reported diagnoses of nine different mental health conditions, and obtain high-quality labeled data without the need for manual labelling. We introduce the SMHD (Self-reported Mental Health Diagnoses) dataset and make it available. SMHD is a novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users. We examine distinctions in users language, as measured by linguistic and psychological variables. We further explore text classification methods to identify individuals with mental conditions through their language.</p>
<p>Keywords:</p>
<h3 id="127. Crowdsourcing a Large Corpus of Clickbait on Twitter.">127. Crowdsourcing a Large Corpus of Clickbait on Twitter.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1127/">Paper Link</a>    Pages:1498-1507</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Potthast:Martin">Martin Potthast</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gollub:Tim">Tim Gollub</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Komlossy:Kristof">Kristof Komlossy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schuster:Sebastian">Sebastian Schuster</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wiegmann:Matti">Matti Wiegmann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fernandez:Erika_Patricia_Garces">Erika Patricia Garces Fernandez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hagen:Matthias">Matthias Hagen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stein_0001:Benno">Benno Stein</a></p>
<p>Abstract:
Clickbait has become a nuisance on social media. To address the urging task of clickbait detection, we constructed a new corpus of 38,517 annotated Twitter tweets, the Webis Clickbait Corpus 2017. To avoid biases in terms of publisher and topic, tweets were sampled from the top 27 most retweeted news publishers, covering a period of 150 days. Each tweet has been annotated on 4-point scale by five annotators recruited at Amazons Mechanical Turk. The corpus has been employed to evaluate 12 clickbait detectors submitted to the Clickbait Challenge 2017. Download: <a href="https://webis.de/data/webis-clickbait-17.html">https://webis.de/data/webis-clickbait-17.html</a> Challenge: <a href="https://clickbait-challenge.org">https://clickbait-challenge.org</a></p>
<p>Keywords:</p>
<h3 id="128. Cross-lingual Knowledge Projection Using Machine Translation and Target-side Knowledge Base Completion.">128. Cross-lingual Knowledge Projection Using Machine Translation and Target-side Knowledge Base Completion.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1128/">Paper Link</a>    Pages:1508-1520</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/o/Otani:Naoki">Naoki Otani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kiyomaru:Hirokazu">Hirokazu Kiyomaru</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kawahara:Daisuke">Daisuke Kawahara</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kurohashi:Sadao">Sadao Kurohashi</a></p>
<p>Abstract:
Considerable effort has been devoted to building commonsense knowledge bases. However, they are not available in many languages because the construction of KBs is expensive. To bridge the gap between languages, this paper addresses the problem of projecting the knowledge in English, a resource-rich language, into other languages, where the main challenge lies in projection ambiguity. This ambiguity is partially solved by machine translation and target-side knowledge base completion, but neither of them is adequately reliable by itself. We show their combination can project English commonsense knowledge into Japanese and Chinese with high precision. Our method also achieves a top-10 accuracy of 90% on the crowdsourced EnglishJapanese benchmark. Furthermore, we use our method to obtain 18,747 facts of accurate Japanese commonsense within a very short period.</p>
<p>Keywords:</p>
<h3 id="129. Assessing Quality Estimation Models for Sentence-Level Prediction.">129. Assessing Quality Estimation Models for Sentence-Level Prediction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1129/">Paper Link</a>    Pages:1521-1533</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cuong:Hoang">Hoang Cuong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jia">Jia Xu</a></p>
<p>Abstract:
This paper provides an evaluation of a wide range of advanced sentence-level Quality Estimation models, including Support Vector Regression, Ride Regression, Neural Networks, Gaussian Processes, Bayesian Neural Networks, Deep Kernel Learning and Deep Gaussian Processes. Beside the accurateness, our main concerns are also the robustness of Quality Estimation models. Our work raises the difficulty in building strong models. Specifically, we show that Quality Estimation models often behave differently in Quality Estimation feature space, depending on whether the scale of feature space is small, medium or large. We also show that Quality Estimation models often behave differently in evaluation settings, depending on whether test data come from the same domain as the training data or not. Our work suggests several strong candidates to use in different circumstances.</p>
<p>Keywords:</p>
<h3 id="130. User-Level Race and Ethnicity Predictors from Twitter Text.">130. User-Level Race and Ethnicity Predictors from Twitter Text.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1130/">Paper Link</a>    Pages:1534-1545</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Preotiuc=Pietro:Daniel">Daniel Preotiuc-Pietro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Ungar:Lyle_H=">Lyle H. Ungar</a></p>
<p>Abstract:
User demographic inference from social media text has the potential to improve a range of downstream applications, including real-time passive polling or quantifying demographic bias. This study focuses on developing models for user-level race and ethnicity prediction. We introduce a data set of users who self-report their race/ethnicity through a survey, in contrast to previous approaches that use distantly supervised data or perceived labels. We develop predictive models from text which accurately predict the membership of a user to the four largest racial and ethnic groups with up to .884 AUC and make these available to the research community.</p>
<p>Keywords:</p>
<h3 id="131. Multi-Source Multi-Class Fake News Detection.">131. Multi-Source Multi-Class Fake News Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1131/">Paper Link</a>    Pages:1546-1557</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Karimi:Hamid">Hamid Karimi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roy:Proteek">Proteek Roy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Saba=Sadiya:Sari">Sari Saba-Sadiya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Jiliang">Jiliang Tang</a></p>
<p>Abstract:
Fake news spreading through media outlets poses a real threat to the trustworthiness of information and detecting fake news has attracted increasing attention in recent years. Fake news is typically written intentionally to mislead readers, which determines that fake news detection merely based on news content is tremendously challenging. Meanwhile, fake news could contain true evidence to mock true news and presents different degrees of fakeness, which further exacerbates the detection difficulty. On the other hand, the spread of fake news produces various types of data from different perspectives. These multiple sources provide rich contextual information about fake news and offer unprecedented opportunities for advanced fake news detection. In this paper, we study fake news detection with different degrees of fakeness by integrating multiple sources. In particular, we introduce approaches to combine information from multiple sources and to discriminate between different degrees of fakeness, and propose a Multi-source Multi-class Fake news Detection framework MMFD, which combines automated feature extraction, multi-source fusion and automated degrees of fakeness detection into a coherent and interpretable model. Experimental results on the real-world data demonstrate the effectiveness of the proposed framework and extensive experiments are further conducted to understand the working of the proposed framework.</p>
<p>Keywords:</p>
<h3 id="132. Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal Language Detection.">132. Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal Language Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1132/">Paper Link</a>    Pages:1558-1569</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dinh:Erik=L=acirc=n_Do">Erik-Ln Do Dinh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Eger:Steffen">Steffen Eger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
Non-literal language phenomena such as idioms or metaphors are commonly studied in isolation from each other in NLP. However, often similar definitions and features are being used for different phenomena, challenging the distinction. Instead, we propose to view the detection problem as a generalized non-literal language classification problem. In this paper we investigate multi-task learning for related non-literal language phenomena. We show that in contrast to simply joining the data of multiple tasks, multi-task learning consistently improves upon four metaphor and idiom detection tasks in two languages, English and German. Comparing two state-of-the-art multi-task learning architectures, we also investigate when soft parameter sharing and learned information flow can be beneficial for our related tasks. We make our adapted code publicly available.</p>
<p>Keywords:</p>
<h3 id="133. Twitter corpus of Resource-Scarce Languages for Sentiment Analysis and Multilingual Emoji Prediction.">133. Twitter corpus of Resource-Scarce Languages for Sentiment Analysis and Multilingual Emoji Prediction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1133/">Paper Link</a>    Pages:1570-1577</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Choudhary:Nurendra">Nurendra Choudhary</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh:Rajat">Rajat Singh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rao:Vijjini_Anvesh">Vijjini Anvesh Rao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shrivastava_0001:Manish">Manish Shrivastava</a></p>
<p>Abstract:
In this paper, we leverage social media platforms such as twitter for developing corpus across multiple languages. The corpus creation methodology is applicable for resource-scarce languages provided the speakers of that particular language are active users on social media platforms. We present an approach to extract social media microblogs such as tweets (Twitter). In this paper, we create corpus for multilingual sentiment analysis and emoji prediction in Hindi, Bengali and Telugu. Further, we perform and analyze multiple NLP tasks utilizing the corpus to get interesting observations.</p>
<p>Keywords:</p>
<h3 id="134. Towards identifying the optimal datasize for lexically-based Bayesian inference of linguistic phylogenies.">134. Towards identifying the optimal datasize for lexically-based Bayesian inference of linguistic phylogenies.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1134/">Paper Link</a>    Pages:1578-1590</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rama:Taraka">Taraka Rama</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wichmann:S=oslash=ren">Sren Wichmann</a></p>
<p>Abstract:
Bayesian linguistic phylogenies are standardly based on cognate matrices for words referring to a fix set of meaningstypically around 100-200. To this day there has not been any empirical investigation into which datasize is optimal. Here we determine, across a set of language families, the optimal number of meanings required for the best performance in Bayesian phylogenetic inference. We rank meanings by stability, infer phylogenetic trees using first the most stable meaning, then the two most stable meanings, and so on, computing the quartet distance of the resulting tree to the tree proposed by language family experts at each step of datasize increase. When a gold standard tree is not available we propose to instead compute the quartet distance between the tree based on the n-most stable meaning and the one based on the n + 1-most stable meanings, increasing n from 1 to N  1, where N is the total number of meanings. The assumption here is that the value of n for which the quartet distance begins to stabilize is also the value at which the quality of the tree ceases to improve. We show that this assumption is borne out. The results of the two methods vary across families, and the optimal number of meanings appears to correlate with the number of languages under consideration.</p>
<p>Keywords:</p>
<h3 id="135. The Road to Success: Assessing the Fate of Linguistic Innovations in Online Communities.">135. The Road to Success: Assessing the Fate of Linguistic Innovations in Online Communities.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1135/">Paper Link</a>    Pages:1591-1603</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tredici:Marco_Del">Marco Del Tredici</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fern=aacute=ndez:Raquel">Raquel Fernndez</a></p>
<p>Abstract:
We investigate the birth and diffusion of lexical innovations in a large dataset of online social communities. We build on sociolinguistic theories and focus on the relation between the spread of a novel term and the social role of the individuals who use it, uncovering characteristics of innovators and adopters. Finally, we perform a prediction task that allows us to anticipate whether an innovation will successfully spread within a community.</p>
<p>Keywords:</p>
<h3 id="136. Ab Initio: Automatic Latin Proto-word Reconstruction.">136. Ab Initio: Automatic Latin Proto-word Reconstruction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1136/">Paper Link</a>    Pages:1604-1614</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Ciobanu:Alina_Maria">Alina Maria Ciobanu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dinu:Liviu_P=">Liviu P. Dinu</a></p>
<p>Abstract:
Proto-word reconstruction is central to the study of language evolution. It consists of recreating the words in an ancient language from its modern daughter languages. In this paper we investigate automatic word form reconstruction for Latin proto-words. Having modern word forms in multiple Romance languages (French, Italian, Spanish, Portuguese and Romanian), we infer the form of their common Latin ancestors. Our approach relies on the regularities that occurred when the Latin words entered the modern languages. We leverage information from all modern languages, building an ensemble system for proto-word reconstruction. We use conditional random fields for sequence labeling, but we conduct preliminary experiments with recurrent neural networks as well. We apply our method on multiple datasets, showing that our method improves on previous results, having also the advantage of requiring less input data, which is essential in historical linguistics, where resources are generally scarce.</p>
<p>Keywords:</p>
<h3 id="137. A Computational Model for the Linguistic Notion of Morphological Paradigm.">137. A Computational Model for the Linguistic Notion of Morphological Paradigm.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1137/">Paper Link</a>    Pages:1615-1626</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Silfverberg:Miikka">Miikka Silfverberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Ling">Ling Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hulden:Mans">Mans Hulden</a></p>
<p>Abstract:
In supervised learning of morphological patterns, the strategy of generalizing inflectional tables into more abstract paradigms through alignment of the longest common subsequence found in an inflection table has been proposed as an efficient method to deduce the inflectional behavior of unseen word forms. In this paper, we extend this notion of morphological paradigm from earlier work and provide a formalization that more accurately matches linguist intuitions about what an inflectional paradigm is. Additionally, we propose and evaluate a mechanism for learning full human-readable paradigm specifications from incomplete dataa scenario when we only have access to a few inflected forms for each lexeme, and want to reconstruct the missing inflections as well as generalize and group the witnessed patterns into a model of more abstract paradigmatic behavior of lexemes.</p>
<p>Keywords:</p>
<h3 id="138. Relation Induction in Word Embeddings Revisited.">138. Relation Induction in Word Embeddings Revisited.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1138/">Paper Link</a>    Pages:1627-1637</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bouraoui:Zied">Zied Bouraoui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jameel:Shoaib">Shoaib Jameel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schockaert:Steven">Steven Schockaert</a></p>
<p>Abstract:
Given a set of instances of some relation, the relation induction task is to predict which other word pairs are likely to be related in the same way. While it is natural to use word embeddings for this task, standard approaches based on vector translations turn out to perform poorly. To address this issue, we propose two probabilistic relation induction models. The first model is based on translations, but uses Gaussians to explicitly model the variability of these translations and to encode soft constraints on the source and target words that may be chosen. In the second model, we use Bayesian linear regression to encode the assumption that there is a linear relationship between the vector representations of related words, which is considerably weaker than the assumption underlying translation based models.</p>
<p>Keywords:</p>
<h3 id="139. Contextual String Embeddings for Sequence Labeling.">139. Contextual String Embeddings for Sequence Labeling.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1139/">Paper Link</a>    Pages:1638-1649</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Akbik:Alan">Alan Akbik</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blythe:Duncan">Duncan Blythe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vollgraf:Roland">Roland Vollgraf</a></p>
<p>Abstract:
Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: <a href="https://github.com/zalandoresearch/flair">https://github.com/zalandoresearch/flair</a></p>
<p>Keywords:</p>
<h3 id="140. Learning Word Meta-Embeddings by Autoencoding.">140. Learning Word Meta-Embeddings by Autoencoding.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1140/">Paper Link</a>    Pages:1650-1661</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bollegala:Danushka">Danushka Bollegala</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bao:Cong">Cong Bao</a></p>
<p>Abstract:
Distributed word embeddings have shown superior performances in numerous Natural Language Processing (NLP) tasks. However, their performances vary significantly across different tasks, implying that the word embeddings learnt by those methods capture complementary aspects of lexical semantics. Therefore, we believe that it is important to combine the existing word embeddings to produce more accurate and complete meta-embeddings of words. We model the meta-embedding learning problem as an autoencoding problem, where we would like to learn a meta-embedding space that can accurately reconstruct all source embeddings simultaneously. Thereby, the meta-embedding space is enforced to capture complementary information in different source embeddings via a coherent common embedding space. We propose three flavours of autoencoded meta-embeddings motivated by different requirements that must be satisfied by a meta-embedding. Our experimental results on a series of benchmark evaluations show that the proposed autoencoded meta-embeddings outperform the existing state-of-the-art meta-embeddings in multiple tasks.</p>
<p>Keywords:</p>
<h3 id="141. GenSense: A Generalized Sense Retrofitting Model.">141. GenSense: A Generalized Sense Retrofitting Model.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1141/">Paper Link</a>    Pages:1662-1671</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Yang=Yin">Yang-Yin Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yen:Ting=Yu">Ting-Yu Yen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Hen=Hsen">Hen-Hsen Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shiue:Yow=Ting">Yow-Ting Shiue</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Hsin=Hsi">Hsin-Hsi Chen</a></p>
<p>Abstract:
With the aid of recently proposed word embedding algorithms, the study of semantic similarity has progressed and advanced rapidly. However, many natural language processing tasks need sense level representation. To address this issue, some researches propose sense embedding learning algorithms. In this paper, we present a generalized model from existing sense retrofitting model. The generalization takes three major components: semantic relations between the senses, the relation strength and the semantic strength. In the experiment, we show that the generalized model can outperform previous approaches in three types of experiment: semantic relatedness, contextual word similarity and semantic difference.</p>
<p>Keywords:</p>
<h3 id="142. Variational Attention for Sequence-to-Sequence Models.">142. Variational Attention for Sequence-to-Sequence Models.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1142/">Paper Link</a>    Pages:1672-1682</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bahuleyan:Hareesh">Hareesh Bahuleyan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mou:Lili">Lili Mou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vechtomova:Olga">Olga Vechtomova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poupart:Pascal">Pascal Poupart</a></p>
<p>Abstract:
The variational encoder-decoder (VED) encodes source information as a set of random variables using a neural network, which in turn is decoded into target data using another neural network. In natural language processing, sequence-to-sequence (Seq2Seq) models typically serve as encoder-decoder networks. When combined with a traditional (deterministic) attention mechanism, the variational latent space may be bypassed by the attention model, and thus becomes ineffective. In this paper, we propose a variational attention mechanism for VED, where the attention vector is also modeled as Gaussian distributed random variables. Results on two experiments show that, without loss of quality, our proposed method alleviates the bypassing phenomenon as it increases the diversity of generated sentences.</p>
<p>Keywords:</p>
<h3 id="143. A New Concept of Deep Reinforcement Learning based Augmented General Tagging System.">143. A New Concept of Deep Reinforcement Learning based Augmented General Tagging System.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1143/">Paper Link</a>    Pages:1683-1693</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yu">Yu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Patel:Abhishek">Abhishek Patel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Hongxia">Hongxia Jin</a></p>
<p>Abstract:
In this paper, a new deep reinforcement learning based augmented general tagging system is proposed. The new system contains two parts: a deep neural network (DNN) based sequence labeling model and a deep reinforcement learning (DRL) based augmented tagger. The augmented tagger helps improve system performance by modeling the data with minority tags. The new system is evaluated on SLU and NLU sequence labeling tasks using ATIS and CoNLL-2003 benchmark datasets, to demonstrate the new systems outstanding performance on general tagging tasks. Evaluated by F1 scores, it shows that the new system outperforms the current state-of-the-art model on ATIS dataset by 1.9% and that on CoNLL-2003 dataset by 1.4%.</p>
<p>Keywords:</p>
<h3 id="144. Learning from Measurements in Crowdsourcing Models: Inferring Ground Truth from Diverse Annotation Types.">144. Learning from Measurements in Crowdsourcing Models: Inferring Ground Truth from Diverse Annotation Types.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1144/">Paper Link</a>    Pages:1694-1704</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Felt:Paul">Paul Felt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ringger:Eric_K=">Eric K. Ringger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Boyd=Graber:Jordan_L=">Jordan L. Boyd-Graber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Seppi:Kevin_D=">Kevin D. Seppi</a></p>
<p>Abstract:
Annotated corpora enable supervised machine learning and data analysis. To reduce the cost of manual annotation, tasks are often assigned to internet workers whose judgments are reconciled by crowdsourcing models. We approach the problem of crowdsourcing using a framework for learning from rich prior knowledge, and we identify a family of crowdsourcing models with the novel ability to combine annotations with differing structures: e.g., document labels and word labels. Annotator judgments are given in the form of the predicted expected value of measurement functions computed over annotations and the data, unifying annotation models. Our model, a specific instance of this framework, compares favorably with previous work. Furthermore, it enables active sample selection, jointly selecting annotator, data item, and annotation structure to reduce annotation effort.</p>
<p>Keywords:</p>
<h3 id="145. Reproducing and Regularizing the SCRN Model.">145. Reproducing and Regularizing the SCRN Model.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1145/">Paper Link</a>    Pages:1705-1716</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kabdolov:Olzhas">Olzhas Kabdolov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Assylbekov:Zhenisbek">Zhenisbek Assylbekov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Takhanov:Rustem">Rustem Takhanov</a></p>
<p>Abstract:
We reproduce the Structurally Constrained Recurrent Network (SCRN) model, and then regularize it using the existing widespread techniques, such as naive dropout, variational dropout, and weight tying. We show that when regularized and optimized appropriately the SCRN model can achieve performance comparable with the ubiquitous LSTM model in language modeling task on English data, while outperforming it on non-English data.</p>
<p>Keywords:</p>
<h3 id="146. Structure-Infused Copy Mechanisms for Abstractive Summarization.">146. Structure-Infused Copy Mechanisms for Abstractive Summarization.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1146/">Paper Link</a>    Pages:1717-1729</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Song:Kaiqiang">Kaiqiang Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Lin">Lin Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0004:Fei">Fei Liu</a></p>
<p>Abstract:
Seq2seq learning has produced promising results on summarization. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the syntactic structure of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence. The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the system, and our proposed approach compares favorably to state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="147. Measuring the Diversity of Automatic Image Descriptions.">147. Measuring the Diversity of Automatic Image Descriptions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1147/">Paper Link</a>    Pages:1730-1741</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Miltenburg:Emiel_van">Emiel van Miltenburg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Elliott:Desmond">Desmond Elliott</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vossen:Piek">Piek Vossen</a></p>
<p>Abstract:
Automatic image description systems typically produce generic sentences that only make use of a small subset of the vocabulary available to them. In this paper, we consider the production of generic descriptions as a lack of diversity in the output, which we quantify using established metrics and two new metrics that frame image description as a word recall task. This framing allows us to evaluate system performance on the head of the vocabulary, as well as on the long tail, where system performance degrades. We use these metrics to examine the diversity of the sentences generated by nine state-of-the-art systems on the MS COCO data set. We find that the systems trained with maximum likelihood objectives produce less diverse output than those trained with additional adversarial objectives. However, the adversarially-trained models only produce more types from the head of the vocabulary and not the tail. Besides vocabulary-based methods, we also look at the compositional capacity of the systems, specifically their ability to create compound nouns and prepositional phrases of different lengths. We conclude that there is still much room for improvement, and offer a toolkit to measure progress towards the goal of generating more diverse image descriptions.</p>
<p>Keywords:</p>
<h3 id="148. Extractive Headline Generation Based on Learning to Rank for Community Question Answering.">148. Extractive Headline Generation Based on Learning to Rank for Community Question Answering.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1148/">Paper Link</a>    Pages:1742-1753</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Higurashi:Tatsuru">Tatsuru Higurashi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kobayashi:Hayato">Hayato Kobayashi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Masuyama:Takeshi">Takeshi Masuyama</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Murao:Kazuma">Kazuma Murao</a></p>
<p>Abstract:
User-generated content such as the questions on community question answering (CQA) forums does not always come with appropriate headlines, in contrast to the news articles used in various headline generation tasks. In such cases, we cannot use paired supervised data, e.g., pairs of articles and headlines, to learn a headline generation model. To overcome this problem, we propose an extractive headline generation method based on learning to rank for CQA that extracts the most informative substring from each question as its headline. Experimental results show that our method outperforms several baselines, including a prefix-based method, which is widely used in real services.</p>
<p>Keywords:</p>
<h3 id="149. A Multi-Attention based Neural Network with External Knowledge for Story Ending Predicting Task.">149. A Multi-Attention based Neural Network with External Knowledge for Story Ending Predicting Task.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1149/">Paper Link</a>    Pages:1754-1762</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Qian">Qian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Ziwei">Ziwei Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Jin=Mao">Jin-Mao Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Yanhui">Yanhui Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jatowt:Adam">Adam Jatowt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Zhenglu">Zhenglu Yang</a></p>
<p>Abstract:
Enabling a mechanism to understand a temporal story and predict its ending is an interesting issue that has attracted considerable attention, as in case of the ROC Story Cloze Task (SCT). In this paper, we develop a multi-attention-based neural network (MANN) with well-designed optimizations, like Highway Network, and concatenated features with embedding representations into the hierarchical neural network model. Considering the particulars of the specific task, we thoughtfully extend MANN with external knowledge resources, exceeding state-of-the-art results obviously. Furthermore, we develop a thorough understanding of our model through a careful hand analysis on a subset of the stories. We identify what traits of MANN contribute to its outperformance and how external knowledge is obtained in such an ending prediction task.</p>
<p>Keywords:</p>
<h3 id="150. A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators.">150. A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1150/">Paper Link</a>    Pages:1763-1774</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fan:Zhihao">Zhihao Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Zhongyu">Zhongyu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Siyuan">Siyuan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0004:Yang">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>Abstract:
Visual Question Generation (VQG) aims to ask natural questions about an image automatically. Existing research focus on training model to fit the annotated data set that makes it indifferent from other language generation tasks. We argue that natural questions need to have two specific attributes from the perspectives of content and linguistic respectively, namely, natural and human-written. Inspired by the setting of discriminator in adversarial learning, we propose two discriminators, one for each attribute, to enhance the training. We then use the reinforcement learning framework to incorporate scores from the two discriminators as the reward to guide the training of the question generator. Experimental results on a benchmark VQG dataset show the effectiveness and robustness of our model compared to some state-of-the-art models in terms of both automatic and human evaluation metrics.</p>
<p>Keywords:</p>
<h3 id="151. Embedding Words as Distributions with a Bayesian Skip-gram Model.">151. Embedding Words as Distributions with a Bayesian Skip-gram Model.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1151/">Paper Link</a>    Pages:1775-1789</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Brazinskas:Arthur">Arthur Brazinskas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Havrylov:Serhii">Serhii Havrylov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Titov:Ivan">Ivan Titov</a></p>
<p>Abstract:
We introduce a method for embedding words as probability densities in a low-dimensional space. Rather than assuming that a word embedding is fixed across the entire text collection, as in standard word embedding methods, in our Bayesian model we generate it from a word-specific prior density for each occurrence of a given word. Intuitively, for each word, the prior density encodes the distribution of its potential meanings. These prior densities are conceptually similar to Gaussian embeddings of wcitevilnis2014word. Interestingly, unlike the Gaussian embeddings, we can also obtain context-specific densities: they encode uncertainty about the sense of a word given its context and correspond to the approximate posterior distributions within our model. The context-dependent densities have many potential applications: for example, we show that they can be directly used in the lexical substitution task. We describe an effective estimation method based on the variational autoencoding framework. We demonstrate the effectiveness of our embedding technique on a range of standard benchmarks.</p>
<p>Keywords:</p>
<h3 id="152. Assessing Composition in Sentence Vector Representations.">152. Assessing Composition in Sentence Vector Representations.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1152/">Paper Link</a>    Pages:1790-1801</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Ettinger:Allyson">Allyson Ettinger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Elgohary:Ahmed">Ahmed Elgohary</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Phillips:Colin">Colin Phillips</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Resnik:Philip">Philip Resnik</a></p>
<p>Abstract:
An important component of achieving language understanding is mastering the composition of sentence meaning, but an immediate challenge to solving this problem is the opacity of sentence vector representations produced by current neural sentence composition models. We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control. To enable the creation of these controlled tasks, we introduce a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints. We describe the details of the method and generation system, and then present results of experiments applying our method to probe for compositional information in embeddings from a number of existing sentence composition models. We find that the method is able to extract useful information about the differing capacities of these models, and we discuss the implications of our results with respect to these systems capturing of sentence information. We make available for public use the datasets used for these experiments, as well as the generation system.</p>
<p>Keywords:</p>
<h3 id="153. Subword-augmented Embedding for Cloze Reading Comprehension.">153. Subword-augmented Embedding for Cloze Reading Comprehension.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1153/">Paper Link</a>    Pages:1802-1814</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Zhuosheng">Zhuosheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Yafang">Yafang Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a></p>
<p>Abstract:
Representation learning is the foundation of machine reading comprehension. In state-of-the-art models, deep learning methods broadly use word and character level representations. However, character is not naturally the minimal linguistic unit. In addition, with a simple concatenation of character and word embedding, previous models actually give suboptimal solution. In this paper, we propose to use subword rather than character for word embedding enhancement. We also empirically explore different augmentation strategies on subword-augmented embedding to enhance the cloze-style reading comprehension model (reader). In detail, we present a reader that uses subword-level representation to augment word embedding with a short list to handle rare words effectively. A thorough examination is conducted to evaluate the comprehensive performance and generalization ability of the proposed reader. Experimental results show that the proposed approach helps the reader significantly outperform the state-of-the-art baselines on various public datasets.</p>
<p>Keywords:</p>
<h3 id="154. Enhancing Sentence Embedding with Generalized Pooling.">154. Enhancing Sentence Embedding with Generalized Pooling.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1154/">Paper Link</a>    Pages:1815-1826</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Qian">Qian Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Ling:Zhen=Hua">Zhen-Hua Ling</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Xiaodan">Xiaodan Zhu</a></p>
<p>Abstract:
Pooling is an essential component of a wide variety of sentence representation and embedding models. This paper explores generalized pooling methods to enhance sentence embedding. We propose vector-based multi-head attention that includes the widely used max pooling, mean pooling, and scalar self-attention as special cases. The model benefits from properly designed penalization terms to reduce redundancy in multi-head attention. We evaluate the proposed model on three different tasks: natural language inference (NLI), author profiling, and sentiment classification. The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets. The proposed approach can be easily implemented for more problems than we discuss in this paper.</p>
<p>Keywords:</p>
<h3 id="155. Treat us like the sequences we are: Prepositional Paraphrasing of Noun Compounds using LSTM.">155. Treat us like the sequences we are: Prepositional Paraphrasing of Noun Compounds using LSTM.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1155/">Paper Link</a>    Pages:1827-1836</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Ponkiya:Girishkumar">Girishkumar Ponkiya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Patel:Kevin">Kevin Patel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Palshikar:Girish_K=">Girish K. Palshikar</a></p>
<p>Abstract:
Interpreting noun compounds is a challenging task. It involves uncovering the underlying predicate which is dropped in the formation of the compound. In most cases, this predicate is of the form VERB+PREP. It has been observed that uncovering the preposition is a significant step towards uncovering the predicate. In this paper, we attempt to paraphrase noun compounds using prepositions. We consider noun compounds and their corresponding prepositional paraphrases as parallelly aligned sequences of words. This enables us to adapt different architectures from cross-lingual embedding literature. We choose the architecture where we create representations of both noun compound (source sequence) and its corresponding prepositional paraphrase (target sequence), such that their sim- ilarity is high. We use LSTMs to learn these representations. We use these representations to decide the correct preposition. Our experiments show that this approach performs considerably well on different datasets of noun compounds that are manually annotated with prepositions.</p>
<p>Keywords:</p>
<h3 id="156. CASCADE: Contextual Sarcasm Detection in Online Discussion Forums.">156. CASCADE: Contextual Sarcasm Detection in Online Discussion Forums.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1156/">Paper Link</a>    Pages:1837-1848</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hazarika:Devamanyu">Devamanyu Hazarika</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poria:Soujanya">Soujanya Poria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gorantla:Sruthi">Sruthi Gorantla</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cambria:Erik">Erik Cambria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zimmermann:Roger">Roger Zimmermann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mihalcea:Rada">Rada Mihalcea</a></p>
<p>Abstract:
The literature in automated sarcasm detection has mainly focused on lexical-, syntactic- and semantic-level analysis of text. However, a sarcastic sentence can be expressed with contextual presumptions, background and commonsense knowledge. In this paper, we propose a ContextuAl SarCasm DEtector (CASCADE), which adopts a hybrid approach of both content- and context-driven modeling for sarcasm detection in online social media discussions. For the latter, CASCADE aims at extracting contextual information from the discourse of a discussion thread. Also, since the sarcastic nature and form of expression can vary from person to person, CASCADE utilizes user embeddings that encode stylometric and personality features of users. When used along with content-based feature extractors such as convolutional neural networks, we see a significant boost in the classification performance on a large Reddit corpus.</p>
<p>Keywords:</p>
<h3 id="157. Recognizing Humour using Word Associations and Humour Anchor Extraction.">157. Recognizing Humour using Word Associations and Humour Anchor Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1157/">Paper Link</a>    Pages:1849-1858</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cattle:Andrew">Andrew Cattle</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Xiaojuan">Xiaojuan Ma</a></p>
<p>Abstract:
This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors (Yang et al., 2015), for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.</p>
<p>Keywords:</p>
<h3 id="158. A Retrospective Analysis of the Fake News Challenge Stance-Detection Task.">158. A Retrospective Analysis of the Fake News Challenge Stance-Detection Task.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1158/">Paper Link</a>    Pages:1859-1874</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hanselowski:Andreas">Andreas Hanselowski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/S=:Avinesh_P=_V=">Avinesh P. V. S.</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schiller:Benjamin">Benjamin Schiller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Caspelherr:Felix">Felix Caspelherr</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chaudhuri:Debanjan">Debanjan Chaudhuri</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meyer:Christian_M=">Christian M. Meyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a stance classification task as a crucial first step towards detecting fake news. To date, there is no in-depth analysis paper to critically discuss FNC-1s experimental setup, reproduce the results, and draw conclusions for next-generation stance classification methods. In this paper, we provide such an in-depth analysis for the three top-performing systems. We first find that FNC-1s proposed evaluation metric favors the majority class, which can be easily classified, and thus overestimates the true discriminative power of the methods. Therefore, we propose a new F1-based metric yielding a changed system ranking. Next, we compare the features and architectures used, which leads to a novel feature-rich stacked LSTM model that performs on par with the best systems, but is superior in predicting minority classes. To understand the methods ability to generalize, we derive a new dataset and perform both in-domain and cross-domain experiments. Our qualitative and quantitative study helps interpreting the original FNC-1 scores and understand which features help improving performance and why. Our new dataset and all source code used during the reproduction study are publicly available for future research.</p>
<p>Keywords:</p>
<h3 id="159. Exploiting Syntactic Structures for Humor Recognition.">159. Exploiting Syntactic Structures for Humor Recognition.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1159/">Paper Link</a>    Pages:1875-1883</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Lizhen">Lizhen Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Donghai">Donghai Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Wei">Wei Song</a></p>
<p>Abstract:
Humor recognition is an interesting and challenging task in natural language processing. This paper proposes to exploit syntactic structure features to enhance humor recognition. Our method achieves significant improvements compared with humor theory driven baselines. We found that some syntactic structure features consistently correlate with humor, which indicate interesting linguistic phenomena. Both the experimental results and the analysis demonstrate that humor can be viewed as a kind of style and content independent syntactic structures can help identify humor and have good interpretability.</p>
<p>Keywords:</p>
<h3 id="160. An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review Detection.">160. An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1160/">Paper Link</a>    Pages:1884-1895</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/You:Zhenni">Zhenni You</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qian:Tieyun">Tieyun Qian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Bing">Bing Liu</a></p>
<p>Abstract:
Spam detection has long been a research topic in both academic and industry due to its wide applications. Previous studies are mainly focused on extracting linguistic or behavior features to distinguish the spam and legitimate reviews. Such features are either ineffective or take long time to collect and thus are hard to be applied to cold-start spam review detection tasks. Recent advance leveraged the neural network to encode the textual and behavior features for the cold-start problem. However, the abundant attribute information are largely neglected by the existing framework. In this paper, we propose a novel deep learning architecture for incorporating entities and their inherent attributes from various domains into a unified framework. Specifically, our model not only encodes the entities of reviewer, item, and review, but also their attributes such as location, date, price ranges. Furthermore, we present a domain classifier to adapt the knowledge from one domain to the other. With the abundant attributes in existing entities and knowledge in other domains, we successfully solve the problem of data scarcity in the cold-start settings. Experimental results on two Yelp datasets prove that our proposed framework significantly outperforms the state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="161. Robust Lexical Features for Improved Neural Network Named-Entity Recognition.">161. Robust Lexical Features for Improved Neural Network Named-Entity Recognition.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1161/">Paper Link</a>    Pages:1896-1907</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ghaddar:Abbas">Abbas Ghaddar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Langlais:Philippe">Philippe Langlais</a></p>
<p>Abstract:
Neural network approaches to Named-Entity Recognition reduce the need for carefully hand-crafted features. While some features do remain in state-of-the-art systems, lexical features have been mostly discarded, with the exception of gazetteers. In this work, we show that this is unfair: lexical features are actually quite useful. We propose to embed words and entity types into a low-dimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia. From this, we compute  offline  a feature vector representing each word. When used with a vanilla recurrent neural network model, this representation yields substantial improvements. We establish a new state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance with a F1 score of 91.73 on the over-studied CONLL-2003 dataset.</p>
<p>Keywords:</p>
<h3 id="162. A Pseudo Label based Dataless Naive Bayes Algorithm for Text Classification with Seed Words.">162. A Pseudo Label based Dataless Naive Bayes Algorithm for Text Classification with Seed Words.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1162/">Paper Link</a>    Pages:1908-1917</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Ximing">Ximing Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Bo">Bo Yang</a></p>
<p>Abstract:
Traditional supervised text classifiers require a large number of manually labeled documents, which are often expensive to obtain. Recently, dataless text classification has attracted more attention, since it only requires very few seed words of categories that are much cheaper. In this paper, we develop a pseudo-label based dataless Naive Bayes (PL-DNB) classifier with seed words. We initialize pseudo-labels for each document using seed word occurrences, and employ the expectation maximization algorithm to train PL-DNB in a semi-supervised manner. The pseudo-labels are iteratively updated using a mixture of seed word occurrences and estimations of label posteriors. To avoid noisy pseudo-labels, we also consider the information of nearest neighboring documents in the pseudo-label update step, i.e., preserving local neighborhood structure of documents. We empirically show that PL-DNB outperforms traditional dataless text classification algorithms with seed words. Especially, PL-DNB performs well on the imbalanced dataset.</p>
<p>Keywords:</p>
<h3 id="163. Visual Question Answering Dataset for Bilingual Image Understanding: A Study of Cross-Lingual Transfer Using Attention Maps.">163. Visual Question Answering Dataset for Bilingual Image Understanding: A Study of Cross-Lingual Transfer Using Attention Maps.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1163/">Paper Link</a>    Pages:1918-1928</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shimizu:Nobuyuki">Nobuyuki Shimizu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rong:Na">Na Rong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miyazaki:Takashi">Takashi Miyazaki</a></p>
<p>Abstract:
Visual question answering (VQA) is a challenging task that requires a computer system to understand both a question and an image. While there is much research on VQA in English, there is a lack of datasets for other languages, and English annotation is not directly applicable in those languages. To deal with this, we have created a Japanese VQA dataset by using crowdsourced annotation with images from the Visual Genome dataset. This is the first such dataset in Japanese. As another contribution, we propose a cross-lingual method for making use of English annotation to improve a Japanese VQA system. The proposed method is based on a popular VQA method that uses an attention mechanism. We use attention maps generated from English questions to help improve the Japanese VQA task. The proposed method experimentally performed better than simply using a monolingual corpus, which demonstrates the effectiveness of using attention maps to transfer cross-lingual information.</p>
<p>Keywords:</p>
<h3 id="164. Style Detection for Free Verse Poetry from Text and Speech.">164. Style Detection for Free Verse Poetry from Text and Speech.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1164/">Paper Link</a>    Pages:1929-1940</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Baumann:Timo">Timo Baumann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hussein:Hussein">Hussein Hussein</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meyer=Sickendiek:Burkhard">Burkhard Meyer-Sickendiek</a></p>
<p>Abstract:
Modern and post-modern free verse poems feature a large and complex variety in their poetic prosodies that falls along a continuum from a more fluent to a more disfluent and choppy style. As the poets of modernism overcame rhyme and meter, they oriented themselves in these two opposing directions, creating a free verse spectrum that calls for new analyses of prosodic forms. We present a method, grounded in philological analysis and current research on cognitive (dis)fluency, for automatically analyzing this spectrum. We define and relate six classes of poetic styles (ranging from parlando to lettristic decomposition) by their gradual differentiation. Based on this discussion, we present a model for automatic prosodic classification of spoken free verse poetry that uses deep hierarchical attention networks to integrate the source text and audio and predict the assigned class. We evaluate our model on a large corpus of German author-read post-modern poetry and find that classes can reliably be differentiated, reaching a weighted f-measure of 0.73, when combining textual and phonetic evidence. In our further analyses, we validate the models decision-making process, the philologically hypothesized continuum of fluency and investigate the relative importance of various features.</p>
<p>Keywords:</p>
<h3 id="165. A Neural Question Answering Model Based on Semi-Structured Tables.">165. A Neural Question Answering Model Based on Semi-Structured Tables.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1165/">Paper Link</a>    Pages:1941-1951</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Hao">Hao Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xiaodong">Xiaodong Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Shuming">Shuming Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Mengxiang">Mengxiang Wang</a></p>
<p>Abstract:
Most question answering (QA) systems are based on raw text and structured knowledge graph. However, raw text corpora are hard for QA system to understand, and structured knowledge graph needs intensive manual work, while it is relatively easy to obtain semi-structured tables from many sources directly, or build them automatically. In this paper, we build an end-to-end system to answer multiple choice questions with semi-structured tables as its knowledge. Our system answers queries by two steps. First, it finds the most similar tables. Then the system measures the relevance between each question and candidate table cells, and choose the most related cell as the source of answer. The system is evaluated with TabMCQ dataset, and gets a huge improvement compared to the state of the art.</p>
<p>Keywords:</p>
<h3 id="166. LCQMC: A Large-scale Chinese Question Matching Corpus.">166. LCQMC: A Large-scale Chinese Question Matching Corpus.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1166/">Paper Link</a>    Pages:1952-1962</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xin">Xin Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Qingcai">Qingcai Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deng:Chong">Chong Deng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Huajun">Huajun Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jing">Jing Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Dongfang">Dongfang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Buzhou">Buzhou Tang</a></p>
<p>Abstract:
The lack of large-scale question matching corpora greatly limits the development of matching methods in question answering (QA) system, especially for non-English languages. To ameliorate this situation, in this paper, we introduce a large-scale Chinese question matching corpus (named LCQMC), which is released to the public1. LCQMC is more general than paraphrase corpus as it focuses on intent matching rather than paraphrase. How to collect a large number of question pairs in variant linguistic forms, which may present the same intent, is the key point for such corpus construction. In this paper, we first use a search engine to collect large-scale question pairs related to high-frequency words from various domains, then filter irrelevant pairs by the Wasserstein distance, and finally recruit three annotators to manually check the left pairs. After this process, a question matching corpus that contains 260,068 question pairs is constructed. In order to verify the LCQMC corpus, we split it into three parts, i.e., a training set containing 238,766 question pairs, a development set with 8,802 question pairs, and a test set with 12,500 question pairs, and test several well-known sentence matching methods on it. The experimental results not only demonstrate the good quality of LCQMC but also provide solid baseline performance for further researches on this corpus.</p>
<p>Keywords:</p>
<h3 id="167. Genre Identification and the Compositional Effect of Genre in Literature.">167. Genre Identification and the Compositional Effect of Genre in Literature.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1167/">Paper Link</a>    Pages:1963-1973</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Worsham:Joseph">Joseph Worsham</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kalita:Jugal">Jugal Kalita</a></p>
<p>Abstract:
Recent advances in Natural Language Processing are finding ways to place an emphasis on the hierarchical nature of text instead of representing language as a flat sequence or unordered collection of words or letters. A human reader must capture multiple levels of abstraction and meaning in order to formulate an understanding of a document. In this paper, we address the problem of developing approaches which are capable of working with extremely large and complex literary documents to perform Genre Identification. The task is to assign the literary classification to a full-length book belonging to a corpus of literature, where the works on average are well over 200,000 words long and genre is an abstract thematic concept. We introduce the Gutenberg Dataset for Genre Identification. Additionally, we present a study on how current deep learning models compare to traditional methods for this task. The results are presented as a baseline along with findings on how using an ensemble of chapters can significantly improve results in deep learning methods. The motivation behind the ensemble of chapters method is discussed as the compositionality of subtexts which make up a larger work and contribute to the overall genre.</p>
<p>Keywords:</p>
<h3 id="168. Transfer Learning for Entity Recognition of Novel Classes.">168. Transfer Learning for Entity Recognition of Novel Classes.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1168/">Paper Link</a>    Pages:1974-1985</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rodr=iacute=guez:Juan_Diego">Juan Diego Rodrguez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Caldwell:Adam">Adam Caldwell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Alexander">Alexander Liu</a></p>
<p>Abstract:
In this reproduction paper, we replicate and extend several past studies on transfer learning for entity recognition. In particular, we are interested in entity recognition problems where the class labels in the source and target domains are different. Our work is the first direct comparison of these previously published approaches in this problem setting. In addition, we perform experiments on seven new source/target corpus pairs, nearly doubling the total number of corpus pairs that have been studied in all past work combined. Our results empirically demonstrate when each of the published approaches tends to do well. In particular, simpler approaches often work best when there is very little labeled target data, while neural transfer approaches tend to do better when there is more labeled target data.</p>
<p>Keywords:</p>
<h3 id="169. Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models.">169. Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1169/">Paper Link</a>    Pages:1986-1997</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Al=Olimat:Hussein">Hussein Al-Olimat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thirunarayan:Krishnaprasad">Krishnaprasad Thirunarayan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shalin:Valerie_L=">Valerie L. Shalin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sheth:Amit_P=">Amit P. Sheth</a></p>
<p>Abstract:
Extracting location names from informal and unstructured social media data requires the identification of referent boundaries and partitioning compound names. Variability, particularly systematic variability in location names (Carroll, 1983), challenges the identification task. Some of this variability can be anticipated as operations within a statistical language model, in this case drawn from gazetteers such as OpenStreetMap (OSM), Geonames, and DBpedia. This permits evaluation of an observed n-gram in Twitter targeted text as a legitimate location name variant from the same location-context. Using n-gram statistics and location-related dictionaries, our Location Name Extraction tool (LNEx) handles abbreviations and automatically filters and augments the location names in gazetteers (handling name contractions and auxiliary contents) to help detect the boundaries of multi-word location names and thereby delimit them in texts. We evaluated our approach on 4,500 event-specific tweets from three targeted streams to compare the performance of LNEx against that of ten state-of-the-art taggers that rely on standard semantic, syntactic and/or orthographic features. LNEx improved the average F-Score by 33-179%, outperforming all taggers. Further, LNEx is capable of stream processing.</p>
<p>Keywords:</p>
<h3 id="170. The APVA-TURBO Approach To Question Answering in Knowledge Base.">170. The APVA-TURBO Approach To Question Answering in Knowledge Base.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1170/">Paper Link</a>    Pages:1998-2009</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yue">Yue Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Richong">Richong Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Cheng">Cheng Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mao:Yongyi">Yongyi Mao</a></p>
<p>Abstract:
In this paper, we study the problem of question answering over knowledge base. We identify that the primary bottleneck in this problem is the difficulty in accurately predicting the relations connecting the subject entity to the object entities. We advocate a new model architecture, APVA, which includes a verification mechanism responsible for checking the correctness of predicted relations. The APVA framework naturally supports a well-principled iterative training procedure, which we call turbo training. We demonstrate via experiments that the APVA-TUBRO approach drastically improves the question answering performance.</p>
<p>Keywords:</p>
<h3 id="171. An Interpretable Reasoning Network for Multi-Relation Question Answering.">171. An Interpretable Reasoning Network for Multi-Relation Question Answering.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1171/">Paper Link</a>    Pages:2010-2022</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Mantong">Mantong Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Minlie">Minlie Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu_0001:Xiaoyan">Xiaoyan Zhu</a></p>
<p>Abstract:
Multi-relation Question Answering is a challenging task, due to the requirement of elaborated analysis on questions and reasoning over multiple fact triples in knowledge base. In this paper, we present a novel model called Interpretable Reasoning Network that employs an interpretable, hop-by-hop reasoning process for question answering. The model dynamically decides which part of an input question should be analyzed at each hop; predicts a relation that corresponds to the current parsed results; utilizes the predicted relation to update the question representation and the state of the reasoning process; and then drives the next-hop reasoning. Experiments show that our model yields state-of-the-art results on two datasets. More interestingly, the model can offer traceable and observable intermediate predictions for reasoning analysis and failure diagnosis, thereby allowing manual manipulation in predicting the final answer.</p>
<p>Keywords:</p>
<h3 id="172. Task-oriented Word Embedding for Text Classification.">172. Task-oriented Word Embedding for Text Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1172/">Paper Link</a>    Pages:2023-2032</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qian">Qian Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Heyan">Heyan Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao_0016:Yang">Yang Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Xiaochi">Xiaochi Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tian:Yuxin">Yuxin Tian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Luyang">Luyang Liu</a></p>
<p>Abstract:
Distributed word representation plays a pivotal role in various natural language processing tasks. In spite of its success, most existing methods only consider contextual information, which is suboptimal when used in various tasks due to a lack of task-specific features. The rational word embeddings should have the ability to capture both the semantic features and task-specific features of words. In this paper, we propose a task-oriented word embedding method and apply it to the text classification task. With the function-aware component, our method regularizes the distribution of words to enable the embedding space to have a clear classification boundary. We evaluate our method using five text classification datasets. The experiment results show that our method significantly outperforms the state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="173. Adaptive Learning of Local Semantic and Global Structure Representations for Text Classification.">173. Adaptive Learning of Local Semantic and Global Structure Representations for Text Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1173/">Paper Link</a>    Pages:2033-2043</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Jianyu">Jianyu Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhan:Zhiqiang">Zhiqiang Zhan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Qichuan">Qichuan Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yang">Yang Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Changjian">Changjian Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zhensheng">Zhensheng Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Liuxin">Liuxin Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Zhiqiang">Zhiqiang He</a></p>
<p>Abstract:
Representation learning is a key issue for most Natural Language Processing (NLP) tasks. Most existing representation models either learn little structure information or just rely on pre-defined structures, leading to degradation of performance and generalization capability. This paper focuses on learning both local semantic and global structure representations for text classification. In detail, we propose a novel Sandwich Neural Network (SNN) to learn semantic and structure representations automatically without relying on parsers. More importantly, semantic and structure information contribute unequally to the text representation at corpus and instance level. To solve the fusion problem, we propose two strategies: Adaptive Learning Sandwich Neural Network (AL-SNN) and Self-Attention Sandwich Neural Network (SA-SNN). The former learns the weights at corpus level, and the latter further combines attention mechanism to assign the weights at instance level. Experimental results demonstrate that our approach achieves competitive performance on several text classification tasks, including sentiment analysis, question type classification and subjectivity classification. Specifically, the accuracies are MR (82.1%), SST-5 (50.4%), TREC (96%) and SUBJ (93.9%).</p>
<p>Keywords:</p>
<h3 id="174. Lyrics Segmentation: Textual Macrostructure Detection using Convolutions.">174. Lyrics Segmentation: Textual Macrostructure Detection using Convolutions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1174/">Paper Link</a>    Pages:2044-2054</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fell:Michael">Michael Fell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nechaev:Yaroslav">Yaroslav Nechaev</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cabrio:Elena">Elena Cabrio</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gandon:Fabien">Fabien Gandon</a></p>
<p>Abstract:
Lyrics contain repeated patterns that are correlated with the repetitions found in the music they accompany. Repetitions in song texts have been shown to enable lyrics segmentation  a fundamental prerequisite of automatically detecting the building blocks (e.g. chorus, verse) of a song text. In this article we improve on the state-of-the-art in lyrics segmentation by applying a convolutional neural network to the task, and experiment with novel features as a step towards deeper macrostructure detection of lyrics.</p>
<p>Keywords:</p>
<h3 id="175. Learning What to Share: Leaky Multi-Task Network for Text Classification.">175. Learning What to Share: Leaky Multi-Task Network for Text Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1175/">Paper Link</a>    Pages:2055-2065</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Liqiang">Liqiang Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Honglun">Honglun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Wenqing">Wenqing Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yongkun">Yongkun Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Yaohui">Yaohui Jin</a></p>
<p>Abstract:
Neural network based multi-task learning has achieved great success on many NLP problems, which focuses on sharing knowledge among tasks by linking some layers to enhance the performance. However, most existing approaches suffer from the interference between tasks because they lack of selection mechanism for feature sharing. In this way, the feature spaces of tasks may be easily contaminated by helpless features borrowed from others, which will confuse the models for making correct prediction. In this paper, we propose a multi-task convolutional neural network with the Leaky Unit, which has memory and forgetting mechanism to filter the feature flows between tasks. Experiments on five different datasets for text classification validate the benefits of our approach.</p>
<p>Keywords:</p>
<h3 id="176. Towards an argumentative content search engine using weak supervision.">176. Towards an argumentative content search engine using weak supervision.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1176/">Paper Link</a>    Pages:2066-2081</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Levy:Ran">Ran Levy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bogin:Ben">Ben Bogin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gretz:Shai">Shai Gretz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aharonov:Ranit">Ranit Aharonov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Slonim:Noam">Noam Slonim</a></p>
<p>Abstract:
Searching for sentences containing claims in a large text corpus is a key component in developing an argumentative content search engine. Previous works focused on detecting claims in a small set of documents or within documents enriched with argumentative content. However, pinpointing relevant claims in massive unstructured corpora, received little attention. A step in this direction was taken in (Levy et al. 2017), where the authors suggested using a weak signal to develop a relatively strict query for claimsentence detection. Here, we leverage this work to define weak signals for training DNNs to obtain significantly greater performance. This approach allows to relax the query and increase the potential coverage. Our results clearly indicate that the system is able to successfully generalize from the weak signal, outperforming previously reported results in terms of both precision and coverage. Finally, we adapt our system to solve a recent argument mining task of identifying argumentative sentences in Web texts retrieved from heterogeneous sources, and obtain F1 scores comparable to the supervised baseline.</p>
<p>Keywords:</p>
<h3 id="177. Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological Tags.">177. Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological Tags.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1177/">Paper Link</a>    Pages:2082-2092</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/G=uuml=ng=ouml=r_0001:Onur">Onur Gngr</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Uskudarli:Suzan">Suzan Uskudarli</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gungor:Tunga">Tunga Gungor</a></p>
<p>Abstract:
Previous studies have shown that linguistic features of a word such as possession, genitive or other grammatical cases can be employed in word representations of a named entity recognition (NER) tagger to improve the performance for morphologically rich languages. However, these taggers require external morphological disambiguation (MD) tools to function which are hard to obtain or non-existent for many languages. In this work, we propose a model which alleviates the need for such disambiguators by jointly learning NER and MD taggers in languages for which one can provide a list of candidate morphological analyses. We show that this can be done independent of the morphological annotation schemes, which differ among languages. Our experiments employing three different model architectures that join these two tasks show that joint learning improves NER performance. Furthermore, the morphological disambiguators performance is shown to be competitive.</p>
<p>Keywords:</p>
<h3 id="178. Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia.">178. Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1178/">Paper Link</a>    Pages:2093-2103</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Azmy:Michael">Michael Azmy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Peng">Peng Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Jimmy">Jimmy Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Ilyas:Ihab_F=">Ihab F. Ilyas</a></p>
<p>Abstract:
Question answering over knowledge graphs is an important problem of interest both commercially and academically. There is substantial interest in the class of natural language questions that can be answered via the lookup of a single fact, driven by the availability of the popular SimpleQuestions dataset. The problem with this dataset, however, is that answer triples are provided from Freebase, which has been defunct for several years. As a result, it is difficult to build real-world question answering systems that are operationally deployable. Furthermore, a defunct knowledge graph means that much of the infrastructure for querying, browsing, and manipulating triples no longer exists. To address this problem, we present SimpleDBpediaQA, a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from Freebase to DBpedia. Although this mapping is conceptually straightforward, there are a number of nuances that make the task non-trivial, owing to the different conceptual organizations of the two knowledge graphs. To lay the foundation for future research using this dataset, we leverage recent work to provide simple yet strong baselines with and without neural networks.</p>
<p>Keywords:</p>
<h3 id="179. An Analysis of Annotated Corpora for Emotion Classification in Text.">179. An Analysis of Annotated Corpora for Emotion Classification in Text.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1179/">Paper Link</a>    Pages:2104-2119</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bostan:Laura_Ana_Maria">Laura Ana Maria Bostan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klinger:Roman">Roman Klinger</a></p>
<p>Abstract:
Several datasets have been annotated and published for classification of emotions. They differ in several ways: (1) the use of different annotation schemata (e. g., discrete label sets, including joy, anger, fear, or sadness or continuous values including valence, or arousal), (2) the domain, and, (3) the file formats. This leads to several research gaps: supervised models often only use a limited set of available resources. Additionally, no previous work has compared emotion corpora in a systematic manner. We aim at contributing to this situation with a survey of the datasets, and aggregate them in a common file format with a common annotation schema. Based on this aggregation, we perform the first cross-corpus classification experiments in the spirit of future research enabled by this paper, in order to gain insight and a better understanding of differences of models inferred from the data. This work also simplifies the choice of the most appropriate resources for developing a model for a novel domain. One result from our analysis is that a subset of corpora is better classified with models trained on a different corpus. For none of the corpora, training on all data altogether is better than using a subselection of the resources. Our unified corpus is available at <a href="http://www.ims.uni-stuttgart.de/data/unifyemotion">http://www.ims.uni-stuttgart.de/data/unifyemotion</a>.</p>
<p>Keywords:</p>
<h3 id="180. Investigating the Working of Text Classifiers.">180. Investigating the Working of Text Classifiers.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1180/">Paper Link</a>    Pages:2120-2131</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sachan:Devendra_Singh">Devendra Singh Sachan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zaheer:Manzil">Manzil Zaheer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Salakhutdinov:Ruslan">Ruslan Salakhutdinov</a></p>
<p>Abstract:
Text classification is one of the most widely studied tasks in natural language processing. Motivated by the principle of compositionality, large multilayer neural network models have been employed for this task in an attempt to effectively utilize the constituent expressions. Almost all of the reported work train large networks using discriminative approaches, which come with a caveat of no proper capacity control, as they tend to latch on to any signal that may not generalize. Using various recent state-of-the-art approaches for text classification, we explore whether these models actually learn to compose the meaning of the sentences or still just focus on some keywords or lexicons for classifying the document. To test our hypothesis, we carefully construct datasets where the training and test splits have no direct overlap of such lexicons, but overall language structure would be similar. We study various text classifiers and observe that there is a big performance drop on these datasets. Finally, we show that even simple models with our proposed regularization techniques, which disincentivize focusing on key lexicons, can substantially improve classification accuracy.</p>
<p>Keywords:</p>
<h3 id="181. A Review on Deep Learning Techniques Applied to Answer Selection.">181. A Review on Deep Learning Techniques Applied to Answer Selection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1181/">Paper Link</a>    Pages:2132-2144</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lai:Tuan_Manh">Tuan Manh Lai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bui:Trung">Trung Bui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0001:Sheng">Sheng Li</a></p>
<p>Abstract:
Given a question and a set of candidate answers, answer selection is the task of identifying which of the candidates answers the question correctly. It is an important problem in natural language processing, with applications in many areas. Recently, many deep learning based methods have been proposed for the task. They produce impressive performance without relying on any feature engineering or expensive external resources. In this paper, we aim to provide a comprehensive review on deep learning methods applied to answer selection.</p>
<p>Keywords:</p>
<h3 id="182. A Survey on Recent Advances in Named Entity Recognition from Deep Learning models.">182. A Survey on Recent Advances in Named Entity Recognition from Deep Learning models.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1182/">Paper Link</a>    Pages:2145-2158</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yadav:Vikas">Vikas Yadav</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bethard:Steven">Steven Bethard</a></p>
<p>Abstract:
Named Entity Recognition (NER) is a key component in NLP systems for question answering, information retrieval, relation extraction, etc. NER systems have been studied and developed widely for decades, but accurate systems using deep neural networks (NN) have only been introduced in the last few years. We present a comprehensive survey of deep neural network architectures for NER, and contrast them with previous approaches to NER based on feature engineering and other supervised or semi-supervised learning algorithms. Our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based NER systems can yield further improvements.</p>
<p>Keywords:</p>
<h3 id="183. Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning.">183. Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1183/">Paper Link</a>    Pages:2159-2169</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:YaoSheng">YaoSheng Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Wenliang">Wenliang Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zhenghua">Zhenghua Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Zhengqiu">Zhengqiu He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a></p>
<p>Abstract:
A bottleneck problem with Chinese named entity recognition (NER) in new domains is the lack of annotated data. One solution is to utilize the method of distant supervision, which has been widely used in relation extraction, to automatically populate annotated training data without humancost. The distant supervision assumption here is that if a string in text is included in a predefined dictionary of entities, the string might be an entity. However, this kind of auto-generated data suffers from two main problems: incomplete and noisy annotations, which affect the performance of NER models. In this paper, we propose a novel approach which can partially solve the above problems of distant supervision for NER. In our approach, to handle the incomplete problem, we apply partial annotation learning to reduce the effect of unknown labels of characters. As for noisy annotation, we design an instance selector based on reinforcement learning to distinguish positive sentences from auto-generated annotations. In experiments, we create two datasets for Chinese named entity recognition in two domains with the help of distant supervision. The experimental results show that the proposed approach obtains better performance than the comparison systems on both two datasets.</p>
<p>Keywords:</p>
<h3 id="184. Joint Neural Entity Disambiguation with Output Space Search.">184. Joint Neural Entity Disambiguation with Output Space Search.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1184/">Paper Link</a>    Pages:2170-2180</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shahbazi:Hamed">Hamed Shahbazi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fern:Xiaoli_Z=">Xiaoli Z. Fern</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Ghaeini:Reza">Reza Ghaeini</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma_0001:Chao">Chao Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Obeidat:Rasha_Mohammad">Rasha Mohammad Obeidat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tadepalli:Prasad">Prasad Tadepalli</a></p>
<p>Abstract:
In this paper, we present a novel model for entity disambiguation that combines both local contextual information and global evidences through Limited Discrepancy Search (LDS). Given an input document, we start from a complete solution constructed by a local model and conduct a search in the space of possible corrections to improve the local solution from a global view point. Our search utilizes a heuristic function to focus more on the least confident local decisions and a pruning function to score the global solutions based on their local fitness and the global coherences among the predicted entities. Experimental results on CoNLL 2003 and TAC 2010 benchmarks verify the effectiveness of our model.</p>
<p>Keywords:</p>
<h3 id="185. Learning to Progressively Recognize New Named Entities with Sequence to Sequence Models.">185. Learning to Progressively Recognize New Named Entities with Sequence to Sequence Models.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1185/">Paper Link</a>    Pages:2181-2191</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Lingzhen">Lingzhen Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moschitti:Alessandro">Alessandro Moschitti</a></p>
<p>Abstract:
In this paper, we propose to use a sequence to sequence model for Named Entity Recognition (NER) and we explore the effectiveness of such model in a progressive NER setting  a Transfer Learning (TL) setting. We train an initial model on source data and transfer it to a model that can recognize new NE categories in the target data during a subsequent step, when the source data is no longer available. Our solution consists in: (i) to reshape and re-parametrize the output layer of the first learned model to enable the recognition of new NEs; (ii) to leave the rest of the architecture unchanged, such that it is initialized with parameters transferred from the initial model; and (iii) to fine tune the network on the target data. Most importantly, we design a new NER approach based on sequence to sequence (Seq2Seq) models, which can intuitively work better in our progressive setting. We compare our approach with a Bidirectional LSTM, which is a strong neural NER model. Our experiments show that the Seq2Seq model performs very well on the standard NER setting and it is more robust in the progressive setting. Our approach can recognize previously unseen NE categories while preserving the knowledge of the seen data.</p>
<p>Keywords:</p>
<h3 id="186. Responding E-commerce Product Questions via Exploiting QA Collections and Reviews.">186. Responding E-commerce Product Questions via Exploiting QA Collections and Reviews.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1186/">Paper Link</a>    Pages:2192-2203</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Qian">Qian Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lam:Wai">Wai Lam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zihao">Zihao Wang</a></p>
<p>Abstract:
Providing instant responses for product questions in E-commerce sites can significantly improve satisfaction of potential consumers. We propose a new framework for automatically responding product questions newly posed by users via exploiting existing QA collections and review collections in a coordinated manner. Our framework can return a ranked list of snippets serving as the automated response for a given question, where each snippet can be a sentence from reviews or an existing question-answer pair. One major subtask in our framework is question-based response review ranking. Learning for response review ranking is challenging since there is no labeled response review available. The collection of existing QA pairs are exploited as distant supervision for learning to rank responses. With proposed distant supervision paradigm, the learned response ranking model makes use of the knowledge in the QA pairs and the corresponding retrieved review lists. Extensive experiments on datasets collected from a real-world commercial E-commerce site demonstrate the effectiveness of our proposed framework.</p>
<p>Keywords:</p>
<h3 id="187. Aff2Vec: Affect-Enriched Distributional Word Representations.">187. Aff2Vec: Affect-Enriched Distributional Word Representations.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1187/">Paper Link</a>    Pages:2204-2218</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Khosla:Sopan">Sopan Khosla</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chhaya:Niyati">Niyati Chhaya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chawla:Kushal">Kushal Chawla</a></p>
<p>Abstract:
Human communication includes information, opinions and reactions. Reactions are often captured by the affective-messages in written as well as verbal communications. While there has been work in affect modeling and to some extent affective content generation, the area of affective word distributions is not well studied. Synsets and lexica capture semantic relationships across words. These models, however, lack in encoding affective or emotional word interpretations. Our proposed model, Aff2Vec, provides a method for enriched word embeddings that are representative of affective interpretations of words. Aff2Vec outperforms the state-of-the-art in intrinsic word-similarity tasks. Further, the use of Aff2Vec representations outperforms baseline embeddings in downstream natural language understanding tasks including sentiment analysis, personality detection, and frustration prediction.</p>
<p>Keywords:</p>
<h3 id="188. Aspect-based summarization of pros and cons in unstructured product reviews.">188. Aspect-based summarization of pros and cons in unstructured product reviews.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1188/">Paper Link</a>    Pages:2219-2229</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kunneman:Florian">Florian Kunneman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wubben:Sander">Sander Wubben</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bosch:Antal_van_den">Antal van den Bosch</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Krahmer:Emiel">Emiel Krahmer</a></p>
<p>Abstract:
We developed three systems for generating pros and cons summaries of product reviews. Automating this task eases the writing of product reviews, and offers readers quick access to the most important information. We compared SynPat, a system based on syntactic phrases selected on the basis of valence scores, against a neural-network-based system trained to map bag-of-words representations of reviews directly to pros and cons, and the same neural system trained on clusters of word-embedding encodings of similar pros and cons. We evaluated the systems in two ways: first on held-out reviews with gold-standard pros and cons, and second by asking human annotators to rate the systems output on relevance and completeness. In the second evaluation, the gold-standard pros and cons were assessed along with the system output. We find that the human-generated summaries are not deemed as significantly more relevant or complete than the SynPat systems; the latter are scored higher than the human-generated summaries on a precision metric. The neural approaches yield a lower performance in the human assessment, and are outperformed by the baseline.</p>
<p>Keywords:</p>
<h3 id="189. Learning Sentiment Composition from Sentiment Lexicons.">189. Learning Sentiment Composition from Sentiment Lexicons.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1189/">Paper Link</a>    Pages:2230-2241</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Toledo=Ronen:Orith">Orith Toledo-Ronen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bar=Haim:Roy">Roy Bar-Haim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Halfon:Alon">Alon Halfon</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jochim:Charles">Charles Jochim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Menczel:Amir">Amir Menczel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aharonov:Ranit">Ranit Aharonov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Slonim:Noam">Noam Slonim</a></p>
<p>Abstract:
Sentiment composition is a fundamental sentiment analysis problem. Previous work relied on manual rules and manually-created lexical resources such as negator lists, or learned a composition function from sentiment-annotated phrases or sentences. We propose a new approach for learning sentiment composition from a large, unlabeled corpus, which only requires a word-level sentiment lexicon for supervision. We automatically generate large sentiment lexicons of bigrams and unigrams, from which we induce a set of lexicons for a variety of sentiment composition processes. The effectiveness of our approach is confirmed through manual annotation, as well as sentiment classification experiments with both phrase-level and sentence-level benchmarks.</p>
<p>Keywords:</p>
<h3 id="190. Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from Modern Hebrew.">190. Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from Modern Hebrew.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1190/">Paper Link</a>    Pages:2242-2252</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Amram:Adam">Adam Amram</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Ben=David:Anat">Anat Ben-David</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsarfaty:Reut">Reut Tsarfaty</a></p>
<p>Abstract:
This paper empirically studies the effects of representation choices on neural sentiment analysis for Modern Hebrew, a morphologically rich language (MRL) for which no sentiment analyzer currently exists. We study two dimensions of representational choices: (i) the granularity of the input signal (token-based vs. morpheme-based), and (ii) the level of encoding of vocabulary items (string-based vs. character-based). We hypothesise that for MRLs, languages where multiple meaning-bearing elements may be carried by a single space-delimited token, these choices will have measurable effects on task perfromance, and that these effects may vary for different architectural designs  fully-connected, convolutional or recurrent. Specifically, we hypothesize that morpheme-based representations will have advantages in terms of their generalization capacity and task accuracy, due to their better OOV coverage. To empirically study these effects, we develop a new sentiment analysis benchmark for Hebrew, based on 12K social media comments, and provide two instances of these data: in token-based and morpheme-based settings. Our experiments show that representation choices empirical effects vary with architecture type. While fully-connected and convolutional networks slightly prefer token-based settings, RNNs benefit from a morpheme-based representation, in accord with the hypothesis that explicit morphological information may help generalize. Our endeavour also delivers the first state-of-the-art broad-coverage sentiment analyzer for Hebrew, with over 89% accuracy, alongside an established benchmark to further study the effects of linguistic representation choices on neural networks task performance.</p>
<p>Keywords:</p>
<h3 id="191. Scoring and Classifying Implicit Positive Interpretations: A Challenge of Class Imbalance.">191. Scoring and Classifying Implicit Positive Interpretations: A Challenge of Class Imbalance.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1191/">Paper Link</a>    Pages:2253-2264</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Son:Chantal_van">Chantal van Son</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morante:Roser">Roser Morante</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aroyo:Lora">Lora Aroyo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vossen:Piek">Piek Vossen</a></p>
<p>Abstract:
This paper reports on a reimplementation of a system on detecting implicit positive meaning from negated statements. In the original regression experiment, different positive interpretations per negation are scored according to their likelihood. We convert the scores to classes and report our results on both the regression and classification tasks. We show that a baseline taking the mean score or most frequent class is hard to beat because of class imbalance in the dataset. Our error analysis indicates that an approach that takes the information structure into account (i.e. which information is new or contrastive) may be promising, which requires looking beyond the syntactic and semantic characteristics of negated statements.</p>
<p>Keywords:</p>
<h3 id="192. Exploratory Neural Relation Classification for Domain Knowledge Acquisition.">192. Exploratory Neural Relation Classification for Domain Knowledge Acquisition.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1192/">Paper Link</a>    Pages:2265-2276</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fan:Yan">Yan Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0001:Chengyu">Chengyu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Xiaofeng">Xiaofeng He</a></p>
<p>Abstract:
The state-of-the-art methods for relation classification are primarily based on deep neural net- works. This kind of supervised learning method suffers from not only limited training data, but also the large number of low-frequency relations in specific domains. In this paper, we propose the task of exploratory relation classification for domain knowledge harvesting. The goal is to learn a classifier on pre-defined relations and discover new relations expressed in texts. A dynamically structured neural network is introduced to classify entity pairs to a continuously expanded relation set. We further propose the similarity sensitive Chinese restaurant process to discover new relations. Experiments conducted on a large corpus show the effectiveness of our neural network, while new relations are discovered with high precision and recall.</p>
<p>Keywords:</p>
<h3 id="193. Who is Killed by Police: Introducing Supervised Attention for Hierarchical LSTMs.">193. Who is Killed by Police: Introducing Supervised Attention for Hierarchical LSTMs.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1193/">Paper Link</a>    Pages:2277-2287</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen_0004:Minh">Minh Nguyen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Thien">Thien Nguyen</a></p>
<p>Abstract:
Finding names of people killed by police has become increasingly important as police shootings get more and more public attention (police killing detection). Unfortunately, there has been not much work in the literature addressing this problem. The early work in this field (Keith etal., 2017) proposed a distant supervision framework based on Expectation Maximization (EM) to deal with the multiple appearances of the names in documents. However, such EM-based framework cannot take full advantages of deep learning models, necessitating the use of handdesigned features to improve the detection performance. In this work, we present a novel deep learning method to solve the problem of police killing recognition. The proposed method relies on hierarchical LSTMs to model the multiple sentences that contain the person names of interests, and introduce supervised attention mechanisms based on semantical word lists and dependency trees to upweight the important contextual words. Our experiments demonstrate the benefits of the proposed model and yield the state-of-the-art performance for police killing detection.</p>
<p>Keywords:</p>
<h3 id="194. Open Information Extraction from Conjunctive Sentences.">194. Open Information Extraction from Conjunctive Sentences.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1194/">Paper Link</a>    Pages:2288-2299</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Saha:Swarnadeep">Swarnadeep Saha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mausam:">Mausam</a></p>
<p>Abstract:
We develop CALM, a coordination analyzer that improves upon the conjuncts identified from dependency parses. It uses a language model based scoring and several linguistic constraints to search over hierarchical conjunct boundaries (for nested coordination). By splitting a conjunctive sentence around these conjuncts, CALM outputs several simple sentences. We demonstrate the value of our coordination analyzer in the end task of Open Information Extraction (Open IE). State-of-the-art Open IE systems lose substantial yield due to ineffective processing of conjunctive sentences. Our Open IE system, CALMIE, performs extraction over the simple sentences identified by CALM to obtain up to 1.8x yield with a moderate increase in precision compared to extractions from original sentences.</p>
<p>Keywords:</p>
<h3 id="195. Graphene: Semantically-Linked Propositions in Open Information Extraction.">195. Graphene: Semantically-Linked Propositions in Open Information Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1195/">Paper Link</a>    Pages:2300-2311</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cetto:Matthias">Matthias Cetto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Niklaus:Christina">Christina Niklaus</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Freitas:Andr=eacute=">Andr Freitas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Handschuh:Siegfried">Siegfried Handschuh</a></p>
<p>Abstract:
We present an Open Information Extraction (IE) approach that uses a two-layered transformation stage consisting of a clausal disembedding layer and a phrasal disembedding layer, together with rhetorical relation identification. In that way, we convert sentences that present a complex linguistic structure into simplified, syntactically sound sentences, from which we can extract propositions that are represented in a two-layered hierarchy in the form of core relational tuples and accompanying contextual information which are semantically linked via rhetorical relations. In a comparative evaluation, we demonstrate that our reference implementation Graphene outperforms state-of-the-art Open IE systems in the construction of correct n-ary predicate-argument structures. Moreover, we show that existing Open IE approaches can benefit from the transformation process of our framework.</p>
<p>Keywords:</p>
<h3 id="196. An Exploration of Three Lightly-supervised Representation Learning Approaches for Named Entity Classification.">196. An Exploration of Three Lightly-supervised Representation Learning Approaches for Named Entity Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1196/">Paper Link</a>    Pages:2312-2324</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nagesh:Ajay">Ajay Nagesh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Surdeanu:Mihai">Mihai Surdeanu</a></p>
<p>Abstract:
Several semi-supervised representation learning methods have been proposed recently that mitigate the drawbacks of traditional bootstrapping: they reduce the amount of semantic drift introduced by iterative approaches through one-shot learning; others address the sparsity of data through the learning of custom, dense representation for the information modeled. In this work, we are the first to adapt three of these methods, most of which have been originally proposed for image processing, to an information extraction task, specifically, named entity classification. Further, we perform a rigorous comparative analysis on two distinct datasets. Our analysis yields several important observations. First, all representation learning methods outperform state-of-the-art semi-supervised methods that do not rely on representation learning. To the best of our knowledge, we report the latest state-of-the-art results on the semi-supervised named entity classification task. Second, one-shot learning methods clearly outperform iterative representation learning approaches. Lastly, one of the best performers relies on the mean teacher framework (Tarvainen and Valpola, 2017), a simple teacher/student approach that is independent of the underlying task-specific model.</p>
<p>Keywords:</p>
<h3 id="197. Multimodal Grounding for Language Processing.">197. Multimodal Grounding for Language Processing.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1197/">Paper Link</a>    Pages:2325-2339</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Beinborn:Lisa">Lisa Beinborn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Botschen:Teresa">Teresa Botschen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
This survey discusses how recent developments in multimodal processing facilitate conceptual grounding of language. We categorize the information flow in multimodal processing with respect to cognitive models of human information processing and analyze different methods for combining multimodal representations. Based on this methodological inventory, we discuss the benefit of multimodal grounding for a variety of language processing tasks and the challenges that arise. We particularly focus on multimodal grounding of verbs which play a crucial role for the compositional power of language.</p>
<p>Keywords:</p>
<h3 id="198. Stress Test Evaluation for Natural Language Inference.">198. Stress Test Evaluation for Natural Language Inference.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1198/">Paper Link</a>    Pages:2340-2353</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Naik:Aakanksha">Aakanksha Naik</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ravichander:Abhilasha">Abhilasha Ravichander</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sadeh:Norman_M=">Norman M. Sadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ros=eacute=:Carolyn_Penstein">Carolyn Penstein Ros</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a></p>
<p>Abstract:
Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed stress tests that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area.</p>
<p>Keywords:</p>
<h3 id="199. Grounded Textual Entailment.">199. Grounded Textual Entailment.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1199/">Paper Link</a>    Pages:2354-2368</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vu:Hoa_Trong">Hoa Trong Vu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Greco_0002:Claudio">Claudio Greco</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Erofeeva:Aliia">Aliia Erofeeva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jafaritazehjan:Somayeh">Somayeh Jafaritazehjan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Linders:Guido">Guido Linders</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tanti:Marc">Marc Tanti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Testoni:Alberto">Alberto Testoni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bernardi:Raffaella">Raffaella Bernardi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gatt:Albert">Albert Gatt</a></p>
<p>Abstract:
Capturing semantic relations between sentences, such as entailment, is a long-standing challenge for computational semantics. Logic-based models analyse entailment in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant world or situation). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare blind and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing grounding in an optimal fashion.</p>
<p>Keywords:</p>
<h3 id="200. Recurrent One-Hop Predictions for Reasoning over Knowledge Graphs.">200. Recurrent One-Hop Predictions for Reasoning over Knowledge Graphs.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1200/">Paper Link</a>    Pages:2369-2378</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yin_0001:Wenpeng">Wenpeng Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yaghoobzadeh:Yadollah">Yadollah Yaghoobzadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schtze</a></p>
<p>Abstract:
Large scale knowledge graphs (KGs) such as Freebase are generally incomplete. Reasoning over multi-hop (mh) KG paths is thus an important capability that is needed for question answering or other NLP tasks that require knowledge about the world. mh-KG reasoning includes diverse scenarios, e.g., given a head entity and a relation path, predict the tail entity; or given two entities connected by some relation paths, predict the unknown relation between them. We present ROPs, recurrent one-hop predictors, that predict entities at each step of mh-KB paths by using recurrent neural networks and vector representations of entities and relations, with two benefits: (i) modeling mh-paths of arbitrary lengths while updating the entity and relation representations by the training signal at each step; (ii) handling different types of mh-KG reasoning in a unified framework. Our models show state-of-the-art for two important multi-hop KG reasoning tasks: Knowledge Base Completion and Path Query Answering.</p>
<p>Keywords:</p>
<h3 id="201. Hybrid Attention based Multimodal Network for Spoken Language Classification.">201. Hybrid Attention based Multimodal Network for Spoken Language Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1201/">Paper Link</a>    Pages:2379-2390</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Yue">Yue Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Kangning">Kangning Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Shiyu">Shiyu Fu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Shuhong">Shuhong Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xinyu">Xinyu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marsic:Ivan">Ivan Marsic</a></p>
<p>Abstract:
We examine the utility of linguistic content and vocal characteristics for multimodal deep learning in human spoken language understanding. We present a deep multimodal network with both feature attention and modality attention to classify utterance-level speech data. The proposed hybrid attention architecture helps the system focus on learning informative representations for both modality-specific feature extraction and model fusion. The experimental results show that our system achieves state-of-the-art or competitive results on three published multimodal datasets. We also demonstrated the effectiveness and generalization of our system on a medical speech dataset from an actual trauma scenario. Furthermore, we provided a detailed comparison and analysis of traditional approaches and deep learning methods on both feature extraction and fusion.</p>
<p>Keywords:</p>
<h3 id="202. Exploring the Influence of Spelling Errors on Lexical Variation Measures.">202. Exploring the Influence of Spelling Errors on Lexical Variation Measures.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1202/">Paper Link</a>    Pages:2391-2398</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nagata:Ryo">Ryo Nagata</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sato:Taisei">Taisei Sato</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Takamura:Hiroya">Hiroya Takamura</a></p>
<p>Abstract:
This paper explores the influence of spelling errors on lexical variation measures. Lexical richness measures such as Type-Token Ration (TTR) and Yules K are often used for learner English analysis and assessment. When applied to learner English, however, they can be unreliable because of the spelling errors appearing in it. Namely, they are, directly or indirectly, based on the counts of distinct word types, and spelling errors undesirably increase the number of distinct words. This paper introduces and examines the hypothesis that lexical richness measures become unstable in learner English because of spelling errors. Specifically, it tests the hypothesis on English learner corpora of three groups (middle school, high school, and college students). To be precise, it estimates the difference in TTR and Yules K caused by spelling errors, by calculating their values before and after spelling errors are manually corrected. Furthermore, it examines the results theoretically and empirically to deepen the understanding of the influence of spelling errors on them.</p>
<p>Keywords:</p>
<h3 id="203. Stance Detection with Hierarchical Attention Network.">203. Stance Detection with Hierarchical Attention Network.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1203/">Paper Link</a>    Pages:2399-2409</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Qingying">Qingying Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhongqing">Zhongqing Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Qiaoming">Qiaoming Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>Abstract:
Stance detection aims to assign a stance label (for or against) to a post toward a specific target. Recently, there is a growing interest in using neural models to detect stance of documents. Most of these works model the sequence of words to learn document representation. However, much linguistic information, such as polarity and arguments of the document, is correlated with the stance of the document, and can inspire us to explore the stance. Hence, we present a neural model to fully employ various linguistic information to construct the document representation. In addition, since the influences of different linguistic information are different, we propose a hierarchical attention network to weigh the importance of various linguistic information, and learn the mutual attention between the document and the linguistic information. The experimental results on two datasets demonstrate the effectiveness of the proposed hierarchical attention neural model.</p>
<p>Keywords:</p>
<h3 id="204. Correcting Chinese Word Usage Errors for Learning Chinese as a Second Language.">204. Correcting Chinese Word Usage Errors for Learning Chinese as a Second Language.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1204/">Paper Link</a>    Pages:2410-2422</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shiue:Yow=Ting">Yow-Ting Shiue</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Hen=Hsen">Hen-Hsen Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Hsin=Hsi">Hsin-Hsi Chen</a></p>
<p>Abstract:
With more and more people around the world learning Chinese as a second language, the need of Chinese error correction tools is increasing. In the HSK dynamic composition corpus, word usage error (WUE) is the most common error type. In this paper, we build a neural network model that considers both target erroneous token and context to generate a correction vector and compare it against a candidate vocabulary to propose suitable corrections. To deal with potential alternative corrections, the top five proposed candidates are judged by native Chinese speakers. For more than 91% of the cases, our system can propose at least one acceptable correction within a list of five candidates. To the best of our knowledge, this is the first research addressing general-type Chinese WUE correction. Our system can help non-native Chinese learners revise their sentences by themselves.</p>
<p>Keywords:</p>
<h3 id="205. Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations.">205. Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1205/">Paper Link</a>    Pages:2423-2436</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lengerich:Benjamin_J=">Benjamin J. Lengerich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Maas:Andrew_L=">Andrew L. Maas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Potts:Christopher">Christopher Potts</a></p>
<p>Abstract:
Knowledge graphs are a versatile framework to encode richly structured data relationships, but it can be challenging to combine these graphs with unstructured data. Methods for retrofitting pre-trained entity representations to the structure of a knowledge graph typically assume that entities are embedded in a connected space and that relations imply similarity. However, useful knowledge graphs often contain diverse entities and relations (with potentially disjoint underlying corpora) which do not accord with these assumptions. To overcome these limitations, we present Functional Retrofitting, a framework that generalizes current retrofitting methods by explicitly modeling pairwise relations. Our framework can directly incorporate a variety of pairwise penalty functions previously developed for knowledge graph completion. Further, it allows users to encode, learn, and extract information about relation semantics. We present both linear and neural instantiations of the framework. Functional Retrofitting significantly outperforms existing retrofitting methods on complex knowledge graphs and loses no accuracy on simpler graphs (in which relations do imply similarity). Finally, we demonstrate the utility of the framework by predicting new drugdisease treatment pairs in a large, complex health knowledge graph.</p>
<p>Keywords:</p>
<h3 id="206. Context-Sensitive Generation of Open-Domain Conversational Responses.">206. Context-Sensitive Generation of Open-Domain Conversational Responses.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1206/">Paper Link</a>    Pages:2437-2447</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0003:Weinan">Weinan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cui:Yiming">Yiming Cui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yifa">Yifa Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Qingfu">Qingfu Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Lingzhi">Lingzhi Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Lianqiang">Lianqiang Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>Abstract:
Despite the success of existing works on single-turn conversation generation, taking the coherence in consideration, human conversing is actually a context-sensitive process. Inspired by the existing studies, this paper proposed the static and dynamic attention based approaches for context-sensitive generation of open-domain conversational responses. Experimental results on two public datasets show that the proposed static attention based approach outperforms all the baselines on automatic and human evaluation.</p>
<p>Keywords:</p>
<h3 id="207. A LSTM Approach with Sub-Word Embeddings for Mongolian Phrase Break Prediction.">207. A LSTM Approach with Sub-Word Embeddings for Mongolian Phrase Break Prediction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1207/">Paper Link</a>    Pages:2448-2455</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0008:Rui">Rui Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bao:Feilong">Feilong Bao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Guanglai">Guanglai Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0031:Hui">Hui Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yonghe">Yonghe Wang</a></p>
<p>Abstract:
In this paper, we first utilize the word embedding that focuses on sub-word units to the Mongolian Phrase Break (PB) prediction task by using Long-Short-Term-Memory (LSTM) model. Mongolian is an agglutinative language. Each root can be followed by several suffixes to form probably millions of words, but the existing Mongolian corpus is not enough to build a robust entire word embedding, thus it suffers a serious data sparse problem and brings a great difficulty for Mongolian PB prediction. To solve this problem, we look at sub-word units in Mongolian word, and encode their information to a meaningful representation, then fed it to LSTM to decode the best corresponding PB label. Experimental results show that the proposed model significantly outperforms traditional CRF model using manually features and obtains 7.49% F-Measure gain.</p>
<p>Keywords:</p>
<h3 id="208. Synonymy in Bilingual Context: The CzEngClass Lexicon.">208. Synonymy in Bilingual Context: The CzEngClass Lexicon.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1208/">Paper Link</a>    Pages:2456-2469</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/u/Uresov=aacute=:Zdenka">Zdenka Uresov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fuc=iacute=kov=aacute=:Eva">Eva Fuckov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hajicov=aacute=:Eva">Eva Hajicov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hajic:Jan">Jan Hajic</a></p>
<p>Abstract:
This paper describes CzEngClass, a bilingual lexical resource being built to investigate verbal synonymy in bilingual context and to relate semantic roles common to one synonym class to verb arguments (verb valency). In addition, the resource is linked to existing resources with the same of a similar aim: English and Czech WordNet, FrameNet, PropBank, VerbNet (SemLink), and valency lexicons for Czech and English (PDT-Vallex, Vallex, and EngVallex). There are several goals of this work and resource: (a) to provide gold standard data for automatic experiments in the future (such as automatic discovery of synonym classes, word sense disambiguation, assignment of classes to occurrences of verbs in text, coreferential linking of verb and event arguments in text, etc.), (b) to build a core (bilingual) lexicon linked to existing resources, for comparative studies and possibly for training automatic tools, and (c) to enrich the annotation of a parallel treebank, the Prague Czech English Dependency Treebank, which so far contained valency annotation but has not linked synonymous senses of verbs together. The method used for extracting the synonym classes is a semi-automatic process with a substantial amount of manual work during filtering, role assignment to classes and individual Class members arguments, and linking to the external lexical resources. We present the first version with 200 classes (about 1800 verbs) and evaluate interannotator agreement using several metrics.</p>
<p>Keywords:</p>
<h3 id="209. Convolutional Neural Network for Universal Sentence Embeddings.">209. Convolutional Neural Network for Universal Sentence Embeddings.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1209/">Paper Link</a>    Pages:2470-2481</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiao:Xiaoqi">Xiaoqi Jiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0001:Fang">Fang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng_0001:Dan">Dan Feng</a></p>
<p>Abstract:
This paper proposes a simple CNN model for creating general-purpose sentence embeddings that can transfer easily across domains and can also act as effective initialization for downstream tasks. Recently, averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings. However, these models represent a sentence, only in terms of features of words or uni-grams in it. In contrast, our model (CSE) utilizes both features of words and n-grams to encode sentences, which is actually a generalization of these bag-of-words models. The extensive experiments demonstrate that CSE performs better than average models in transfer learning setting and exceeds the state of the art in supervised learning setting by initializing the parameters with the pre-trained sentence embeddings.</p>
<p>Keywords:</p>
<h3 id="210. Rich Character-Level Information for Korean Morphological Analysis and Part-of-Speech Tagging.">210. Rich Character-Level Information for Korean Morphological Analysis and Part-of-Speech Tagging.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1210/">Paper Link</a>    Pages:2482-2492</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Matteson:Andrew">Andrew Matteson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Chanhee">Chanhee Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Young=Bum">Young-Bum Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lim:Heuiseok">Heuiseok Lim</a></p>
<p>Abstract:
Due to the fact that Korean is a highly agglutinative, character-rich language, previous work on Korean morphological analysis typically employs the use of sub-character features known as graphemes or otherwise utilizes comprehensive prior linguistic knowledge (i.e., a dictionary of known morphological transformation forms, or actions). These models have been created with the assumption that character-level, dictionary-less morphological analysis was intractable due to the number of actions required. We present, in this study, a multi-stage action-based model that can perform morphological transformation and part-of-speech tagging using arbitrary units of input and apply it to the case of character-level Korean morphological analysis. Among models that do not employ prior linguistic knowledge, we achieve state-of-the-art word and sentence-level tagging accuracy with the Sejong Korean corpus using our proposed data-driven Bi-LSTM model.</p>
<p>Keywords:</p>
<h3 id="211. Why does PairDiff work? - A Mathematical Analysis of Bilinear Relational Compositional Operators for Analogy Detection.">211. Why does PairDiff work? - A Mathematical Analysis of Bilinear Relational Compositional Operators for Analogy Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1211/">Paper Link</a>    Pages:2493-2504</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hakami:Huda">Huda Hakami</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hayashi:Kohei">Kohei Hayashi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bollegala:Danushka">Danushka Bollegala</a></p>
<p>Abstract:
Representing the semantic relations that exist between two given words (or entities) is an important first step in a wide-range of NLP applications such as analogical reasoning, knowledge base completion and relational information retrieval. A simple, yet surprisingly accurate method for representing a relation between two words is to compute the vector offset (PairDiff) between their corresponding word embeddings. Despite the empirical success, it remains unclear as to whether PairDiff is the best operator for obtaining a relational representation from word embeddings. We conduct a theoretical analysis of generalised bilinear operators that can be used to measure the l2 relational distance between two word-pairs. We show that, if the word embed- dings are standardised and uncorrelated, such an operator will be independent of bilinear terms, and can be simplified to a linear form, where PairDiff is a special case. For numerous word embedding types, we empirically verify the uncorrelation assumption, demonstrating the general applicability of our theoretical result. Moreover, we experimentally discover PairDiff from the bilinear relational compositional operator on several benchmark analogy datasets.</p>
<p>Keywords:</p>
<h3 id="212. Real-time Change Point Detection using On-line Topic Models.">212. Real-time Change Point Detection using On-line Topic Models.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1212/">Paper Link</a>    Pages:2505-2515</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yunli">Yunli Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goutte:Cyril">Cyril Goutte</a></p>
<p>Abstract:
Detecting changes within an unfolding event in real time from news articles or social media enables to react promptly to serious issues in public safety, public health or natural disasters. In this study, we use on-line Latent Dirichlet Allocation (LDA) to model shifts in topics, and apply on-line change point detection (CPD) algorithms to detect when significant changes happen. We describe an on-line Bayesian change point detection algorithm that we use to detect topic changes from on-line LDA output. Extensive experiments on social media data and news articles show the benefits of on-line LDA versus standard LDA, and of on-line change point detection compared to off-line algorithms. This yields F-scores up to 52% on the detection of significant real-life changes from these document streams.</p>
<p>Keywords:</p>
<h3 id="213. Automatically Creating a Lexicon of Verbal Polarity Shifters: Mono- and Cross-lingual Methods for German.">213. Automatically Creating a Lexicon of Verbal Polarity Shifters: Mono- and Cross-lingual Methods for German.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1213/">Paper Link</a>    Pages:2516-2528</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schulder:Marc">Marc Schulder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wiegand:Michael">Michael Wiegand</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruppenhofer:Josef">Josef Ruppenhofer</a></p>
<p>Abstract:
In this paper we use methods for creating a large lexicon of verbal polarity shifters and apply them to German. Polarity shifters are content words that can move the polarity of a phrase towards its opposite, such as the verb abandon in abandon all hope. This is similar to how negation words like not can influence polarity. Both shifters and negation are required for high precision sentiment analysis. Lists of negation words are available for many languages, but the only language for which a sizable lexicon of verbal polarity shifters exists is English. This lexicon was created by bootstrapping a sample of annotated verbs with a supervised classifier that uses a set of data- and resource-driven features. We reproduce and adapt this approach to create a German lexicon of verbal polarity shifters. Thereby, we confirm that the approach works for multiple languages. We further improve classification by leveraging cross-lingual information from the English shifter lexicon. Using this improved approach, we bootstrap a large number of German verbal polarity shifters, reducing the annotation effort drastically. The resulting German lexicon of verbal polarity shifters is made publicly available.</p>
<p>Keywords:</p>
<h3 id="214. Part-of-Speech Tagging on an Endangered Language: a Parallel Griko-Italian Resource.">214. Part-of-Speech Tagging on an Endangered Language: a Parallel Griko-Italian Resource.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1214/">Paper Link</a>    Pages:2529-2539</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Anastasopoulos:Antonios">Antonios Anastasopoulos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lekakou:Marika">Marika Lekakou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Quer:Josep">Josep Quer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zimianiti:Eleni">Eleni Zimianiti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/DeBenedetto:Justin">Justin DeBenedetto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chiang_0001:David">David Chiang</a></p>
<p>Abstract:
Most work on part-of-speech (POS) tagging is focused on high resource languages, or examines low-resource and active learning settings through simulated studies. We evaluate POS tagging techniques on an actual endangered language, Griko. We present a resource that contains 114 narratives in Griko, along with sentence-level translations in Italian, and provides gold annotations for the test set. Based on a previously collected small corpus, we investigate several traditional methods, as well as methods that take advantage of monolingual data or project cross-lingual POS tags. We show that the combination of a semi-supervised method with cross-lingual transfer is more appropriate for this extremely challenging setting, with the best tagger achieving an accuracy of 72.9%. With an applied active learning scheme, which we use to collect sentence-level annotations over the test set, we achieve improvements of more than 21 percentage points.</p>
<p>Keywords:</p>
<h3 id="215. One vs. Many QA Matching with both Word-level and Sentence-level Attention Network.">215. One vs. Many QA Matching with both Word-level and Sentence-level Attention Network.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1215/">Paper Link</a>    Pages:2540-2550</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Lu">Lu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Shoushan">Shoushan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Changlong">Changlong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xiaozhong">Xiaozhong Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>Abstract:
Question-Answer (QA) matching is a fundamental task in the Natural Language Processing community. In this paper, we first build a novel QA matching corpus with informal text which is collected from a product reviewing website. Then, we propose a novel QA matching approach, namely One vs. Many Matching, which aims to address the novel scenario where one question sentence often has an answer with multiple sentences. Furthermore, we improve our matching approach by employing both word-level and sentence-level attentions for solving the noisy problem in the informal text. Empirical studies demonstrate the effectiveness of the proposed approach to question-answer matching.</p>
<p>Keywords:</p>
<h3 id="216. Learning to Generate Word Representations using Subword Information.">216. Learning to Generate Word Representations using Subword Information.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1216/">Paper Link</a>    Pages:2551-2561</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Yeachan">Yeachan Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Kang=Min">Kang-Min Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Ji=Min">Ji-Min Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee_0001:SangKeun">SangKeun Lee</a></p>
<p>Abstract:
Distributed representations of words play a major role in the field of natural language processing by encoding semantic and syntactic information of words. However, most existing works on learning word representations typically regard words as individual atomic units and thus are blind to subword information in words. This further gives rise to a difficulty in representing out-of-vocabulary (OOV) words. In this paper, we present a character-based word representation approach to deal with this limitation. The proposed model learns to generate word representations from characters. In our model, we employ a convolutional neural network and a highway network over characters to extract salient features effectively. Unlike previous models that learn word representations from a large corpus, we take a set of pre-trained word embeddings and generalize it to word entries, including OOV words. To demonstrate the efficacy of the proposed model, we perform both an intrinsic and an extrinsic task which are word similarity and language modeling, respectively. Experimental results show clearly that the proposed model significantly outperforms strong baseline models that regard words or their subwords as atomic units. For example, we achieve as much as 18.5% improvement on average in perplexity for morphologically rich languages compared to strong baselines in the language modeling task.</p>
<p>Keywords:</p>
<h3 id="217. Urdu Word Segmentation using Conditional Random Fields (CRFs).">217. Urdu Word Segmentation using Conditional Random Fields (CRFs).</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1217/">Paper Link</a>    Pages:2562-2569</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zia:Haris_Bin">Haris Bin Zia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Raza:Agha_Ali">Agha Ali Raza</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Athar:Awais">Awais Athar</a></p>
<p>Abstract:
State-of-the-art Natural Language Processing algorithms rely heavily on efficient word segmentation. Urdu is amongst languages for which word segmentation is a complex task as it exhibits space omission as well as space insertion issues. This is partly due to the Arabic script which although cursive in nature, consists of characters that have inherent joining and non-joining attributes regardless of word boundary. This paper presents a word segmentation system for Urdu which uses a Conditional Random Field sequence modeler with orthographic, linguistic and morphological features. Our proposed model automatically learns to predict white space as word boundary as well as Zero Width Non-Joiner (ZWNJ) as sub-word boundary. Using a manually annotated corpus, our model achieves F1 score of 0.97 for word boundary identification and 0.85 for sub-word boundary identification tasks. We have made our code and corpus publicly available to make our results reproducible.</p>
<p>Keywords:</p>
<h3 id="218. ReSyf: a French lexicon with ranked synonyms.">218. ReSyf: a French lexicon with ranked synonyms.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1218/">Paper Link</a>    Pages:2570-2581</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Billami:Mokhtar_Boumedyen">Mokhtar Boumedyen Billami</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fran=ccedil=ois:Thomas">Thomas Franois</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gala:Nuria">Nuria Gala</a></p>
<p>Abstract:
In this article, we present ReSyf, a lexical resource of monolingual synonyms ranked according to their difficulty to be read and understood by native learners of French. The synonyms come from an existing lexical network and they have been semantically disambiguated and refined. A ranking algorithm, based on a wide range of linguistic features and validated through an evaluation campaign with human annotators, automatically sorts the synonyms corresponding to a given word sense by reading difficulty. ReSyf is freely available and will be integrated into a web platform for reading assistance. It can also be applied to perform lexical simplification of French texts.</p>
<p>Keywords:</p>
<h3 id="219. If you've seen some, you've seen them all: Identifying variants of multiword expressions.">219. If you've seen some, you've seen them all: Identifying variants of multiword expressions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1219/">Paper Link</a>    Pages:2582-2594</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pasquer:Caroline">Caroline Pasquer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Savary:Agata">Agata Savary</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ramisch:Carlos">Carlos Ramisch</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Antoine:Jean=Yves">Jean-Yves Antoine</a></p>
<p>Abstract:
Multiword expressions, especially verbal ones (VMWEs), show idiosyncratic variability, which is challenging for NLP applications, hence the need for VMWE identification. We focus on the task of variant identification, i.e. identifying variants of previously seen VMWEs, whatever their surface form. We model the problem as a classification task. Syntactic subtrees with previously seen combinations of lemmas are first extracted, and then classified on the basis of features relevant to morpho-syntactic variation of VMWEs. Feature values are both absolute, i.e. hold for a particular VMWE candidate, and relative, i.e. based on comparing a candidate with previously seen VMWEs. This approach outperforms a baseline by 4 percent points of F-measure on a French corpus.</p>
<p>Keywords:</p>
<h3 id="220. Learning Multilingual Topics from Incomparable Corpora.">220. Learning Multilingual Topics from Incomparable Corpora.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1220/">Paper Link</a>    Pages:2595-2609</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hao:Shudong">Shudong Hao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Paul:Michael_J=">Michael J. Paul</a></p>
<p>Abstract:
Multilingual topic models enable crosslingual tasks by extracting consistent topics from multilingual corpora. Most models require parallel or comparable training corpora, which limits their ability to generalize. In this paper, we first demystify the knowledge transfer mechanism behind multilingual topic models by defining an alternative but equivalent formulation. Based on this analysis, we then relax the assumption of training data required by most existing models, creating a model that only requires a dictionary for training. Experiments show that our new method effectively learns coherent multilingual topics from partially and fully incomparable corpora with limited amounts of dictionary resources.</p>
<p>Keywords:</p>
<h3 id="221. Using Word Embeddings for Unsupervised Acronym Disambiguation.">221. Using Word Embeddings for Unsupervised Acronym Disambiguation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1221/">Paper Link</a>    Pages:2610-2619</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Charbonnier:Jean">Jean Charbonnier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wartena:Christian">Christian Wartena</a></p>
<p>Abstract:
Scientific papers from all disciplines contain many abbreviations and acronyms. In many cases these acronyms are ambiguous. We present a method to choose the contextual correct definition of an acronym that does not require training for each acronym and thus can be applied to a large number of different acronyms with only few instances. We constructed a set of 19,954 examples of 4,365 ambiguous acronyms from image captions in scientific papers along with their contextually correct definition from different domains. We learn word embeddings for all words in the corpus and compare the averaged context vector of the words in the expansion of an acronym with the weighted average vector of the words in the context of the acronym. We show that this method clearly outperforms (classical) cosine similarity. Furthermore, we show that word embeddings learned from a 1 billion word corpus of scientific texts outperform word embeddings learned on much large general corpora.</p>
<p>Keywords:</p>
<h3 id="222. Indigenous language technologies in Canada: Assessment, challenges, and successes.">222. Indigenous language technologies in Canada: Assessment, challenges, and successes.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1222/">Paper Link</a>    Pages:2620-2632</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Littell:Patrick">Patrick Littell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kazantseva:Anna">Anna Kazantseva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kuhn:Roland">Roland Kuhn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pine:Aidan">Aidan Pine</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Arppe:Antti">Antti Arppe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cox:Christopher">Christopher Cox</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Junker:Marie=Odile">Marie-Odile Junker</a></p>
<p>Abstract:
In this article, we discuss which text, speech, and image technologies have been developed, and would be feasible to develop, for the approximately 60 Indigenous languages spoken in Canada. In particular, we concentrate on technologies that may be feasible to develop for most or all of these languages, not just those that may be feasible for the few most-resourced of these. We assess past achievements and consider future horizons for Indigenous language transliteration, text prediction, spell-checking, approximate search, machine translation, speech recognition, speaker diarization, speech synthesis, optical character recognition, and computer-aided language learning.</p>
<p>Keywords:</p>
<h3 id="223. Pluralizing Nouns across Agglutinating Bantu Languages.">223. Pluralizing Nouns across Agglutinating Bantu Languages.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1223/">Paper Link</a>    Pages:2633-2643</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Byamugisha:Joan">Joan Byamugisha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Keet:C=_Maria">C. Maria Keet</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/DeRenzi:Brian">Brian DeRenzi</a></p>
<p>Abstract:
Text generation may require the pluralization of nouns, such as in context-sensitive user interfaces and in natural language generation more broadly. While this has been solved for the widely-used languages, this is still a challenge for the languages in the Bantu language family. Pluralization results obtained for isiZulu and Runyankore showed there were similarities in approach, including the need to combine syntax with semantics, despite belonging to different language zones. This suggests that bootstrapping and generalizability might be feasible. We investigated this systematically for seven languages across three different Guthrie language zones. The first outcome is that Meinhofs 1948 specification of the noun classes are indeed inadequate for computational purposes for all examined languages, due to non-determinism in prefixes, and we thus redefined the characteristic noun class tables of 29 noun classes into 53. The second main result is that the generic pluralizer achieved over 93% accuracy in coverage testing and over 94% on a random sample. This is comparable to the language-specific isiZulu and Runyankore pluralizers.</p>
<p>Keywords:</p>
<h3 id="224. Automatically Extracting Qualia Relations for the Rich Event Ontology.">224. Automatically Extracting Qualia Relations for the Rich Event Ontology.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1224/">Paper Link</a>    Pages:2644-2652</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kazeminejad:Ghazaleh">Ghazaleh Kazeminejad</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bonial:Claire">Claire Bonial</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brown:Susan_Windisch">Susan Windisch Brown</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Palmer:Martha">Martha Palmer</a></p>
<p>Abstract:
Commonsense, real-world knowledge about the events that entities or things in the world are typically involved in, as well as part-whole relationships, is valuable for allowing computational systems to draw everyday inferences about the world. Here, we focus on automatically extracting information about (1) the events that typically bring about certain entities (origins), (2) the events that are the typical functions of entities, and (3) part-whole relationships in entities. These correspond to the agentive, telic and constitutive qualia central to the Generative Lexicon. We describe our motivations and methods for extracting these qualia relations from the Suggested Upper Merged Ontology (SUMO) and show that human annotators overwhelmingly find the information extracted to be reasonable. Because ontologies provide a way of structuring this information and making it accessible to agents and computational systems generally, efforts are underway to incorporate the extracted information to an ontology hub of Natural Language Processing semantic role labeling resources, the Rich Event Ontology.</p>
<p>Keywords:</p>
<h3 id="225. SeVeN: Augmenting Word Embeddings with Unsupervised Relation Vectors.">225. SeVeN: Augmenting Word Embeddings with Unsupervised Relation Vectors.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1225/">Paper Link</a>    Pages:2653-2665</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Anke:Luis_Espinosa">Luis Espinosa Anke</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schockaert:Steven">Steven Schockaert</a></p>
<p>Abstract:
We present SeVeN (Semantic Vector Networks), a hybrid resource that encodes relationships between words in the form of a graph. Different from traditional semantic networks, these relations are represented as vectors in a continuous vector space. We propose a simple pipeline for learning such relation vectors, which is based on word vector averaging in combination with an ad hoc autoencoder. We show that by explicitly encoding relational information in a dedicated vector space we can capture aspects of word meaning that are complementary to what is captured by word embeddings. For example, by examining clusters of relation vectors, we observe that relational similarities can be identified at a more abstract level than with traditional word vector differences. Finally, we test the effectiveness of semantic vector networks in two tasks: measuring word similarity and neural text categorization. SeVeN is available at bitbucket.org/luisespinosa/seven.</p>
<p>Keywords:</p>
<h3 id="226. Evaluation of Unsupervised Compositional Representations.">226. Evaluation of Unsupervised Compositional Representations.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1226/">Paper Link</a>    Pages:2666-2677</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Aldarmaki:Hanan">Hanan Aldarmaki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Diab:Mona_T=">Mona T. Diab</a></p>
<p>Abstract:
We evaluated various compositional models, from bag-of-words representations to compositional RNN-based models, on several extrinsic supervised and unsupervised evaluation benchmarks. Our results confirm that weighted vector averaging can outperform context-sensitive models in most benchmarks, but structural features encoded in RNN models can also be useful in certain classification tasks. We analyzed some of the evaluation datasets to identify the aspects of meaning they measure and the characteristics of the various models that explain their performance variance.</p>
<p>Keywords:</p>
<h3 id="227. Using Formulaic Expressions in Writing Assistance Systems.">227. Using Formulaic Expressions in Writing Assistance Systems.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1227/">Paper Link</a>    Pages:2678-2689</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/i/Iwatsuki:Kenichi">Kenichi Iwatsuki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aizawa:Akiko">Akiko Aizawa</a></p>
<p>Abstract:
Formulaic expressions (FEs) used in scholarly papers, such as there has been little discussion about, are helpful for non-native English speakers. However, it is time-consuming for users to manually search for an appropriate expression every time they want to consult FE dictionaries. For this reason, we tackle the task of semantic searches of FE dictionaries. At the start of our research, we identified two salient difficulties in this task. First, the paucity of example sentences in existing FE dictionaries results in a shortage of context information, which is necessary for acquiring semantic representation of FEs. Second, while a semantic category label is assigned to each FE in many FE dictionaries, it is difficult to predict the labels from user input, forcing users to manually designate the semantic category when searching. To address these difficulties, we propose a new framework for semantic searches of FEs and propose a new method to leverage both existing dictionaries and domain sentence corpora. Further, we expand an existing FE dictionary to consider building a more comprehensive and domain-specific FE dictionary and to verify the effectiveness of our method.</p>
<p>Keywords:</p>
<h3 id="228. What's in Your Embedding, And How It Predicts Task Performance.">228. What's in Your Embedding, And How It Predicts Task Performance.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1228/">Paper Link</a>    Pages:2690-2703</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rogers:Anna">Anna Rogers</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Ananthakrishna:Shashwath_Hosur">Shashwath Hosur Ananthakrishna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rumshisky:Anna">Anna Rumshisky</a></p>
<p>Abstract:
Attempts to find a single technique for general-purpose intrinsic evaluation of word embeddings have so far not been successful. We present a new approach based on scaled-up qualitative analysis of word vector neighborhoods that quantifies interpretable characteristics of a given model (e.g. its preference for synonyms or shared morphological forms as nearest neighbors). We analyze 21 such factors and show how they correlate with performance on 14 extrinsic and intrinsic task datasets (and also explain the lack of correlation between some of them). Our approach enables multi-faceted evaluation, parameter search, and generally  a more principled, hypothesis-driven approach to development of distributional semantic representations.</p>
<p>Keywords:</p>
<h3 id="229. Word Sense Disambiguation Based on Word Similarity Calculation Using Word Vector Representation from a Knowledge-based Graph.">229. Word Sense Disambiguation Based on Word Similarity Calculation Using Word Vector Representation from a Knowledge-based Graph.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1229/">Paper Link</a>    Pages:2704-2714</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/o/O:Dongsuk">Dongsuk O</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kwon:Sunjae">Sunjae Kwon</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Kyungsun">Kyungsun Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Ko:Youngjoong">Youngjoong Ko</a></p>
<p>Abstract:
Word sense disambiguation (WSD) is the task to determine the word sense according to its context. Many existing WSD studies have been using an external knowledge-based unsupervised approach because it has fewer word set constraints than supervised approaches requiring training data. In this paper, we propose a new WSD method to generate the context of an ambiguous word by using similarities between an ambiguous word and words in the input document. In addition, to leverage our WSD method, we further propose a new word similarity calculation method based on the semantic network structure of BabelNet. We evaluate the proposed methods on the SemEval-13 and SemEval-15 for English WSD dataset. Experimental results demonstrate that the proposed WSD method significantly improves the baseline WSD method. Furthermore, our WSD system outperforms the state-of-the-art WSD systems in the Semeval-13 dataset. Finally, it has higher performance than the state-of-the-art unsupervised knowledge-based WSD system in the average performance of both datasets.</p>
<p>Keywords:</p>
<h3 id="230. Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator.">230. Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1230/">Paper Link</a>    Pages:2715-2729</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Patro:Badri_Narayana">Badri Narayana Patro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kurmi:Vinod_Kumar">Vinod Kumar Kurmi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kumar:Sandeep">Sandeep Kumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Namboodiri:Vinay_P=">Vinay P. Namboodiri</a></p>
<p>Abstract:
In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.</p>
<p>Keywords:</p>
<h3 id="231. A Reassessment of Reference-Based Grammatical Error Correction Metrics.">231. A Reassessment of Reference-Based Grammatical Error Correction Metrics.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1231/">Paper Link</a>    Pages:2730-2741</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chollampatt:Shamil">Shamil Chollampatt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ng:Hwee_Tou">Hwee Tou Ng</a></p>
<p>Abstract:
Several metrics have been proposed for evaluating grammatical error correction (GEC) systems based on grammaticality, fluency, and adequacy of the output sentences. Previous studies of the correlation of these metrics with human quality judgments were inconclusive, due to the lack of appropriate significance tests, discrepancies in the methods, and choice of datasets used. In this paper, we re-evaluate reference-based GEC metrics by measuring the system-level correlations with humans on a large dataset of human judgments of GEC outputs, and by properly conducting statistical significance tests. Our results show no significant advantage of GLEU over MaxMatch (M2), contradicting previous studies that claim GLEU to be superior. For a finer-grained analysis, we additionally evaluate these metrics for their agreement with human judgments at the sentence level. Our sentence-level analysis indicates that comparing GLEU and M2, one metric may be more useful than the other depending on the scenario. We further qualitatively analyze these metrics and our findings show that apart from being less interpretable and non-deterministic, GLEU also produces counter-intuitive scores in commonly occurring test examples.</p>
<p>Keywords:</p>
<h3 id="232. Information Aggregation via Dynamic Routing for Sequence Encoding.">232. Information Aggregation via Dynamic Routing for Sequence Encoding.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1232/">Paper Link</a>    Pages:2742-2752</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gong:Jingjing">Jingjing Gong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qiu:Xipeng">Xipeng Qiu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Shaojing">Shaojing Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>Abstract:
While much progress has been made in how to encode a text sequence into a sequence of vectors, less attention has been paid to how to aggregate these preceding vectors (outputs of RNN/CNN) into fixed-size encoding vector. Usually, a simple max or average pooling is used, which is a bottom-up and passive way of aggregation and lack of guidance by task information. In this paper, we propose an aggregation mechanism to obtain a fixed-size encoding with a dynamic routing policy. The dynamic routing policy is dynamically deciding that what and how much information need be transferred from each word to the final encoding of the text sequence. Following the work of Capsule Network, we design two dynamic routing policies to aggregate the outputs of RNN/CNN encoding layer into a final encoding vector. Compared to the other aggregation methods, dynamic routing can refine the messages according to the state of final encoding vector. Experimental results on five text classification tasks show that our method outperforms other aggregating models by a significant margin. Related source code is released on our github page.Related source code is released on our github page.</p>
<p>Keywords:</p>
<h3 id="233. A Full End-to-End Semantic Role Labeler, Syntactic-agnostic Over Syntactic-aware?">233. A Full End-to-End Semantic Role Labeler, Syntactic-agnostic Over Syntactic-aware?</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1233/">Paper Link</a>    Pages:2753-2765</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cai:Jiaxun">Jiaxun Cai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Shexia">Shexia He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zuchao">Zuchao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a></p>
<p>Abstract:
Semantic role labeling (SRL) is to recognize the predicate-argument structure of a sentence, including subtasks of predicate disambiguation and argument labeling. Previous studies usually formulate the entire SRL problem into two or more subtasks. For the first time, this paper introduces an end-to-end neural model which unifiedly tackles the predicate disambiguation and the argument labeling in one shot. Using a biaffine scorer, our model directly predicts all semantic role labels for all given word pairs in the sentence without relying on any syntactic parse information. Specifically, we augment the BiLSTM encoder with a non-linear transformation to further distinguish the predicate and the argument in a given sentence, and model the semantic role labeling process as a word pair classification task by employing the biaffine attentional mechanism. Though the proposed model is syntax-agnostic with local decoder, it outperforms the state-of-the-art syntax-aware SRL systems on the CoNLL-2008, 2009 benchmarks for both English and Chinese. To our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models.</p>
<p>Keywords:</p>
<h3 id="234. Authorship Attribution By Consensus Among Multiple Features.">234. Authorship Attribution By Consensus Among Multiple Features.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1234/">Paper Link</a>    Pages:2766-2777</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Patchala:Jagadeesh">Jagadeesh Patchala</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhatnagar:Raj">Raj Bhatnagar</a></p>
<p>Abstract:
Most existing research on authorship attribution uses various lexical, syntactic and semantic features. In this paper we demonstrate an effective template-based approach for combining various syntactic features of a document for authorship analysis. The parse-tree based features that we propose are independent of the topic of a document and reflect the innate writing styles of authors. We show that the use of templates including sub-trees of parse trees in conjunction with other syntactic features result in improved author attribution rates. Another contribution is the demonstration that Dempsters rule based combination of evidence from syntactic features performs better than other evidence-combination methods. We also demonstrate that our methodology works well for the case where actual author is not included in the candidate author set.</p>
<p>Keywords:</p>
<h3 id="235. Modeling with Recurrent Neural Networks for Open Vocabulary Slots.">235. Modeling with Recurrent Neural Networks for Open Vocabulary Slots.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1235/">Paper Link</a>    Pages:2778-2790</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Jun=Seong">Jun-Seong Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Junghoe">Junghoe Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Park:SeungUn">SeungUn Park</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Kwangyong">Kwangyong Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Yoonju">Yoonju Lee</a></p>
<p>Abstract:
Dealing with open-vocabulary slots has been among the challenges in the natural language area. While recent studies on attention-based recurrent neural network (RNN) models have performed well in completing several language related tasks such as spoken language understanding and dialogue systems, there has been a lack of attempts to address filling slots that take on values from a virtually unlimited set. In this paper, we propose a new RNN model that can capture the vital concept: Understanding the role of a word may vary according to how long a reader focuses on a particular part of a sentence. The proposed model utilizes a long-term aware attention structure, positional encoding primarily considering the relative distance between words, and multi-task learning of a character-based language model and an intent detection model. We show that the model outperforms the existing RNN models with respect to discovering open-vocabulary slots without any external information, such as a named entity database or knowledge base. In particular, we confirm that it performs better with a greater number of slots in a dataset, including unknown words, by evaluating the models on a dataset of several domains. In addition, the proposed model also demonstrates superior performance with regard to intent detection.</p>
<p>Keywords:</p>
<h3 id="236. Challenges and Opportunities of Applying Natural Language Processing in Business Process Management.">236. Challenges and Opportunities of Applying Natural Language Processing in Business Process Management.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1236/">Paper Link</a>    Pages:2791-2801</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Aa:Han_van_der">Han van der Aa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carmona:Josep">Josep Carmona</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Leopold:Henrik">Henrik Leopold</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mendling:Jan">Jan Mendling</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Padr=oacute=:Llu=iacute=s">Llus Padr</a></p>
<p>Abstract:
The Business Process Management (BPM) field focuses in the coordination of labor so that organizational processes are smoothly executed in a way that products and services are properly delivered. At the same time, NLP has reached a maturity level that enables its widespread application in many contexts, thanks to publicly available frameworks. In this position paper, we show how NLP has potential in raising the benefits of BPM practices at different levels. Instead of being exhaustive, we show selected key challenges were a successful application of NLP techniques would facilitate the automation of particular tasks that nowadays require a significant effort to accomplish. Finally, we report on applications that consider both the process perspective and its enhancement through NLP.</p>
<p>Keywords:</p>
<h3 id="237. Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection.">237. Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1237/">Paper Link</a>    Pages:2802-2813</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ghosal:Tirthankar">Tirthankar Ghosal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Edithal:Vignesh">Vignesh Edithal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Ekbal:Asif">Asif Ekbal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsatsaronis:George">George Tsatsaronis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chivukula:Srinivasa_Satya_Sameer_Kumar">Srinivasa Satya Sameer Kumar Chivukula</a></p>
<p>Abstract:
The rapid growth of documents across the web has necessitated finding means of discarding redundant documents and retaining novel ones. Capturing redundancy is challenging as it may involve investigating at a deep semantic level. Techniques for detecting such semantic redundancy at the document level are scarce. In this work we propose a deep Convolutional Neural Networks (CNN) based model to classify a document as novel or redundant with respect to a set of relevant documents already seen by the system. The system is simple and do not require any manual feature engineering. Our novel scheme encodes relevant and relative information from both source and target texts to generate an intermediate representation which we coin as the Relative Document Vector (RDV). The proposed method outperforms the existing state-of-the-art on a document-level novelty detection dataset by a margin of 5% in terms of accuracy. We further demonstrate the effectiveness of our approach on a standard paraphrase detection dataset where paraphrased passages closely resemble to semantically redundant documents.</p>
<p>Keywords:</p>
<h3 id="238. What represents "style" in authorship attribution?">238. What represents "style" in authorship attribution?</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1238/">Paper Link</a>    Pages:2814-2822</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sundararajan:Kalaivani">Kalaivani Sundararajan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Woodard:Damon_L=">Damon L. Woodard</a></p>
<p>Abstract:
Authorship attribution typically uses all information representing both content and style whereas attribution based only on stylistic aspects may be robust in cross-domain settings. This paper analyzes different linguistic aspects that may help represent style. Specifically, we study the role of syntax and lexical words (nouns, verbs, adjectives and adverbs) in representing style. We use a purely syntactic language model to study the significance of sentence structures in both single-domain and cross-domain attribution, i.e. cross-topic and cross-genre attribution. We show that syntax may be helpful for cross-genre attribution while cross-topic attribution and single-domain may benefit from additional lexical information. Further, pure syntactic models may not be effective by themselves and need to be used in combination with other robust models. To study the role of word choice, we perform attribution by masking all words or specific topic words corresponding to nouns, verbs, adjectives and adverbs. Using a single-domain dataset, IMDB1M reviews, we demonstrate the heavy influence of common nouns and proper nouns in attribution, thereby highlighting topic interference. Using cross-domain Guardian10 dataset, we show that some common nouns, verbs, adjectives and adverbs may help with stylometric attribution as demonstrated by masking topic words corresponding to these parts-of-speech. As expected, it was observed that proper nouns are heavily influenced by content and cross-domain attribution will benefit from completely masking them.</p>
<p>Keywords:</p>
<h3 id="239. Learning Target-Specific Representations of Financial News Documents For Cumulative Abnormal Return Prediction.">239. Learning Target-Specific Representations of Financial News Documents For Cumulative Abnormal Return Prediction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1239/">Paper Link</a>    Pages:2823-2833</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Duan:Junwen">Junwen Duan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Xiao">Xiao Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Ching=Yun">Ching-Yun Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>Abstract:
Texts from the Internet serve as important data sources for financial market modeling. Early statistical approaches rely on manually defined features to capture lexical, sentiment and event information, which suffers from feature sparsity. Recent work has considered learning dense representations for news titles and abstracts. Compared to news titles, full documents can contain more potentially helpful information, but also noise compared to events and sentences, which has been less investigated in previous work. To fill this gap, we propose a novel target-specific abstract-guided news document representation model. The model uses a target-sensitive representation of the news abstract to weigh sentences in the news content, so as to select and combine the most informative sentences for market modeling. Results show that document representations can give better performance for estimating cumulative abnormal returns of companies when compared to titles and abstracts. Our model is especially effective when it used to combine information from multiple document sources compared to the sentence-level baselines.</p>
<p>Keywords:</p>
<h3 id="240. Model-Free Context-Aware Word Composition.">240. Model-Free Context-Aware Word Composition.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1240/">Paper Link</a>    Pages:2834-2845</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/An:Bo">Bo An</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xianpei">Xianpei Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a></p>
<p>Abstract:
Word composition is a promising technique for representation learning of large linguistic units (e.g., phrases, sentences and documents). However, most of the current composition models do not take the ambiguity of words and the context outside of a linguistic unit into consideration for learning representations, and consequently suffer from the inaccurate representation of semantics. To address this issue, we propose a model-free context-aware word composition model, which employs the latent semantic information as global context for learning representations. The proposed model attempts to resolve the word sense disambiguation and word composition in a unified framework. Extensive evaluation shows consistent improvements over various strong word representation/composition models at different granularities (including word, phrase and sentence), demonstrating the effectiveness of our proposed method.</p>
<p>Keywords:</p>
<h3 id="241. Learning Features from Co-occurrences: A Theoretical Analysis.">241. Learning Features from Co-occurrences: A Theoretical Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1241/">Paper Link</a>    Pages:2846-2854</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yanpeng">Yanpeng Li</a></p>
<p>Abstract:
Representing a word by its co-occurrences with other words in context is an effective way to capture the meaning of the word. However, the theory behind remains a challenge. In this work, taking the example of a word classification task, we give a theoretical analysis of the approaches that represent a word X by a function f(P(C|X)), where C is a context feature, P(C|X) is the conditional probability estimated from a text corpus, and the function f maps the co-occurrence measure to a prediction score. We investigate the impact of context feature C and the function f . We also explain the reasons why using the co-occurrences with multiple context features may be better than just using a single one. In addition, based on the analysis, we propose a hypothesis about the conditional probability on zero probability events.</p>
<p>Keywords:</p>
<h3 id="242. Towards a unified framework for bilingual terminology extraction of single-word and multi-word terms.">242. Towards a unified framework for bilingual terminology extraction of single-word and multi-word terms.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1242/">Paper Link</a>    Pages:2855-2866</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jingshu">Jingshu Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morin:Emmanuel">Emmanuel Morin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Saldarriaga:Sebasti=aacute=n_Pe=ntilde=a">Sebastin Pea Saldarriaga</a></p>
<p>Abstract:
Extracting a bilingual terminology for multi-word terms from comparable corpora has not been widely researched. In this work we propose a unified framework for aligning bilingual terms independently of the term lengths. We also introduce some enhancements to the context-based and the neural network based approaches. Our experiments show the effectiveness of our enhancements of previous works and the system can be adapted in specialized domains.</p>
<p>Keywords:</p>
<h3 id="243. Neural Activation Semantic Models: Computational lexical semantic models of localized neural activations.">243. Neural Activation Semantic Models: Computational lexical semantic models of localized neural activations.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1243/">Paper Link</a>    Pages:2867-2878</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Athanasiou:Nikos">Nikos Athanasiou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Iosif:Elias">Elias Iosif</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Potamianos:Alexandros">Alexandros Potamianos</a></p>
<p>Abstract:
Neural activation models have been proposed in the literature that use a set of example words for which fMRI measurements are available in order to find a mapping between word semantics and localized neural activations. Successful mappings let us expand to the full lexicon of concrete nouns using the assumption that similarity of meaning implies similar neural activation patterns. In this paper, we propose a computational model that estimates semantic similarity in the neural activation space and investigates the relative performance of this model for various natural language processing tasks. Despite the simplicity of the proposed model and the very small number of example words used to bootstrap it, the neural activation semantic model performs surprisingly well compared to state-of-the-art word embeddings. Specifically, the neural activation semantic model performs better than the state-of-the-art for the task of semantic similarity estimation between very similar or very dissimilar words, while performing well on other tasks such as entailment and word categorization. These are strong indications that neural activation semantic models can not only shed some light into human cognition but also contribute to computation models for certain tasks.</p>
<p>Keywords:</p>
<h3 id="244. Folksonomication: Predicting Tags for Movies from Plot Synopses using Emotion Flow Encoded Neural Network.">244. Folksonomication: Predicting Tags for Movies from Plot Synopses using Emotion Flow Encoded Neural Network.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1244/">Paper Link</a>    Pages:2879-2891</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kar:Sudipta">Sudipta Kar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Maharjan:Suraj">Suraj Maharjan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Solorio:Thamar">Thamar Solorio</a></p>
<p>Abstract:
Folksonomy of movies covers a wide range of heterogeneous information about movies, like the genre, plot structure, visual experiences, soundtracks, metadata, and emotional experiences from watching a movie. Being able to automatically generate or predict tags for movies can help recommendation engines improve retrieval of similar movies, and help viewers know what to expect from a movie in advance. In this work, we explore the problem of creating tags for movies from plot synopses. We propose a novel neural network model that merges information from synopses and emotion flows throughout the plots to predict a set of tags for movies. We compare our system with multiple baselines and found that the addition of emotion flows boosts the performance of the network by learning 18% more tags than a traditional machine learning system.</p>
<p>Keywords:</p>
<h3 id="245. Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level.">245. Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1245/">Paper Link</a>    Pages:2892-2904</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Buechel:Sven">Sven Buechel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hahn:Udo">Udo Hahn</a></p>
<p>Abstract:
Emotion Representation Mapping (ERM) has the goal to convert existing emotion ratings from one representation format into another one, e.g., mapping Valence-Arousal-Dominance annotations for words or sentences into Ekmans Basic Emotions and vice versa. ERM can thus not only be considered as an alternative to Word Emotion Induction (WEI) techniques for automatic emotion lexicon construction but may also help mitigate problems that come from the proliferation of emotion representation formats in recent years. We propose a new neural network approach to ERM that not only outperforms the previous state-of-the-art. Equally important, we present a refined evaluation methodology and gather strong evidence that our model yields results which are (almost) as reliable as human annotations, even in cross-lingual settings. Based on these results we generate new emotion ratings for 13 typologically diverse languages and claim that they have near-gold quality, at least.</p>
<p>Keywords:</p>
<h3 id="246. Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning.">246. Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1246/">Paper Link</a>    Pages:2905-2913</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tafreshi:Shabnam">Shabnam Tafreshi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Diab:Mona_T=">Mona T. Diab</a></p>
<p>Abstract:
Detection and classification of emotion categories expressed by a sentence is a challenging task due to subjectivity of emotion. To date, most of the models are trained and evaluated on single genre and when used to predict emotion in different genre their performance drops by a large margin. To address the issue of robustness, we model the problem within a joint multi-task learning framework. We train this model with a multigenre emotion corpus to predict emotions across various genre. Each genre is represented as a separate task, we use soft parameter shared layers across the various tasks. our experimental results show that this model improves the results across the various genres, compared to a single genre training in the same neural net architecture.</p>
<p>Keywords:</p>
<h3 id="247. How emotional are you? Neural Architectures for Emotion Intensity Prediction in Microblogs.">247. How emotional are you? Neural Architectures for Emotion Intensity Prediction in Microblogs.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1247/">Paper Link</a>    Pages:2914-2926</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kulshreshtha:Devang">Devang Kulshreshtha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goel:Pranav">Pranav Goel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh:Anil_Kumar">Anil Kumar Singh</a></p>
<p>Abstract:
Social media based micro-blogging sites like Twitter have become a common source of real-time information (impacting organizations and their strategies, and are used for expressing emotions and opinions. Automated analysis of such content therefore rises in importance. To this end, we explore the viability of using deep neural networks on the specific task of emotion intensity prediction in tweets. We propose a neural architecture combining convolutional and fully connected layers in a non-sequential manner - done for the first time in context of natural language based tasks. Combined with lexicon-based features along with transfer learning, our model achieves state-of-the-art performance, outperforming the previous system by 0.044 or 4.4% Pearson correlation on the WASSA17 EmoInt shared task dataset. We investigate the performance of deep multi-task learning models trained for all emotions at once in a unified architecture and get encouraging results. Experiments performed on evaluating correlation between emotion pairs offer interesting insights into the relationship between them.</p>
<p>Keywords:</p>
<h3 id="248. Expressively vulgar: The socio-dynamics of vulgarity and its effects on sentiment analysis in social media.">248. Expressively vulgar: The socio-dynamics of vulgarity and its effects on sentiment analysis in social media.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1248/">Paper Link</a>    Pages:2927-2938</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cachola:Isabel">Isabel Cachola</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Holgate:Eric">Eric Holgate</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Preotiuc=Pietro:Daniel">Daniel Preotiuc-Pietro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Junyi_Jessy">Junyi Jessy Li</a></p>
<p>Abstract:
Vulgarity is a common linguistic expression and is used to perform several linguistic functions. Understanding their usage can aid both linguistic and psychological phenomena as well as benefit downstream natural language processing applications such as sentiment analysis. This study performs a large-scale, data-driven empirical analysis of vulgar words using social media data. We analyze the socio-cultural and pragmatic aspects of vulgarity using tweets from users with known demographics. Further, we collect sentiment ratings for vulgar tweets to study the relationship between the use of vulgar words and perceived sentiment and show that explicitly modeling vulgar words can boost sentiment analysis performance.</p>
<p>Keywords:</p>
<h3 id="249. Clausal Modifiers in the Grammar Matrix.">249. Clausal Modifiers in the Grammar Matrix.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1249/">Paper Link</a>    Pages:2939-2952</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Howell:Kristen">Kristen Howell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zamaraeva:Olga">Olga Zamaraeva</a></p>
<p>Abstract:
We extend the coverage of an existing grammar customization system to clausal modifiers, also referred to as adverbial clauses. We present an analysis, taking a typologically-driven approach to account for this phenomenon across the worlds languages, which we implement in the Grammar Matrix customization system (Bender et al., 2002, 2010). Testing our analysis on testsuites from five genetically and geographically diverse languages that were not considered in development, we achieve 88.4% coverage and 1.5% overgeneration.</p>
<p>Keywords:</p>
<h3 id="250. Sliced Recurrent Neural Networks.">250. Sliced Recurrent Neural Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1250/">Paper Link</a>    Pages:2953-2964</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Zeping">Zeping Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Gongshen">Gongshen Liu</a></p>
<p>Abstract:
Recurrent neural networks have achieved great success in many NLP tasks. However, they have difficulty in parallelization because of the recurrent structure, so it takes much time to train RNNs. In this paper, we introduce sliced recurrent neural networks (SRNNs), which could be parallelized by slicing the sequences into many subsequences. SRNNs have the ability to obtain high-level information through multiple layers with few extra parameters. We prove that the standard RNN is a special case of the SRNN when we use linear activation functions. Without changing the recurrent units, SRNNs are 136 times as fast as standard RNNs and could be even faster when we train longer sequences. Experiments on six large-scale sentiment analysis datasets show that SRNNs achieve better performance than standard RNNs.</p>
<p>Keywords:</p>
<h3 id="251. Multi-Task Learning for Sequence Tagging: An Empirical Study.">251. Multi-Task Learning for Sequence Tagging: An Empirical Study.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1251/">Paper Link</a>    Pages:2965-2977</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Changpinyo:Soravit">Soravit Changpinyo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Hexiang">Hexiang Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sha:Fei">Fei Sha</a></p>
<p>Abstract:
We study three general multi-task learning (MTL) approaches on 11 sequence tagging tasks. Our extensive empirical results show that in about 50% of the cases, jointly learning all 11 tasks improves upon either independent or pairwise learning of the tasks. We also show that pairwise MTL can inform us what tasks can benefit others or what tasks can be benefited if they are learned jointly. In particular, we identify tasks that can always benefit others as well as tasks that can always be harmed by others. Interestingly, one of our MTL approaches yields embeddings of the tasks that reveal the natural clustering of semantic and syntactic tasks. Our inquiries have opened the doors to further utilization of MTL in NLP.</p>
<p>Keywords:</p>
<h3 id="252. Using J-K-fold Cross Validation To Reduce Variance When Tuning NLP Models.">252. Using J-K-fold Cross Validation To Reduce Variance When Tuning NLP Models.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1252/">Paper Link</a>    Pages:2978-2989</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Moss:Henry_B=">Henry B. Moss</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Leslie:David_S=">David S. Leslie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rayson:Paul">Paul Rayson</a></p>
<p>Abstract:
K-fold cross validation (CV) is a popular method for estimating the true performance of machine learning models, allowing model selection and parameter tuning. However, the very process of CV requires random partitioning of the data and so our performance estimates are in fact stochastic, with variability that can be substantial for natural language processing tasks. We demonstrate that these unstable estimates cannot be relied upon for effective parameter tuning. The resulting tuned parameters are highly sensitive to how our data is partitioned, meaning that we often select sub-optimal parameter choices and have serious reproducibility issues. Instead, we propose to use the less variable J-K-fold CV, in which J independent K-fold cross validations are used to assess performance. Our main contributions are extending J-K-fold CV from performance estimation to parameter tuning and investigating how to choose J and K. We argue that variability is more important than bias for effective tuning and so advocate lower choices of K than are typically seen in the NLP literature and instead use the saved computation to increase J. To demonstrate the generality of our recommendations we investigate a wide range of case-studies: sentiment classification (both general and target-specific), part-of-speech tagging and document classification.</p>
<p>Keywords:</p>
<h3 id="253. Incremental Natural Language Processing: Challenges, Strategies, and Evaluation.">253. Incremental Natural Language Processing: Challenges, Strategies, and Evaluation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1253/">Paper Link</a>    Pages:2990-3003</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/K=ouml=hn:Arne">Arne Khn</a></p>
<p>Abstract:
Incrementality is ubiquitous in human-human interaction and beneficial for human-computer interaction. It has been a topic of research in different parts of the NLP community, mostly with focus on the specific topic at hand even though incremental systems have to deal with similar challenges regardless of domain. In this survey, I consolidate and categorize the approaches, identifying similarities and differences in the computation and data, and show trade-offs that have to be considered. A focus lies on evaluating incremental systems because the standard metrics often fail to capture the incremental properties of a system and coming up with a suitable evaluation scheme is non-trivial.</p>
<p>Keywords:</p>
<h3 id="254. Gold Standard Annotations for Preposition and Verb Sense with Semantic Role Labels in Adult-Child Interactions.">254. Gold Standard Annotations for Preposition and Verb Sense with Semantic Role Labels in Adult-Child Interactions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1254/">Paper Link</a>    Pages:3004-3014</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Moon:Lori">Lori Moon</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Christodoulopoulos:Christos">Christos Christodoulopoulos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fisher:Cynthia">Cynthia Fisher</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Franco:Sandra">Sandra Franco</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>Abstract:
This paper describes the augmentation of an existing corpus of child-directed speech. The resulting corpus is a gold-standard labeled corpus for supervised learning of semantic role labels in adult-child dialogues. Semantic role labeling (SRL) models assign semantic roles to sentence constituents, thus indicating who has done what to whom (and in what way). The current corpus is derived from the Adam files in the Brown corpus (Brown 1973) of the CHILDES corpora, and augments the partial annotation described in Connor et al. (2010). It provides labels for both semantic arguments of verbs and semantic arguments of prepositions. The semantic role labels and senses of verbs follow Propbank guidelines Kingsbury and Palmer, 2002; Gildea and Palmer 2002; Palmer et al., 2005) and those for prepositions follow Srikumar and Roth (2011). The corpus was annotated by two annotators. Inter-annotator agreement is given separately for prepositions and verbs, and for adult speech and child speech. Overall, across child and adult samples, including verbs and prepositions, the kappa score for sense is 72.6, for the number of semantic-role-bearing arguments, the kappa score is 77.4, for identical semantic role labels on a given argument, the kappa score is 91.1, for the span of semantic role labels, and the kappa for agreement is 93.9. The sense and number of arguments was often open to multiple interpretations in child speech, due to the rapidly changing discourse and omission of constituents in production. Annotators used a discourse context window of ten sentences before and ten sentences after the target utterance to determine the annotation labels. The derived corpus is available for use in CHAT (MacWhinney, 2000) and XML format.</p>
<p>Keywords:</p>
<h3 id="255. Multi-layer Representation Fusion for Neural Machine Translation.">255. Multi-layer Representation Fusion for Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1255/">Paper Link</a>    Pages:3015-3026</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Qiang">Qiang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Fuxue">Fuxue Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Tong">Tong Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yanyang">Yanyang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yinqiao">Yinqiao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Jingbo">Jingbo Zhu</a></p>
<p>Abstract:
Neural machine translation systems require a number of stacked layers for deep models. But the prediction depends on the sentence representation of the top-most layer with no access to low-level representations. This makes it more difficult to train the model and poses a risk of information loss to prediction. In this paper, we propose a multi-layer representation fusion (MLRF) approach to fusing stacked layers. In particular, we design three fusion functions to learn a better representation from the stack. Experimental results show that our approach yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-of-the-art in German-English translation.</p>
<p>Keywords:</p>
<h3 id="256. Toward Better Loanword Identification in Uyghur Using Cross-lingual Word Embeddings.">256. Toward Better Loanword Identification in Uyghur Using Cross-lingual Word Embeddings.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1256/">Paper Link</a>    Pages:3027-3037</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mi:Chenggang">Chenggang Mi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yating">Yating Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0065:Lei">Lei Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Xi">Xi Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Tonghai">Tonghai Jiang</a></p>
<p>Abstract:
To enrich vocabulary of low resource settings, we proposed a novel method which identify loanwords in monolingual corpora. More specifically, we first use cross-lingual word embeddings as the core feature to generate semantically related candidates based on comparable corpora and a small bilingual lexicon; then, a log-linear model which combines several shallow features such as pronunciation similarity and hybrid language model features to predict the final results. In this paper, we use Uyghur as the receipt language and try to detect loanwords in four donor languages: Arabic, Chinese, Persian and Russian. We conduct two groups of experiments to evaluate the effectiveness of our proposed approach: loanword identification and OOV translation in four language pairs and eight translation directions (Uyghur-Arabic, Arabic-Uyghur, Uyghur-Chinese, Chinese-Uyghur, Uyghur-Persian, Persian-Uyghur, Uyghur-Russian, and Russian-Uyghur). Experimental results on loanword identification show that our method outperforms other baseline models significantly. Neural machine translation models integrating results of loanword identification experiments achieve the best results on OOV translation(with 0.5-0.9 BLEU improvements)</p>
<p>Keywords:</p>
<h3 id="257. Adaptive Weighting for Neural Machine Translation.">257. Adaptive Weighting for Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1257/">Paper Link</a>    Pages:3038-3048</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yachao">Yachao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Junhui">Junhui Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a></p>
<p>Abstract:
In the popular sequence to sequence (seq2seq) neural machine translation (NMT), there exist many weighted sum models (WSMs), each of which takes a set of input and generates one output. However, the weights in a WSM are independent of each other and fixed for all inputs, suggesting that by ignoring different needs of inputs, the WSM lacks effective control on the influence of each input. In this paper, we propose adaptive weighting for WSMs to control the contribution of each input. Specifically, we apply adaptive weighting for both GRU and the output state in NMT. Experimentation on Chinese-to-English translation and English-to-German translation demonstrates that the proposed adaptive weighting is able to much improve translation accuracy by achieving significant improvement of 1.49 and 0.92 BLEU points for the two translation tasks. Moreover, we discuss in-depth on what type of information is encoded in the encoder and how information influences the generation of target words in the decoder.</p>
<p>Keywords:</p>
<h3 id="258. Generic refinement of expressive grammar formalisms with an application to discontinuous constituent parsing.">258. Generic refinement of expressive grammar formalisms with an application to discontinuous constituent parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1258/">Paper Link</a>    Pages:3049-3063</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gebhardt:Kilian">Kilian Gebhardt</a></p>
<p>Abstract:
We formulate a generalization of Petrov et al. (2006)s split/merge algorithm for interpreted regular tree grammars (Koller and Kuhlmann, 2011), which capture a large class of grammar formalisms. We evaluate its effectiveness empirically on the task of discontinuous constituent parsing with two mildly context-sensitive grammar formalisms: linear context-free rewriting systems (Vijay-Shanker et al., 1987) as well as hybrid grammars (Nederhof and Vogler, 2014).</p>
<p>Keywords:</p>
<h3 id="259. Double Path Networks for Sequence to Sequence Learning.">259. Double Path Networks for Sequence to Sequence Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1259/">Paper Link</a>    Pages:3064-3074</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Song:Kaitao">Kaitao Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Xu">Xu Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Di">Di He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0003:Jianfeng">Jianfeng Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin:Tao">Tao Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tie=Yan">Tie-Yan Liu</a></p>
<p>Abstract:
Encoder-decoder based Sequence to Sequence learning (S2S) has made remarkable progress in recent years. Different network architectures have been used in the encoder/decoder. Among them, Convolutional Neural Networks (CNN) and Self Attention Networks (SAN) are the prominent ones. The two architectures achieve similar performances but use very different ways to encode and decode context: CNN use convolutional layers to focus on the local connectivity of the sequence, while SAN uses self-attention layers to focus on global semantics. In this work we propose Double Path Networks for Sequence to Sequence learning (DPN-S2S), which leverage the advantages of both models by using double path information fusion. During the encoding step, we develop a double path architecture to maintain the information coming from different paths with convolutional layers and self-attention layers separately. To effectively use the encoded context, we develop a gated attention fusion module and use it to automatically pick up the information needed during the decoding step, which is also a double path network. By deeply integrating the two paths, both types of information are combined and well exploited. Experiments show that our proposed method can significantly improve the performance of sequence to sequence learning over state-of-the-art systems.</p>
<p>Keywords:</p>
<h3 id="260. An Empirical Investigation of Error Types in Vietnamese Parsing.">260. An Empirical Investigation of Error Types in Vietnamese Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1260/">Paper Link</a>    Pages:3075-3089</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Quy">Quy Nguyen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miyao:Yusuke">Yusuke Miyao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Noji:Hiroshi">Hiroshi Noji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Nhung">Nhung Nguyen</a></p>
<p>Abstract:
Syntactic parsing plays a crucial role in improving the quality of natural language processing tasks. Although there have been several research projects on syntactic parsing in Vietnamese, the parsing quality has been far inferior than those reported in major languages, such as English and Chinese. In this work, we evaluated representative constituency parsing models on a Vietnamese Treebank to look for the most suitable parsing method for Vietnamese. We then combined the advantages of automatic and manual analysis to investigate errors produced by the experimented parsers and find the reasons for them. Our analysis focused on three possible sources of parsing errors, namely limited training data, part-of-speech (POS) tagging errors, and ambiguous constructions. As a result, we found that the last two sources, which frequently appear in Vietnamese text, significantly attributed to the poor performance of Vietnamese parsing.</p>
<p>Keywords:</p>
<h3 id="261. Learning with Noise-Contrastive Estimation: Easing training by learning to scale.">261. Learning with Noise-Contrastive Estimation: Easing training by learning to scale.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1261/">Paper Link</a>    Pages:3090-3101</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Labeau:Matthieu">Matthieu Labeau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Allauzen:Alexandre">Alexandre Allauzen</a></p>
<p>Abstract:
Noise-Contrastive Estimation (NCE) is a learning criterion that is regularly used to train neural language models in place of Maximum Likelihood Estimation, since it avoids the computational bottleneck caused by the output softmax. In this paper, we analyse and explain some of the weaknesses of this objective function, linked to the mechanism of self-normalization, by closely monitoring comparative experiments. We then explore several remedies and modifications to propose tractable and efficient NCE training strategies. In particular, we propose to make the scaling factor a trainable parameter of the model, and to use the noise distribution to initialize the output bias. These solutions, yet simple, yield stable and competitive performances in either small and large scale language modelling tasks.</p>
<p>Keywords:</p>
<h3 id="262. Parallel Corpora for bi-lingual English-Ethiopian Languages Statistical Machine Translation.">262. Parallel Corpora for bi-lingual English-Ethiopian Languages Statistical Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1262/">Paper Link</a>    Pages:3102-3111</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Abate:Solomon_Teferra">Solomon Teferra Abate</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Melese:Michael">Michael Melese</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tachbelie:Martha_Yifiru">Martha Yifiru Tachbelie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meshesha:Million">Million Meshesha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Atinafu:Solomon">Solomon Atinafu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mulugeta:Wondwossen">Wondwossen Mulugeta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Assabie:Yaregal">Yaregal Assabie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abera:Hafte">Hafte Abera</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Seyoum:Binyam_Ephrem">Binyam Ephrem Seyoum</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abebe:Tewodros">Tewodros Abebe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsegaye:Wondimagegnhue">Wondimagegnhue Tsegaye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lemma:Amanuel">Amanuel Lemma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Andargie:Tsegaye">Tsegaye Andargie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shifaw:Seifedin">Seifedin Shifaw</a></p>
<p>Abstract:
In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Geez. The corpora are used for conducting a bi-directional statistical machine translation experiments. The BLEU scores of the bi-directional Statistical Machine Translation (SMT) systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT specially when the targets are Ethiopian languages. Now we are working towards an optimal alignment for a bi-directional English-Ethiopian languages SMT.</p>
<p>Keywords:</p>
<h3 id="263. Multilingual Neural Machine Translation with Task-Specific Attention.">263. Multilingual Neural Machine Translation with Task-Specific Attention.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1263/">Paper Link</a>    Pages:3112-3122</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Blackwood:Graeme_W=">Graeme W. Blackwood</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Ballesteros:Miguel">Miguel Ballesteros</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Ward:Todd">Todd Ward</a></p>
<p>Abstract:
Multilingual machine translation addresses the task of translating between multiple source and target languages. We propose task-specific attention models, a simple but effective technique for improving the quality of sequence-to-sequence neural multilingual translation. Our approach seeks to retain as much of the parameter sharing generalization of NMT models as possible, while still allowing for language-specific specialization of the attention model to a particular language-pair or task. Our experiments on four languages of the Europarl corpus show that using a target-specific model of attention provides consistent gains in translation quality for all possible translation directions, compared to a model in which all parameters are shared. We observe improved translation quality even in the (extreme) low-resource zero-shot translation directions for which the model never saw explicitly paired parallel data.</p>
<p>Keywords:</p>
<h3 id="264. Combining Information-Weighted Sequence Alignment and Sound Correspondence Models for Improved Cognate Detection.">264. Combining Information-Weighted Sequence Alignment and Sound Correspondence Models for Improved Cognate Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1264/">Paper Link</a>    Pages:3123-3133</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dellert:Johannes">Johannes Dellert</a></p>
<p>Abstract:
Methods for automated cognate detection in historical linguistics invariably build on some measure of form similarity which is designed to capture the remaining systematic similarities between cognate word forms after thousands of years of divergence. A wide range of clustering and classification algorithms has been explored for the purpose, whereas possible improvements on the level of pairwise form similarity measures have not been the main focus of research. The approach presented in this paper improves on this core component of cognate detection systems by a novel combination of information weighting, a technique for putting less weight on reoccurring morphological material, with sound correspondence modeling by means of pointwise mutual information. In evaluations on expert cognacy judgments over a subset of the IPA-encoded NorthEuraLex database, the combination of both techniques is shown to lead to considerable improvements in average precision for binary cognate detection, and modest improvements for distance-based cognate clustering.</p>
<p>Keywords:</p>
<h3 id="265. Tailoring Neural Architectures for Translating from Morphologically Rich Languages.">265. Tailoring Neural Architectures for Translating from Morphologically Rich Languages.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1265/">Paper Link</a>    Pages:3134-3145</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Passban:Peyman">Peyman Passban</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Way:Andy">Andy Way</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qun">Qun Liu</a></p>
<p>Abstract:
A morphologically complex word (MCW) is a hierarchical constituent with meaning-preserving subunits, so word-based models which rely on surface forms might not be powerful enough to translate such structures. When translating from morphologically rich languages (MRLs), a source word could be mapped to several words or even a full sentence on the target side, which means an MCW should not be treated as an atomic unit. In order to provide better translations for MRLs, we boost the existing neural machine translation (NMT) architecture with a double- channel encoder and a double-attentive decoder. The main goal targeted in this research is to provide richer information on the encoder side and redesign the decoder accordingly to benefit from such information. Our experimental results demonstrate that we could achieve our goal as the proposed model outperforms existing subword- and character-based architectures and showed significant improvements on translating from German, Russian, and Turkish into English.</p>
<p>Keywords:</p>
<h3 id="266. deepQuest: A Framework for Neural-based Quality Estimation.">266. deepQuest: A Framework for Neural-based Quality Estimation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1266/">Paper Link</a>    Pages:3146-3157</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/i/Ive:Julia">Julia Ive</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blain:Fr=eacute=d=eacute=ric">Frdric Blain</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Specia:Lucia">Lucia Specia</a></p>
<p>Abstract:
Predicting Machine Translation (MT) quality can help in many practical tasks such as MT post-editing. The performance of Quality Estimation (QE) methods has drastically improved recently with the introduction of neural approaches to the problem. However, thus far neural approaches have only been designed for word and sentence-level prediction. We present a neural framework that is able to accommodate neural QE approaches at these fine-grained levels and generalize them to the level of documents. We test the framework with two sentence-level neural QE approaches: a state of the art approach that requires extensive pre-training, and a new light-weight approach that we propose, which employs basic encoders. Our approach is significantly faster and yields performance improvements for a range of document-level quality estimation tasks. To our knowledge, this is the first neural architecture for document-level QE. In addition, for the first time we apply QE models to the output of both statistical and neural MT systems for a series of European languages and highlight the new challenges resulting from the use of neural MT.</p>
<p>Keywords:</p>
<h3 id="267. Butterfly Effects in Frame Semantic Parsing: impact of data processing on model ranking.">267. Butterfly Effects in Frame Semantic Parsing: impact of data processing on model ranking.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1267/">Paper Link</a>    Pages:3158-3169</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kabbach:Alexandre">Alexandre Kabbach</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ribeyre:Corentin">Corentin Ribeyre</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Herbelot:Aur=eacute=lie">Aurlie Herbelot</a></p>
<p>Abstract:
Knowing the state-of-the-art for a particular task is an essential component of any computational linguistics investigation. But can we be truly confident that the current state-of-the-art is indeed the best performing model? In this paper, we study the case of frame semantic parsing, a well-established task with multiple shared datasets. We show that in spite of all the care taken to provide a standard evaluation resource, small variations in data processing can have dramatic consequences for ranking parser performance. This leads us to propose an open-source standardized processing pipeline, which can be shared and reused for robust model comparison.</p>
<p>Keywords:</p>
<h3 id="268. Sensitivity to Input Order: Evaluation of an Incremental and Memory-Limited Bayesian Cross-Situational Word Learning Model.">268. Sensitivity to Input Order: Evaluation of an Incremental and Memory-Limited Bayesian Cross-Situational Word Learning Model.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1268/">Paper Link</a>    Pages:3170-3180</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sadeghi:Sepideh">Sepideh Sadeghi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Scheutz:Matthias">Matthias Scheutz</a></p>
<p>Abstract:
We present a variation of the incremental and memory-limited algorithm in (Sadeghi et al., 2017) for Bayesian cross-situational word learning and evaluate the model in terms of its functional performance and its sensitivity to input order. We show that the functional performance of our sub-optimal model on corpus data is close to that of its optimal counterpart (Frank et al., 2009), while only the sub-optimal model is capable of predicting the input order effects reported in experimental studies.</p>
<p>Keywords:</p>
<h3 id="269. Sentence Weighting for Neural Machine Translation Domain Adaptation.">269. Sentence Weighting for Neural Machine Translation Domain Adaptation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1269/">Paper Link</a>    Pages:3181-3190</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Shiqi">Shiqi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a></p>
<p>Abstract:
In this paper, we propose a new sentence weighting method for the domain adaptation of neural machine translation. We introduce a domain similarity metric to evaluate the relevance between a sentence and an available entire domain dataset. The similarity of each sentence to the target domain is calculated with various methods. The computed similarity is then integrated into the training objective to weight sentences. The adaptation results on both IWSLT Chinese-English TED task and a task with only synthetic training parallel data show that our sentence weighting method is able to achieve an significant improvement over strong baselines.</p>
<p>Keywords:</p>
<h3 id="270. Quantifying training challenges of dependency parsers.">270. Quantifying training challenges of dependency parsers.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1270/">Paper Link</a>    Pages:3191-3202</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Aufrant:Lauriane">Lauriane Aufrant</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wisniewski:Guillaume">Guillaume Wisniewski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yvon:Fran=ccedil=ois">Franois Yvon</a></p>
<p>Abstract:
Not all dependencies are equal when training a dependency parser: some are straightforward enough to be learned with only a sample of data, others embed more complexity. This work introduces a series of metrics to quantify those differences, and thereby to expose the shortcomings of various parsing algorithms and strategies. Apart from a more thorough comparison of parsing systems, these new tools also prove useful for characterizing the information conveyed by cross-lingual parsers, in a quantitative but still interpretable way.</p>
<p>Keywords:</p>
<h3 id="271. Seq2seq Dependency Parsing.">271. Seq2seq Dependency Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1271/">Paper Link</a>    Pages:3203-3214</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zuchao">Zuchao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cai:Jiaxun">Jiaxun Cai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Shexia">Shexia He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a></p>
<p>Abstract:
This paper presents a sequence to sequence (seq2seq) dependency parser by directly predicting the relative position of head for each given word, which therefore results in a truly end-to-end seq2seq dependency parser for the first time. Enjoying the advantage of seq2seq modeling, we enrich a series of embedding enhancement, including firstly introduced subword and node2vec augmentation. Meanwhile, we propose a beam search decoder with tree constraint and subroot decomposition over the sequence to furthermore enhance our seq2seq parser. Our parser is evaluated on benchmark treebanks, being on par with the state-of-the-art parsers by achieving 94.11% UAS on PTB and 88.78% UAS on CTB, respectively.</p>
<p>Keywords:</p>
<h3 id="272. Revisiting the Hierarchical Multiscale LSTM.">272. Revisiting the Hierarchical Multiscale LSTM.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1272/">Paper Link</a>    Pages:3215-3227</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/K=aacute=d=aacute=r:=Aacute=kos">kos Kdr</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/C=ocirc=t=eacute=:Marc=Alexandre">Marc-Alexandre Ct</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chrupala:Grzegorz">Grzegorz Chrupala</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Alishahi:Afra">Afra Alishahi</a></p>
<p>Abstract:
Hierarchical Multiscale LSTM (Chung et. al., 2016) is a state-of-the-art language model that learns interpretable structure from character-level input. Such models can provide fertile ground for (cognitive) computational linguistics studies. However, the high complexity of the architecture, training and implementations might hinder its applicability. We provide a detailed reproduction and ablation study of the architecture, shedding light on some of the potential caveats of re-purposing complex deep-learning architectures. We further show that simplifying certain aspects of the architecture can in fact improve its performance. We also investigate the linguistic units (segments) learned by various levels of the model, and argue that their quality does not correlate with the overall performance of the model on language modeling.</p>
<p>Keywords:</p>
<h3 id="273. Character-Level Feature Extraction with Densely Connected Networks.">273. Character-Level Feature Extraction with Densely Connected Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1273/">Paper Link</a>    Pages:3228-3239</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Chanhee">Chanhee Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Young=Bum">Young-Bum Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Dongyub">Dongyub Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lim:Heuiseok">Heuiseok Lim</a></p>
<p>Abstract:
Generating character-level features is an important step for achieving good results in various natural language processing tasks. To alleviate the need for human labor in generating hand-crafted features, methods that utilize neural architectures such as Convolutional Neural Network (CNN) or Recurrent Neural Network (RNN) to automatically extract such features have been proposed and have shown great results. However, CNN generates position-independent features, and RNN is slow since it needs to process the characters sequentially. In this paper, we propose a novel method of using a densely connected network to automatically extract character-level features. The proposed method does not require any language or task specific assumptions, and shows robustness and effectiveness while being faster than CNN- or RNN-based methods. Evaluating this method on three sequence labeling tasks - slot tagging, Part-of-Speech (POS) tagging, and Named-Entity Recognition (NER) - we obtain state-of-the-art performance with a 96.62 F1-score and 97.73% accuracy on slot tagging and POS tagging, respectively, and comparable performance to the state-of-the-art 91.13 F1-score on NER.</p>
<p>Keywords:</p>
<h3 id="274. Neural Machine Translation Incorporating Named Entity.">274. Neural Machine Translation Incorporating Named Entity.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1274/">Paper Link</a>    Pages:3240-3250</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/u/Ugawa:Arata">Arata Ugawa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tamura:Akihiro">Akihiro Tamura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ninomiya:Takashi">Takashi Ninomiya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Takamura:Hiroya">Hiroya Takamura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Okumura:Manabu">Manabu Okumura</a></p>
<p>Abstract:
This study proposes a new neural machine translation (NMT) model based on the encoder-decoder model that incorporates named entity (NE) tags of source-language sentences. Conventional NMT models have two problems enumerated as follows: (i) they tend to have difficulty in translating words with multiple meanings because of the high ambiguity, and (ii) these modelsabilitytotranslatecompoundwordsseemschallengingbecausetheencoderreceivesaword, a part of the compound word, at each time step. To alleviate these problems, the encoder of the proposed model encodes the input word on the basis of its NE tag at each time step, which could reduce the ambiguity of the input word. Furthermore,the encoder introduces a chunk-level LSTM layer over a word-level LSTM layer and hierarchically encodes a source-language sentence to capture a compound NE as a chunk on the basis of the NE tags. We evaluate the proposed model on an English-to-Japanese translation task with the ASPEC, and English-to-Bulgarian and English-to-Romanian translation tasks with the Europarl corpus. The evaluation results show that the proposed model achieves up to 3.11 point improvement in BLEU.</p>
<p>Keywords:</p>
<h3 id="275. Semantic Parsing for Technical Support Questions.">275. Semantic Parsing for Technical Support Questions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1275/">Paper Link</a>    Pages:3251-3259</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Abhirut">Abhirut Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ray:Anupama">Anupama Ray</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dasgupta:Gargi">Gargi Dasgupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh:Gautam">Gautam Singh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aggarwal:Pooja">Pooja Aggarwal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mohapatra:Prateeti">Prateeti Mohapatra</a></p>
<p>Abstract:
Technical support problems are very complex. In contrast to regular web queries (that contain few keywords) or factoid questions (which are a few sentences), these problems usually include attributes like a detailed description of what is failing (symptom), steps taken in an effort to remediate the failure (activity), and sometimes a specific request or ask (intent). Automating support is the task of automatically providing answers to these problems given a corpus of solution documents. Traditional approaches to this task rely on information retrieval and are keyword based; looking for keyword overlap between the question and solution documents and ignoring these attributes. We present an approach for semantic parsing of technical questions that uses grammatical structure to extract these attributes as a baseline, and a CRF based model that can improve performance considerably in the presence of annotated data for training. We also demonstrate that combined with reasoning, these attributes help outperform retrieval baselines.</p>
<p>Keywords:</p>
<h3 id="276. Deconvolution-Based Global Decoding for Neural Machine Translation.">276. Deconvolution-Based Global Decoding for Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1276/">Paper Link</a>    Pages:3260-3271</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Junyang">Junyang Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xuancheng">Xuancheng Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Shuming">Shuming Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Jinsong">Jinsong Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Qi">Qi Su</a></p>
<p>Abstract:
A great proportion of sequence-to-sequence (Seq2Seq) models for Neural Machine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate translation word by word following a sequential order. As the studies of linguistics have proved that language is not linear word sequence but sequence of complex structure, translation at each step should be conditioned on the whole target-side context. To tackle the problem, we propose a new NMT model that decodes the sequence with the guidance of its structural prediction of the context of the target sequence. Our model generates translation based on the structural prediction of the target-side context so that the translation can be freed from the bind of sequential order. Experimental results demonstrate that our model is more competitive compared with the state-of-the-art methods, and the analysis reflects that our model is also robust to translating sentences of different lengths and it also reduces repetition with the instruction from the target-side context for decoding.</p>
<p>Keywords:</p>
<h3 id="277. Pattern-revising Enhanced Simple Question Answering over Knowledge Bases.">277. Pattern-revising Enhanced Simple Question Answering over Knowledge Bases.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1277/">Paper Link</a>    Pages:3272-3282</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hao:Yanchao">Yanchao Hao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Hao">Hao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Shizhu">Shizhu He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Kang">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Jun">Jun Zhao</a></p>
<p>Abstract:
Question Answering over Knowledge Bases (KB-QA), which automatically answer natural language questions based on the facts contained by a knowledge base, is one of the most important natural language processing (NLP) tasks. Simple questions constitute a large part of questions queried on the web, still being a challenge to QA systems. In this work, we propose to conduct pattern extraction and entity linking first, and put forward pattern revising procedure to mitigate the error propagation problem. In order to learn to rank candidate subject-predicate pairs to enable the relevant facts retrieval given a question, we propose to do joint fact selection enhanced by relation detection. Multi-level encodings and multi-dimension information are leveraged to strengthen the whole procedure. The experimental results demonstrate that our approach sets a new record in this task, outperforming the current state-of-the-art by an absolute large margin.</p>
<p>Keywords:</p>
<h3 id="278. Integrating Question Classification and Deep Learning for improved Answer Selection.">278. Integrating Question Classification and Deep Learning for improved Answer Selection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1278/">Paper Link</a>    Pages:3283-3294</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Madabushi:Harish_Tayyar">Harish Tayyar Madabushi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Mark">Mark Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Barnden:John">John Barnden</a></p>
<p>Abstract:
We present a system for Answer Selection that integrates fine-grained Question Classification with a Deep Learning model designed for Answer Selection. We detail the necessary changes to the Question Classification taxonomy and system, the creation of a new Entity Identification system and methods of highlighting entities to achieve this objective. Our experiments show that Question Classes are a strong signal to Deep Learning models for Answer Selection, and enable us to outperform the current state of the art in all variations of our experiments except one. In the best configuration, our MRR and MAP scores outperform the current state of the art by between 3 and 5 points on both versions of the TREC Answer Selection test set, a standard dataset for this task.</p>
<p>Keywords:</p>
<h3 id="279. Knowledge as A Bridge: Improving Cross-domain Answer Selection with External Knowledge.">279. Knowledge as A Bridge: Improving Cross-domain Answer Selection with External Knowledge.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1279/">Paper Link</a>    Pages:3295-3305</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Deng:Yang">Yang Deng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Ying">Ying Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0007:Min">Min Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yaliang">Yaliang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Du:Nan">Nan Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fan_0001:Wei">Wei Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lei:Kai">Kai Lei</a></p>
<p>Abstract:
Answer selection is an important but challenging task. Significant progresses have been made in domains where a large amount of labeled training data is available. However, obtaining rich annotated data is a time-consuming and expensive process, creating a substantial barrier for applying answer selection models to a new domain which has limited labeled data. In this paper, we propose Knowledge-aware Attentive Network (KAN), a transfer learning framework for cross-domain answer selection, which uses the knowledge base as a bridge to enable knowledge transfer from the source domain to the target domains. Specifically, we design a knowledge module to integrate the knowledge-based representational learning into answer selection models. The learned knowledge-based representations are shared by source and target domains, which not only leverages large amounts of cross-domain data, but also benefits from a regularization effect that leads to more general representations to help tasks in new domains. To verify the effectiveness of our model, we use SQuAD-T dataset as the source domain and three other datasets (i.e., Yahoo QA, TREC QA and InsuranceQA) as the target domains. The experimental results demonstrate that KAN has remarkable applicability and generality, and consistently outperforms the strong competitors by a noticeable margin for cross-domain answer selection.</p>
<p>Keywords:</p>
<h3 id="280. Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering.">280. Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1280/">Paper Link</a>    Pages:3306-3317</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sorokin:Daniil">Daniil Sorokin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
The most approaches to Knowledge Base Question Answering are based on semantic parsing. In this paper, we address the problem of learning vector representations for complex semantic parses that consist of multiple entities and relations. Previous work largely focused on selecting the correct semantic relations for a question and disregarded the structure of the semantic parse: the connections between entities and the directions of the relations. We propose to use Gated Graph Neural Networks to encode the graph structure of the semantic parse. We show on two data sets that the graph networks outperform all baseline models that do not explicitly model the structure. The error analysis confirms that our approach can successfully process complex semantic parses.</p>
<p>Keywords:</p>
<h3 id="281. Rethinking the Agreement in Human Evaluation Tasks.">281. Rethinking the Agreement in Human Evaluation Tasks.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1281/">Paper Link</a>    Pages:3318-3329</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Amidei:Jacopo">Jacopo Amidei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Piwek:Paul">Paul Piwek</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Willis:Alistair">Alistair Willis</a></p>
<p>Abstract:
Human evaluations are broadly thought to be more valuable the higher the inter-annotator agreement. In this paper we examine this idea. We will describe our experiments and analysis within the area of Automatic Question Generation. Our experiments show how annotators diverge in language annotation tasks due to a range of ineliminable factors. For this reason, we believe that annotation schemes for natural language generation tasks that are aimed at evaluating language quality need to be treated with great care. In particular, an unchecked focus on reduction of disagreement among annotators runs the danger of creating generation goals that reward output that is more distant from, rather than closer to, natural human-like language. We conclude the paper by suggesting a new approach to the use of the agreement metrics in natural language generation evaluation tasks.</p>
<p>Keywords:</p>
<h3 id="282. Dependent Gated Reading for Cloze-Style Question Answering.">282. Dependent Gated Reading for Cloze-Style Question Answering.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1282/">Paper Link</a>    Pages:3330-3345</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ghaeini:Reza">Reza Ghaeini</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fern:Xiaoli_Z=">Xiaoli Z. Fern</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shahbazi:Hamed">Hamed Shahbazi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tadepalli:Prasad">Prasad Tadepalli</a></p>
<p>Abstract:
We present a novel deep learning architecture to address the cloze-style question answering task. Existing approaches employ reading mechanisms that do not fully exploit the interdependency between the document and the query. In this paper, we propose a novel dependent gated reading bidirectional GRU network (DGR) to efficiently model the relationship between the document and the query during encoding and decision making. Our evaluation shows that DGR obtains highly competitive performance on well-known machine comprehension benchmarks such as the Childrens Book Test (CBT-NE and CBT-CN) and Who DiD What (WDW, Strict and Relaxed). Finally, we extensively analyze and validate our model by ablation and attention studies.</p>
<p>Keywords:</p>
<h3 id="283. Automated Fact Checking: Task Formulations, Methods and Future Directions.">283. Automated Fact Checking: Task Formulations, Methods and Future Directions.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1283/">Paper Link</a>    Pages:3346-3359</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Thorne:James">James Thorne</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vlachos:Andreas">Andreas Vlachos</a></p>
<p>Abstract:
The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.</p>
<p>Keywords:</p>
<h3 id="284. Can Rumour Stance Alone Predict Veracity?">284. Can Rumour Stance Alone Predict Veracity?</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1284/">Paper Link</a>    Pages:3360-3370</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dungs:Sebastian">Sebastian Dungs</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aker:Ahmet">Ahmet Aker</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fuhr:Norbert">Norbert Fuhr</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bontcheva:Kalina">Kalina Bontcheva</a></p>
<p>Abstract:
Prior manual studies of rumours suggested that crowd stance can give insights into the actual rumour veracity. Even though numerous studies of automatic veracity classification of social media rumours have been carried out, none explored the effectiveness of leveraging crowd stance to determine veracity. We use stance as an additional feature to those commonly used in earlier studies. We also model the veracity of a rumour using variants of Hidden Markov Models (HMM) and the collective stance information. This paper demonstrates that HMMs that use stance and tweets times as the only features for modelling true and false rumours achieve F1 scores in the range of 80%, outperforming those approaches where stance is used jointly with content and user based features.</p>
<p>Keywords:</p>
<h3 id="285. Attending Sentences to detect Satirical Fake News.">285. Attending Sentences to detect Satirical Fake News.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1285/">Paper Link</a>    Pages:3371-3380</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sarkar:Sohan_De">Sohan De Sarkar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Fan">Fan Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mukherjee:Arjun">Arjun Mukherjee</a></p>
<p>Abstract:
Satirical news detection is important in order to prevent the spread of misinformation over the Internet. Existing approaches to capture news satire use machine learning models such as SVM and hierarchical neural networks along with hand-engineered features, but do not explore sentence and document difference. This paper proposes a robust, hierarchical deep neural network approach for satire detection, which is capable of capturing satire both at the sentence level and at the document level. The architecture incorporates pluggable generic neural networks like CNN, GRU, and LSTM. Experimental results on real world news satire dataset show substantial performance gains demonstrating the effectiveness of our proposed approach. An inspection of the learned models reveals the existence of key sentences that control the presence of satire in news.</p>
<p>Keywords:</p>
<h3 id="286. Predicting Stances from Social Media Posts using Factorization Machines.">286. Predicting Stances from Social Media Posts using Factorization Machines.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1286/">Paper Link</a>    Pages:3381-3390</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sasaki:Akira">Akira Sasaki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hanawa:Kazuaki">Kazuaki Hanawa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Okazaki:Naoaki">Naoaki Okazaki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Inui:Kentaro">Kentaro Inui</a></p>
<p>Abstract:
Social media provide platforms to express, discuss, and shape opinions about events and issues in the real world. An important step to analyze the discussions on social media and to assist in healthy decision-making is stance detection. This paper presents an approach to detect the stance of a user toward a topic based on their stances toward other topics and the social media posts of the user. We apply factorization machines, a widely used method in item recommendation, to model user preferences toward topics from the social media data. The experimental results demonstrate that users posts are useful to model topic preferences and therefore predict stances of silent users.</p>
<p>Keywords:</p>
<h3 id="287. Automatic Detection of Fake News.">287. Automatic Detection of Fake News.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1287/">Paper Link</a>    Pages:3391-3401</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/P=eacute=rez=Rosas:Ver=oacute=nica">Vernica Prez-Rosas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kleinberg:Bennett">Bennett Kleinberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lefevre:Alexandra">Alexandra Lefevre</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mihalcea:Rada">Rada Mihalcea</a></p>
<p>Abstract:
The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76%. In addition, we provide comparative analyses of the automatic and manual identification of fake news.</p>
<p>Keywords:</p>
<h3 id="288. All-in-one: Multi-task Learning for Rumour Verification.">288. All-in-one: Multi-task Learning for Rumour Verification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1288/">Paper Link</a>    Pages:3402-3413</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kochkina:Elena">Elena Kochkina</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liakata:Maria">Maria Liakata</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zubiaga:Arkaitz">Arkaitz Zubiaga</a></p>
<p>Abstract:
Automatic resolution of rumours is a challenging task that can be broken down into smaller components that make up a pipeline, including rumour detection, rumour tracking and stance classification, leading to the final outcome of determining the veracity of a rumour. In previous work, these steps in the process of rumour verification have been developed as separate components where the output of one feeds into the next. We propose a multi-task learning approach that allows joint training of the main and auxiliary tasks, improving the performance of rumour verification. We examine the connection between the dataset properties and the outcomes of the multi-task learning models used.</p>
<p>Keywords:</p>
<h3 id="289. Open Information Extraction on Scientific Text: An Evaluation.">289. Open Information Extraction on Scientific Text: An Evaluation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1289/">Paper Link</a>    Pages:3414-3423</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Groth:Paul_T=">Paul T. Groth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lauruhn:Michael">Michael Lauruhn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Scerri:Antony">Antony Scerri</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Daniel_Jr=:Ron">Ron Daniel Jr.</a></p>
<p>Abstract:
Open Information Extraction (OIE) is the task of the unsupervised creation of structured information from text. OIE is often used as a starting point for a number of downstream tasks including knowledge base construction, relation extraction, and question answering. While OIE methods are targeted at being domain independent, they have been evaluated primarily on newspaper, encyclopedic or general web text. In this article, we evaluate the performance of OIE on scientific texts originating from 10 different disciplines. To do so, we use two state-of-the-art OIE systems using a crowd-sourcing approach. We find that OIE systems perform significantly worse on scientific text than encyclopedic text. We also provide an error analysis and suggest areas of work to reduce errors. Our corpus of sentences and judgments are made available.</p>
<p>Keywords:</p>
<h3 id="290. Simple Algorithms For Sentiment Analysis On Sentiment Rich, Data Poor Domains.">290. Simple Algorithms For Sentiment Analysis On Sentiment Rich, Data Poor Domains.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1290/">Paper Link</a>    Pages:3424-3435</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sarma:Prathusha_K=">Prathusha K. Sarma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sethares:William_A=">William A. Sethares</a></p>
<p>Abstract:
Standard word embedding algorithms learn vector representations from large corpora of text documents in an unsupervised fashion. However, the quality of word embeddings learned from these algorithms is affected by the size of training data sets. Thus, applications of these algorithms in domains with only moderate amounts of available data is limited. In this paper we introduce an algorithm that learns word embeddings jointly with a classifier. Our algorithm is called SWESA (Supervised Word Embeddings for Sentiment Analysis). SWESA leverages document label information to learn vector representations of words from a modest corpus of text documents by solving an optimization problem that minimizes a cost function with respect to both word embeddings and the weight vector used for classification. Experiments on several real world data sets show that SWESA has superior performance on domains with limited data, when compared to previously suggested approaches to word embeddings and sentiment analysis tasks.</p>
<p>Keywords:</p>
<h3 id="291. Word-Level Loss Extensions for Neural Temporal Relation Classification.">291. Word-Level Loss Extensions for Neural Temporal Relation Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1291/">Paper Link</a>    Pages:3436-3447</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Leeuwenberg:Artuur">Artuur Leeuwenberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moens:Marie=Francine">Marie-Francine Moens</a></p>
<p>Abstract:
Unsupervised pre-trained word embeddings are used effectively for many tasks in natural language processing to leverage unlabeled textual data. Often these embeddings are either used as initializations or as fixed word representations for task-specific classification models. In this work, we extend our classification models task loss with an unsupervised auxiliary loss on the word-embedding level of the model. This is to ensure that the learned word representations contain both task-specific features, learned from the supervised loss component, and more general features learned from the unsupervised loss component. We evaluate our approach on the task of temporal relation extraction, in particular, narrative containment relation extraction from clinical records, and show that continued training of the embeddings on the unsupervised objective together with the task objective gives better task-specific embeddings, and results in an improvement over the state of the art on the THYME dataset, using only a general-domain part-of-speech tagger as linguistic resource.</p>
<p>Keywords:</p>
<h3 id="292. Personalized Text Retrieval for Learners of Chinese as a Foreign Language.">292. Personalized Text Retrieval for Learners of Chinese as a Foreign Language.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1292/">Paper Link</a>    Pages:3448-3455</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yeung:Chak_Yan">Chak Yan Yeung</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee_0001:John">John Lee</a></p>
<p>Abstract:
This paper describes a personalized text retrieval algorithm that helps language learners select the most suitable reading material in terms of vocabulary complexity. The user first rates their knowledge of a small set of words, chosen by a graph-based active learning model. The system trains a complex word identification model on this set, and then applies the model to find texts that contain the desired proportion of new, challenging, and familiar vocabulary. In an evaluation on learners of Chinese as a foreign language, we show that this algorithm is effective in identifying simpler texts for low-proficiency learners, and more challenging ones for high-proficiency learners.</p>
<p>Keywords:</p>
<h3 id="293. Punctuation as Native Language Interference.">293. Punctuation as Native Language Interference.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1293/">Paper Link</a>    Pages:3456-3466</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Markov:Ilia">Ilia Markov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nastase:Vivi">Vivi Nastase</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Strapparava:Carlo">Carlo Strapparava</a></p>
<p>Abstract:
In this paper, we describe experiments designed to explore and evaluate the impact of punctuation marks on the task of native language identification. Punctuation is specific to each language, and is part of the indicators that overtly represent the manner in which each language organizes and conveys information. Our experiments are organized in various set-ups: the usual multi-class classification for individual languages, also considering classification by language groups, across different proficiency levels, topics and even cross-corpus. The results support our hypothesis that punctuation marks are persistent and robust indicators of the native language of the author, which do not diminish in influence even when a high proficiency level in a non-native language is achieved.</p>
<p>Keywords:</p>
<h3 id="294. Investigating Productive and Receptive Knowledge: A Profile for Second Language Learning.">294. Investigating Productive and Receptive Knowledge: A Profile for Second Language Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1294/">Paper Link</a>    Pages:3467-3478</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zilio:Leonardo">Leonardo Zilio</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wilkens:Rodrigo">Rodrigo Wilkens</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fairon:C=eacute=drick">Cdrick Fairon</a></p>
<p>Abstract:
The literature frequently addresses the differences in receptive and productive vocabulary, but grammar is often left unacknowledged in second language acquisition studies. In this paper, we used two corpora to investigate the divergences in the behavior of pedagogically relevant grammatical structures in reception and production texts. We further improved the divergence scores observed in this investigation by setting a polarity to them that indicates whether there is overuse or underuse of a grammatical structure by language learners. This led to the compilation of a language profile that was later combined with vocabulary and readability features for classifying reception and production texts in three classes: beginner, intermediate, and advanced. The results of the automatic classification task in both production (0.872 of F-measure) and reception (0.942 of F-measure) were comparable to the current state of the art. We also attempted to automatically attribute a score to texts produced by learners, and the correlation results were encouraging, but there is still a good amount of room for improvement in this task. The developed language profile will serve as input for a system that helps language learners to activate more of their passive knowledge in writing texts.</p>
<p>Keywords:</p>
<h3 id="295. iParaphrasing: Extracting Visually Grounded Paraphrases via an Image.">295. iParaphrasing: Extracting Visually Grounded Paraphrases via an Image.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1295/">Paper Link</a>    Pages:3479-3492</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chu:Chenhui">Chenhui Chu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Otani:Mayu">Mayu Otani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nakashima:Yuta">Yuta Nakashima</a></p>
<p>Abstract:
A paraphrase is a restatement of the meaning of a text in other words. Paraphrases have been studied to enhance the performance of many natural language processing tasks. In this paper, we propose a novel task iParaphrasing to extract visually grounded paraphrases (VGPs), which are different phrasal expressions describing the same visual concept in an image. These extracted VGPs have the potential to improve language and image multimodal tasks such as visual question answering and image captioning. How to model the similarity between VGPs is the key of iParaphrasing. We apply various existing methods as well as propose a novel neural network-based method with image attention, and report the results of the first attempt toward iParaphrasing.</p>
<p>Keywords:</p>
<h3 id="296. MCDTB: A Macro-level Chinese Discourse TreeBank.">296. MCDTB: A Macro-level Chinese Discourse TreeBank.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1296/">Paper Link</a>    Pages:3493-3504</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Feng">Feng Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Sheng">Sheng Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chu:Xiaomin">Xiaomin Chu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Peifeng">Peifeng Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Qiaoming">Qiaoming Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>Abstract:
In view of the differences between the annotations of micro and macro discourse rela-tionships, this paper describes the relevant experiments on the construction of the Macro Chinese Discourse Treebank (MCDTB), a higher-level Chinese discourse corpus. Fol-lowing RST (Rhetorical Structure Theory), we annotate the macro discourse information, including discourse structure, nuclearity and relationship, and the additional discourse information, including topic sentences, lead and abstract, to make the macro discourse annotation more objective and accurate. Finally, we annotated 720 articles with a Kappa value greater than 0.6. Preliminary experiments on this corpus verify the computability of MCDTB.</p>
<p>Keywords:</p>
<h3 id="297. Corpus-based Content Construction.">297. Corpus-based Content Construction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1297/">Paper Link</a>    Pages:3505-3515</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Srinivasan:Balaji_Vasan">Balaji Vasan Srinivasan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Maneriker:Pranav">Pranav Maneriker</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Krishna:Kundan">Kundan Krishna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Modani:Natwar">Natwar Modani</a></p>
<p>Abstract:
Enterprise content writers are engaged in writing textual content for various purposes. Often, the text being written may already be present in the enterprise corpus in the form of past articles and can be re-purposed for the current needs. In the absence of suitable tools, authors manually curate/create such content (sometimes from scratch) which reduces their productivity. To address this, we propose an automatic approach to generate an initial version of the authors intended text based on an input content snippet. Starting with a set of extracted textual fragments related to the snippet based on the query words in it, the proposed approach builds the desired text from these fragment by simultaneously optimizing the information coverage, relevance, diversity and coherence in the generated content. Evaluations on standard datasets shows improved performance against existing baselines on several metrics.</p>
<p>Keywords:</p>
<h3 id="298. Bridging resolution: Task definition, corpus resources and rule-based experiments.">298. Bridging resolution: Task definition, corpus resources and rule-based experiments.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1298/">Paper Link</a>    Pages:3516-3528</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/R=ouml=siger:Ina">Ina Rsiger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riester:Arndt">Arndt Riester</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kuhn:Jonas">Jonas Kuhn</a></p>
<p>Abstract:
Recent work on bridging resolution has so far been based on the corpus ISNotes (Markert et al. 2012), as this was the only corpus available with unrestricted bridging annotation. Hou et al. 2014s rule-based system currently achieves state-of-the-art performance on this corpus, as learning-based approaches suffer from the lack of available training data. Recently, a number of new corpora with bridging annotations have become available. To test the generalisability of the approach by Hou et al. 2014, we apply a slightly extended rule-based system to these corpora. Besides the expected out-of-domain effects, we also observe low performance on some of the in-domain corpora. Our analysis shows that this is the result of two very different phenomena being defined as bridging, namely referential and lexical bridging. We also report that filtering out gold or predicted coreferent anaphors before applying the bridging resolution system helps improve bridging resolution.</p>
<p>Keywords:</p>
<h3 id="299. Semi-Supervised Disfluency Detection.">299. Semi-Supervised Disfluency Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1299/">Paper Link</a>    Pages:3529-3538</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0023:Feng">Feng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0048:Wei">Wei Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0007:Zhen">Zhen Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Qianqian">Qianqian Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Shuang">Shuang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0002:Bo">Bo Xu</a></p>
<p>Abstract:
While the disfluency detection has achieved notable success in the past years, it still severely suffers from the data scarcity. To tackle this problem, we propose a novel semi-supervised approach which can utilize large amounts of unlabelled data. In this work, a light-weight neural net is proposed to extract the hidden features based solely on self-attention without any Recurrent Neural Network (RNN) or Convolutional Neural Network (CNN). In addition, we use the unlabelled corpus to enhance the performance. Besides, the Generative Adversarial Network (GAN) training is applied to enforce the similar distribution between the labelled and unlabelled data. The experimental results show that our approach achieves significant improvements over strong baselines.</p>
<p>Keywords:</p>
<h3 id="300. ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents.">300. ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1300/">Paper Link</a>    Pages:3539-3551</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mezza:Stefano">Stefano Mezza</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cervone:Alessandra">Alessandra Cervone</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stepanov:Evgeny_A=">Evgeny A. Stepanov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tortoreto:Giuliano">Giuliano Tortoreto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riccardi:Giuseppe">Giuseppe Riccardi</a></p>
<p>Abstract:
Dialogue Act (DA) tagging is crucial for spoken language understanding systems, as it provides a general representation of speakers intents, not bound to a particular dialogue system. Unfortunately, publicly available data sets with DA annotation are all based on different annotation schemes and thus incompatible with each other. Moreover, their schemes often do not cover all aspects necessary for open-domain human-machine interaction. In this paper, we propose a methodology to map several publicly available corpora to a subset of the ISO standard, in order to create a large task-independent training corpus for DA classification. We show the feasibility of using this corpus to train a domain-independent DA tagger testing it on out-of-domain conversational data, and argue the importance of training on multiple corpora to achieve robustness across different DA categories.</p>
<p>Keywords:</p>
<h3 id="301. Arrows are the Verbs of Diagrams.">301. Arrows are the Verbs of Diagrams.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1301/">Paper Link</a>    Pages:3552-3563</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Alikhani:Malihe">Malihe Alikhani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stone:Matthew">Matthew Stone</a></p>
<p>Abstract:
Arrows are a key ingredient of schematic pictorial communication. This paper investigates the interpretation of arrows through linguistic, crowdsourcing and machine-learning methodology. Our work establishes a novel analogy between arrows and verbs: we advocate representing arrows in terms of qualitatively different structural and semantic frames, and resolving frames to specific interpretations using shallow world knowledge.</p>
<p>Keywords:</p>
<h3 id="302. Improving Feature Extraction for Pathology Reports with Precise Negation Scope Detection.">302. Improving Feature Extraction for Pathology Reports with Precise Negation Scope Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1302/">Paper Link</a>    Pages:3564-3575</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zamaraeva:Olga">Olga Zamaraeva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Howell:Kristen">Kristen Howell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rhine:Adam">Adam Rhine</a></p>
<p>Abstract:
We use a broad coverage, linguistically precise English Resource Grammar (ERG) to detect negation scope in sentences taken from pathology reports. We show that incorporating this information in feature extraction has a positive effect on classification of the reports with respect to cancer laterality compared with NegEx, a commonly used tool for negation detection. We analyze the differences between NegEx and ERG results on our dataset and how these differences indicate some directions for future work.</p>
<p>Keywords:</p>
<h3 id="303. Bridge Video and Text with Cascade Syntactic Structure.">303. Bridge Video and Text with Cascade Syntactic Structure.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1303/">Paper Link</a>    Pages:3576-3585</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Guolong">Guolong Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin:Zheng">Zheng Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Kaiping">Kaiping Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Kai">Kai Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Ye:Shuxiong">Shuxiong Ye</a></p>
<p>Abstract:
We present a video captioning approach that encodes features by progressively completing syntactic structure (LSTM-CSS). To construct basic syntactic structure (i.e., subject, predicate, and object), we use a Conditional Random Field to label semantic representations (i.e., motions, objects). We argue that in order to improve the comprehensiveness of the description, the local features within object regions can be used to generate complementary syntactic elements (e.g., attribute, adverbial). Inspired by redundancy of human receptors, we utilize a Region Proposal Network to focus on the object regions. To model the final temporal dynamics, Recurrent Neural Network with Path Embeddings is adopted. We demonstrate the effectiveness of LSTM-CSS on generating natural sentences: 42.3% and 28.5% in terms of BLEU@4 and METEOR. Superior performance when compared to state-of-the-art methods are reported on a large video description dataset (i.e., MSR-VTT-2016).</p>
<p>Keywords:</p>
<h3 id="304. Multi-task and Multi-lingual Joint Learning of Neural Lexical Utterance Classification based on Partially-shared Modeling.">304. Multi-task and Multi-lingual Joint Learning of Neural Lexical Utterance Classification based on Partially-shared Modeling.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1304/">Paper Link</a>    Pages:3586-3596</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Masumura:Ryo">Ryo Masumura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tanaka:Tomohiro">Tomohiro Tanaka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Higashinaka:Ryuichiro">Ryuichiro Higashinaka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Masataki:Hirokazu">Hirokazu Masataki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aono:Yushi">Yushi Aono</a></p>
<p>Abstract:
This paper is an initial study on multi-task and multi-lingual joint learning for lexical utterance classification. A major problem in constructing lexical utterance classification modules for spoken dialogue systems is that individual data resources are often limited or unbalanced among tasks and/or languages. Various studies have examined joint learning using neural-network based shared modeling; however, previous joint learning studies focused on either cross-task or cross-lingual knowledge transfer. In order to simultaneously support both multi-task and multi-lingual joint learning, our idea is to explicitly divide state-of-the-art neural lexical utterance classification into language-specific components that can be shared between different tasks and task-specific components that can be shared between different languages. In addition, in order to effectively transfer knowledge between different task data sets and different language data sets, this paper proposes a partially-shared modeling method that possesses both shared components and components specific to individual data sets. We demonstrate the effectiveness of proposed method using Japanese and English data sets with three different lexical utterance classification tasks.</p>
<p>Keywords:</p>
<h3 id="305. Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language.">305. Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1305/">Paper Link</a>    Pages:3597-3607</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bai:He">He Bai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Yu">Yu Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Liang">Liang Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hwang:Mei=Yuh">Mei-Yuh Hwang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>Abstract:
To deploy a spoken language understanding (SLU) model to a new language, language transferring is desired to avoid the trouble of acquiring and labeling a new big SLU corpus. An SLU corpus is a monolingual corpus with domain/intent/slot labels. Translating the original SLU corpus into the target language is an attractive strategy. However, SLU corpora consist of plenty of semantic labels (slots), which general-purpose translators cannot handle well, not to mention additional culture differences. This paper focuses on the language transferring task given a small in-domain parallel SLU corpus. The in-domain parallel corpus can be used as the first adaptation on the general translator. But more importantly, we show how to use reinforcement learning (RL) to further adapt the adapted translator, where translated sentences with more proper slot tags receive higher rewards. Our reward is derived from the source input sentence exclusively, unlike reward via actor-critical methods or computing reward with a ground truth target sentence. Hence we can adapt the translator the second time, using the big monolingual SLU corpus from the source language. We evaluate our approach on Chinese to English language transferring for SLU systems. The experimental results show that the generated English SLU corpus via adaptation and reinforcement learning gives us over 97% in the slot F1 score and over 84% accuracy in domain classification. It demonstrates the effectiveness of the proposed language transferring method. Compared with naive translation, our proposed method improves domain classification accuracy by relatively 22%, and the slot filling F1 score by relatively more than 71%.</p>
<p>Keywords:</p>
<h3 id="306. A Prospective-Performance Network to Alleviate Myopia in Beam Search for Response Generation.">306. A Prospective-Performance Network to Alleviate Myopia in Beam Search for Response Generation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1306/">Paper Link</a>    Pages:3608-3618</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zongsheng">Zongsheng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bai:Yunzhi">Yunzhi Bai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Bowen">Bowen Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0003:Zhen">Zhen Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhuoran">Zhuoran Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Baoxun">Baoxun Wang</a></p>
<p>Abstract:
Generative dialog models usually adopt beam search as the inference method to generate responses. However, small-width beam search only focuses on the limited current optima. This deficiency named as myopic bias ultimately suppresses the diversity and probability of generated responses. Although increasing the beam width mitigates the myopic bias, it also proportionally slows down the inference efficiency. To alleviate the myopic bias in small-width beam search, this paper proposes a Prospective-Performance Network (PPN) to predict the future reward of the given partially-generated response, and the future reward is defined by the expectation of the partial response appearing in the top-ranked responses given by a larger-width beam search. Enhanced by PPN, the decoder can promote the results with great potential during the beam search phase. The experimental results on both Chinese and English corpora show that our method is promising to increase the quality and diversity of generated responses, with inference efficiency well maintained.</p>
<p>Keywords:</p>
<h3 id="307. Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical Text.">307. Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical Text.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1307/">Paper Link</a>    Pages:3619-3630</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xing:Junjie">Junjie Xing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Kenny">Kenny Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Shaodian">Shaodian Zhang</a></p>
<p>Abstract:
Chinese word segmentation (CWS) trained from open source corpus faces dramatic performance drop when dealing with domain text, especially for a domain with lots of special terms and diverse writing styles, such as the biomedical domain. However, building domain-specific CWS requires extremely high annotation cost. In this paper, we propose an approach by exploiting domain-invariant knowledge from high resource to low resource domains. Extensive experiments show that our model achieves consistently higher accuracy than the single-task CWS and other transfer learning baselines, especially when there is a large disparity between source and target domains.</p>
<p>Keywords:</p>
<h3 id="308. Addressee and Response Selection for Multilingual Conversation.">308. Addressee and Response Selection for Multilingual Conversation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1308/">Paper Link</a>    Pages:3631-3644</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sato:Motoki">Motoki Sato</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ouchi:Hiroki">Hiroki Ouchi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsuboi:Yuta">Yuta Tsuboi</a></p>
<p>Abstract:
Developing conversational systems that can converse in many languages is an interesting challenge for natural language processing. In this paper, we introduce multilingual addressee and response selection. In this task, a conversational system predicts an appropriate addressee and response for an input message in multiple languages. A key to developing such multilingual responding systems is how to utilize high-resource language data to compensate for low-resource language data. We present several knowledge transfer methods for conversational systems. To evaluate our methods, we create a new multilingual conversation dataset. Experiments on the dataset demonstrate the effectiveness of our methods.</p>
<p>Keywords:</p>
<h3 id="309. Graph Based Decoding for Event Sequencing and Coreference Resolution.">309. Graph Based Decoding for Event Sequencing and Coreference Resolution.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1309/">Paper Link</a>    Pages:3645-3657</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhengzhong">Zhengzhong Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitamura:Teruko">Teruko Mitamura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>Abstract:
Events in text documents are interrelated in complex ways. In this paper, we study two types of relation: Event Coreference and Event Sequencing. We show that the popular tree-like decoding structure for automated Event Coreference is not suitable for Event Sequencing. To this end, we propose a graph-based decoding algorithm that is applicable to both tasks. The new decoding algorithm supports flexible feature sets for both tasks. Empirically, our event coreference system has achieved state-of-the-art performance on the TAC-KBP 2015 event coreference task and our event sequencing system beats a strong temporal-based, oracle-informed baseline. We discuss the challenges of studying these event relations.</p>
<p>Keywords:</p>
<h3 id="310. DIDEC: The Dutch Image Description and Eye-tracking Corpus.">310. DIDEC: The Dutch Image Description and Eye-tracking Corpus.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1310/">Paper Link</a>    Pages:3658-3669</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Miltenburg:Emiel_van">Emiel van Miltenburg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/K=aacute=d=aacute=r:=Aacute=kos">kos Kdr</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Koolen:Ruud">Ruud Koolen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Krahmer:Emiel">Emiel Krahmer</a></p>
<p>Abstract:
We present a corpus of spoken Dutch image descriptions, paired with two sets of eye-tracking data: Free viewing, where participants look at images without any particular purpose, and Description viewing, where we track eye movements while participants produce spoken descriptions of the images they are viewing. This paper describes the data collection procedure and the corpus itself, and provides an initial analysis of self-corrections in image descriptions. We also present two studies showing the potential of this data. Though these studies mainly serve as an example, we do find two interesting results: (1) the eye-tracking data for the description viewing task is more coherent than for the free-viewing task; (2) variation in image descriptions (also called image specificity; Jas and Parikh, 2015) is only moderately correlated across different languages. Our corpus can be used to gain a deeper understanding of the image description task, particularly how visual attention is correlated with the image description process.</p>
<p>Keywords:</p>
<h3 id="311. Narrative Schema Stability in News Text.">311. Narrative Schema Stability in News Text.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1311/">Paper Link</a>    Pages:3670-3680</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Simonson:Dan">Dan Simonson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Davis:Anthony">Anthony Davis</a></p>
<p>Abstract:
We investigate the stability of narrative schemas (Chambers and Jurafsky, 2009) automatically induced from a news corpus, representing recurring narratives in a corpus. If such techniques produce meaningful results, we should expect that small changes to the corpus will result in only small changes to the induced schemas. We describe experiments involving successive ablation of a corpus and cross-validation at each stage of ablation, on schemas generated by three different techniques over a general news corpus and topically-specific subcorpora. We also develop a method for evaluating the similarity between sets of narrative schemas, and thus the stability of the schema induction algorithms. This stability analysis affirms the heterogeneous/homogeneous document category hypothesis first presented in Simonson and Davis (2016), whose technique is problematically limited. Additionally, increased ablation leads to increasing stability, so the smaller the remaining corpus, the more stable schema generation appears to be. We surmise that as a corpus grows larger, novel and more varied narratives continue to appear and stability declines, though at some point this decline levels off as new additions to the corpus consist essentially of more of the same.</p>
<p>Keywords:</p>
<h3 id="312. NIPS Conversational Intelligence Challenge 2017 Winner System: Skill-based Conversational Agent with Supervised Dialog Manager.">312. NIPS Conversational Intelligence Challenge 2017 Winner System: Skill-based Conversational Agent with Supervised Dialog Manager.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1312/">Paper Link</a>    Pages:3681-3692</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yusupov:Idris">Idris Yusupov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kuratov:Yurii">Yurii Kuratov</a></p>
<p>Abstract:
We present bot#1337: a dialog system developed for the 1st NIPS Conversational Intelligence Challenge 2017 (ConvAI). The aim of the competition was to implement a bot capable of conversing with humans based on a given passage of text. To enable conversation, we implemented a set of skills for our bot, including chit-chat, topic detection, text summarization, question answering and question generation. The system has been trained in a supervised setting using a dialogue manager to select an appropriate skill for generating a response. The latter allows a developer to focus on the skill implementation rather than the finite state machine based dialog manager. The proposed system bot#1337 won the competition with an average dialogue quality score of 2.78 out of 5 given by human evaluators. Source code and trained models for the bot#1337 are available on GitHub.</p>
<p>Keywords:</p>
<h3 id="313. AMR Beyond the Sentence: the Multi-sentence AMR corpus.">313. AMR Beyond the Sentence: the Multi-sentence AMR corpus.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1313/">Paper Link</a>    Pages:3693-3702</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/o/O=Gorman:Tim">Tim O&apos;Gorman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Regan:Michael">Michael Regan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Griffitt:Kira">Kira Griffitt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hermjakob:Ulf">Ulf Hermjakob</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Knight:Kevin">Kevin Knight</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Palmer:Martha">Martha Palmer</a></p>
<p>Abstract:
There are few corpora that endeavor to represent the semantic content of entire documents. We present a corpus that accomplishes one way of capturing document level semantics, by annotating coreference and similar phenomena (bridging and implicit roles) on top of gold Abstract Meaning Representations of sentence-level semantics. We present a new corpus of this annotation, with analysis of its quality, alongside a plausible baseline for comparison. It is hoped that this Multi-Sentence AMR corpus (MS-AMR) may become a feasible method for developing rich representations of document meaning, useful for tasks such as information extraction and question answering.</p>
<p>Keywords:</p>
<h3 id="314. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model.">314. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1314/">Paper Link</a>    Pages:3703-3714</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Lu">Lu Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Zhongyu">Zhongyu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Xiangkun">Xiangkun Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0004:Yang">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Qi">Qi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>Abstract:
In this paper, we investigate the issue of persuasiveness evaluation for argumentative comments. Most of the existing research explores different text features of reply comments on word level and ignores interactions between participants. In general, viewpoints are usually expressed by multiple arguments and exchanged on argument level. To better model the process of dialogical argumentation, we propose a novel co-attention mechanism based neural network to capture the interactions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply.</p>
<p>Keywords:</p>
<h3 id="315. Learning Visually-Grounded Semantics from Contrastive Adversarial Samples.">315. Learning Visually-Grounded Semantics from Contrastive Adversarial Samples.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1315/">Paper Link</a>    Pages:3715-3727</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Haoyue">Haoyue Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mao:Jiayuan">Jiayuan Mao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Tete">Tete Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Yuning">Yuning Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0015:Jian">Jian Sun</a></p>
<p>Abstract:
We study the problem of grounding distributional representations of texts on the visual domain, namely visual-semantic embeddings (VSE for short). Begin with an insightful adversarial attack on VSE embeddings, we show the limitation of current frameworks and image-text datasets (e.g., MS-COCO) both quantitatively and qualitatively. The large gap between the number of possible constitutions of real-world semantics and the size of parallel data, to a large extent, restricts the model to establish a strong link between textual semantics and visual concepts. We alleviate this problem by augmenting the MS-COCO image captioning datasets with textual contrastive adversarial samples. These samples are synthesized using language priors of human and the WordNet knowledge base, and enforce the model to ground learned embeddings to concrete concepts within the image. This simple but powerful technique brings a noticeable improvement over the baselines on a diverse set of downstream tasks, in addition to defending known-type adversarial attacks. Codes are available at <a href="https://github.com/ExplorerFreda/VSE-C">https://github.com/ExplorerFreda/VSE-C</a>.</p>
<p>Keywords:</p>
<h3 id="316. Structured Representation Learning for Online Debate Stance Prediction.">316. Structured Representation Learning for Online Debate Stance Prediction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1316/">Paper Link</a>    Pages:3728-3739</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Chang">Chang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Porco:Aldo">Aldo Porco</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goldwasser:Dan">Dan Goldwasser</a></p>
<p>Abstract:
Online debates can help provide valuable information about various perspectives on a wide range of issues. However, understanding the stances expressed in these debates is a highly challenging task, which requires modeling both textual content and users conversational interactions. Current approaches take a collective classification approach, which ignores the relationships between different debate topics. In this work, we suggest to view this task as a representation learning problem, and embed the text and authors jointly based on their interactions. We evaluate our model over the Internet Argumentation Corpus, and compare different approaches for structural information embedding. Experimental results show that our model can achieve significantly better results compared to previous competitive models.</p>
<p>Keywords:</p>
<h3 id="317. Modeling Multi-turn Conversation with Deep Utterance Aggregation.">317. Modeling Multi-turn Conversation with Deep Utterance Aggregation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1317/">Paper Link</a>    Pages:3740-3752</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Zhuosheng">Zhuosheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jiangtong">Jiangtong Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Pengfei">Pengfei Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Gongshen">Gongshen Liu</a></p>
<p>Abstract:
Multi-turn conversation understanding is a major challenge for building intelligent dialogue systems. This work focuses on retrieval-based response matching for multi-turn conversation whose related work simply concatenates the conversation utterances, ignoring the interactions among previous utterances for context modeling. In this paper, we formulate previous utterances into context using a proposed deep utterance aggregation model to form a fine-grained context representation. In detail, a self-matching attention is first introduced to route the vital information in each utterance. Then the model matches a response with each refined utterance and the final matching score is obtained after attentive turns aggregation. Experimental results show our model outperforms the state-of-the-art methods on three multi-turn conversation benchmarks, including a newly introduced e-commerce dialogue corpus.</p>
<p>Keywords:</p>
<h3 id="318. Argumentation Synthesis following Rhetorical Strategies.">318. Argumentation Synthesis following Rhetorical Strategies.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1318/">Paper Link</a>    Pages:3753-3765</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wachsmuth:Henning">Henning Wachsmuth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stede:Manfred">Manfred Stede</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baff:Roxanne_El">Roxanne El Baff</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khatib:Khalid_Al">Khalid Al Khatib</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Skeppstedt:Maria">Maria Skeppstedt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stein_0001:Benno">Benno Stein</a></p>
<p>Abstract:
Persuasion is rarely achieved through a loose set of arguments alone. Rather, an effective delivery of arguments follows a rhetorical strategy, combining logical reasoning with appeals to ethics and emotion. We argue that such a strategy means to select, arrange, and phrase a set of argumentative discourse units. In this paper, we model rhetorical strategies for the computational synthesis of effective argumentation. In a study, we let 26 experts synthesize argumentative texts with different strategies for 10 topics. We find that the experts agree in the selection significantly more when following the same strategy. While the texts notably vary for different strategies, especially their arrangement remains stable. The results suggest that our model enables a strategical synthesis.</p>
<p>Keywords:</p>
<h3 id="319. A Dataset for Building Code-Mixed Goal Oriented Conversation Systems.">319. A Dataset for Building Code-Mixed Goal Oriented Conversation Systems.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1319/">Paper Link</a>    Pages:3766-3780</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Banerjee:Suman">Suman Banerjee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moghe:Nikita">Nikita Moghe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Arora:Siddhartha">Siddhartha Arora</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khapra:Mitesh_M=">Mitesh M. Khapra</a></p>
<p>Abstract:
There is an increasing demand for goal-oriented conversation systems which can assist users in various day-to-day activities such as booking tickets, restaurant reservations, shopping, etc. Most of the existing datasets for building such conversation systems focus on monolingual conversations and there is hardly any work on multilingual and/or code-mixed conversations. Such datasets and systems thus do not cater to the multilingual regions of the world, such as India, where it is very common for people to speak more than one language and seamlessly switch between them resulting in code-mixed conversations. For example, a Hindi speaking user looking to book a restaurant would typically ask, Kya tum is restaurant mein ek table book karne mein meri help karoge? (Can you help me in booking a table at this restaurant?). To facilitate the development of such code-mixed conversation models, we build a goal-oriented dialog dataset containing code-mixed conversations. Specifically, we take the text from the DSTC2 restaurant reservation dataset and create code-mixed versions of it in Hindi-English, Bengali-English, Gujarati-English and Tamil-English. We also establish initial baselines on this dataset using existing state of the art models. This dataset along with our baseline implementations will be made publicly available for research purposes.</p>
<p>Keywords:</p>
<h3 id="320. Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation.">320. Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1320/">Paper Link</a>    Pages:3781-3792</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wen:Haoyang">Haoyang Wen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yijia">Yijia Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Che:Wanxiang">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin:Libo">Libo Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>Abstract:
Classic pipeline models for task-oriented dialogue system require explicit modeling the dialogue states and hand-crafted action spaces to query a domain-specific knowledge base. Conversely, sequence-to-sequence models learn to map dialogue history to the response in current turn without explicit knowledge base querying. In this work, we propose a novel framework that leverages the advantages of classic pipeline and sequence-to-sequence models. Our framework models a dialogue state as a fixed-size distributed representation and use this representation to query a knowledge base via an attention mechanism. Experiment on Stanford Multi-turn Multi-domain Task-oriented Dialogue Dataset shows that our framework significantly outperforms other sequence-to-sequence based baseline models on both automatic and human evaluation.</p>
<p>Keywords:</p>
<h3 id="321. Incorporating Deep Visual Features into Multiobjective based Multi-view Search Results Clustering.">321. Incorporating Deep Visual Features into Multiobjective based Multi-view Search Results Clustering.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1321/">Paper Link</a>    Pages:3793-3805</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mitra:Sayantan">Sayantan Mitra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hasanuzzaman:Mohammed">Mohammed Hasanuzzaman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Saha_0001:Sriparna">Sriparna Saha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Way:Andy">Andy Way</a></p>
<p>Abstract:
Current paper explores the use of multi-view learning for search result clustering. A web-snippet can be represented using multiple views. Apart from textual view cued by both the semantic and syntactic information, a complimentary view extracted from images contained in the web-snippets is also utilized in the current framework. A single consensus partitioning is finally obtained after consulting these two individual views by the deployment of a multiobjective based clustering technique. Several objective functions including the values of a cluster quality measure measuring the goodness of partitionings obtained using different views and an agreement-disagreement index, quantifying the amount of oneness among multiple views in generating partitionings are optimized simultaneously using AMOSA. In order to detect the number of clusters automatically, concepts of variable length solutions and a vast range of permutation operators are introduced in the clustering process. Finally, a set of alternative partitioning are obtained on the final Pareto front by the proposed multi-view based multiobjective technique. Experimental results by the proposed approach on several benchmark test datasets of SRC with respect to different performance metrics evidently establish the power of visual and text-based views in achieving better search result clustering.</p>
<p>Keywords:</p>
<h3 id="322. Integrating Tree Structures and Graph Structures with Neural Networks to Classify Discussion Discourse Acts.">322. Integrating Tree Structures and Graph Structures with Neural Networks to Classify Discussion Discourse Acts.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1322/">Paper Link</a>    Pages:3806-3818</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Miura:Yasuhide">Yasuhide Miura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kano:Ryuji">Ryuji Kano</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Taniguchi:Motoki">Motoki Taniguchi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Taniguchi:Tomoki">Tomoki Taniguchi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Misawa:Shotaro">Shotaro Misawa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ohkuma:Tomoko">Tomoko Ohkuma</a></p>
<p>Abstract:
We proposed a model that integrates discussion structures with neural networks to classify discourse acts. Several attempts have been made in earlier works to analyze texts that are used in various discussions. The importance of discussion structures has been explored in those works but their methods required a sophisticated design to combine structural features with a classifier. Our model introduces tree learning approaches and a graph learning approach to directly capture discussion structures without structural features. In an evaluation to classify discussion discourse acts in Reddit, the model achieved improvements of 1.5% in accuracy and 2.2 in FB1 score compared to the previous best model. We further analyzed the model using an attention mechanism to inspect interactions among different learning approaches.</p>
<p>Keywords:</p>
<h3 id="323. AnlamVer: Semantic Model Evaluation Dataset for Turkish - Word Similarity and Relatedness.">323. AnlamVer: Semantic Model Evaluation Dataset for Turkish - Word Similarity and Relatedness.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1323/">Paper Link</a>    Pages:3819-3836</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Ercan:G=ouml=khan">Gkhan Ercan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yildiz:Olcay_Taner">Olcay Taner Yildiz</a></p>
<p>Abstract:
In this paper, we present AnlamVer, which is a semantic model evaluation dataset for Turkish designed to evaluate word similarity and word relatedness tasks while discriminating those two relations from each other. Our dataset consists of 500 word-pairs annotated by 12 human subjects, and each pair has two distinct scores for similarity and relatedness. Word-pairs are selected to enable the evaluation of distributional semantic models by multiple attributes of words and word-pair relations such as frequency, morphology, concreteness and relation types (e.g., synonymy, antonymy). Our aim is to provide insights to semantic model researchers by evaluating models in multiple attributes. We balance dataset word-pairs by their frequencies to evaluate the robustness of semantic models concerning out-of-vocabulary and rare words problems, which are caused by the rich derivational and inflectional morphology of the Turkish language.</p>
<p>Keywords:</p>
<h3 id="324. Arguments and Adjuncts in Universal Dependencies.">324. Arguments and Adjuncts in Universal Dependencies.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1324/">Paper Link</a>    Pages:3837-3852</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Przepi=oacute=rkowski:Adam">Adam Przepirkowski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Patejuk:Agnieszka">Agnieszka Patejuk</a></p>
<p>Abstract:
The aim of this paper is to argue for a coherent Universal Dependencies approach to the core vs. non-core distinction. We demonstrate inconsistencies in the current version 2 of UD in this respect  mostly resulting from the preservation of the argumentadjunct dichotomy despite the declared avoidance of this distinction  and propose a relatively conservative modification of UD that is free from these problems.</p>
<p>Keywords:</p>
<h3 id="325. Distinguishing affixoid formations from compounds.">325. Distinguishing affixoid formations from compounds.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1325/">Paper Link</a>    Pages:3853-3865</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ruppenhofer:Josef">Josef Ruppenhofer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wiegand:Michael">Michael Wiegand</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wilm:Rebecca">Rebecca Wilm</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Markert:Katja">Katja Markert</a></p>
<p>Abstract:
We study German affixoids, a type of morpheme in between affixes and free stems. Several properties have been associated with them  increased productivity; a bleached semantics, which is often evaluative and/or intensifying and thus of relevance to sentiment analysis; and the existence of a free morpheme counterpart  but not been validated empirically. In experiments on a new data set that we make available, we put these key assumptions from the morphological literature to the test and show that despite the fact that affixoids generate many low-frequency formations, we can classify these as affixoid or non-affixoid instances with a best F1-score of 74%.</p>
<p>Keywords:</p>
<h3 id="326. A Survey on Open Information Extraction.">326. A Survey on Open Information Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1326/">Paper Link</a>    Pages:3866-3878</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Niklaus:Christina">Christina Niklaus</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cetto:Matthias">Matthias Cetto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Freitas:Andr=eacute=">Andr Freitas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Handschuh:Siegfried">Siegfried Handschuh</a></p>
<p>Abstract:
We provide a detailed overview of the various approaches that were proposed to date to solve the task of Open Information Extraction. We present the major challenges that such systems face, show the evolution of the suggested approaches over time and depict the specific issues they address. In addition, we provide a critique of the commonly applied evaluation procedures for assessing the performance of Open IE systems and highlight some directions for future work.</p>
<p>Keywords:</p>
<h3 id="327. Design Challenges and Misconceptions in Neural Sequence Labeling.">327. Design Challenges and Misconceptions in Neural Sequence Labeling.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1327/">Paper Link</a>    Pages:3879-3889</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0039:Jie">Jie Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Shuailong">Shuailong Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a></p>
<p>Abstract:
We investigate the design challenges of constructing effective and efficient neural sequence labeling systems, by reproducing twelve neural sequence labeling models, which include most of the state-of-the-art structures, and conduct a systematic model comparison on three benchmarks (i.e. NER, Chunking, and POS tagging). Misconceptions and inconsistent conclusions in existing literature are examined and clarified under statistical experiments. In the comparison and analysis process, we reach several practical conclusions which can be useful to practitioners.</p>
<p>Keywords:</p>
<h3 id="328. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering.">328. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1328/">Paper Link</a>    Pages:3890-3902</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lan:Wuwei">Wuwei Lan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0004:Wei">Wei Xu</a></p>
<p>Abstract:
In this paper, we analyze several neural network designs (and their variations) for sentence pair modeling and compare their performance extensively across eight datasets, including paraphrase identification, semantic textual similarity, natural language inference, and question answering tasks. Although most of these models have claimed state-of-the-art performance, the original papers often reported on only one or two selected datasets. We provide a systematic study and show that (i) encoding contextual information by LSTM and inter-sentence interactions are critical, (ii) Tree-LSTM does not help as much as previously claimed but surprisingly improves performance on Twitter datasets, (iii) the Enhanced Sequential Inference Model is the best so far for larger datasets, while the Pairwise Word Interaction Model achieves the best performance when less data is available. We release our implementations as an open-source toolkit.</p>
<p>Keywords:</p>
<h3 id="329. Authorless Topic Models: Biasing Models Away from Known Structure.">329. Authorless Topic Models: Biasing Models Away from Known Structure.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1329/">Paper Link</a>    Pages:3903-3914</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Thompson:Laure">Laure Thompson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mimno:David">David Mimno</a></p>
<p>Abstract:
Most previous work in unsupervised semantic modeling in the presence of metadata has assumed that our goal is to make latent dimensions more correlated with metadata, but in practice the exact opposite is often true. Some users want topic models that highlight differences between, for example, authors, but others seek more subtle connections across authors. We introduce three metrics for identifying topics that are highly correlated with metadata, and demonstrate that this problem affects between 30 and 50% of the topics in models trained on two real-world collections, regardless of the size of the model. We find that we can predict which words cause this phenomenon and that by selectively subsampling these words we dramatically reduce topic-metadata correlation, improve topic stability, and maintain or even improve model quality.</p>
<p>Keywords:</p>
<h3 id="330. SGM: Sequence Generation Model for Multi-label Classification.">330. SGM: Sequence Generation Model for Multi-label Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/C18-1330/">Paper Link</a>    Pages:3915-3926</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Pengcheng">Pengcheng Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0101:Wei">Wei Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Shuming">Shuming Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Wei">Wei Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a></p>
<p>Abstract:
Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it. Extensive experimental results show that our proposed methods outperform previous work by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels.</p>
<p>Keywords:</p>
 

<div class="home">
<i title='' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title=''><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="https://github.com/huntercmd/ccf"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
