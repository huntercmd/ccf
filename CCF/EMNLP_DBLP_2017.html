 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="cssfile" rel="stylesheet" type="text/css" href="https://rawcdn.githack.com/huntercmd/blog/master/config/css/light.css">
<script src="https://rawcdn.githack.com/huntercmd/blog/d9beff1/config/css/skin.js"></script>
<script src="https://rawcdn.githack.com/huntercmd/blog/master/config/css/classie.js"></script>

<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#EMNLP 2017:Copenhagen, Denmark">EMNLP 2017:Copenhagen, Denmark</a><ul>
<li><a href="#Paper Num: 322 || Session Num: 0">Paper Num: 322 || Session Num: 0</a><ul>
<li><a href="#1. Monolingual Phrase Alignment on Parse Forests.">1. Monolingual Phrase Alignment on Parse Forests.</a></li>
<li><a href="#2. Fast(er">2. Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set.</a> Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set.)</li>
<li><a href="#3. Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs.">3. Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs.</a></li>
<li><a href="#4. Position-aware Attention and Supervised Data Improve Slot Filling.">4. Position-aware Attention and Supervised Data Improve Slot Filling.</a></li>
<li><a href="#5. Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach.">5. Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach.</a></li>
<li><a href="#6. Integrating Order Information and Event Relation for Script Event Prediction.">6. Integrating Order Information and Event Relation for Script Event Prediction.</a></li>
<li><a href="#7. Entity Linking for Queries by Searching Wikipedia Sentences.">7. Entity Linking for Queries by Searching Wikipedia Sentences.</a></li>
<li><a href="#8. Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data.">8. Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data.</a></li>
<li><a href="#9. Universal Semantic Parsing.">9. Universal Semantic Parsing.</a></li>
<li><a href="#10. Mimicking Word Embeddings using Subword RNNs.">10. Mimicking Word Embeddings using Subword RNNs.</a></li>
<li><a href="#11. Past, Present, Future: A Computational Investigation of the Typology of Tense in 1000 Languages.">11. Past, Present, Future: A Computational Investigation of the Typology of Tense in 1000 Languages.</a></li>
<li><a href="#12. Neural Machine Translation with Source-Side Latent Graph Parsing.">12. Neural Machine Translation with Source-Side Latent Graph Parsing.</a></li>
<li><a href="#13. Neural Machine Translation with Word Predictions.">13. Neural Machine Translation with Word Predictions.</a></li>
<li><a href="#14. Towards Decoding as Continuous Optimisation in Neural Machine Translation.">14. Towards Decoding as Continuous Optimisation in Neural Machine Translation.</a></li>
<li><a href="#15. Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space.">15. Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space.</a></li>
<li><a href="#16. Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks.">16. Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks.</a></li>
<li><a href="#17. Obj2Text: Generating Visually Descriptive Language from Object Layouts.">17. Obj2Text: Generating Visually Descriptive Language from Object Layouts.</a></li>
<li><a href="#18. End-to-end Neural Coreference Resolution.">18. End-to-end Neural Coreference Resolution.</a></li>
<li><a href="#19. Neural Net Models of Open-domain Discourse Coherence.">19. Neural Net Models of Open-domain Discourse Coherence.</a></li>
<li><a href="#20. Affinity-Preserving Random Walk for Multi-Document Summarization.">20. Affinity-Preserving Random Walk for Multi-Document Summarization.</a></li>
<li><a href="#21. A Mention-Ranking Model for Abstract Anaphora Resolution.">21. A Mention-Ranking Model for Abstract Anaphora Resolution.</a></li>
<li><a href="#22. Hierarchical Embeddings for Hypernymy Detection and Directionality.">22. Hierarchical Embeddings for Hypernymy Detection and Directionality.</a></li>
<li><a href="#23. Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics.">23. Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics.</a></li>
<li><a href="#24. Dict2vec : Learning Word Embeddings using Lexical Dictionaries.">24. Dict2vec : Learning Word Embeddings using Lexical Dictionaries.</a></li>
<li><a href="#25. Learning Chinese Word Representations From Glyphs Of Characters.">25. Learning Chinese Word Representations From Glyphs Of Characters.</a></li>
<li><a href="#26. Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext.">26. Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext.</a></li>
<li><a href="#27. Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components.">27. Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components.</a></li>
<li><a href="#28. Exploiting Morphological Regularities in Distributional Word Representations.">28. Exploiting Morphological Regularities in Distributional Word Representations.</a></li>
<li><a href="#29. Exploiting Word Internal Structures for Generic Chinese Sentence Representation.">29. Exploiting Word Internal Structures for Generic Chinese Sentence Representation.</a></li>
<li><a href="#30. High-risk learning: acquiring new word vectors from tiny data.">30. High-risk learning: acquiring new word vectors from tiny data.</a></li>
<li><a href="#31. Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding.">31. Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding.</a></li>
<li><a href="#32. VecShare: A Framework for Sharing Word Representation Vectors.">32. VecShare: A Framework for Sharing Word Representation Vectors.</a></li>
<li><a href="#33. Word Re-Embedding via Manifold Dimensionality Retention.">33. Word Re-Embedding via Manifold Dimensionality Retention.</a></li>
<li><a href="#34. MUSE: Modularizing Unsupervised Sense Embeddings.">34. MUSE: Modularizing Unsupervised Sense Embeddings.</a></li>
<li><a href="#35. Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging.">35. Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging.</a></li>
<li><a href="#36. Learning What's Easy: Fully Differentiable Neural Easy-First Taggers.">36. Learning What's Easy: Fully Differentiable Neural Easy-First Taggers.</a></li>
<li><a href="#37. Incremental Skip-gram Model with Negative Sampling.">37. Incremental Skip-gram Model with Negative Sampling.</a></li>
<li><a href="#38. Learning to select data for transfer learning with Bayesian Optimization.">38. Learning to select data for transfer learning with Bayesian Optimization.</a></li>
<li><a href="#39. Unsupervised Pretraining for Sequence to Sequence Learning.">39. Unsupervised Pretraining for Sequence to Sequence Learning.</a></li>
<li><a href="#40. Efficient Attention using a Fixed-Size Memory Representation.">40. Efficient Attention using a Fixed-Size Memory Representation.</a></li>
<li><a href="#41. Rotated Word Vector Representations and their Interpretability.">41. Rotated Word Vector Representations and their Interpretability.</a></li>
<li><a href="#42. A causal framework for explaining the predictions of black-box sequence-to-sequence models.">42. A causal framework for explaining the predictions of black-box sequence-to-sequence models.</a></li>
<li><a href="#43. Piecewise Latent Variables for Neural Variational Text Processing.">43. Piecewise Latent Variables for Neural Variational Text Processing.</a></li>
<li><a href="#44. Learning the Structure of Variable-Order CRFs: a finite-state perspective.">44. Learning the Structure of Variable-Order CRFs: a finite-state perspective.</a></li>
<li><a href="#45. Sparse Communication for Distributed Gradient Descent.">45. Sparse Communication for Distributed Gradient Descent.</a></li>
<li><a href="#46. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks.">46. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks.</a></li>
<li><a href="#47. Why ADAGRAD Fails for Online Topic Modeling.">47. Why ADAGRAD Fails for Online Topic Modeling.</a></li>
<li><a href="#48. Recurrent Attention Network on Memory for Aspect Sentiment Analysis.">48. Recurrent Attention Network on Memory for Aspect Sentiment Analysis.</a></li>
<li><a href="#49. A Cognition Based Attention Model for Sentiment Analysis.">49. A Cognition Based Attention Model for Sentiment Analysis.</a></li>
<li><a href="#50. Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews.">50. Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews.</a></li>
<li><a href="#51. Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal.">51. Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal.</a></li>
<li><a href="#52. Identifying Humor in Reviews using Background Text Sources.">52. Identifying Humor in Reviews using Background Text Sources.</a></li>
<li><a href="#53. Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision.">53. Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision.</a></li>
<li><a href="#54. Towards a Universal Sentiment Classifier in Multiple languages.">54. Towards a Universal Sentiment Classifier in Multiple languages.</a></li>
<li><a href="#55. Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network.">55. Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network.</a></li>
<li><a href="#56. Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters.">56. Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters.</a></li>
<li><a href="#57. Refining Word Embeddings for Sentiment Analysis.">57. Refining Word Embeddings for Sentiment Analysis.</a></li>
<li><a href="#58. A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis.">58. A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis.</a></li>
<li><a href="#59. Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings.">59. Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings.</a></li>
<li><a href="#60. Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association.">60. Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association.</a></li>
<li><a href="#61. DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning.">61. DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning.</a></li>
<li><a href="#62. Task-Oriented Query Reformulation with Reinforcement Learning.">62. Task-Oriented Query Reformulation with Reinforcement Learning.</a></li>
<li><a href="#63. Sentence Simplification with Deep Reinforcement Learning.">63. Sentence Simplification with Deep Reinforcement Learning.</a></li>
<li><a href="#64. Learning how to Active Learn: A Deep Reinforcement Learning Approach.">64. Learning how to Active Learn: A Deep Reinforcement Learning Approach.</a></li>
<li><a href="#65. Split and Rephrase.">65. Split and Rephrase.</a></li>
<li><a href="#66. Neural Response Generation via GAN with an Approximate Embedding Layer.">66. Neural Response Generation via GAN with an Approximate Embedding Layer.</a></li>
<li><a href="#67. A Hybrid Convolutional Variational Autoencoder for Text Generation.">67. A Hybrid Convolutional Variational Autoencoder for Text Generation.</a></li>
<li><a href="#68. Filling the Blanks (hint: plural noun">68. Filling the Blanks (hint: plural noun) for Mad Libs Humor.</a> for Mad Libs Humor.)</li>
<li><a href="#69. Measuring Thematic Fit with Distributional Feature Overlap.">69. Measuring Thematic Fit with Distributional Feature Overlap.</a></li>
<li><a href="#70. SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations.">70. SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations.</a></li>
<li><a href="#71. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.">71. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.</a></li>
<li><a href="#72. Determining Semantic Textual Similarity using Natural Deduction Proofs.">72. Determining Semantic Textual Similarity using Natural Deduction Proofs.</a></li>
<li><a href="#73. Multi-Grained Chinese Word Segmentation.">73. Multi-Grained Chinese Word Segmentation.</a></li>
<li><a href="#74. Don't Throw Those Morphological Analyzers Away Just Yet: Neural Morphological Disambiguation for Arabic.">74. Don't Throw Those Morphological Analyzers Away Just Yet: Neural Morphological Disambiguation for Arabic.</a></li>
<li><a href="#75. Paradigm Completion for Derivational Morphology.">75. Paradigm Completion for Derivational Morphology.</a></li>
<li><a href="#76. A Sub-Character Architecture for Korean Language Processing.">76. A Sub-Character Architecture for Korean Language Processing.</a></li>
<li><a href="#77. Do LSTMs really work so well for PoS tagging? - A replication study.">77. Do LSTMs really work so well for PoS tagging? - A replication study.</a></li>
<li><a href="#78. The Labeled Segmentation of Printed Books.">78. The Labeled Segmentation of Printed Books.</a></li>
<li><a href="#79. Cross-lingual Character-Level Neural Morphological Tagging.">79. Cross-lingual Character-Level Neural Morphological Tagging.</a></li>
<li><a href="#80. Word-Context Character Embeddings for Chinese Word Segmentation.">80. Word-Context Character Embeddings for Chinese Word Segmentation.</a></li>
<li><a href="#81. Segmentation-Free Word Embedding for Unsegmented Languages.">81. Segmentation-Free Word Embedding for Unsegmented Languages.</a></li>
<li><a href="#82. From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems.">82. From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems.</a></li>
<li><a href="#83. RACE: Large-scale ReAding Comprehension Dataset From Examinations.">83. RACE: Large-scale ReAding Comprehension Dataset From Examinations.</a></li>
<li><a href="#84. Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers.">84. Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers.</a></li>
<li><a href="#85. Learning Fine-Grained Expressions to Solve Math Word Problems.">85. Learning Fine-Grained Expressions to Solve Math Word Problems.</a></li>
<li><a href="#86. Structural Embedding of Syntactic Trees for Machine Comprehension.">86. Structural Embedding of Syntactic Trees for Machine Comprehension.</a></li>
<li><a href="#87. World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions.">87. World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions.</a></li>
<li><a href="#88. Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension.">88. Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension.</a></li>
<li><a href="#89. Deep Neural Solver for Math Word Problems.">89. Deep Neural Solver for Math Word Problems.</a></li>
<li><a href="#90. Latent Space Embedding for Retrieval in Question-Answer Archives.">90. Latent Space Embedding for Retrieval in Question-Answer Archives.</a></li>
<li><a href="#91. Question Generation for Question Answering.">91. Question Generation for Question Answering.</a></li>
<li><a href="#92. Learning to Paraphrase for Question Answering.">92. Learning to Paraphrase for Question Answering.</a></li>
<li><a href="#93. Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture.">93. Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture.</a></li>
<li><a href="#94. Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification Model.">94. Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification Model.</a></li>
<li><a href="#95. Recovering Question Answering Errors via Query Revision.">95. Recovering Question Answering Errors via Query Revision.</a></li>
<li><a href="#96. An empirical study on the effectiveness of images in Multimodal Neural Machine Translation.">96. An empirical study on the effectiveness of images in Multimodal Neural Machine Translation.</a></li>
<li><a href="#97. Sound-Word2Vec: Learning Word Representations Grounded in Sounds.">97. Sound-Word2Vec: Learning Word Representations Grounded in Sounds.</a></li>
<li><a href="#98. The Promise of Premise: Harnessing Question Premises in Visual Question Answering.">98. The Promise of Premise: Harnessing Question Premises in Visual Question Answering.</a></li>
<li><a href="#99. Guided Open Vocabulary Image Captioning with Constrained Beam Search.">99. Guided Open Vocabulary Image Captioning with Constrained Beam Search.</a></li>
<li><a href="#100. Zero-Shot Activity Recognition with Verb Attribute Induction.">100. Zero-Shot Activity Recognition with Verb Attribute Induction.</a></li>
<li><a href="#101. Deriving continous grounded meaning representations from referentially structured multimodal contexts.">101. Deriving continous grounded meaning representations from referentially structured multimodal contexts.</a></li>
<li><a href="#102. Hierarchically-Attentive RNN for Album Summarization and Storytelling.">102. Hierarchically-Attentive RNN for Album Summarization and Storytelling.</a></li>
<li><a href="#103. Video Highlight Prediction Using Audience Chat Reactions.">103. Video Highlight Prediction Using Audience Chat Reactions.</a></li>
<li><a href="#104. Reinforced Video Captioning with Entailment Rewards.">104. Reinforced Video Captioning with Entailment Rewards.</a></li>
<li><a href="#105. Evaluating Hierarchies of Verb Argument Structure with Hierarchical Clustering.">105. Evaluating Hierarchies of Verb Argument Structure with Hierarchical Clustering.</a></li>
<li><a href="#106. Incorporating Global Visual Features into Attention-based Neural Machine Translation.">106. Incorporating Global Visual Features into Attention-based Neural Machine Translation.</a></li>
<li><a href="#107. Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.">107. Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.</a></li>
<li><a href="#108. An analysis of eye-movements during reading for the detection of mild cognitive impairment.">108. An analysis of eye-movements during reading for the detection of mild cognitive impairment.</a></li>
<li><a href="#109. A Structured Learning Approach to Temporal Relation Extraction.">109. A Structured Learning Approach to Temporal Relation Extraction.</a></li>
<li><a href="#110. Importance sampling for unbiased on-demand evaluation of knowledge base population.">110. Importance sampling for unbiased on-demand evaluation of knowledge base population.</a></li>
<li><a href="#111. PACRR: A Position-Aware Neural IR Model for Relevance Matching.">111. PACRR: A Position-Aware Neural IR Model for Relevance Matching.</a></li>
<li><a href="#112. Globally Normalized Reader.">112. Globally Normalized Reader.</a></li>
<li><a href="#113. Speech segmentation with a neural encoder model of working memory.">113. Speech segmentation with a neural encoder model of working memory.</a></li>
<li><a href="#114. Speaking, Seeing, Understanding: Correlating semantic models with conceptual representation in the brain.">114. Speaking, Seeing, Understanding: Correlating semantic models with conceptual representation in the brain.</a></li>
<li><a href="#115. Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video.">115. Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video.</a></li>
<li><a href="#116. Tensor Fusion Network for Multimodal Sentiment Analysis.">116. Tensor Fusion Network for Multimodal Sentiment Analysis.</a></li>
<li><a href="#117. ConStance: Modeling Annotation Contexts to Improve Stance Classification.">117. ConStance: Modeling Annotation Contexts to Improve Stance Classification.</a></li>
<li><a href="#118. Deeper Attention to Abusive User Content Moderation.">118. Deeper Attention to Abusive User Content Moderation.</a></li>
<li><a href="#119. Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models.">119. Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models.</a></li>
<li><a href="#120. Human Centered NLP with User-Factor Adaptation.">120. Human Centered NLP with User-Factor Adaptation.</a></li>
<li><a href="#121. Neural Sequence Learning Models for Word Sense Disambiguation.">121. Neural Sequence Learning Models for Word Sense Disambiguation.</a></li>
<li><a href="#122. Learning Word Relatedness over Time.">122. Learning Word Relatedness over Time.</a></li>
<li><a href="#123. Inter-Weighted Alignment Network for Sentence Pair Modeling.">123. Inter-Weighted Alignment Network for Sentence Pair Modeling.</a></li>
<li><a href="#124. A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources and Recent Advances.">124. A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources and Recent Advances.</a></li>
<li><a href="#125. Idiom-Aware Compositional Distributed Semantics.">125. Idiom-Aware Compositional Distributed Semantics.</a></li>
<li><a href="#126. Macro Grammars and Holistic Triggering for Efficient Semantic Parsing.">126. Macro Grammars and Holistic Triggering for Efficient Semantic Parsing.</a></li>
<li><a href="#127. A Continuously Growing Dataset of Sentential Paraphrases.">127. A Continuously Growing Dataset of Sentential Paraphrases.</a></li>
<li><a href="#128. Cross-domain Semantic Parsing via Paraphrasing.">128. Cross-domain Semantic Parsing via Paraphrasing.</a></li>
<li><a href="#129. A Joint Sequential and Relational Model for Frame-Semantic Parsing.">129. A Joint Sequential and Relational Model for Frame-Semantic Parsing.</a></li>
<li><a href="#130. Getting the Most out of AMR Parsing.">130. Getting the Most out of AMR Parsing.</a></li>
<li><a href="#131. AMR Parsing using Stack-LSTMs.">131. AMR Parsing using Stack-LSTMs.</a></li>
<li><a href="#132. An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective.">132. An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective.</a></li>
<li><a href="#133. Predicting Word Association Strengths.">133. Predicting Word Association Strengths.</a></li>
<li><a href="#134. Learning Contextually Informed Representations for Linear-Time Discourse Parsing.">134. Learning Contextually Informed Representations for Linear-Time Discourse Parsing.</a></li>
<li><a href="#135. Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification.">135. Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification.</a></li>
<li><a href="#136. Chinese Zero Pronoun Resolution with Deep Memory Network.">136. Chinese Zero Pronoun Resolution with Deep Memory Network.</a></li>
<li><a href="#137. How much progress have we made on RST discourse parsing? A replication study of recent results on the RST-DT.">137. How much progress have we made on RST discourse parsing? A replication study of recent results on the RST-DT.</a></li>
<li><a href="#138. What is it? Disambiguating the different readings of the pronoun 'it'.">138. What is it? Disambiguating the different readings of the pronoun 'it'.</a></li>
<li><a href="#139. Revisiting Selectional Preferences for Coreference Resolution.">139. Revisiting Selectional Preferences for Coreference Resolution.</a></li>
<li><a href="#140. Learning to Rank Semantic Coherence for Topic Segmentation.">140. Learning to Rank Semantic Coherence for Topic Segmentation.</a></li>
<li><a href="#141. GRASP: Rich Patterns for Argumentation Mining.">141. GRASP: Rich Patterns for Argumentation Mining.</a></li>
<li><a href="#142. Patterns of Argumentation Strategies across Topics.">142. Patterns of Argumentation Strategies across Topics.</a></li>
<li><a href="#143. Using Argument-based Features to Predict and Analyse Review Helpfulness.">143. Using Argument-based Features to Predict and Analyse Review Helpfulness.</a></li>
<li><a href="#144. Here's My Point: Joint Pointer Architecture for Argument Mining.">144. Here's My Point: Joint Pointer Architecture for Argument Mining.</a></li>
<li><a href="#145. Identifying attack and support argumentative relations using deep learning.">145. Identifying attack and support argumentative relations using deep learning.</a></li>
<li><a href="#146. Neural Lattice-to-Sequence Models for Uncertain Inputs.">146. Neural Lattice-to-Sequence Models for Uncertain Inputs.</a></li>
<li><a href="#147. Memory-augmented Neural Machine Translation.">147. Memory-augmented Neural Machine Translation.</a></li>
<li><a href="#148. Dynamic Data Selection for Neural Machine Translation.">148. Dynamic Data Selection for Neural Machine Translation.</a></li>
<li><a href="#149. Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search.">149. Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search.</a></li>
<li><a href="#150. Translating Phrases in Neural Machine Translation.">150. Translating Phrases in Neural Machine Translation.</a></li>
<li><a href="#151. Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation.">151. Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation.</a></li>
<li><a href="#152. Learning Translations via Matrix Completion.">152. Learning Translations via Matrix Completion.</a></li>
<li><a href="#153. Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback.">153. Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback.</a></li>
<li><a href="#154. Towards Compact and Fast Neural Machine Translation Using a Combined Method.">154. Towards Compact and Fast Neural Machine Translation Using a Combined Method.</a></li>
<li><a href="#155. Instance Weighting for Neural Machine Translation Domain Adaptation.">155. Instance Weighting for Neural Machine Translation Domain Adaptation.</a></li>
<li><a href="#156. Regularization techniques for fine-tuning in neural machine translation.">156. Regularization techniques for fine-tuning in neural machine translation.</a></li>
<li><a href="#157. Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical Comparison of Two Phrase-Based Decoding Algorithms.">157. Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical Comparison of Two Phrase-Based Decoding Algorithms.</a></li>
<li><a href="#158. Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning.">158. Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning.</a></li>
<li><a href="#159. Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling.">159. Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling.</a></li>
<li><a href="#160. Neural Semantic Parsing with Type Constraints for Semi-Structured Tables.">160. Neural Semantic Parsing with Type Constraints for Semi-Structured Tables.</a></li>
<li><a href="#161. Joint Concept Learning and Semantic Parsing from Natural Language Explanations.">161. Joint Concept Learning and Semantic Parsing from Natural Language Explanations.</a></li>
<li><a href="#162. Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection.">162. Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection.</a></li>
<li><a href="#163. Identifying civilians killed by police with distantly supervised entity-event extraction.">163. Identifying civilians killed by police with distantly supervised entity-event extraction.</a></li>
<li><a href="#164. Asking too much? The rhetorical role of questions in political discourse.">164. Asking too much? The rhetorical role of questions in political discourse.</a></li>
<li><a href="#165. Detecting Perspectives in Political Debates.">165. Detecting Perspectives in Political Debates.</a></li>
<li><a href="#166. "i have a feeling trump will win..................": Forecasting Winners and Losers from User Predictions on Twitter.">166. "i have a feeling trump will win..................": Forecasting Winners and Losers from User Predictions on Twitter.</a></li>
<li><a href="#167. A Question Answering Approach for Emotion Cause Extraction.">167. A Question Answering Approach for Emotion Cause Extraction.</a></li>
<li><a href="#168. Story Comprehension for Predicting What Happens Next.">168. Story Comprehension for Predicting What Happens Next.</a></li>
<li><a href="#169. Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm.">169. Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm.</a></li>
<li><a href="#170. Opinion Recommendation Using A Neural Model.">170. Opinion Recommendation Using A Neural Model.</a></li>
<li><a href="#171. CRF Autoencoder for Unsupervised Dependency Parsing.">171. CRF Autoencoder for Unsupervised Dependency Parsing.</a></li>
<li><a href="#172. Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence.">172. Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence.</a></li>
<li><a href="#173. Incremental Graph-based Neural Dependency Parsing.">173. Incremental Graph-based Neural Dependency Parsing.</a></li>
<li><a href="#174. Neural Discontinuous Constituency Parsing.">174. Neural Discontinuous Constituency Parsing.</a></li>
<li><a href="#175. Stack-based Multi-layer Attention for Transition-based Dependency Parsing.">175. Stack-based Multi-layer Attention for Transition-based Dependency Parsing.</a></li>
<li><a href="#176. Dependency Grammar Induction with Neural Lexicalization and Big Training Data.">176. Dependency Grammar Induction with Neural Lexicalization and Big Training Data.</a></li>
<li><a href="#177. Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition.">177. Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition.</a></li>
<li><a href="#178. Effective Inference for Generative Neural Parsing.">178. Effective Inference for Generative Neural Parsing.</a></li>
<li><a href="#179. Semi-supervised Structured Prediction with Neural CRF Autoencoder.">179. Semi-supervised Structured Prediction with Neural CRF Autoencoder.</a></li>
<li><a href="#180. TAG Parsing with Neural Networks and Vector Representations of Supertags.">180. TAG Parsing with Neural Networks and Vector Representations of Supertags.</a></li>
<li><a href="#181. Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification.">181. Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification.</a></li>
<li><a href="#182. End-to-End Neural Relation Extraction with Global Optimization.">182. End-to-End Neural Relation Extraction with Global Optimization.</a></li>
<li><a href="#183. KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs.">183. KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs.</a></li>
<li><a href="#184. Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short.">184. Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short.</a></li>
<li><a href="#185. Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations.">185. Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations.</a></li>
<li><a href="#186. Incorporating Relation Paths in Neural Relation Extraction.">186. Incorporating Relation Paths in Neural Relation Extraction.</a></li>
<li><a href="#187. Adversarial Training for Relation Extraction.">187. Adversarial Training for Relation Extraction.</a></li>
<li><a href="#188. Context-Aware Representations for Knowledge Base Relation Extraction.">188. Context-Aware Representations for Knowledge Base Relation Extraction.</a></li>
<li><a href="#189. A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction.">189. A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction.</a></li>
<li><a href="#190. A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events.">190. A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events.</a></li>
<li><a href="#191. Deep Residual Learning for Weakly-Supervised Relation Extraction.">191. Deep Residual Learning for Weakly-Supervised Relation Extraction.</a></li>
<li><a href="#192. Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective.">192. Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective.</a></li>
<li><a href="#193. Exploring Vector Spaces for Semantic Relations.">193. Exploring Vector Spaces for Semantic Relations.</a></li>
<li><a href="#194. Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants.">194. Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants.</a></li>
<li><a href="#195. Dynamic Entity Representations in Neural Language Models.">195. Dynamic Entity Representations in Neural Language Models.</a></li>
<li><a href="#196. Towards Quantum Language Models.">196. Towards Quantum Language Models.</a></li>
<li><a href="#197. Reference-Aware Language Models.">197. Reference-Aware Language Models.</a></li>
<li><a href="#198. A Simple Language Model based on PMI Matrix Approximations.">198. A Simple Language Model based on PMI Matrix Approximations.</a></li>
<li><a href="#199. Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones.">199. Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones.</a></li>
<li><a href="#200. Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels.">200. Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels.</a></li>
<li><a href="#201. Initializing Convolutional Filters with Semantic Features for Text Classification.">201. Initializing Convolutional Filters with Semantic Features for Text Classification.</a></li>
<li><a href="#202. Shortest-Path Graph Kernels for Document Similarity.">202. Shortest-Path Graph Kernels for Document Similarity.</a></li>
<li><a href="#203. Adapting Topic Models using Lexical Associations with Tree Priors.">203. Adapting Topic Models using Lexical Associations with Tree Priors.</a></li>
<li><a href="#204. Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation for Crowdsourced Data.">204. Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation for Crowdsourced Data.</a></li>
<li><a href="#205. CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles.">205. CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles.</a></li>
<li><a href="#206. Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon Induction.">206. Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon Induction.</a></li>
<li><a href="#207. Unfolding and Shrinking Neural Machine Translation Ensembles.">207. Unfolding and Shrinking Neural Machine Translation Ensembles.</a></li>
<li><a href="#208. Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.">208. Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.</a></li>
<li><a href="#209. Trainable Greedy Decoding for Neural Machine Translation.">209. Trainable Greedy Decoding for Neural Machine Translation.</a></li>
<li><a href="#210. Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features.">210. Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features.</a></li>
<li><a href="#211. Fine Grained Citation Span for References in Wikipedia.">211. Fine Grained Citation Span for References in Wikipedia.</a></li>
<li><a href="#212. Identifying Semantic Edit Intentions from Revisions in Wikipedia.">212. Identifying Semantic Edit Intentions from Revisions in Wikipedia.</a></li>
<li><a href="#213. Accurate Supervised and Semi-Supervised Machine Reading for Long Documents.">213. Accurate Supervised and Semi-Supervised Machine Reading for Long Documents.</a></li>
<li><a href="#214. Adversarial Examples for Evaluating Reading Comprehension Systems.">214. Adversarial Examples for Evaluating Reading Comprehension Systems.</a></li>
<li><a href="#215. Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension.">215. Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension.</a></li>
<li><a href="#216. Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension.">216. Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension.</a></li>
<li><a href="#217. What is the Essence of a Claim? Cross-Domain Claim Identification.">217. What is the Essence of a Claim? Cross-Domain Claim Identification.</a></li>
<li><a href="#218. Identifying Where to Focus in Reading Comprehension for Neural Question Generation.">218. Identifying Where to Focus in Reading Comprehension for Neural Question Generation.</a></li>
<li><a href="#219. Break it Down for Me: A Study in Automated Lyric Annotation.">219. Break it Down for Me: A Study in Automated Lyric Annotation.</a></li>
<li><a href="#220. Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization.">220. Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization.</a></li>
<li><a href="#221. Deep Recurrent Generative Decoder for Abstractive Text Summarization.">221. Deep Recurrent Generative Decoder for Abstractive Text Summarization.</a></li>
<li><a href="#222. Extractive Summarization Using Multi-Task Learning with Document Classification.">222. Extractive Summarization Using Multi-Task Learning with Document Classification.</a></li>
<li><a href="#223. Towards Automatic Construction of News Overview Articles by News Synthesis.">223. Towards Automatic Construction of News Overview Articles by News Synthesis.</a></li>
<li><a href="#224. Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank.">224. Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank.</a></li>
<li><a href="#225. Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events.">225. Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events.</a></li>
<li><a href="#226. When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size">226. When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size).</a>.)</li>
<li><a href="#227. Steering Output Style and Topic in Neural Response Generation.">227. Steering Output Style and Topic in Neural Response Generation.</a></li>
<li><a href="#228. Preserving Distributional Information in Dialogue Act Classification.">228. Preserving Distributional Information in Dialogue Act Classification.</a></li>
<li><a href="#229. Adversarial Learning for Neural Dialogue Generation.">229. Adversarial Learning for Neural Dialogue Generation.</a></li>
<li><a href="#230. Using Context Information for Dialog Act Classification in DNN Framework.">230. Using Context Information for Dialog Act Classification in DNN Framework.</a></li>
<li><a href="#231. Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences.">231. Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences.</a></li>
<li><a href="#232. Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems.">232. Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems.</a></li>
<li><a href="#233. Affordable On-line Dialogue Policy Learning.">233. Affordable On-line Dialogue Policy Learning.</a></li>
<li><a href="#234. Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models.">234. Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models.</a></li>
<li><a href="#235. Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars.">235. Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars.</a></li>
<li><a href="#236. Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning.">236. Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning.</a></li>
<li><a href="#237. Why We Need New Evaluation Metrics for NLG.">237. Why We Need New Evaluation Metrics for NLG.</a></li>
<li><a href="#238. Challenges in Data-to-Document Generation.">238. Challenges in Data-to-Document Generation.</a></li>
<li><a href="#239. All that is English may be Hindi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media.">239. All that is English may be Hindi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media.</a></li>
<li><a href="#240. Multi-View Unsupervised User Feature Embedding for Social Media-based Substance Use Prediction.">240. Multi-View Unsupervised User Feature Embedding for Social Media-based Substance Use Prediction.</a></li>
<li><a href="#241. Demographic-aware word associations.">241. Demographic-aware word associations.</a></li>
<li><a href="#242. A Factored Neural Network Model for Characterizing Online Discussions in Vector Space.">242. A Factored Neural Network Model for Characterizing Online Discussions in Vector Space.</a></li>
<li><a href="#243. Dimensions of Interpersonal Relationships: Corpus and Experiments.">243. Dimensions of Interpersonal Relationships: Corpus and Experiments.</a></li>
<li><a href="#244. Argument Mining on Twitter: Arguments, Facts and Sources.">244. Argument Mining on Twitter: Arguments, Facts and Sources.</a></li>
<li><a href="#245. Distinguishing Japanese Non-standard Usages from Standard Ones.">245. Distinguishing Japanese Non-standard Usages from Standard Ones.</a></li>
<li><a href="#246. Connotation Frames of Power and Agency in Modern Films.">246. Connotation Frames of Power and Agency in Modern Films.</a></li>
<li><a href="#247. Controlling Human Perception of Basic User Traits.">247. Controlling Human Perception of Basic User Traits.</a></li>
<li><a href="#248. Topic Signatures in Political Campaign Speeches.">248. Topic Signatures in Political Campaign Speeches.</a></li>
<li><a href="#249. Assessing Objective Recommendation Quality through Political Forecasting.">249. Assessing Objective Recommendation Quality through Political Forecasting.</a></li>
<li><a href="#250. Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs Using Set Cover Problem.">250. Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs Using Set Cover Problem.</a></li>
<li><a href="#251. Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision.">251. Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision.</a></li>
<li><a href="#252. The Impact of Modeling Overall Argumentation with Tree Kernels.">252. The Impact of Modeling Overall Argumentation with Tree Kernels.</a></li>
<li><a href="#253. Learning Generic Sentence Representations Using Convolutional Neural Networks.">253. Learning Generic Sentence Representations Using Convolutional Neural Networks.</a></li>
<li><a href="#254. Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks.">254. Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks.</a></li>
<li><a href="#255. Part-of-Speech Tagging for Twitter with Adversarial Neural Networks.">255. Part-of-Speech Tagging for Twitter with Adversarial Neural Networks.</a></li>
<li><a href="#256. Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings.">256. Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings.</a></li>
<li><a href="#257. Does syntax help discourse segmentation? Not so much.">257. Does syntax help discourse segmentation? Not so much.</a></li>
<li><a href="#258. Deal or No Deal? End-to-End Learning of Negotiation Dialogues.">258. Deal or No Deal? End-to-End Learning of Negotiation Dialogues.</a></li>
<li><a href="#259. Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning.">259. Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning.</a></li>
<li><a href="#260. Towards Debate Automation: a Recurrent Model for Predicting Debate Winners.">260. Towards Debate Automation: a Recurrent Model for Predicting Debate Winners.</a></li>
<li><a href="#261. Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation.">261. Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation.</a></li>
<li><a href="#262. A Challenge Set Approach to Evaluating Machine Translation.">262. A Challenge Set Approach to Evaluating Machine Translation.</a></li>
<li><a href="#263. Knowledge Distillation for Bilingual Dictionary Induction.">263. Knowledge Distillation for Bilingual Dictionary Induction.</a></li>
<li><a href="#264. Machine Translation, it's a question of style, innit? The case of English tag questions.">264. Machine Translation, it's a question of style, innit? The case of English tag questions.</a></li>
<li><a href="#265. Deciphering Related Languages.">265. Deciphering Related Languages.</a></li>
<li><a href="#266. Identifying Cognate Sets Across Dictionaries of Related Languages.">266. Identifying Cognate Sets Across Dictionaries of Related Languages.</a></li>
<li><a href="#267. Learning Language Representations for Typology Prediction.">267. Learning Language Representations for Typology Prediction.</a></li>
<li><a href="#268. Cheap Translation for Cross-Lingual Named Entity Recognition.">268. Cheap Translation for Cross-Lingual Named Entity Recognition.</a></li>
<li><a href="#269. Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation.">269. Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation.</a></li>
<li><a href="#270. Classification of telicity using cross-linguistic annotation projection.">270. Classification of telicity using cross-linguistic annotation projection.</a></li>
<li><a href="#271. Counterfactual Learning from Bandit Feedback under Deterministic Logging : A Case Study in Statistical Machine Translation.">271. Counterfactual Learning from Bandit Feedback under Deterministic Logging : A Case Study in Statistical Machine Translation.</a></li>
<li><a href="#272. Learning Fine-grained Relations from Chinese User Generated Categories.">272. Learning Fine-grained Relations from Chinese User Generated Categories.</a></li>
<li><a href="#273. Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures.">273. Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures.</a></li>
<li><a href="#274. Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation.">274. Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation.</a></li>
<li><a href="#275. Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention Separators.">275. Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention Separators.</a></li>
<li><a href="#276. Deep Joint Entity Disambiguation with Local Neural Attention.">276. Deep Joint Entity Disambiguation with Local Neural Attention.</a></li>
<li><a href="#277. MinIE: Minimizing Facts in Open Information Extraction.">277. MinIE: Minimizing Facts in Open Information Extraction.</a></li>
<li><a href="#278. Scientific Information Extraction with Semi-supervised Neural Tagging.">278. Scientific Information Extraction with Semi-supervised Neural Tagging.</a></li>
<li><a href="#279. NITE: A Neural Inductive Teaching Framework for Domain Specific NER.">279. NITE: A Neural Inductive Teaching Framework for Domain Specific NER.</a></li>
<li><a href="#280. Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods.">280. Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods.</a></li>
<li><a href="#281. Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks.">281. Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks.</a></li>
<li><a href="#282. Fast and Accurate Entity Recognition with Iterated Dilated Convolutions.">282. Fast and Accurate Entity Recognition with Iterated Dilated Convolutions.</a></li>
<li><a href="#283. Entity Linking via Joint Encoding of Types, Descriptions, and Context.">283. Entity Linking via Joint Encoding of Types, Descriptions, and Context.</a></li>
<li><a href="#284. An Insight Extraction System on BioMedical Literature with Deep Neural Networks.">284. An Insight Extraction System on BioMedical Literature with Deep Neural Networks.</a></li>
<li><a href="#285. Word Etymology as Native Language Interference.">285. Word Etymology as Native Language Interference.</a></li>
<li><a href="#286. A Simpler and More Generalizable Story Detector using Verb and Character Features.">286. A Simpler and More Generalizable Story Detector using Verb and Character Features.</a></li>
<li><a href="#287. Multi-modular domain-tailored OCR post-correction.">287. Multi-modular domain-tailored OCR post-correction.</a></li>
<li><a href="#288. Learning to Predict Charges for Criminal Cases with Legal Basis.">288. Learning to Predict Charges for Criminal Cases with Legal Basis.</a></li>
<li><a href="#289. Quantifying the Effects of Text Duplication on Semantic Models.">289. Quantifying the Effects of Text Duplication on Semantic Models.</a></li>
<li><a href="#290. Identifying Semantically Deviating Outlier Documents.">290. Identifying Semantically Deviating Outlier Documents.</a></li>
<li><a href="#291. Detecting and Explaining Causes From Text For a Time Series Event.">291. Detecting and Explaining Causes From Text For a Time Series Event.</a></li>
<li><a href="#292. A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC.">292. A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC.</a></li>
<li><a href="#293. Identifying the Provision of Choices in Privacy Policy Text.">293. Identifying the Provision of Choices in Privacy Policy Text.</a></li>
<li><a href="#294. An Empirical Analysis of Edit Importance between Document Versions.">294. An Empirical Analysis of Edit Importance between Document Versions.</a></li>
<li><a href="#295. Transition-Based Disfluency Detection using LSTMs.">295. Transition-Based Disfluency Detection using LSTMs.</a></li>
<li><a href="#296. Neural Sequence-Labelling Models for Grammatical Error Correction.">296. Neural Sequence-Labelling Models for Grammatical Error Correction.</a></li>
<li><a href="#297. Adapting Sequence Models for Sentence Correction.">297. Adapting Sequence Models for Sentence Correction.</a></li>
<li><a href="#298. A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output.">298. A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output.</a></li>
<li><a href="#299. Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU.">299. Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU.</a></li>
<li><a href="#300. Exploiting Cross-Sentence Context for Neural Machine Translation.">300. Exploiting Cross-Sentence Context for Neural Machine Translation.</a></li>
<li><a href="#301. Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources.">301. Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources.</a></li>
<li><a href="#302. Image Pivoting for Learning Multilingual Multimodal Representations.">302. Image Pivoting for Learning Multilingual Multimodal Representations.</a></li>
<li><a href="#303. Neural Machine Translation with Source Dependency Representation.">303. Neural Machine Translation with Source Dependency Representation.</a></li>
<li><a href="#304. Visual Denotations for Recognizing Textual Entailment.">304. Visual Denotations for Recognizing Textual Entailment.</a></li>
<li><a href="#305. Sequence Effects in Crowdsourced Annotations.">305. Sequence Effects in Crowdsourced Annotations.</a></li>
<li><a href="#306. No Need to Pay Attention: Simple Recurrent Neural Networks Work!">306. No Need to Pay Attention: Simple Recurrent Neural Networks Work!</a></li>
<li><a href="#307. The strange geometry of skip-gram with negative sampling.">307. The strange geometry of skip-gram with negative sampling.</a></li>
<li><a href="#308. Natural Language Processing with Small Feed-Forward Networks.">308. Natural Language Processing with Small Feed-Forward Networks.</a></li>
<li><a href="#309. Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction.">309. Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction.</a></li>
<li><a href="#310. Analogs of Linguistic Structure in Deep Representations.">310. Analogs of Linguistic Structure in Deep Representations.</a></li>
<li><a href="#311. A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings.">311. A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings.</a></li>
<li><a href="#312. Learning what to read: Focused machine reading.">312. Learning what to read: Focused machine reading.</a></li>
<li><a href="#313. DOC: Deep Open Classification of Text Documents.">313. DOC: Deep Open Classification of Text Documents.</a></li>
<li><a href="#314. Charmanteau: Character Embedding Models For Portmanteau Creation.">314. Charmanteau: Character Embedding Models For Portmanteau Creation.</a></li>
<li><a href="#315. Using Automated Metaphor Identification to Aid in Detection and Prediction of First-Episode Schizophrenia.">315. Using Automated Metaphor Identification to Aid in Detection and Prediction of First-Episode Schizophrenia.</a></li>
<li><a href="#316. Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking.">316. Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking.</a></li>
<li><a href="#317. Topic-Based Agreement and Disagreement in US Electoral Manifestos.">317. Topic-Based Agreement and Disagreement in US Electoral Manifestos.</a></li>
<li><a href="#318. Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora.">318. Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora.</a></li>
<li><a href="#319. Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps.">319. Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps.</a></li>
<li><a href="#320. Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog.">320. Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog.</a></li>
<li><a href="#321. Depression and Self-Harm Risk Assessment in Online Forums.">321. Depression and Self-Harm Risk Assessment in Online Forums.</a></li>
<li><a href="#322. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints.">322. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="EMNLP 2017:Copenhagen, Denmark">EMNLP 2017:Copenhagen, Denmark</h1>
<p><a href="https://www.aclweb.org/anthology/volumes/D17-1/">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017.</a> Association for Computational Linguistics
<a href="https://dblp.uni-trier.de/db/conf/emnlp/emnlp2017.html">DBLP Link</a></p>
<h2 id="Paper Num: 322 || Session Num: 0">Paper Num: 322 || Session Num: 0</h2>
<h3 id="1. Monolingual Phrase Alignment on Parse Forests.">1. Monolingual Phrase Alignment on Parse Forests.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1001/">Paper Link</a>    Pages:1-11</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Arase:Yuki">Yuki Arase</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsujii:Junichi">Junichi Tsujii</a></p>
<p>Abstract:
We propose an efficient method to conduct phrase alignment on parse forests for paraphrase detection. Unlike previous studies, our method identifies syntactic paraphrases under linguistically motivated grammar. In addition, it allows phrases to non-compositionally align to handle paraphrases with non-homographic phrase correspondences. A dataset that provides gold parse trees and their phrase alignments is created. The experimental results confirm that the proposed method conducts highly accurate phrase alignment compared to human performance.</p>
<p>Keywords:</p>
<h3 id="2. Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set.">2. Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1002/">Paper Link</a>    Pages:12-23</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Tianze">Tianze Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang_0001:Liang">Liang Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Lillian">Lillian Lee</a></p>
<p>Abstract:
We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case O(n3) exact decoders for arc-hybrid and arc-eager transition systems. With our minimal features, we also present O(n3) global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the second-best-in-class result on the English Penn Treebank.</p>
<p>Keywords:</p>
<h3 id="3. Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs.">3. Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1003/">Paper Link</a>    Pages:24-34</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cao:Junjie">Junjie Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Sheng">Sheng Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Weiwei">Weiwei Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>Abstract:
We propose a new Maximum Subgraph algorithm for first-order parsing to 1-endpoint-crossing, pagenumber-2 graphs. Our algorithm has two characteristics: (1) it separates the construction for noncrossing edges and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features. We then introduce a new algorithm for quasi-second-order parsing. Experiments demonstrate that second-order features are helpful for Maximum Subgraph parsing.</p>
<p>Keywords:</p>
<h3 id="4. Position-aware Attention and Supervised Data Improve Slot Filling.">4. Position-aware Attention and Supervised Data Improve Slot Filling.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1004/">Paper Link</a>    Pages:35-45</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yuhao">Yuhao Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhong:Victor">Victor Zhong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Danqi">Danqi Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Angeli:Gabor">Gabor Angeli</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Manning:Christopher_D=">Christopher D. Manning</a></p>
<p>Abstract:
Organized relational knowledge in the form of knowledge graphs is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2% to 26.7%.</p>
<p>Keywords:</p>
<h3 id="5. Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach.">5. Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1005/">Paper Link</a>    Pages:46-56</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Liyuan">Liyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xiang">Xiang Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Qi">Qi Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhi:Shi">Shi Zhi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gui:Huan">Huan Gui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han_0001:Jiawei">Jiawei Han</a></p>
<p>Abstract:
Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art.</p>
<p>Keywords:</p>
<h3 id="6. Integrating Order Information and Event Relation for Script Event Prediction.">6. Integrating Order Information and Event Relation for Script Event Prediction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1006/">Paper Link</a>    Pages:57-67</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhongqing">Zhongqing Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Ching=Yun">Ching-Yun Chang</a></p>
<p>Abstract:
There has been a recent line of work automatically learning scripts from unstructured texts, by modeling narrative event chains. While the dominant approach group events using event pair relations, LSTMs have been used to encode full chains of narrative events. The latter has the advantage of learning long-range temporal orders, yet the former is more adaptive to partial orders. We propose a neural model that leverages the advantages of both methods, by using LSTM hidden states as features for event pair modelling. A dynamic memory network is utilized to automatically induce weights on existing events for inferring a subsequent event. Standard evaluation shows that our method significantly outperforms both methods above, giving the best results reported so far.</p>
<p>Keywords:</p>
<h3 id="7. Entity Linking for Queries by Searching Wikipedia Sentences.">7. Entity Linking for Queries by Searching Wikipedia Sentences.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1007/">Paper Link</a>    Pages:68-77</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Chuanqi">Chuanqi Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Pengjie">Pengjie Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lv:Weifeng">Weifeng Lv</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a></p>
<p>Abstract:
We present a simple yet effective approach for linking entities in queries. The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query. Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework. The advantages of our approach lie in two aspects, which contribute to the ranking process and final linking result. First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query. Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles. We conduct experiments on two benchmark datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that our method outperforms state-of-the-art systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ dataset.</p>
<p>Keywords:</p>
<h3 id="8. Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data.">8. Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1008/">Paper Link</a>    Pages:78-88</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pasini:Tommaso">Tommaso Pasini</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Navigli:Roberto">Roberto Navigli</a></p>
<p>Abstract:
Annotating large numbers of sentences with senses is the heaviest requirement of current Word Sense Disambiguation. We present Train-O-Matic, a language-independent method for generating millions of sense-annotated training instances for virtually all meanings of words in a languages vocabulary. The approach is fully automatic: no human intervention is required and the only type of human knowledge used is a WordNet-like resource. Train-O-Matic achieves consistently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the burden of manual annotation. All the training data is available for research purposes at <a href="http://trainomatic.org">http://trainomatic.org</a>.</p>
<p>Keywords:</p>
<h3 id="9. Universal Semantic Parsing.">9. Universal Semantic Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1009/">Paper Link</a>    Pages:89-101</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Reddy:Siva">Siva Reddy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/T=auml=ckstr=ouml=m:Oscar">Oscar Tckstrm</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Petrov:Slav">Slav Petrov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Steedman:Mark">Mark Steedman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>Abstract:
Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions.</p>
<p>Keywords:</p>
<h3 id="10. Mimicking Word Embeddings using Subword RNNs.">10. Mimicking Word Embeddings using Subword RNNs.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1010/">Paper Link</a>    Pages:102-112</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pinter:Yuval">Yuval Pinter</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guthrie:Robert">Robert Guthrie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Eisenstein:Jacob">Jacob Eisenstein</a></p>
<p>Abstract:
Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised character-based model in low resource settings.</p>
<p>Keywords:</p>
<h3 id="11. Past, Present, Future: A Computational Investigation of the Typology of Tense in 1000 Languages.">11. Past, Present, Future: A Computational Investigation of the Typology of Tense in 1000 Languages.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1011/">Paper Link</a>    Pages:113-124</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Asgari:Ehsaneddin">Ehsaneddin Asgari</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schtze</a></p>
<p>Abstract:
We present SuperPivot, an analysis method for low-resource languages that occur in a superparallel corpus, i.e., in a corpus that contains an order of magnitude more languages than parallel corpora currently in use. We show that SuperPivot performs well for the crosslingual analysis of the linguistic phenomenon of tense. We produce analysis results for more than 1000 languages, conducting  to the best of our knowledge  the largest crosslingual computational study performed to date. We extend existing methodology for leveraging parallel corpora for typological analysis by overcoming a limiting assumption of earlier work: We only require that a linguistic feature is overtly marked in a few of thousands of languages as opposed to requiring that it be marked in all languages under investigation.</p>
<p>Keywords:</p>
<h3 id="12. Neural Machine Translation with Source-Side Latent Graph Parsing.">12. Neural Machine Translation with Source-Side Latent Graph Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1012/">Paper Link</a>    Pages:125-135</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hashimoto:Kazuma">Kazuma Hashimoto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsuruoka:Yoshimasa">Yoshimasa Tsuruoka</a></p>
<p>Abstract:
This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the parser is optimized according to the translation objective. In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our model can be further improved by pre-training it with a small amount of treebank annotations. Our final ensemble model significantly outperforms the previous best models on the standard English-to-Japanese translation dataset.</p>
<p>Keywords:</p>
<h3 id="13. Neural Machine Translation with Word Predictions.">13. Neural Machine Translation with Word Predictions.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1013/">Paper Link</a>    Pages:136-145</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Weng:Rongxiang">Rongxiang Weng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Shujian">Shujian Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zheng:Zaixiang">Zaixiang Zheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Xin=Yu">Xin-Yu Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jiajun">Jiajun Chen</a></p>
<p>Abstract:
In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence. These vectors are generated by parameters which are updated by back-propagation of translation errors through time.We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use word predictions as a mechanism for direct supervision. More specifically, we require these vectors to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English machine translation task show an average BLEU improvement by 4.53, respectively.</p>
<p>Keywords:</p>
<h3 id="14. Towards Decoding as Continuous Optimisation in Neural Machine Translation.">14. Towards Decoding as Continuous Optimisation in Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1014/">Paper Link</a>    Pages:146-156</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hoang:Cong_Duy_Vu">Cong Duy Vu Hoang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haffari:Gholamreza">Gholamreza Haffari</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a></p>
<p>Abstract:
We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We reformulate decoding, a discrete optimization problem, into a continuous problem, such that optimization can make use of efficient gradient-based techniques. Our powerful decoding framework allows for more accurate decoding for standard neural machine translation models, as well as enabling decoding in intractable models such as intersection of several different NMT models. Our empirical results show that our decoding framework is effective, and can leads to substantial improvements in translations, especially in situations where greedy search and beam search are not feasible. Finally, we show how the technique is highly competitive with, and complementary to, reranking.</p>
<p>Keywords:</p>
<h3 id="15. Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space.">15. Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1015/">Paper Link</a>    Pages:157-166</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kitaev:Nikita">Nikita Kitaev</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klein:Dan">Dan Klein</a></p>
<p>Abstract:
We present a model for locating regions in space based on natural language descriptions. Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as on top of or next to, and finally locate the region described in the sentence. All components form a single neural network that is trained end-to-end without prior knowledge of object segmentation. To evaluate our model, we construct and release a new dataset consisting of Minecraft scenes with crowdsourced natural language descriptions. We achieve a 32% relative error reduction compared to a strong neural baseline.</p>
<p>Keywords:</p>
<h3 id="16. Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks.">16. Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1016/">Paper Link</a>    Pages:167-176</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rahimi_0001:Afshin">Afshin Rahimi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldwin:Timothy">Timothy Baldwin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a></p>
<p>Abstract:
We propose a method for embedding two-dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over Twitter data, the proposed model outperforms conventional regression-based geolocation and provides a better estimate of uncertainty. We also show the effectiveness of the representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.</p>
<p>Keywords:</p>
<h3 id="17. Obj2Text: Generating Visually Descriptive Language from Object Layouts.">17. Obj2Text: Generating Visually Descriptive Language from Object Layouts.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1017/">Paper Link</a>    Pages:177-187</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Xuwang">Xuwang Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ordonez:Vicente">Vicente Ordonez</a></p>
<p>Abstract:
Generating captions for images is a task that has recently received considerable attention. Another type of visual inputs are abstract scenes or object layouts where the only information provided is a set of objects and their locations. This type of imagery is commonly found in many applications in computer graphics, virtual reality, and storyboarding. We explore in this paper OBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show in our paper that this model despite using a sequence encoder can effectively represent complex spatial object-object relationships and produce descriptions that are globally coherent and semantically relevant. We test our approach for the task of describing object layouts in the MS-COCO dataset by producing sentences given only object annotations. We additionally show that our model combined with a state-of-the-art object detector can improve the accuracy of an image captioning model.</p>
<p>Keywords:</p>
<h3 id="18. End-to-end Neural Coreference Resolution.">18. End-to-end Neural Coreference Resolution.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1018/">Paper Link</a>    Pages:188-197</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Kenton">Kenton Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Luheng">Luheng He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lewis:Mike">Mike Lewis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a></p>
<p>Abstract:
We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.</p>
<p>Keywords:</p>
<h3 id="19. Neural Net Models of Open-domain Discourse Coherence.">19. Neural Net Models of Open-domain Discourse Coherence.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1019/">Paper Link</a>    Pages:198-209</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jiwei">Jiwei Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jurafsky:Dan">Dan Jurafsky</a></p>
<p>Abstract:
Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) in narrow domains. In this paper, we describe domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of coherence in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.</p>
<p>Keywords:</p>
<h3 id="20. Affinity-Preserving Random Walk for Multi-Document Summarization.">20. Affinity-Preserving Random Walk for Multi-Document Summarization.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1020/">Paper Link</a>    Pages:210-220</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Kexiang">Kexiang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tianyu">Tianyu Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sui:Zhifang">Zhifang Sui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Baobao">Baobao Chang</a></p>
<p>Abstract:
Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinity-preserving random walk to the summarization task, which preserves the affinity relations of sentences by an absorbing random walk model. Meanwhile, we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of summarization in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summarization task show the good performance of our method, which has the best ROUGE-2 recall among the graph-based ranking methods.</p>
<p>Keywords:</p>
<h3 id="21. A Mention-Ranking Model for Abstract Anaphora Resolution.">21. A Mention-Ranking Model for Abstract Anaphora Resolution.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1021/">Paper Link</a>    Pages:221-232</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Marasovic:Ana">Ana Marasovic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Born:Leo">Leo Born</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Opitz:Juri">Juri Opitz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Frank:Anette">Anette Frank</a></p>
<p>Abstract:
Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentenceantecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors. Our model selects syntactically plausible candidates and  if disregarding syntax  discriminates candidates using deeper features.</p>
<p>Keywords:</p>
<h3 id="22. Hierarchical Embeddings for Hypernymy Detection and Directionality.">22. Hierarchical Embeddings for Hypernymy Detection and Directionality.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1022/">Paper Link</a>    Pages:233-243</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Kim_Anh">Kim Anh Nguyen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/K=ouml=per:Maximilian">Maximilian Kper</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Walde:Sabine_Schulte_im">Sabine Schulte im Walde</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vu:Ngoc_Thang">Ngoc Thang Vu</a></p>
<p>Abstract:
We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernymhyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-the-art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.</p>
<p>Keywords:</p>
<h3 id="23. Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics.">23. Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1023/">Paper Link</a>    Pages:244-253</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Zhe">Zhe Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Tao">Tao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Shen">Shen Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Bofang">Bofang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Du:Xiaoyong">Xiaoyong Du</a></p>
<p>Abstract:
The existing word representation methods mostly limit their information source to word co-occurrence statistics. In this paper, we introduce ngrams into four representation methods: SGNS, GloVe, PPMI matrix, and its SVD factorization. Comprehensive experiments are conducted on word analogy and similarity tasks. The results show that improved word representations are learned from ngram co-occurrence statistics. We also demonstrate that the trained ngram representations are useful in many aspects such as finding antonyms and collocations. Besides, a novel approach of building co-occurrence matrix is proposed to alleviate the hardware burdens brought by ngrams.</p>
<p>Keywords:</p>
<h3 id="24. Dict2vec : Learning Word Embeddings using Lexical Dictionaries.">24. Dict2vec : Learning Word Embeddings using Lexical Dictionaries.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1024/">Paper Link</a>    Pages:254-263</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tissier:Julien">Julien Tissier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gravier:Christophe">Christophe Gravier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Habrard:Amaury">Amaury Habrard</a></p>
<p>Abstract:
Learning word embeddings on large unlabeled corpus has been shown to be successful in improving many natural language tasks. The most efficient and popular approaches learn or retrofit such representations using additional external data. Resulting embeddings are generally better than their corpus-only counterparts, although such resources cover a fraction of words in the vocabulary. In this paper, we propose a new approach, Dict2vec, based on one of the largest yet refined datasource for describing words  natural language dictionaries. Dict2vec builds new word pairs from dictionary entries so that semantically-related words are moved closer, and negative sampling filters out pairs whose words are unrelated in dictionaries. We evaluate the word representations obtained using Dict2vec on eleven datasets for the word similarity task and on four datasets for a text classification task.</p>
<p>Keywords:</p>
<h3 id="25. Learning Chinese Word Representations From Glyphs Of Characters.">25. Learning Chinese Word Representations From Glyphs Of Characters.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1025/">Paper Link</a>    Pages:264-273</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Su:Tzu=Ray">Tzu-Ray Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Hung=yi">Hung-yi Lee</a></p>
<p>Abstract:
In this paper, we propose new methods to learn Chinese word representations. Chinese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by convolutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chinese and made them public.</p>
<p>Keywords:</p>
<h3 id="26. Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext.">26. Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1026/">Paper Link</a>    Pages:274-285</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wieting:John">John Wieting</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mallinson:Jonathan">Jonathan Mallinson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gimpel:Kevin">Kevin Gimpel</a></p>
<p>Abstract:
We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and data sources, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from human-written sentences, finding clear differences in length, the amount of repetition, and the use of rare words.</p>
<p>Keywords:</p>
<h3 id="27. Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components.">27. Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1027/">Paper Link</a>    Pages:286-291</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Jinxing">Jinxing Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jian:Xun">Xun Jian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xin:Hao">Hao Xin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yangqiu">Yangqiu Song</a></p>
<p>Abstract:
Word embeddings have attracted much attention recently. Different from alphabetic writing systems, Chinese characters are often composed of subcharacter components which are also semantically informative. In this work, we propose an approach to jointly embed Chinese words as well as their characters and fine-grained subcharacter components. We use three likelihoods to evaluate whether the context words, characters, and components can predict the current target word, and collected 13,253 subcharacter components to demonstrate the existing approaches of decomposing Chinese characters are not enough. Evaluation on both word similarity and word analogy tasks demonstrates the superior performance of our model.</p>
<p>Keywords:</p>
<h3 id="28. Exploiting Morphological Regularities in Distributional Word Representations.">28. Exploiting Morphological Regularities in Distributional Word Representations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1028/">Paper Link</a>    Pages:292-297</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Arihant">Arihant Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Akhtar:Syed_Sarfaraz">Syed Sarfaraz Akhtar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vajpayee:Avijit">Avijit Vajpayee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Srivastava:Arjit">Arjit Srivastava</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jhanwar:Madan_Gopal">Madan Gopal Jhanwar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shrivastava_0001:Manish">Manish Shrivastava</a></p>
<p>Abstract:
We present an unsupervised, language agnostic approach for exploiting morphological regularities present in high dimensional vector spaces. We propose a novel method for generating embeddings of words from their morphological variants using morphological transformation operators. We evaluate this approach on MSR word analogy test set with an accuracy of 85% which is 12% higher than the previous best known system.</p>
<p>Keywords:</p>
<h3 id="29. Exploiting Word Internal Structures for Generic Chinese Sentence Representation.">29. Exploiting Word Internal Structures for Generic Chinese Sentence Representation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1029/">Paper Link</a>    Pages:298-303</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Shaonan">Shaonan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>Abstract:
We introduce a novel mixed characterword architecture to improve Chinese sentence representations, by utilizing rich semantic information of word internal structures. Our architecture uses two key strategies. The first is a mask gate on characters, learning the relation among characters in a word. The second is a maxpooling operation on words, adaptively finding the optimal mixture of the atomic and compositional word representations. Finally, the proposed architecture is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task.</p>
<p>Keywords:</p>
<h3 id="30. High-risk learning: acquiring new word vectors from tiny data.">30. High-risk learning: acquiring new word vectors from tiny data.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1030/">Paper Link</a>    Pages:304-309</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Herbelot:Aur=eacute=lie">Aurlie Herbelot</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baroni:Marco">Marco Baroni</a></p>
<p>Abstract:
Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn a good vector for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.</p>
<p>Keywords:</p>
<h3 id="31. Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding.">31. Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1031/">Paper Link</a>    Pages:310-315</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sanu:Joseph">Joseph Sanu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Mingbin">Mingbin Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang_0001:Hui">Hui Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Quan">Quan Liu</a></p>
<p>Abstract:
In this paper, we propose to learn word embeddings based on the recent fixed-size ordinally forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence into a fixed-size representation. We use FOFE to fully encode the left and right context of each word in a corpus to construct a novel word-context matrix, which is further weighted and factorized using truncated SVD to generate low-dimension word embedding vectors. We evaluate this alternate method in encoding word-context statistics and show the new FOFE method has a notable effect on the resulting word embeddings. Experimental results on several popular word similarity tasks have demonstrated that the proposed method outperforms other SVD models that use canonical count based techniques to generate word context matrices.</p>
<p>Keywords:</p>
<h3 id="32. VecShare: A Framework for Sharing Word Representation Vectors.">32. VecShare: A Framework for Sharing Word Representation Vectors.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1032/">Paper Link</a>    Pages:316-320</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fernandez:Jared">Jared Fernandez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Zhaocheng">Zhaocheng Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Downey:Doug">Doug Downey</a></p>
<p>Abstract:
Many Natural Language Processing (NLP) models rely on distributed vector representations of words. Because the process of training word vectors can require large amounts of data and computation, NLP researchers and practitioners often utilize pre-trained embeddings downloaded from the Web. However, finding the best embeddings for a given task is difficult, and can be computationally prohibitive. We present a framework, called VecShare, that makes it easy to share and retrieve word embeddings on the Web. The framework leverages a public data-sharing infrastructure to host embedding sets, and provides automated mechanisms for retrieving the embeddings most similar to a given corpus. We perform an experimental evaluation of VecShares similarity strategies, and show that they are effective at efficiently retrieving embeddings that boost accuracy in a document classification task. Finally, we provide an open-source Python library for using the VecShare framework.</p>
<p>Keywords:</p>
<h3 id="33. Word Re-Embedding via Manifold Dimensionality Retention.">33. Word Re-Embedding via Manifold Dimensionality Retention.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1033/">Paper Link</a>    Pages:321-326</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hasan:Souleiman">Souleiman Hasan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Curry:Edward">Edward Curry</a></p>
<p>Abstract:
Word embeddings seek to recover a Euclidean metric space by mapping words into vectors, starting from words co-occurrences in a corpus. Word embeddings may underestimate the similarity between nearby words, and overestimate it between distant words in the Euclidean metric space. In this paper, we re-embed pre-trained word embeddings with a stage of manifold learning which retains dimensionality. We show that this approach is theoretically founded in the metric recovery paradigm, and empirically show that it can improve on state-of-the-art embeddings in word similarity tasks 0.5 - 5.0% points depending on the original space.</p>
<p>Keywords:</p>
<h3 id="34. MUSE: Modularizing Unsupervised Sense Embeddings.">34. MUSE: Modularizing Unsupervised Sense Embeddings.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1034/">Paper Link</a>    Pages:327-337</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Guang=He">Guang-He Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Yun=Nung">Yun-Nung Chen</a></p>
<p>Abstract:
This paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts. Prior work focused on designing a single model to deliver both mechanisms, and thus suffered from either coarse-grained representation learning or inefficient sense selection. The proposed modular approach, MUSE, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with linear-time sense selection. We leverage reinforcement learning to enable joint training on the proposed modules, and introduce various exploration techniques on sense selection for better robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on synonym selection as well as on contextual word similarities in terms of MaxSimC.</p>
<p>Keywords:</p>
<h3 id="35. Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging.">35. Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1035/">Paper Link</a>    Pages:338-348</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Reimers:Nils">Nils Reimers</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant (p &lt; 10-4) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.</p>
<p>Keywords:</p>
<h3 id="36. Learning What's Easy: Fully Differentiable Neural Easy-First Taggers.">36. Learning What's Easy: Fully Differentiable Neural Easy-First Taggers.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1036/">Paper Link</a>    Pages:349-362</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Martins:Andr=eacute=_F=_T=">Andr F. T. Martins</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kreutzer:Julia">Julia Kreutzer</a></p>
<p>Abstract:
We introduce a novel neural easy-first decoder that learns to solve sequence tagging tasks in a flexible order. In contrast to previous easy-first decoders, our models are end-to-end differentiable. The decoder iteratively updates a sketch of the predictions over the sequence. At its core is an attention mechanism that controls which parts of the input are strategically the best to process next. We present a new constrained softmax transformation that ensures the same cumulative attention to every word, and show how to efficiently evaluate and backpropagate over it. Our models compare favourably to BILSTM taggers on three sequence tagging tasks.</p>
<p>Keywords:</p>
<h3 id="37. Incremental Skip-gram Model with Negative Sampling.">37. Incremental Skip-gram Model with Negative Sampling.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1037/">Paper Link</a>    Pages:363-371</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kaji:Nobuhiro">Nobuhiro Kaji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kobayashi:Hayato">Hayato Kobayashi</a></p>
<p>Abstract:
This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.</p>
<p>Keywords:</p>
<h3 id="38. Learning to select data for transfer learning with Bayesian Optimization.">38. Learning to select data for transfer learning with Bayesian Optimization.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1038/">Paper Link</a>    Pages:372-382</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ruder:Sebastian">Sebastian Ruder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Plank:Barbara">Barbara Plank</a></p>
<p>Abstract:
Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures areto some degreetransferable across models, domains, and even tasks.</p>
<p>Keywords:</p>
<h3 id="39. Unsupervised Pretraining for Sequence to Sequence Learning.">39. Unsupervised Pretraining for Sequence to Sequence Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1039/">Paper Link</a>    Pages:383-391</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ramachandran:Prajit">Prajit Ramachandran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Peter_J=">Peter J. Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Le:Quoc_V=">Quoc V. Le</a></p>
<p>Abstract:
This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT EnglishGerman task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from th previous best models on both WMT14 and WMT15 EnglishGerman. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.</p>
<p>Keywords:</p>
<h3 id="40. Efficient Attention using a Fixed-Size Memory Representation.">40. Efficient Attention using a Fixed-Size Memory Representation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1040/">Paper Link</a>    Pages:392-400</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Britz:Denny">Denny Britz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guan:Melody_Y=">Melody Y. Guan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luong:Minh=Thang">Minh-Thang Luong</a></p>
<p>Abstract:
The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments.</p>
<p>Keywords:</p>
<h3 id="41. Rotated Word Vector Representations and their Interpretability.">41. Rotated Word Vector Representations and their Interpretability.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1041/">Paper Link</a>    Pages:401-411</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Park:Sungjoon">Sungjoon Park</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bak:JinYeong">JinYeong Bak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Oh:Alice">Alice Oh</a></p>
<p>Abstract:
Vector representation of words improves performance in various NLP tasks, but the high dimensional word vectors are very difficult to interpret. We apply several rotation algorithms to the vector representation of words to improve the interpretability. Unlike previous approaches that induce sparsity, the rotated vectors are interpretable while preserving the expressive performance of the original vectors. Furthermore, any prebuilt word vector representation can be rotated for improved interpretability. We apply rotation to skipgrams and glove and compare the expressive power and interpretability with the original vectors and the sparse overcomplete vectors. The results show that the rotated vectors outperform the original and the sparse overcomplete vectors for interpretability and expressiveness tasks.</p>
<p>Keywords:</p>
<h3 id="42. A causal framework for explaining the predictions of black-box sequence-to-sequence models.">42. A causal framework for explaining the predictions of black-box sequence-to-sequence models.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1042/">Paper Link</a>    Pages:412-421</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Alvarez=Melis:David">David Alvarez-Melis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jaakkola:Tommi_S=">Tommi S. Jaakkola</a></p>
<p>Abstract:
We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an explanation consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.</p>
<p>Keywords:</p>
<h3 id="43. Piecewise Latent Variables for Neural Variational Text Processing.">43. Piecewise Latent Variables for Neural Variational Text Processing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1043/">Paper Link</a>    Pages:422-432</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Serban:Iulian_Vlad">Iulian Vlad Serban</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ororbia_II:Alexander_G=">Alexander G. Ororbia II</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pineau:Joelle">Joelle Pineau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Courville:Aaron_C=">Aaron C. Courville</a></p>
<p>Abstract:
Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.</p>
<p>Keywords:</p>
<h3 id="44. Learning the Structure of Variable-Order CRFs: a finite-state perspective.">44. Learning the Structure of Variable-Order CRFs: a finite-state perspective.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1044/">Paper Link</a>    Pages:433-439</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lavergne:Thomas">Thomas Lavergne</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yvon:Fran=ccedil=ois">Franois Yvon</a></p>
<p>Abstract:
The computational complexity of linear-chain Conditional Random Fields (CRFs) makes it difficult to deal with very large label sets and long range dependencies. Such situations are not rare and arise when dealing with morphologically rich languages or joint labelling tasks. We extend here recent proposals to consider variable order CRFs. Using an effective finite-state representation of variable-length dependencies, we propose new ways to perform feature selection at large scale and report experimental results where we outperform strong baselines on a tagging task.</p>
<p>Keywords:</p>
<h3 id="45. Sparse Communication for Distributed Gradient Descent.">45. Sparse Communication for Distributed Gradient Descent.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1045/">Paper Link</a>    Pages:440-445</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Aji:Alham_Fikri">Alham Fikri Aji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Heafield:Kenneth">Kenneth Heafield</a></p>
<p>Abstract:
We make distributed stochastic gradient descent faster by exchanging sparse updates instead of dense updates. Gradient updates are positively skewed as most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices. This method can be combined with quantization to further improve the compression. We explore different configurations and apply them to neural machine translation and MNIST image classification tasks. Most configurations work on MNIST, whereas different configurations reduce convergence rate on the more complex translation task. Our experiments show that we can achieve up to 49% speed up on MNIST and 22% on NMT without damaging the final accuracy or BLEU.</p>
<p>Keywords:</p>
<h3 id="46. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks.">46. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1206/">Paper Link</a>    Pages:1923-1933</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hashimoto:Kazuma">Kazuma Hashimoto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Caiming">Caiming Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsuruoka:Yoshimasa">Yoshimasa Tsuruoka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Socher:Richard">Richard Socher</a></p>
<p>Abstract:
Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one tasks loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.</p>
<p>Keywords:</p>
<h3 id="47. Why ADAGRAD Fails for Online Topic Modeling.">47. Why ADAGRAD Fails for Online Topic Modeling.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1046/">Paper Link</a>    Pages:446-451</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lu:You">You Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lund:Jeffrey">Jeffrey Lund</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Boyd=Graber:Jordan_L=">Jordan L. Boyd-Graber</a></p>
<p>Abstract:
Online topic modeling, i.e., topic modeling with stochastic variational inference, is a powerful and efficient technique for analyzing large datasets, and ADAGRAD is a widely-used technique for tuning learning rates during online gradient optimization. However, these two techniques do not work well together. We show that this is because ADAGRAD uses accumulation of previous gradients as the learning rates denominators. For online topic modeling, the magnitude of gradients is very large. It causes learning rates to shrink very quickly, so the parameters cannot fully converge until the training ends</p>
<p>Keywords:</p>
<h3 id="48. Recurrent Attention Network on Memory for Aspect Sentiment Analysis.">48. Recurrent Attention Network on Memory for Aspect Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1047/">Paper Link</a>    Pages:452-461</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Peng">Peng Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Zhongqian">Zhongqian Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Wei">Wei Yang</a></p>
<p>Abstract:
We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our model on four datasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data.</p>
<p>Keywords:</p>
<h3 id="49. A Cognition Based Attention Model for Sentiment Analysis.">49. A Cognition Based Attention Model for Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1048/">Paper Link</a>    Pages:462-471</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Long:Yunfei">Yunfei Long</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0001:Qin">Qin Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiang:Rong">Rong Xiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Minglei">Minglei Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Chu=Ren">Chu-Ren Huang</a></p>
<p>Abstract:
Attention models are proposed in sentiment analysis because some words are more important than others. However,most existing methods either use local context based text information or user preference information. In this work, we propose a novel attention model trained by cognition grounded eye-tracking data. A reading prediction model is first built using eye-tracking data as dependent data and other features in the context as independent data. The predicted reading time is then used to build a cognition based attention (CBA) layer for neural sentiment analysis. As a comprehensive model, We can capture attentions of words in sentences as well as sentences in documents. Different attention mechanisms can also be incorporated to capture other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded data can be brought into NLP tasks.</p>
<p>Keywords:</p>
<h3 id="50. Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews.">50. Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1049/">Paper Link</a>    Pages:472-481</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Poddar:Lahari">Lahari Poddar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hsu:Wynne">Wynne Hsu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Mong=Li">Mong-Li Lee</a></p>
<p>Abstract:
User generated content about products and services in the form of reviews are often diverse and even contradictory. This makes it difficult for users to know if an opinion in a review is prevalent or biased. We study the problem of searching for supporting opinions in the context of reviews. We propose a framework called SURF, that first identifies opinions expressed in a review, and then finds similar opinions from other reviews. We design a novel probabilistic graphical model that captures opinions as a combination of aspect, topic and sentiment dimensions, takes into account the preferences of individual authors, as well as the quality of the entity under review, and encodes the flow of thoughts in a review by constraining the aspect distribution dynamically among successive review segments. We derive a similarity measure that considers both lexical and semantic similarity to find supporting opinions. Experiments on TripAdvisor hotel reviews and Yelp restaurant reviews show that our model outperforms existing methods for modeling opinions, and the proposed framework is effective in finding supporting opinions.</p>
<p>Keywords:</p>
<h3 id="51. Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal.">51. Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1050/">Paper Link</a>    Pages:482-491</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ghosh:Aniruddha">Aniruddha Ghosh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Veale:Tony">Tony Veale</a></p>
<p>Abstract:
Sarcasm is a pervasive phenomenon in social media, permitting the concise communication of meaning, affect and attitude. Concision requires wit to produce and wit to understand, which demands from each party knowledge of norms, context and a speakers mindset. Insight into a speakers psychological profile at the time of production is a valuable source of context for sarcasm detection. Using a neural architecture, we show significant gains in detection accuracy when knowledge of the speakers mood at the time of production can be inferred. Our focus is on sarcasm detection on Twitter, and show that the mood exhibited by a speaker over tweets leading up to a new post is as useful a cue for sarcasm as the topical context of the post itself. The work opens the door to an empirical exploration not just of sarcasm in text but of the sarcastic state of mind.</p>
<p>Keywords:</p>
<h3 id="52. Identifying Humor in Reviews using Background Text Sources.">52. Identifying Humor in Reviews using Background Text Sources.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1051/">Paper Link</a>    Pages:492-501</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Morales:Alex">Alex Morales</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhai:Chengxiang">Chengxiang Zhai</a></p>
<p>Abstract:
We study the problem of automatically identifying humorous text from a new kind of text data, i.e., online reviews. We propose a generative language model, based on the theory of incongruity, to model humorous text, which allows us to leverage background text sources, such as Wikipedia entry descriptions, and enables construction of multiple features for identifying humorous reviews. Evaluation of these features using supervised learning for classifying reviews into humorous and non-humorous reviews shows that the features constructed based on the proposed generative model are much more effective than the major features proposed in the existing literature, allowing us to achieve almost 86% accuracy. These humorous review predictions can also supply good indicators for identifying helpful reviews.</p>
<p>Keywords:</p>
<h3 id="53. Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision.">53. Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1052/">Paper Link</a>    Pages:502-510</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Le_Yi">Le Yi Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xia:Rui">Rui Xia</a></p>
<p>Abstract:
Sentiment lexicon is an important tool for identifying the sentiment polarity of words and texts. How to automatically construct sentiment lexicons has become a research topic in the field of sentiment analysis and opinion mining. Recently there were some attempts to employ representation learning algorithms to construct a sentiment lexicon with sentiment-aware word embedding. However, these methods were normally trained under document-level sentiment supervision. In this paper, we develop a neural architecture to train a sentiment-aware word embedding by integrating the sentiment supervision at both document and word levels, to enhance the quality of word embedding as well as the sentiment lexicon. Experiments on the SemEval 2013-2016 datasets indicate that the sentiment lexicon generated by our approach achieves the state-of-the-art performance in both supervised and unsupervised sentiment classification, in comparison with several strong sentiment lexicon construction methods.</p>
<p>Keywords:</p>
<h3 id="54. Towards a Universal Sentiment Classifier in Multiple languages.">54. Towards a Universal Sentiment Classifier in Multiple languages.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1053/">Paper Link</a>    Pages:511-520</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Kui">Kui Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>Abstract:
Existing sentiment classifiers usually work for only one specific language, and different classification models are used in different languages. In this paper we aim to build a universal sentiment classifier with a single classification model in multiple different languages. In order to achieve this goal, we propose to learn multilingual sentiment-aware word embeddings simultaneously based only on the labeled reviews in English and unlabeled parallel data available in a few language pairs. It is not required that the parallel data exist between English and any other language, because the sentiment information can be transferred into any language via pivot languages. We present the evaluation results of our universal sentiment classifier in five languages, and the results are very promising even when the parallel data between English and the target languages are not used. Furthermore, the universal single classifier is compared with a few cross-language sentiment classifiers relying on direct parallel data between the source and target languages, and the results show that the performance of our universal sentiment classifier is very promising compared to that of different cross-language classifiers in multiple target languages.</p>
<p>Keywords:</p>
<h3 id="55. Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network.">55. Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1054/">Paper Link</a>    Pages:521-526</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dou:Zi=Yi">Zi-Yi Dou</a></p>
<p>Abstract:
Document-level sentiment classification is a fundamental problem which aims to predict a users overall sentiment about a product in a document. Several methods have been proposed to tackle the problem whereas most of them fail to consider the influence of users who express the sentiment and products which are evaluated. To address the issue, we propose a deep memory network for document-level sentiment classification which could capture the user and product information at the same time. To prove the effectiveness of our algorithm, we conduct experiments on IMDB and Yelp datasets and the results indicate that our model can achieve better performance than several existing methods.</p>
<p>Keywords:</p>
<h3 id="56. Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters.">56. Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1055/">Paper Link</a>    Pages:527-533</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0007:Min">Min Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mei:Jincheng">Jincheng Mei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Wei">Wei Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Zhou">Zhou Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0006:Xiaojun">Xiaojun Chen</a></p>
<p>Abstract:
We study the problem of identifying the topics and sentiments and tracking their shifts from social media texts in different geographical regions during emergencies and disasters. We propose a location-based dynamic sentiment-topic model (LDST) which can jointly model topic, sentiment, time and Geolocation information. The experimental results demonstrate that LDST performs very well at discovering topics and sentiments from social media and tracking their shifts in different geographical regions during emergencies and disasters. We will release the data and source code after this work is published.</p>
<p>Keywords:</p>
<h3 id="57. Refining Word Embeddings for Sentiment Analysis.">57. Refining Word Embeddings for Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1056/">Paper Link</a>    Pages:534-539</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Liang=Chih">Liang-Chih Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0008:Jin">Jin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lai:K=_Robert">K. Robert Lai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xue=Jie">Xue-Jie Zhang</a></p>
<p>Abstract:
Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).</p>
<p>Keywords:</p>
<h3 id="58. A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis.">58. A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1057/">Paper Link</a>    Pages:540-546</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Akhtar:Md=_Shad">Md. Shad Akhtar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kumar:Abhishek">Abhishek Kumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Ghosal:Deepanway">Deepanway Ghosal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Ekbal:Asif">Asif Ekbal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a></p>
<p>Abstract:
In this paper, we propose a novel method for combining deep learning and classical feature based models using a Multi-Layer Perceptron (MLP) network for financial sentiment analysis. We develop various deep learning models based on Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). These are trained on top of pre-trained, autoencoder-based, financial word embeddings and lexicon features. An ensemble is constructed by combining these deep learning models and a classical supervised model based on Support Vector Regression (SVR). We evaluate our proposed technique on a benchmark dataset of SemEval-2017 shared task on financial sentiment analysis. The propose model shows impressive results on two datasets, i.e. microblogs and news headlines datasets. Comparisons show that our proposed model performs better than the existing state-of-the-art systems for the above two datasets by 2.0 and 4.1 cosine points, respectively.</p>
<p>Keywords:</p>
<h3 id="59. Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings.">59. Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1058/">Paper Link</a>    Pages:547-552</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sharma:Raksha">Raksha Sharma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Somani:Arpan">Arpan Somani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kumar:Lakshya">Lakshya Kumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a></p>
<p>Abstract:
Identification of intensity ordering among polar (positive or negative) words which have the same semantics can lead to a fine-grained sentiment analysis. For example, master, seasoned and familiar point to different intensity levels, though they all convey the same meaning (semantics), i.e., expertise: having a good knowledge of. In this paper, we propose a semi-supervised technique that uses sentiment bearing word embeddings to produce a continuous ranking among adjectives that share common semantics. Our system demonstrates a strong Spearmans rank correlation of 0.83 with the gold standard ranking. We show that sentiment bearing word embeddings facilitate a more accurate intensity ranking system than other standard word embeddings (word2vec and GloVe). Word2vec is the state-of-the-art for intensity ordering task.</p>
<p>Keywords:</p>
<h3 id="60. Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association.">60. Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1059/">Paper Link</a>    Pages:553-563</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yasheng">Yasheng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yang">Yang Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Bing">Bing Liu</a></p>
<p>Abstract:
Although many sentiment lexicons in different languages exist, most are not comprehensive. In a recent sentiment analysis application, we used a large Chinese sentiment lexicon and found that it missed a large number of sentiment words in social media. This prompted us to make a new attempt to study sentiment lexicon expansion. This paper first poses the problem as a PU learning problem, which is a new formulation. It then proposes a new PU learning method suitable for our problem using a neural network. The results are enhanced further with a new dictionary-based technique and a novel polarity classification technique. Experimental results show that the proposed approach outperforms baseline methods greatly.</p>
<p>Keywords:</p>
<h3 id="61. DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning.">61. DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1060/">Paper Link</a>    Pages:564-573</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Wenhan">Wenhan Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hoang:Thien">Thien Hoang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>Abstract:
We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.</p>
<p>Keywords:</p>
<h3 id="62. Task-Oriented Query Reformulation with Reinforcement Learning.">62. Task-Oriented Query Reformulation with Reinforcement Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1061/">Paper Link</a>    Pages:574-583</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nogueira:Rodrigo">Rodrigo Nogueira</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a></p>
<p>Abstract:
Search engines play an important role in our everyday lives by assisting us in finding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20% in terms of recall. Furthermore, we present a simple method to estimate a conservative upper-bound performance of a model in a particular environment and verify that there is still large room for improvements.</p>
<p>Keywords:</p>
<h3 id="63. Sentence Simplification with Deep Reinforcement Learning.">63. Sentence Simplification with Deep Reinforcement Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1062/">Paper Link</a>    Pages:584-594</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xingxing">Xingxing Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>Abstract:
Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.</p>
<p>Keywords:</p>
<h3 id="64. Learning how to Active Learn: A Deep Reinforcement Learning Approach.">64. Learning how to Active Learn: A Deep Reinforcement Learning Approach.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1063/">Paper Link</a>    Pages:595-605</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fang:Meng">Meng Fang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0012:Yuan">Yuan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a></p>
<p>Abstract:
Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation to one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning algorithms.</p>
<p>Keywords:</p>
<h3 id="65. Split and Rephrase.">65. Split and Rephrase.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1064/">Paper Link</a>    Pages:606-616</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Narayan:Shashi">Shashi Narayan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gardent:Claire">Claire Gardent</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shimorina:Anastasia">Anastasia Shimorina</a></p>
<p>Abstract:
We propose a new sentence simplification task (Split-and-Rephrase) where the aim is to split a complex sentence into a meaning preserving sequence of shorter sentences. Like sentence simplification, splitting-and-rephrasing has the potential of benefiting both natural language processing and societal applications. Because shorter sentences are generally better processed by NLP systems, it could be used as a preprocessing step which facilitates and improves the performance of parsers, semantic role labellers and machine translation systems. It should also be of use for people with reading disabilities because it allows the conversion of longer sentences into shorter ones. This paper makes two contributions towards this new task. First, we create and make available a benchmark consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning. Second, we propose five models (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task.</p>
<p>Keywords:</p>
<h3 id="66. Neural Response Generation via GAN with an Approximate Embedding Layer.">66. Neural Response Generation via GAN with an Approximate Embedding Layer.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1065/">Paper Link</a>    Pages:617-626</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0003:Zhen">Zhen Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Bingquan">Bingquan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Baoxun">Baoxun Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Chengjie">Chengjie Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0001:Xiaolong">Xiaolong Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhuoran">Zhuoran Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qi:Chao">Chao Qi</a></p>
<p>Abstract:
This paper presents a Generative Adversarial Network (GAN) to model single-turn short-text conversations, which trains a sequence-to-sequence (Seq2Seq) network for response generation simultaneously with a discriminative classifier that measures the differences between human-produced responses and machine-generated ones. In addition, the proposed method introduces an approximate embedding layer to solve the non-differentiable problem caused by the sampling-based output decoding procedure in the Seq2Seq generative model. The GAN setup provides an effective way to avoid noninformative responses (a.k.a safe responses), which are frequently observed in traditional neural response generators. The experimental results show that the proposed approach significantly outperforms existing neural response generation models in diversity metrics, with slight increases in relevance scores as well, when evaluated on both a Mandarin corpus and an English corpus.</p>
<p>Keywords:</p>
<h3 id="67. A Hybrid Convolutional Variational Autoencoder for Text Generation.">67. A Hybrid Convolutional Variational Autoencoder for Text Generation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1066/">Paper Link</a>    Pages:627-637</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Semeniuta:Stanislau">Stanislau Semeniuta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Severyn:Aliaksei">Aliaksei Severyn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Barth:Erhardt">Erhardt Barth</a></p>
<p>Abstract:
In this paper we explore the effect of architectural choices on learning a variational autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid the issue of the VAE collapsing to a deterministic model.</p>
<p>Keywords:</p>
<h3 id="68. Filling the Blanks (hint: plural noun) for Mad Libs Humor.">68. Filling the Blanks (hint: plural noun) for Mad Libs Humor.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1067/">Paper Link</a>    Pages:638-647</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hossain:Nabil">Nabil Hossain</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Krumm:John">John Krumm</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vanderwende:Lucy">Lucy Vanderwende</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Horvitz:Eric">Eric Horvitz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kautz:Henry_A=">Henry A. Kautz</a></p>
<p>Abstract:
Computerized generation of humor is a notoriously difficult AI problem. We develop an algorithm called Libitum that helps humans generate humor in a Mad Lib, which is a popular fill-in-the-blank game. The algorithm is based on a machine learned classifier that determines whether a potential fill-in word is funny in the context of the Mad Lib story. We use Amazon Mechanical Turk to create ground truth data and to judge humor for our classifier to mimic, and we make this data freely available. Our testing shows that Libitum successfully aids humans in filling in Mad Libs that are usually judged funnier than those filled in by humans with no computerized help. We go on to analyze why some words are better than others at making a Mad Lib funny.</p>
<p>Keywords:</p>
<h3 id="69. Measuring Thematic Fit with Distributional Feature Overlap.">69. Measuring Thematic Fit with Distributional Feature Overlap.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1068/">Paper Link</a>    Pages:648-658</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Santus:Enrico">Enrico Santus</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chersoni:Emmanuele">Emmanuele Chersoni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lenci:Alessandro">Alessandro Lenci</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blache:Philippe">Philippe Blache</a></p>
<p>Abstract:
In this paper, we introduce a new distributional method for modeling predicate-argument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems. Moreover, it provides an explicit representation of the features characterizing verb-specific semantic roles.</p>
<p>Keywords:</p>
<h3 id="70. SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations.">70. SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1069/">Paper Link</a>    Pages:659-669</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mekala:Dheeraj">Dheeraj Mekala</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gupta_0001:Vivek">Vivek Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Paranjape:Bhargavi">Bhargavi Paranjape</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Karnick:Harish">Harish Karnick</a></p>
<p>Abstract:
We present a feature vector formation technique for documents - Sparse Composite Document Vector (SCDV) - which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation. In SCDV, word embeddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that can express complex, multi-topic documents. Through extensive experiments on multi-class and multi-label classification tasks, we outperform the previous state-of-the-art method, NTSG. We also show that SCDV embeddings perform well on heterogeneous tasks like Topic Coherence, context-sensitive Learning and Information Retrieval. Moreover, we achieve a significant reduction in training and prediction times compared to other representation methods. SCDV achieves best of both worlds - better performance with lower time and space complexity.</p>
<p>Keywords:</p>
<h3 id="71. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.">71. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1070/">Paper Link</a>    Pages:670-680</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Conneau:Alexis">Alexis Conneau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kiela:Douwe">Douwe Kiela</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwenk:Holger">Holger Schwenk</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Barrault:Lo=iuml=c">Loc Barrault</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bordes:Antoine">Antoine Bordes</a></p>
<p>Abstract:
Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.</p>
<p>Keywords:</p>
<h3 id="72. Determining Semantic Textual Similarity using Natural Deduction Proofs.">72. Determining Semantic Textual Similarity using Natural Deduction Proofs.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1071/">Paper Link</a>    Pages:681-691</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yanaka:Hitomi">Hitomi Yanaka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mineshima:Koji">Koji Mineshima</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mart=iacute=nez=G=oacute=mez:Pascual">Pascual Martnez-Gmez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bekki:Daisuke">Daisuke Bekki</a></p>
<p>Abstract:
Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logic-based systems and that features derived from the proofs are effective for learning textual similarity.</p>
<p>Keywords:</p>
<h3 id="73. Multi-Grained Chinese Word Segmentation.">73. Multi-Grained Chinese Word Segmentation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1072/">Paper Link</a>    Pages:692-703</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gong:Chen">Chen Gong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zhenghua">Zhenghua Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Xinzhou">Xinzhou Jiang</a></p>
<p>Abstract:
Traditionally, word segmentation (WS) adopts the single-grained formalism, where a sentence corresponds to a single word sequence. However, Sproat et al. (1997) show that the inter-native-speaker consistency ratio over Chinese word boundaries is only 76%, indicating single-grained WS (SWS) imposes unnecessary challenges on both manual annotation and statistical modeling. Moreover, WS results of different granularities can be complementary and beneficial for high-level applications. This work proposes and addresses multi-grained WS (MWS). We build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling. Experiments and analysis lead to many interesting findings.</p>
<p>Keywords:</p>
<h3 id="74. Don't Throw Those Morphological Analyzers Away Just Yet: Neural Morphological Disambiguation for Arabic.">74. Don't Throw Those Morphological Analyzers Away Just Yet: Neural Morphological Disambiguation for Arabic.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1073/">Paper Link</a>    Pages:704-713</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zalmout:Nasser">Nasser Zalmout</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Habash:Nizar">Nizar Habash</a></p>
<p>Abstract:
This paper presents a model for Arabic morphological disambiguation based on Recurrent Neural Networks (RNN). We train Long Short-Term Memory (LSTM) cells in several configurations and embedding levels to model the various morphological features. Our experiments show that these models outperform state-of-the-art systems without explicit use of feature engineering. However, adding learning features from a morphological analyzer to model the space of possible analyses provides additional improvement. We make use of the resulting morphological models for scoring and ranking the analyses of the morphological analyzer for morphological disambiguation. The results show significant gains in accuracy across several evaluation metrics. Our system results in 4.4% absolute increase over the state-of-the-art in full morphological analysis accuracy (30.6% relative error reduction), and 10.6% (31.5% relative error reduction) for out-of-vocabulary words.</p>
<p>Keywords:</p>
<h3 id="75. Paradigm Completion for Derivational Morphology.">75. Paradigm Completion for Derivational Morphology.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1074/">Paper Link</a>    Pages:714-720</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cotterell:Ryan">Ryan Cotterell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vylomova:Ekaterina">Ekaterina Vylomova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khayrallah:Huda">Huda Khayrallah</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kirov:Christo">Christo Kirov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yarowsky:David">David Yarowsky</a></p>
<p>Abstract:
The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models adapted from the inflection task are able to learn the range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems.</p>
<p>Keywords:</p>
<h3 id="76. A Sub-Character Architecture for Korean Language Processing.">76. A Sub-Character Architecture for Korean Language Processing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1075/">Paper Link</a>    Pages:721-726</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stratos:Karl">Karl Stratos</a></p>
<p>Abstract:
We introduce a novel sub-character architecture that exploits a unique compositional structure of the Korean language. Our method decomposes each character into a small set of primitive phonetic units called jamo letters from which character- and word-level representations are induced. The jamo letters divulge syntactic and semantic information that is difficult to access with conventional character-level units. They greatly alleviate the data sparsity problem, reducing the observation space to 1.6% of the original while increasing accuracy in our experiments. We apply our architecture to dependency parsing and achieve dramatic improvement over strong lexical baselines.</p>
<p>Keywords:</p>
<h3 id="77. Do LSTMs really work so well for PoS tagging? - A replication study.">77. Do LSTMs really work so well for PoS tagging? - A replication study.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1076/">Paper Link</a>    Pages:727-736</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Horsmann:Tobias">Tobias Horsmann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zesch:Torsten">Torsten Zesch</a></p>
<p>Abstract:
A recent study by Plank et al. (2016) found that LSTM-based PoS taggers considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset. We replicate this study using a fresh collection of 27 corpora of 21 languages that are annotated with fine-grained tagsets of varying size. Our replication confirms the result in general, and we additionally find that the advantage of LSTMs is even bigger for larger tagsets. However, we also find that for the very large tagsets of morphologically rich languages, hand-crafted morphological lexicons are still necessary to reach state-of-the-art performance.</p>
<p>Keywords:</p>
<h3 id="78. The Labeled Segmentation of Printed Books.">78. The Labeled Segmentation of Printed Books.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1077/">Paper Link</a>    Pages:737-747</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/McConnaughey:Lara">Lara McConnaughey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Jennifer">Jennifer Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bamman:David">David Bamman</a></p>
<p>Abstract:
We introduce the task of book structure labeling: segmenting and assigning a fixed category (such as Table of Contents, Preface, Index) to the document structure of printed books. We manually annotate the page-level structural categories for a large dataset totaling 294,816 pages in 1,055 books evenly sampled from 1750-1922, and present empirical results comparing the performance of several classes of models. The best-performing model, a bidirectional LSTM with rich features, achieves an overall accuracy of 95.8 and a class-balanced macro F-score of 71.4.</p>
<p>Keywords:</p>
<h3 id="79. Cross-lingual Character-Level Neural Morphological Tagging.">79. Cross-lingual Character-Level Neural Morphological Tagging.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1078/">Paper Link</a>    Pages:748-759</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cotterell:Ryan">Ryan Cotterell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Heigold:Georg">Georg Heigold</a></p>
<p>Abstract:
Even for common NLP tasks, sufficient supervision is not available in many languages  morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones.</p>
<p>Keywords:</p>
<h3 id="80. Word-Context Character Embeddings for Chinese Word Segmentation.">80. Word-Context Character Embeddings for Chinese Word Segmentation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1079/">Paper Link</a>    Pages:760-766</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Hao">Hao Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Zhenting">Zhenting Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Shujian">Shujian Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Xin=Yu">Xin-Yu Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jiajun">Jiajun Chen</a></p>
<p>Abstract:
Neural parsers have benefited from automatically labeled data via dependency-context word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method improves state-of-the-art neural word segmentation models significantly, beating tri-training baselines for leveraging auto-segmented data.</p>
<p>Keywords:</p>
<h3 id="81. Segmentation-Free Word Embedding for Unsegmented Languages.">81. Segmentation-Free Word Embedding for Unsegmented Languages.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1080/">Paper Link</a>    Pages:767-772</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/o/Oshikiri:Takamasa">Takamasa Oshikiri</a></p>
<p>Abstract:
In this paper, we propose a new pipeline of word embedding for unsegmented languages, called segmentation-free word embedding, which does not require word segmentation as a preprocessing step. Unlike space-delimited languages, unsegmented languages, such as Chinese and Japanese, require word segmentation as a preprocessing step. However, word segmentation, that often requires manually annotated resources, is difficult and expensive, and unavoidable errors in word segmentation affect downstream tasks. To avoid these problems in learning word vectors of unsegmented languages, we consider word co-occurrence statistics over all possible candidates of segmentations based on frequent character n-grams instead of segmented sentences provided by conventional word segmenters. Our experiments of noun category prediction tasks on raw Twitter, Weibo, and Wikipedia corpora show that the proposed method outperforms the conventional approaches that require word segmenters.</p>
<p>Keywords:</p>
<h3 id="82. From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems.">82. From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1081/">Paper Link</a>    Pages:773-784</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sachan:Mrinmaya">Mrinmaya Sachan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dubey:Avinava">Avinava Dubey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xing:Eric_P=">Eric P. Xing</a></p>
<p>Abstract:
Textbooks are rich sources of information. Harvesting structured knowledge from textbooks is a key challenge in many educational applications. As a case study, we present an approach for harvesting structured axiomatic knowledge from math textbooks. Our approach uses rich contextual and typographical features extracted from raw textbooks. It leverages the redundancy and shared ordering across multiple textbooks to further refine the harvested axioms. These axioms are then parsed into rules that are used to improve the state-of-the-art in solving geometry problems.</p>
<p>Keywords:</p>
<h3 id="83. RACE: Large-scale ReAding Comprehension Dataset From Examinations.">83. RACE: Large-scale ReAding Comprehension Dataset From Examinations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1082/">Paper Link</a>    Pages:785-794</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lai:Guokun">Guokun Lai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Qizhe">Qizhe Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Hanxiao">Hanxiao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yiming">Yiming Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>Abstract:
We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at <a href="http://www.cs.cmu.edu/~glai1/data/race/and">http://www.cs.cmu.edu/~glai1/data/race/and</a> the code is available at <a href="https://github.com/qizhex/RACE_AR_baselines">https://github.com/qizhex/RACE_AR_baselines</a>.</p>
<p>Keywords:</p>
<h3 id="84. Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers.">84. Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1083/">Paper Link</a>    Pages:795-804</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hopkins:Mark">Mark Hopkins</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Petrescu=Prahova:Cristian">Cristian Petrescu-Prahova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Levin:Roie">Roie Levin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bras:Ronan_Le">Ronan Le Bras</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Herrasti:Alvaro">Alvaro Herrasti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Joshi:Vidur">Vidur Joshi</a></p>
<p>Abstract:
We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich source of such questions  the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our system propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first-ever results 43% recall and 91% precision) on SAT algebra word problems. We also apply our system to the public Dolphin algebra question set, and improve the state-of-the-art F1-score from 73.9% to 77.0%.</p>
<p>Keywords:</p>
<h3 id="85. Learning Fine-Grained Expressions to Solve Math Word Problems.">85. Learning Fine-Grained Expressions to Solve Math Word Problems.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1084/">Paper Link</a>    Pages:805-814</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Danqing">Danqing Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Shuming">Shuming Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Chin=Yew">Chin-Yew Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Jian">Jian Yin</a></p>
<p>Abstract:
This paper presents a novel template-based method to solve math word problems. This method learns the mappings between math concept phrases in math word problems and their math expressions from training data. For each equation template, we automatically construct a rich template sketch by aggregating information from various problems with the same template. Our approach is implemented in a two-stage system. It first retrieves a few relevant equation system templates and aligns numbers in math word problems to those templates for candidate equation generation. It then does a fine-grained inference to obtain the final answer. Experiment results show that our method achieves an accuracy of 28.4% on the linear Dolphin18K benchmark, which is 10% (54% relative) higher than previous state-of-the-art systems while achieving an accuracy increase of 12% (59% relative) on the TS6 benchmark subset.</p>
<p>Keywords:</p>
<h3 id="86. Structural Embedding of Syntactic Trees for Machine Comprehension.">86. Structural Embedding of Syntactic Trees for Machine Comprehension.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1085/">Paper Link</a>    Pages:815-824</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Rui">Rui Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Junjie">Junjie Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Wei">Wei Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Zi">Zi Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nyberg:Eric">Eric Nyberg</a></p>
<p>Abstract:
Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.</p>
<p>Keywords:</p>
<h3 id="87. World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions.">87. World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1086/">Paper Link</a>    Pages:825-834</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Long:Teng">Teng Long</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bengio:Emmanuel">Emmanuel Bengio</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lowe:Ryan">Ryan Lowe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheung:Jackie_Chi_Kit">Jackie Chi Kit Cheung</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Precup:Doina">Doina Precup</a></p>
<p>Abstract:
Humans interpret texts with respect to some background information, or world knowledge, and we would like to develop automatic reading comprehension systems that can do the same. In this paper, we introduce a task and several models to drive progress towards this goal. In particular, we propose the task of rare entity prediction: given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This task is challenging due to the diversity of language styles and the extremely large number of rare entities. We propose two recurrent neural network architectures which make use of external knowledge in the form of entity descriptions. Our experiments show that our hierarchical LSTM model performs significantly better at the rare entity prediction task than those that do not make use of external resources.</p>
<p>Keywords:</p>
<h3 id="88. Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension.">88. Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1087/">Paper Link</a>    Pages:835-844</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Golub:David">David Golub</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Po=Sen">Po-Sen Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He_0001:Xiaodong">Xiaodong He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deng_0001:Li">Li Deng</a></p>
<p>Abstract:
We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network. Given a high performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed synthesis network with a pretrained model on the SQuAD dataset, we achieve an F1 measure of 46.6% on the challenging NewsQA dataset, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline by 7.6%, without use of provided annotations.</p>
<p>Keywords:</p>
<h3 id="89. Deep Neural Solver for Math Word Problems.">89. Deep Neural Solver for Math Word Problems.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1088/">Paper Link</a>    Pages:845-854</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yan">Yan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xiaojiang">Xiaojiang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Shuming">Shuming Shi</a></p>
<p>Abstract:
This paper presents a deep neural solver to automatically solve math word problems. In contrast to previous statistical learning approaches, we directly translate math word problems to equation templates using a recurrent neural network (RNN) model, without sophisticated feature engineering. We further design a hybrid model that combines the RNN model and a similarity-based retrieval model to achieve additional performance improvement. Experiments conducted on a large dataset show that the RNN model and the hybrid model significantly outperform state-of-the-art statistical learning methods for math word problem solving.</p>
<p>Keywords:</p>
<h3 id="90. Latent Space Embedding for Retrieval in Question-Answer Archives.">90. Latent Space Embedding for Retrieval in Question-Answer Archives.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1089/">Paper Link</a>    Pages:855-865</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/P_0001:Deepak">Deepak P</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Garg:Dinesh">Dinesh Garg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shevade:Shirish_K=">Shirish K. Shevade</a></p>
<p>Abstract:
Community-driven Question Answering (CQA) systems such as Yahoo! Answers have become valuable sources of reusable information. CQA retrieval enables usage of historical CQA archives to solve new questions posed by users. This task has received much recent attention, with methods building upon literature from translation models, topic models, and deep learning. In this paper, we devise a CQA retrieval technique, LASER-QA, that embeds question-answer pairs within a unified latent space preserving the local neighborhood structure of question and answer spaces. The idea is that such a space mirrors semantic similarity among questions as well as answers, thereby enabling high quality retrieval. Through an empirical analysis on various real-world QA datasets, we illustrate the improved effectiveness of LASER-QA over state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="91. Question Generation for Question Answering.">91. Question Generation for Question Answering.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1090/">Paper Link</a>    Pages:866-874</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Duan:Nan">Nan Duan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Duyu">Duyu Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Peng">Peng Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a></p>
<p>Abstract:
This paper presents how to generate questions from given passages using neural networks, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as training data. The contribution of the paper is 2-fold: First, two types of question generation approaches are proposed, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN); Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved.</p>
<p>Keywords:</p>
<h3 id="92. Learning to Paraphrase for Question Answering.">92. Learning to Paraphrase for Question Answering.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1091/">Paper Link</a>    Pages:875-886</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dong_0004:Li">Li Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mallinson:Jonathan">Jonathan Mallinson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reddy:Siva">Siva Reddy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>Abstract:
Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-to-end using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.</p>
<p>Keywords:</p>
<h3 id="93. Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture.">93. Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1092/">Paper Link</a>    Pages:887-896</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Meng:Yuanliang">Yuanliang Meng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rumshisky:Anna">Anna Rumshisky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Romanov:Alexey">Alexey Romanov</a></p>
<p>Abstract:
In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A double-checking technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin. We also conduct intrinsic evaluation and post state-of-the-art results on Timebank-Dense.</p>
<p>Keywords:</p>
<h3 id="94. Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification Model.">94. Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification Model.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1093/">Paper Link</a>    Pages:897-902</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tymoshenko:Kateryna">Kateryna Tymoshenko</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bonadiman:Daniele">Daniele Bonadiman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moschitti:Alessandro">Alessandro Moschitti</a></p>
<p>Abstract:
Recent work has shown that Tree Kernels (TKs) and Convolutional Neural Networks (CNNs) obtain the state of the art in answer sentence reranking. Additionally, their combination used in Support Vector Machines (SVMs) is promising as it can exploit both the syntactic patterns captured by TKs and the embeddings learned by CNNs. However, the embeddings are constructed according to a classification function, which is not directly exploitable in the preference ranking algorithm of SVMs. In this work, we propose a new hybrid approach combining preference ranking applied to TKs and pointwise ranking applied to CNNs. We show that our approach produces better results on two well-known and rather different datasets: WikiQA for answer sentence selection and SemEval cQA for comment selection in Community Question Answering.</p>
<p>Keywords:</p>
<h3 id="95. Recovering Question Answering Errors via Query Revision.">95. Recovering Question Answering Errors via Query Revision.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1094/">Paper Link</a>    Pages:903-909</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yavuz:Semih">Semih Yavuz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gur:Izzeddin">Izzeddin Gur</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su_0001:Yu">Yu Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Xifeng">Xifeng Yan</a></p>
<p>Abstract:
The existing factoid QA systems often lack a post-inspection component that can help models recover from their own mistakes. In this work, we propose to crosscheck the corresponding KB relations behind the predicted answers and identify potential inconsistencies. Instead of developing a new model that accepts evidences collected from these relations, we choose to plug them back to the original questions directly and check if the revised question makes sense or not. A bidirectional LSTM is applied to encode revised questions. We develop a scoring mechanism over the revised question encodings to refine the predictions of a base QA system. This approach can improve the F1 score of STAGG (Yih et al., 2015), one of the leading QA systems, from 52.5% to 53.9% on WEBQUESTIONS data.</p>
<p>Keywords:</p>
<h3 id="96. An empirical study on the effectiveness of images in Multimodal Neural Machine Translation.">96. An empirical study on the effectiveness of images in Multimodal Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1095/">Paper Link</a>    Pages:910-919</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Delbrouck:Jean=Benoit">Jean-Benoit Delbrouck</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dupont:St=eacute=phane">Stphane Dupont</a></p>
<p>Abstract:
In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multi-modal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multi-modal translation task (English, image  German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.</p>
<p>Keywords:</p>
<h3 id="97. Sound-Word2Vec: Learning Word Representations Grounded in Sounds.">97. Sound-Word2Vec: Learning Word Representations Grounded in Sounds.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1096/">Paper Link</a>    Pages:920-925</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vijayakumar:Ashwin_K=">Ashwin K. Vijayakumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vedantam:Ramakrishna">Ramakrishna Vedantam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Parikh:Devi">Devi Parikh</a></p>
<p>Abstract:
To be able to interact better with humans, it is crucial for machines to understand sound  a primary modality of human perception. Previous works have used sound to learn embeddings for improved generic semantic similarity assessment. In this work, we treat sound as a first-class citizen, studying downstream 6textual tasks which require aural grounding. To this end, we propose sound-word2vec  a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering Foley sound effects (used in movies). Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.</p>
<p>Keywords:</p>
<h3 id="98. The Promise of Premise: Harnessing Question Premises in Visual Question Answering.">98. The Promise of Premise: Harnessing Question Premises in Visual Question Answering.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1097/">Paper Link</a>    Pages:926-935</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mahendru:Aroma">Aroma Mahendru</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prabhu:Viraj">Viraj Prabhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mohapatra:Akrit">Akrit Mohapatra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Batra:Dhruv">Dhruv Batra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Stefan">Stefan Lee</a></p>
<p>Abstract:
In this paper, we make a simple observation that questions about images often contain premises  objects and relationships implied by the question  and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions. When presented with a question that is irrelevant to an image, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in non-sensical or even misleading answers. We note that a visual question is irrelevant to an image if at least one of its premises is false (i.e. not depicted in the image). We leverage this observation to construct a dataset for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not. We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.</p>
<p>Keywords:</p>
<h3 id="99. Guided Open Vocabulary Image Captioning with Constrained Beam Search.">99. Guided Open Vocabulary Image Captioning with Constrained Beam Search.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1098/">Paper Link</a>    Pages:936-945</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Anderson_0001:Peter">Peter Anderson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fernando:Basura">Basura Fernando</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Johnson_0001:Mark">Mark Johnson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gould:Stephen">Stephen Gould</a></p>
<p>Abstract:
Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.</p>
<p>Keywords:</p>
<h3 id="100. Zero-Shot Activity Recognition with Verb Attribute Induction.">100. Zero-Shot Activity Recognition with Verb Attribute Induction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1099/">Paper Link</a>    Pages:946-958</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zellers:Rowan">Rowan Zellers</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a></p>
<p>Abstract:
In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb salute has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.</p>
<p>Keywords:</p>
<h3 id="101. Deriving continous grounded meaning representations from referentially structured multimodal contexts.">101. Deriving continous grounded meaning representations from referentially structured multimodal contexts.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1100/">Paper Link</a>    Pages:959-965</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zarrie=szlig=:Sina">Sina Zarrie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schlangen:David">David Schlangen</a></p>
<p>Abstract:
Corpora of referring expressions paired with their visual referents are a good source for learning word meanings directly grounded in visual representations. Here, we explore additional ways of extracting from them word representations linked to multi-modal context: through expressions that refer to the same object, and through expressions that refer to different objects in the same scene. We show that continuous meaning representations derived from these contexts capture complementary aspects of similarity, , even if not outperforming textual embeddings trained on very large amounts of raw text when tested on standard similarity benchmarks. We propose a new task for evaluating grounded meaning representationsdetection of potentially co-referential phrasesand show that it requires precise denotational representations of attribute meanings, which our method provides.</p>
<p>Keywords:</p>
<h3 id="102. Hierarchically-Attentive RNN for Album Summarization and Storytelling.">102. Hierarchically-Attentive RNN for Album Summarization and Storytelling.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1101/">Paper Link</a>    Pages:966-971</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Licheng">Licheng Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berg:Tamara_L=">Tamara L. Berg</a></p>
<p>Abstract:
We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.</p>
<p>Keywords:</p>
<h3 id="103. Video Highlight Prediction Using Audience Chat Reactions.">103. Video Highlight Prediction Using Audience Chat Reactions.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1102/">Paper Link</a>    Pages:972-978</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Cheng=Yang">Cheng-Yang Fu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Joon">Joon Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berg:Alexander_C=">Alexander C. Berg</a></p>
<p>Abstract:
Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.</p>
<p>Keywords:</p>
<h3 id="104. Reinforced Video Captioning with Entailment Rewards.">104. Reinforced Video Captioning with Entailment Rewards.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1103/">Paper Link</a>    Pages:979-985</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pasunuru:Ramakanth">Ramakanth Pasunuru</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a></p>
<p>Abstract:
Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.</p>
<p>Keywords:</p>
<h3 id="105. Evaluating Hierarchies of Verb Argument Structure with Hierarchical Clustering.">105. Evaluating Hierarchies of Verb Argument Structure with Hierarchical Clustering.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1104/">Paper Link</a>    Pages:986-991</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mu:Jesse">Jesse Mu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hartshorne:Joshua_K=">Joshua K. Hartshorne</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/O=Donnell:Timothy">Timothy O&apos;Donnell</a></p>
<p>Abstract:
Verbs can only be used with a few specific arrangements of their arguments (syntactic frames). Most theorists note that verbs can be organized into a hierarchy of verb classes based on the frames they admit. Here we show that such a hierarchy is objectively well-supported by the patterns of verbs and frames in English, since a systematic hierarchical clustering algorithm converges on the same structure as the handcrafted taxonomy of VerbNet, a broad-coverage verb lexicon. We also show that the hierarchies capture meaningful psychological dimensions of generalization by predicting novel verb coercions by human participants. We discuss limitations of a simple hierarchical representation and suggest similar approaches for identifying the representations underpinning verb argument structure.</p>
<p>Keywords:</p>
<h3 id="106. Incorporating Global Visual Features into Attention-based Neural Machine Translation.">106. Incorporating Global Visual Features into Attention-based Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1105/">Paper Link</a>    Pages:992-1003</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Calixto:Iacer">Iacer Calixto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qun">Qun Liu</a></p>
<p>Abstract:
We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. Global image features are extracted using a pre-trained convolutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate translations into English and German, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal NMT models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.</p>
<p>Keywords:</p>
<h3 id="107. Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.">107. Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1106/">Paper Link</a>    Pages:1004-1015</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Misra:Dipendra_Kumar">Dipendra Kumar Misra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Langford_0001:John">John Langford</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Artzi:Yoav">Yoav Artzi</a></p>
<p>Abstract:
We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agents exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.</p>
<p>Keywords:</p>
<h3 id="108. An analysis of eye-movements during reading for the detection of mild cognitive impairment.">108. An analysis of eye-movements during reading for the detection of mild cognitive impairment.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1107/">Paper Link</a>    Pages:1016-1026</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fraser:Kathleen_C=">Kathleen C. Fraser</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fors:Kristina_Lundholm">Kristina Lundholm Fors</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kokkinakis:Dimitrios">Dimitrios Kokkinakis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nordlund:Arto">Arto Nordlund</a></p>
<p>Abstract:
We present a machine learning analysis of eye-tracking data for the detection of mild cognitive impairment, a decline in cognitive abilities that is associated with an increased risk of developing dementia. We compare two experimental configurations (reading aloud versus reading silently), as well as two methods of combining information from the two trials (concatenation and merging). Additionally, we annotate the words being read with information about their frequency and syntactic category, and use these annotations to generate new features. Ultimately, we are able to distinguish between participants with and without cognitive impairment with up to 86% accuracy.</p>
<p>Keywords:</p>
<h3 id="109. A Structured Learning Approach to Temporal Relation Extraction.">109. A Structured Learning Approach to Temporal Relation Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1108/">Paper Link</a>    Pages:1027-1037</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Ning:Qiang">Qiang Ning</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Zhili">Zhili Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>Abstract:
Identifying temporal relations between events is an essential step towards natural language understanding. However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events. Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators. This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem.</p>
<p>Keywords:</p>
<h3 id="110. Importance sampling for unbiased on-demand evaluation of knowledge base population.">110. Importance sampling for unbiased on-demand evaluation of knowledge base population.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1109/">Paper Link</a>    Pages:1038-1048</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chaganty:Arun_Tejasvi">Arun Tejasvi Chaganty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Paranjape:Ashwin">Ashwin Paranjape</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Manning:Christopher_D=">Christopher D. Manning</a></p>
<p>Abstract:
Knowledge base population (KBP) systems take in a large document corpus and extract entities and their relations. Thus far, KBP evaluation has relied on judgements on the pooled predictions of existing systems. We show that this evaluation is problematic: when a new system predicts a previously unseen relation, it is penalized even if it is correct. This leads to significant bias against new systems, which counterproductively discourages innovation in the field. Our first contribution is a new importance-sampling based evaluation which corrects for this bias by annotating a new systems predictions on-demand via crowdsourcing. We show this eliminates bias and reduces variance using data from the 2015 TAC KBP task. Our second contribution is an implementation of our method made publicly available as an online KBP evaluation service. We pilot the service by testing diverse state-of-the-art systems on the TAC KBP 2016 corpus and obtain accurate scores in a cost effective manner.</p>
<p>Keywords:</p>
<h3 id="111. PACRR: A Position-Aware Neural IR Model for Relevance Matching.">111. PACRR: A Position-Aware Neural IR Model for Relevance Matching.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1110/">Paper Link</a>    Pages:1049-1058</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hui:Kai">Kai Hui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yates:Andrew">Andrew Yates</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berberich:Klaus">Klaus Berberich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Melo:Gerard_de">Gerard de Melo</a></p>
<p>Abstract:
In order to adopt deep learning for information retrieval, models are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored. In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document. Extensive experiments on six years TREC Web Track data confirm that the proposed model yields better results under multiple benchmarks.</p>
<p>Keywords:</p>
<h3 id="112. Globally Normalized Reader.">112. Globally Normalized Reader.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1111/">Paper Link</a>    Pages:1059-1069</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Raiman:Jonathan">Jonathan Raiman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miller:John">John Miller</a></p>
<p>Abstract:
Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answers sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type. This method improves the performance of all models considered in this work and is of independent interest for a variety of NLP tasks.</p>
<p>Keywords:</p>
<h3 id="113. Speech segmentation with a neural encoder model of working memory.">113. Speech segmentation with a neural encoder model of working memory.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1112/">Paper Link</a>    Pages:1070-1080</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Elsner:Micha">Micha Elsner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shain:Cory">Cory Shain</a></p>
<p>Abstract:
We present the first unsupervised LSTM speech segmenter as a cognitive model of the acquisition of words from unsegmented input. Cognitive biases toward phonological and syntactic predictability in speech are rooted in the limitations of human memory (Baddeley et al., 1998); compressed representations are easier to acquire and retain in memory. To model the biases introduced by these memory limitations, our system uses an LSTM-based encoder-decoder with a small number of hidden units, then searches for a segmentation that minimizes autoencoding loss. Linguistically meaningful segments (e.g. words) should share regular patterns of features that facilitate decoder performance in comparison to random segmentations, and we show that our learner discovers these patterns when trained on either phoneme sequences or raw acoustics. To our knowledge, ours is the first fully unsupervised system to be able to segment both symbolic and acoustic representations of speech.</p>
<p>Keywords:</p>
<h3 id="114. Speaking, Seeing, Understanding: Correlating semantic models with conceptual representation in the brain.">114. Speaking, Seeing, Understanding: Correlating semantic models with conceptual representation in the brain.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1113/">Paper Link</a>    Pages:1081-1091</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bulat:Luana">Luana Bulat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Clark:Stephen">Stephen Clark</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shutova:Ekaterina">Ekaterina Shutova</a></p>
<p>Abstract:
Research in computational semantics is increasingly guided by our understanding of human semantic processing. However, semantic models are typically studied in the context of natural language processing system performance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, state-of-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience.</p>
<p>Keywords:</p>
<h3 id="115. Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video.">115. Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1114/">Paper Link</a>    Pages:1092-1102</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Haoran">Haoran Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Junnan">Junnan Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Cong">Cong Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>Abstract:
The rapid increase of the multimedia data over the Internet necessitates multi-modal summarization from collections of text, image, audio and video. In this work, we propose an extractive Multi-modal Summarization (MMS) method which can automatically generate a textual summary given a set of documents, images, audios and videos related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal contents. For audio information, we design an approach to selectively use its transcription. For vision information, we learn joint representations of texts and images using a neural network. Finally, all the multi-modal aspects are considered to generate the textural summary by maximizing the salience, non-redundancy, readability and coverage through budgeted optimization of submodular functions. We further introduce an MMS corpus in English and Chinese. The experimental results on this dataset demonstrate that our method outperforms other competitive baseline methods.</p>
<p>Keywords:</p>
<h3 id="116. Tensor Fusion Network for Multimodal Sentiment Analysis.">116. Tensor Fusion Network for Multimodal Sentiment Analysis.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1115/">Paper Link</a>    Pages:1103-1114</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zadeh_0001:Amir">Amir Zadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Minghai">Minghai Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poria:Soujanya">Soujanya Poria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cambria:Erik">Erik Cambria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morency:Louis=Philippe">Louis-Philippe Morency</a></p>
<p>Abstract:
Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Networks, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.</p>
<p>Keywords:</p>
<h3 id="117. ConStance: Modeling Annotation Contexts to Improve Stance Classification.">117. ConStance: Modeling Annotation Contexts to Improve Stance Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1116/">Paper Link</a>    Pages:1115-1124</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Joseph:Kenneth">Kenneth Joseph</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Friedland:Lisa">Lisa Friedland</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hobbs:William">William Hobbs</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lazer:David">David Lazer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsur:Oren">Oren Tsur</a></p>
<p>Abstract:
Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the models interpretable parameters shed light on the effects of each context.</p>
<p>Keywords:</p>
<h3 id="118. Deeper Attention to Abusive User Content Moderation.">118. Deeper Attention to Abusive User Content Moderation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1117/">Paper Link</a>    Pages:1125-1135</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pavlopoulos:John">John Pavlopoulos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Malakasiotis:Prodromos">Prodromos Malakasiotis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Androutsopoulos:Ion">Ion Androutsopoulos</a></p>
<p>Abstract:
Experimenting with a new dataset of 1.6M user comments from a news portal and an existing dataset of 115K Wikipedia talk page comments, we show that an RNN operating on word embeddings outpeforms the previous state of the art in moderation, which used logistic regression or an MLP classifier with character or word n-grams. We also compare against a CNN operating on word embeddings, and a word-list baseline. A novel, deep, classificationspecific attention mechanism improves the performance of the RNN further, and can also highlight suspicious words for free, without including highlighted words in the training data. We consider both fully automatic and semi-automatic moderation.</p>
<p>Keywords:</p>
<h3 id="119. Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models.">119. Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1118/">Paper Link</a>    Pages:1136-1145</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dubossarsky:Haim">Haim Dubossarsky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weinshall:Daphna">Daphna Weinshall</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grossman:Eitan">Eitan Grossman</a></p>
<p>Abstract:
This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.</p>
<p>Keywords:</p>
<h3 id="120. Human Centered NLP with User-Factor Adaptation.">120. Human Centered NLP with User-Factor Adaptation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1119/">Paper Link</a>    Pages:1146-1155</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lynn:Veronica_E=">Veronica E. Lynn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Son:Youngseo">Youngseo Son</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kulkarni:Vivek">Vivek Kulkarni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Balasubramanian:Niranjan">Niranjan Balasubramanian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:H=_Andrew">H. Andrew Schwartz</a></p>
<p>Abstract:
We pose the general task of user-factor adaptation  adapting supervised learning models to real-valued user factors inferred from a background of their language, reflecting the idea that a piece of text should be understood within the context of the user that wrote it. We introduce a continuous adaptation technique, suited for real-valued user factors that are common in social science and bringing us closer to personalized NLP, adapting to each user uniquely. We apply this technique with known user factors including age, gender, and personality traits, as well as latent factors, evaluating over five tasks: POS tagging, PP-attachment, sentiment analysis, sarcasm detection, and stance detection. Adaptation provides statistically significant benefits for 3 of the 5 tasks: up to +1.2 points for PP-attachment, +3.4 points for sarcasm, and +3.0 points for stance.</p>
<p>Keywords:</p>
<h3 id="121. Neural Sequence Learning Models for Word Sense Disambiguation.">121. Neural Sequence Learning Models for Word Sense Disambiguation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1120/">Paper Link</a>    Pages:1156-1167</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Raganato:Alessandro">Alessandro Raganato</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bovi:Claudio_Delli">Claudio Delli Bovi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Navigli:Roberto">Roberto Navigli</a></p>
<p>Abstract:
Word Sense Disambiguation models exist in many flavors. Even though supervised ones tend to perform best in terms of accuracy, they often lose ground to more flexible knowledge-based solutions, which do not require training by a word expert for every disambiguation target. To bridge this gap we adopt a different perspective and rely on sequence learning to frame the disambiguation problem: we propose and study in depth a series of end-to-end neural architectures directly tailored to the task, from bidirectional Long Short-Term Memory to encoder-decoder models. Our extensive evaluation over standard benchmarks and in multiple languages shows that sequence learning enables more versatile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features.</p>
<p>Keywords:</p>
<h3 id="122. Learning Word Relatedness over Time.">122. Learning Word Relatedness over Time.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1121/">Paper Link</a>    Pages:1168-1178</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rosin:Guy_D=">Guy D. Rosin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Adar:Eytan">Eytan Adar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Radinsky:Kira">Kira Radinsky</a></p>
<p>Abstract:
Search systems are often focused on providing relevant results for the now, assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the Web to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as query expansion, leverage the relationship between terms but cannot function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from longitudinal data collections. The model supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this task and show its application for the task of query expansion, achieving high gain.</p>
<p>Keywords:</p>
<h3 id="123. Inter-Weighted Alignment Network for Sentence Pair Modeling.">123. Inter-Weighted Alignment Network for Sentence Pair Modeling.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1122/">Paper Link</a>    Pages:1179-1189</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Gehui">Gehui Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yunlun">Yunlun Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deng:Zhi=Hong">Zhi-Hong Deng</a></p>
<p>Abstract:
Sentence pair modeling is a crucial problem in the field of natural language processing. In this paper, we propose a model to measure the similarity of a sentence pair focusing on the interaction information. We utilize the word level similarity matrix to discover fine-grained alignment of two sentences. It should be emphasized that each word in a sentence has a different importance from the perspective of semantic composition, so we exploit two novel and efficient strategies to explicitly calculate a weight for each word. Although the proposed model only use a sequential LSTM for sentence modeling without any external resource such as syntactic parser tree and additional lexicon features, experimental results show that our model achieves state-of-the-art performance on three datasets of two tasks.</p>
<p>Keywords:</p>
<h3 id="124. A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources and Recent Advances.">124. A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources and Recent Advances.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1123/">Paper Link</a>    Pages:1190-1203</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0001:Chengyu">Chengyu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Xiaofeng">Xiaofeng He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Aoying">Aoying Zhou</a></p>
<p>Abstract:
A taxonomy is a semantic hierarchy, consisting of concepts linked by is-a relations. While a large number of taxonomies have been constructed from human-compiled resources (e.g., Wikipedia), learning taxonomies from text corpora has received a growing interest and is essential for long-tailed and domain-specific knowledge acquisition. In this paper, we overview recent advances on taxonomy construction from free texts, reorganizing relevant subtasks into a complete framework. We also overview resources for evaluation and discuss challenges for future research.</p>
<p>Keywords:</p>
<h3 id="125. Idiom-Aware Compositional Distributed Semantics.">125. Idiom-Aware Compositional Distributed Semantics.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1124/">Paper Link</a>    Pages:1204-1213</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Pengfei">Pengfei Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qian:Kaiyu">Kaiyu Qian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qiu:Xipeng">Xipeng Qiu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>Abstract:
Idioms are peculiar linguistic constructions that impose great challenges for representing the semantics of language, especially in current prevailing end-to-end neural models, which assume that the semantics of a phrase or sentence can be literally composed from its constitutive words. In this paper, we propose an idiom-aware distributed semantic model to build representation of sentences on the basis of understanding their contained idioms. Our models are grounded in the literal-first psycholinguistic hypothesis, which can adaptively learn semantic compositionality of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of idioms. The qualitative and quantitative experimental analyses demonstrate the efficacy of our models.</p>
<p>Keywords:</p>
<h3 id="126. Macro Grammars and Holistic Triggering for Efficient Semantic Parsing.">126. Macro Grammars and Holistic Triggering for Efficient Semantic Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1125/">Paper Link</a>    Pages:1214-1223</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0002:Yuchen">Yuchen Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pasupat:Panupong">Panupong Pasupat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a></p>
<p>Abstract:
To learn a semantic parser from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7% to 42.7%, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7%.</p>
<p>Keywords:</p>
<h3 id="127. A Continuously Growing Dataset of Sentential Paraphrases.">127. A Continuously Growing Dataset of Sentential Paraphrases.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1126/">Paper Link</a>    Pages:1224-1234</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lan:Wuwei">Wuwei Lan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qiu:Siyu">Siyu Qiu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Hua">Hua He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0004:Wei">Wei Xu</a></p>
<p>Abstract:
A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at ~70% precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.</p>
<p>Keywords:</p>
<h3 id="128. Cross-domain Semantic Parsing via Paraphrasing.">128. Cross-domain Semantic Parsing via Paraphrasing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1127/">Paper Link</a>    Pages:1235-1246</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Su_0001:Yu">Yu Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Xifeng">Xifeng Yan</a></p>
<p>Abstract:
Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular Overnight dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement.</p>
<p>Keywords:</p>
<h3 id="129. A Joint Sequential and Relational Model for Frame-Semantic Parsing.">129. A Joint Sequential and Relational Model for Frame-Semantic Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1128/">Paper Link</a>    Pages:1247-1256</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Bishan">Bishan Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitchell:Tom_M=">Tom M. Mitchell</a></p>
<p>Abstract:
We introduce a new method for frame-semantic parsing that significantly improves the prior state of the art. Our model leverages the advantages of a deep bidirectional LSTM network which predicts semantic role labels word by word and a relational network which predicts semantic roles for individual text expressions in relation to a predicate. The two networks are integrated into a single model via knowledge distillation, and a unified graphical model is employed to jointly decode frames and semantic roles during inference. Experiments on the standard FrameNet data show that our model significantly outperforms existing neural and non-neural approaches, achieving a 5.7 F1 gain over the current state of the art, for full frame structure extraction.</p>
<p>Keywords:</p>
<h3 id="130. Getting the Most out of AMR Parsing.">130. Getting the Most out of AMR Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1129/">Paper Link</a>    Pages:1257-1268</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Chuan">Chuan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xue:Nianwen">Nianwen Xue</a></p>
<p>Abstract:
This paper proposes to tackle the AMR parsing bottleneck by improving two components of an AMR parser: concept identification and alignment. We first build a Bidirectional LSTM based concept identifier that is able to incorporate richer contextual information to learn sparse AMR concept labels. We then extend an HMM-based word-to-concept alignment model with graph distance distortion and a rescoring method during decoding to incorporate the structural information in the AMR graph. We show integrating the two components into an existing AMR parser results in consistently better performance over the state of the art on various datasets.</p>
<p>Keywords:</p>
<h3 id="131. AMR Parsing using Stack-LSTMs.">131. AMR Parsing using Stack-LSTMs.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1130/">Paper Link</a>    Pages:1269-1275</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Ballesteros:Miguel">Miguel Ballesteros</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Al=Onaizan:Yaser">Yaser Al-Onaizan</a></p>
<p>Abstract:
We present a transition-based AMR parser that directly generates AMR parses from plain text. We use Stack-LSTMs to represent our parser state and make decisions greedily. In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further.</p>
<p>Keywords:</p>
<h3 id="132. An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective.">132. An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1131/">Paper Link</a>    Pages:1276-1282</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Jie">Jie Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su_0001:Yu">Yu Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guan:Ziyu">Ziyu Guan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Huan">Huan Sun</a></p>
<p>Abstract:
Given a question and a set of answer candidates, answer triggering determines whether the candidate set contains any correct answers. If yes, it then outputs a correct one. In contrast to existing pipeline methods which first consider individual candidate answers separately and then make a prediction based on a threshold, we propose an end-to-end deep neural network framework, which is trained by a novel group-level objective function that directly optimizes the answer triggering performance. Our objective function penalizes three potential types of error and allows training the framework in an end-to-end manner. Experimental results on the WikiQA benchmark show that our framework outperforms the state of the arts by a 6.6% absolute gain under F1 measure.</p>
<p>Keywords:</p>
<h3 id="133. Predicting Word Association Strengths.">133. Predicting Word Association Strengths.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1132/">Paper Link</a>    Pages:1283-1288</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cattle:Andrew">Andrew Cattle</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Xiaojuan">Xiaojuan Ma</a></p>
<p>Abstract:
This paper looks at the task of predicting word association strengths across three datasets; WordNet Evocation (Boyd-Graber et al., 2006), University of Southern Florida Free Association norms (Nelson et al., 2004), and Edinburgh Associative Thesaurus (Kiss et al., 1973). We achieve results of r=0.357 and p=0.379, r=0.344 and p=0.300, and r=0.292 and p=0.363, respectively. We find Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) cosine similarities, as well as vector offsets, to be the highest performing features. Furthermore, we examine the usefulness of Gaussian embeddings (Vilnis and McCallum, 2014) for predicting word association strength, the first work to do so.</p>
<p>Keywords:</p>
<h3 id="134. Learning Contextually Informed Representations for Linear-Time Discourse Parsing.">134. Learning Contextually Informed Representations for Linear-Time Discourse Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1133/">Paper Link</a>    Pages:1289-1298</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0124:Yang">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>Abstract:
Recent advances in RST discourse parsing have focused on two modeling paradigms: (a) high order parsers which jointly predict the tree structure of the discourse and the relations it encodes; or (b) linear-time parsers which are efficient but mostly based on local features. In this work, we propose a linear-time parser with a novel way of representing discourse constituents based on neural networks which takes into account global contextual information and is able to capture long-distance dependencies. Experimental results show that our parser obtains state-of-the art performance on benchmark datasets, while being efficient (with time complexity linear in the number of sentences in the document) and requiring minimal feature engineering.</p>
<p>Keywords:</p>
<h3 id="135. Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification.">135. Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1134/">Paper Link</a>    Pages:1299-1308</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lan:Man">Man Lan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Jianxiang">Jianxiang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Yuanbin">Yuanbin Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Niu:Zheng=Yu">Zheng-Yu Niu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Haifeng">Haifeng Wang</a></p>
<p>Abstract:
We present a novel multi-task attention based neural network model to address implicit discourse relationship representation and identification through two types of representation learning, an attention based neural network for learning discourse relationship representation with two arguments and a multi-task framework for learning knowledge from annotated and unannotated corpora. The extensive experiments have been performed on two benchmark corpora (i.e., PDTB and CoNLL-2016 datasets). Experimental results show that our proposed model outperforms the state-of-the-art systems on benchmark corpora.</p>
<p>Keywords:</p>
<h3 id="136. Chinese Zero Pronoun Resolution with Deep Memory Network.">136. Chinese Zero Pronoun Resolution with Deep Memory Network.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1135/">Paper Link</a>    Pages:1309-1318</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Qingyu">Qingyu Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0030:Yu">Yu Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0003:Weinan">Weinan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>Abstract:
Existing approaches for Chinese zero pronoun resolution typically utilize only syntactical and lexical features while ignoring semantic information. The fundamental reason is that zero pronouns have no descriptive information, which brings difficulty in explicitly capturing their semantic similarities with antecedents. Meanwhile, representing zero pronouns is challenging since they are merely gaps that convey no actual content. In this paper, we address this issue by building a deep memory network that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents. Consequently, our resolver takes advantage of semantic information by using these continuous distributed representations. Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings.</p>
<p>Keywords:</p>
<h3 id="137. How much progress have we made on RST discourse parsing? A replication study of recent results on the RST-DT.">137. How much progress have we made on RST discourse parsing? A replication study of recent results on the RST-DT.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1136/">Paper Link</a>    Pages:1319-1324</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Morey:Mathieu">Mathieu Morey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Muller:Philippe">Philippe Muller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Asher:Nicholas">Nicholas Asher</a></p>
<p>Abstract:
This article evaluates purported progress over the past years in RST discourse parsing. Several studies report a relative error reduction of 24 to 51% on all metrics that authors attribute to the introduction of distributed representations of discourse units. We replicate the standard evaluation of 9 parsers, 5 of which use distributed representations, from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT. Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure. We evaluate all these parsers with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings. Under this more stringent procedure, the gains attributable to distributed representations represent at most a 16% relative error reduction on fully-labelled structures.</p>
<p>Keywords:</p>
<h3 id="138. What is it? Disambiguating the different readings of the pronoun 'it'.">138. What is it? Disambiguating the different readings of the pronoun 'it'.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1137/">Paper Link</a>    Pages:1325-1331</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lo=aacute=iciga:Sharid">Sharid Loiciga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guillou:Liane">Liane Guillou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hardmeier:Christian">Christian Hardmeier</a></p>
<p>Abstract:
In this paper, we address the problem of predicting one of three functions for the English pronoun it: anaphoric, event reference or pleonastic. This disambiguation is valuable in the context of machine translation and coreference resolution. We present experiments using a MAXENT classifier trained on gold-standard data and self-training experiments of an RNN trained on silver-standard data, annotated using the MAXENT classifier. Lastly, we report on an analysis of the strengths of these two models.</p>
<p>Keywords:</p>
<h3 id="139. Revisiting Selectional Preferences for Coreference Resolution.">139. Revisiting Selectional Preferences for Coreference Resolution.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1138/">Paper Link</a>    Pages:1332-1339</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Heinzerling:Benjamin">Benjamin Heinzerling</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moosavi:Nafise_Sadat">Nafise Sadat Moosavi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Strube_0001:Michael">Michael Strube</a></p>
<p>Abstract:
Selectional preferences have long been claimed to be essential for coreference resolution. However, they are modeled only implicitly by current coreference resolvers. We propose a dependency-based embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. Incorporating our model improves performance, matching state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile are such improvements.</p>
<p>Keywords:</p>
<h3 id="140. Learning to Rank Semantic Coherence for Topic Segmentation.">140. Learning to Rank Semantic Coherence for Topic Segmentation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1139/">Paper Link</a>    Pages:1340-1344</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Liang">Liang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Sujian">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lv:Yajuan">Yajuan Lv</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a></p>
<p>Abstract:
Topic segmentation plays an important role for discourse parsing and information retrieval. Due to the absence of training data, previous work mainly adopts unsupervised methods to rank semantic coherence between paragraphs for topic segmentation. In this paper, we present an intuitive and simple idea to automatically create a quasi training dataset, which includes a large amount of text pairs from the same or different documents with different semantic coherence. With the training corpus, we design a symmetric CNN neural network to model text pairs and rank the semantic coherence within the learning to rank framework. Experiments show that our algorithm is able to achieve competitive performance over strong baselines on several real-world datasets.</p>
<p>Keywords:</p>
<h3 id="141. GRASP: Rich Patterns for Argumentation Mining.">141. GRASP: Rich Patterns for Argumentation Mining.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1140/">Paper Link</a>    Pages:1345-1350</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shnarch:Eyal">Eyal Shnarch</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Levy:Ran">Ran Levy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Raykar:Vikas_C=">Vikas C. Raykar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Slonim:Noam">Noam Slonim</a></p>
<p>Abstract:
GRASP (GReedy Augmented Sequential Patterns) is an algorithm for automatically extracting patterns that characterize subtle linguistic phenomena. To that end, GRASP augments each term of input text with multiple layers of linguistic information. These different facets of the text terms are systematically combined to reveal rich patterns. We report highly promising experimental results in several challenging text analysis tasks within the field of Argumentation Mining. We believe that GRASP is general enough to be useful for other domains too. For example, each of the following sentences includes a claim for a [topic]: 1. Opponents often argue that the open primary is unconstitutional. [Open Primaries] 2. Prof. Smith suggested that affirmative action devalues the accomplishments of the chosen. [Affirmative Action] 3. The majority stated that the First Amendment does not guarantee the right to offend others. [Freedom of Speech] These sentences share almost no words in common, however, they are similar at a more abstract level. A human observer may notice the following underlying common structure, or pattern: [someone][argue/suggest/state][that][topic term][sentiment term]. GRASP aims to automatically capture such underlying structures of the given data. For the above examples it finds the pattern [noun][express][that][noun,topic][sentiment], where [express] stands for all its (in)direct hyponyms, and [noun,topic] means a noun which is also related to the topic.</p>
<p>Keywords:</p>
<h3 id="142. Patterns of Argumentation Strategies across Topics.">142. Patterns of Argumentation Strategies across Topics.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1141/">Paper Link</a>    Pages:1351-1357</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Khatib:Khalid_Al">Khalid Al Khatib</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wachsmuth:Henning">Henning Wachsmuth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hagen:Matthias">Matthias Hagen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stein_0001:Benno">Benno Stein</a></p>
<p>Abstract:
This paper presents an analysis of argumentation strategies in news editorials within and across topics. Given nearly 29,000 argumentative editorials from the New York Times, we develop two machine learning models, one for determining an editorials topic, and one for identifying evidence types in the editorial. Based on the distribution and structure of the identified types, we analyze the usage patterns of argumentation strategies among 12 different topics. We detect several common patterns that provide insights into the manifestation of argumentation strategies. Also, our experiments reveal clear correlations between the topics and the detected patterns.</p>
<p>Keywords:</p>
<h3 id="143. Using Argument-based Features to Predict and Analyse Review Helpfulness.">143. Using Argument-based Features to Predict and Analyse Review Helpfulness.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1142/">Paper Link</a>    Pages:1358-1363</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Haijing">Haijing Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao_0021:Yang">Yang Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lv:Pin">Pin Lv</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Mengxue">Mengxue Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Geng:Shiqiang">Shiqiang Geng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Minglan">Minglan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0005:Hao">Hao Wang</a></p>
<p>Abstract:
We study the helpful product reviews identification problem in this paper. We observe that the evidence-conclusion discourse relations, also known as arguments, often appear in product reviews, and we hypothesise that some argument-based features, e.g. the percentage of argumentative sentences, the evidences-conclusions ratios, are good indicators of helpful reviews. To validate this hypothesis, we manually annotate arguments in 110 hotel reviews, and investigate the effectiveness of several combinations of argument-based features. Experiments suggest that, when being used together with the argument-based features, the state-of-the-art baseline features can enjoy a performance boost (in terms of F1) of 11.01% in average.</p>
<p>Keywords:</p>
<h3 id="144. Here's My Point: Joint Pointer Architecture for Argument Mining.">144. Here's My Point: Joint Pointer Architecture for Argument Mining.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1143/">Paper Link</a>    Pages:1364-1373</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Potash:Peter">Peter Potash</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Romanov:Alexey">Alexey Romanov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rumshisky:Anna">Anna Rumshisky</a></p>
<p>Abstract:
In order to determine argument structure in text, one must understand how individual components of the overall argument are linked. This work presents the first neural network-based approach to link extraction in argument mining. Specifically, we propose a novel architecture that applies Pointer Network sequence-to-sequence attention modeling to structural prediction in discourse parsing tasks. We then develop a joint model that extends this architecture to simultaneously address the link extraction task and the classification of argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models. Furthermore, our results demonstrate that jointly optimizing for both tasks is crucial for high performance.</p>
<p>Keywords:</p>
<h3 id="145. Identifying attack and support argumentative relations using deep learning.">145. Identifying attack and support argumentative relations using deep learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1144/">Paper Link</a>    Pages:1374-1379</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cocarascu:Oana">Oana Cocarascu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Toni:Francesca">Francesca Toni</a></p>
<p>Abstract:
We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another, of the kind that naturally occur in a debate. The architecture uses two (unidirectional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.</p>
<p>Keywords:</p>
<h3 id="146. Neural Lattice-to-Sequence Models for Uncertain Inputs.">146. Neural Lattice-to-Sequence Models for Uncertain Inputs.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1145/">Paper Link</a>    Pages:1380-1389</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sperber:Matthias">Matthias Sperber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Niehues:Jan">Jan Niehues</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Waibel:Alex">Alex Waibel</a></p>
<p>Abstract:
The input to a neural sequence-to-sequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTMs child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.</p>
<p>Keywords:</p>
<h3 id="147. Memory-augmented Neural Machine Translation.">147. Memory-augmented Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1146/">Paper Link</a>    Pages:1390-1399</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Feng_0004:Yang">Yang Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Shiyue">Shiyue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Andi">Andi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0013:Dong">Dong Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abel:Andrew">Andrew Abel</a></p>
<p>Abstract:
Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by 9.0 and 2.7 BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.</p>
<p>Keywords:</p>
<h3 id="148. Dynamic Data Selection for Neural Machine Translation.">148. Dynamic Data Selection for Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1147/">Paper Link</a>    Pages:1400-1410</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wees:Marlies_van_der">Marlies van der Wees</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bisazza:Arianna">Arianna Bisazza</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Monz:Christof">Christof Monz</a></p>
<p>Abstract:
Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.</p>
<p>Keywords:</p>
<h3 id="149. Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search.">149. Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1148/">Paper Link</a>    Pages:1411-1420</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dahlmann:Leonard">Leonard Dahlmann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Matusov:Evgeny">Evgeny Matusov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Petrushkov:Pavel">Pavel Petrushkov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khadivi:Shahram">Shahram Khadivi</a></p>
<p>Abstract:
In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German-to-English news domain and English-to-Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong NMT baseline.</p>
<p>Keywords:</p>
<h3 id="150. Translating Phrases in Neural Machine Translation.">150. Translating Phrases in Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1149/">Paper Link</a>    Pages:1421-1431</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0007:Xing">Xing Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Zhaopeng">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a></p>
<p>Abstract:
Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets.</p>
<p>Keywords:</p>
<h3 id="151. Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation.">151. Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1150/">Paper Link</a>    Pages:1432-1441</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Baosong">Baosong Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wong:Derek_F=">Derek F. Wong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Tong">Tong Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chao:Lidia_S=">Lidia S. Chao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Jingbo">Jingbo Zhu</a></p>
<p>Abstract:
This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.</p>
<p>Keywords:</p>
<h3 id="152. Learning Translations via Matrix Completion.">152. Learning Translations via Matrix Completion.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1152/">Paper Link</a>    Pages:1452-1463</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wijaya:Derry">Derry Wijaya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Callahan:Brendan">Brendan Callahan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hewitt:John">John Hewitt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Jie">Jie Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Ling:Xiao">Xiao Ling</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Apidianaki:Marianna">Marianna Apidianaki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Callison=Burch:Chris">Chris Callison-Burch</a></p>
<p>Abstract:
Bilingual Lexicon Induction is the task of learning word translations without bilingual parallel corpora. We model this task as a matrix completion problem, and present an effective and extendable framework for completing the matrix. This method harnesses diverse bilingual and monolingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages.</p>
<p>Keywords:</p>
<h3 id="153. Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback.">153. Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1153/">Paper Link</a>    Pages:1464-1474</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Khanh">Khanh Nguyen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Daum=eacute=_III:Hal">Hal Daum III</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Boyd=Graber:Jordan_L=">Jordan L. Boyd-Graber</a></p>
<p>Abstract:
Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.</p>
<p>Keywords:</p>
<h3 id="154. Towards Compact and Fast Neural Machine Translation Using a Combined Method.">154. Towards Compact and Fast Neural Machine Translation Using a Combined Method.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1154/">Paper Link</a>    Pages:1475-1481</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xiaowei">Xiaowei Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0048:Wei">Wei Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0023:Feng">Feng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Shuang">Shuang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0002:Bo">Bo Xu</a></p>
<p>Abstract:
Neural Machine Translation (NMT) lays intensive burden on computation and memory cost. It is a challenge to deploy NMT models on the devices with limited computation and memory budgets. This paper presents a four stage pipeline to compress model and speed up the decoding for NMT. Our method first introduces a compact architecture based on convolutional encoder and weight shared embeddings. Then weight pruning is applied to obtain a sparse model. Next, we propose a fast sequence interpolation approach which enables the greedy decoding to achieve performance on par with the beam search. Hence, the time-consuming beam search can be replaced by simple greedy decoding. Finally, vocabulary selection is used to reduce the computation of softmax layer. Our final model achieves 10 times speedup, 17 times parameters reduction, less than 35MB storage size and comparable performance compared to the baseline model.</p>
<p>Keywords:</p>
<h3 id="155. Instance Weighting for Neural Machine Translation Domain Adaptation.">155. Instance Weighting for Neural Machine Translation Domain Adaptation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1155/">Paper Link</a>    Pages:1482-1488</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0015:Rui">Rui Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Utiyama:Masao">Masao Utiyama</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Lemao">Lemao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Kehai">Kehai Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sumita:Eiichiro">Eiichiro Sumita</a></p>
<p>Abstract:
Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points.</p>
<p>Keywords:</p>
<h3 id="156. Regularization techniques for fine-tuning in neural machine translation.">156. Regularization techniques for fine-tuning in neural machine translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1156/">Paper Link</a>    Pages:1489-1494</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Barone:Antonio_Valerio_Miceli">Antonio Valerio Miceli Barone</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haddow:Barry">Barry Haddow</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Germann:Ulrich">Ulrich Germann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sennrich:Rico">Rico Sennrich</a></p>
<p>Abstract:
We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for EnglishGerman and EnglishRussian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.</p>
<p>Keywords:</p>
<h3 id="157. Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical Comparison of Two Phrase-Based Decoding Algorithms.">157. Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical Comparison of Two Phrase-Based Decoding Algorithms.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1157/">Paper Link</a>    Pages:1495-1499</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Yin=Wen">Yin-Wen Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Collins_0001:Michael">Michael Collins</a></p>
<p>Abstract:
This paper describes an empirical study of the phrase-based decoding algorithm proposed by Chang and Collins (2017). The algorithm produces a translation by processing the source-language sentence in strictly left-to-right order, differing from commonly used approaches that build the target-language sentence in left-to-right order. Our results show that the new algorithm is competitive with Moses (Koehn et al., 2007) in terms of both speed and BLEU scores.</p>
<p>Keywords:</p>
<h3 id="158. Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning.">158. Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1158/">Paper Link</a>    Pages:1500-1505</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Domhan:Tobias">Tobias Domhan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hieber:Felix">Felix Hieber</a></p>
<p>Abstract:
The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language modeling and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.</p>
<p>Keywords:</p>
<h3 id="159. Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling.">159. Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1159/">Paper Link</a>    Pages:1506-1515</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Marcheggiani:Diego">Diego Marcheggiani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Titov:Ivan">Ivan Titov</a></p>
<p>Abstract:
Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.</p>
<p>Keywords:</p>
<h3 id="160. Neural Semantic Parsing with Type Constraints for Semi-Structured Tables.">160. Neural Semantic Parsing with Type Constraints for Semi-Structured Tables.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1160/">Paper Link</a>    Pages:1516-1526</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Krishnamurthy:Jayant">Jayant Krishnamurthy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dasigi:Pradeep">Pradeep Dasigi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gardner_0001:Matt">Matt Gardner</a></p>
<p>Abstract:
We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations: (1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables. We also introduce a novel method for training our neural model with question-answer supervision. On the WikiTableQuestions data set, our parser achieves a state-of-the-art accuracy of 43.3% for a single model and 45.9% for a 5-model ensemble, improving on the best prior score of 38.7% set by a 15-model ensemble. These results suggest that type constraints and entity linking are valuable components to incorporate in neural semantic parsers.</p>
<p>Keywords:</p>
<h3 id="161. Joint Concept Learning and Semantic Parsing from Natural Language Explanations.">161. Joint Concept Learning and Semantic Parsing from Natural Language Explanations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1161/">Paper Link</a>    Pages:1527-1536</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Srivastava:Shashank">Shashank Srivastava</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Labutov:Igor">Igor Labutov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitchell:Tom_M=">Tom M. Mitchell</a></p>
<p>Abstract:
Natural language constitutes a predominant medium for much of human learning and pedagogy. We consider the problem of concept learning from natural language explanations, and a small number of labeled examples of the concept. For example, in learning the concept of a phishing email, one might say this is a phishing email because it asks for your bank account number. Solving this problem involves both learning to interpret open ended natural language statements, and learning the concept itself. We present a joint model for (1) language interpretation (semantic parsing) and (2) concept learning (classification) that does not require labeling statements with logical forms. Instead, the model prefers discriminative interpretations of statements in context of observable features of the data as a weak signal for parsing. On a dataset of email-related concepts, our approach yields across-the-board improvements in classification performance, with a 30% relative improvement in F1 score over competitive methods in the low data regime.</p>
<p>Keywords:</p>
<h3 id="162. Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection.">162. Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1162/">Paper Link</a>    Pages:1537-1546</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rei:Marek">Marek Rei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bulat:Luana">Luana Bulat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kiela:Douwe">Douwe Kiela</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shutova:Ekaterina">Ekaterina Shutova</a></p>
<p>Abstract:
The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on hand-engineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.</p>
<p>Keywords:</p>
<h3 id="163. Identifying civilians killed by police with distantly supervised entity-event extraction.">163. Identifying civilians killed by police with distantly supervised entity-event extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1163/">Paper Link</a>    Pages:1547-1557</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Keith:Katherine_A=">Katherine A. Keith</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Handler:Abram">Abram Handler</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pinkham:Michael">Michael Pinkham</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Magliozzi:Cara">Cara Magliozzi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McDuffie:Joshua">Joshua McDuffie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/O=Connor:Brendan">Brendan O&apos;Connor</a></p>
<p>Abstract:
We propose a new, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases.</p>
<p>Keywords:</p>
<h3 id="164. Asking too much? The rhetorical role of questions in political discourse.">164. Asking too much? The rhetorical role of questions in political discourse.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1164/">Paper Link</a>    Pages:1558-1572</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Justine">Justine Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Spirling:Arthur">Arthur Spirling</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Danescu=Niculescu=Mizil:Cristian">Cristian Danescu-Niculescu-Mizil</a></p>
<p>Abstract:
Questions play a prominent role in social interactions, performing rhetorical functions that go beyond that of simple informational exchange. The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor. While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied. In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role. By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discoursesuch as the bifurcation in questioning behavior between government and opposition partiesand reveals new insights into the effects of a legislators tenure and political career ambitions.</p>
<p>Keywords:</p>
<h3 id="165. Detecting Perspectives in Political Debates.">165. Detecting Perspectives in Political Debates.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1165/">Paper Link</a>    Pages:1573-1582</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vilares:David">David Vilares</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Yulan">Yulan He</a></p>
<p>Abstract:
We explore how to detect peoples perspectives that occupy a certain proposition. We propose a Bayesian modelling approach where topics (or propositions) and their associated perspectives (or viewpoints) are modeled as latent variables. Words associated with topics or perspectives follow different generative routes. Based on the extracted perspectives, we can extract the top associated sentences from text to generate a succinct summary which allows a quick glimpse of the main viewpoints in a document. The model is evaluated on debates from the House of Commons of the UK Parliament, revealing perspectives from the debates without the use of labelled data and obtaining better results than previous related solutions under a variety of evaluations.</p>
<p>Keywords:</p>
<h3 id="166. "i have a feeling trump will win..................": Forecasting Winners and Losers from User Predictions on Twitter.">166. "i have a feeling trump will win..................": Forecasting Winners and Losers from User Predictions on Twitter.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1166/">Paper Link</a>    Pages:1583-1592</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Swamy:Sandesh">Sandesh Swamy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ritter:Alan">Alan Ritter</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marneffe:Marie=Catherine_de">Marie-Catherine de Marneffe</a></p>
<p>Abstract:
Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome: Leonardo DiCaprio will win Best Actor vs. Leonardo DiCaprio may win or No way Leonardo wins!. Can popular beliefs on social media predict who will win? To answer this question, we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision. We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts predictions and retrospectively identify surprise outcomes.</p>
<p>Keywords:</p>
<h3 id="167. A Question Answering Approach for Emotion Cause Extraction.">167. A Question Answering Approach for Emotion Cause Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1167/">Paper Link</a>    Pages:1593-1602</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gui_0003:Lin">Lin Gui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Jiannan">Jiannan Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Yulan">Yulan He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Ruifeng">Ruifeng Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0001:Qin">Qin Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Du:Jiachen">Jiachen Du</a></p>
<p>Abstract:
Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.</p>
<p>Keywords:</p>
<h3 id="168. Story Comprehension for Predicting What Happens Next.">168. Story Comprehension for Predicting What Happens Next.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1168/">Paper Link</a>    Pages:1603-1614</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chaturvedi:Snigdha">Snigdha Chaturvedi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Haoruo">Haoruo Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>Abstract:
Automatic story comprehension is a fundamental challenge in Natural Language Understanding, and can enable computers to learn about social norms, human behavior and commonsense. In this paper, we present a story comprehension model that explores three distinct semantic aspects: (i) the sequence of events described in the story, (ii) its emotional trajectory, and (iii) its plot consistency. We judge the models understanding of real-world stories by inquiring if, like humans, it can develop an expectation of what will happen next in a given story. Specifically, we use it to predict the correct ending of a given short story from possible alternatives. The model uses a hidden variable to weigh the semantic aspects in the context of the story. Our experiments demonstrate the potential of our approach to characterize these semantic aspects, and the strength of the hidden variable based approach. The model outperforms the state-of-the-art approaches and achieves best results on a publicly available dataset.</p>
<p>Keywords:</p>
<h3 id="169. Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm.">169. Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1169/">Paper Link</a>    Pages:1615-1625</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Felbo:Bjarke">Bjarke Felbo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mislove:Alan">Alan Mislove</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/S=oslash=gaard:Anders">Anders Sgaard</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rahwan:Iyad">Iyad Rahwan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lehmann:Sune">Sune Lehmann</a></p>
<p>Abstract:
NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within emotion, sentiment and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches.</p>
<p>Keywords:</p>
<h3 id="170. Opinion Recommendation Using A Neural Model.">170. Opinion Recommendation Using A Neural Model.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1170/">Paper Link</a>    Pages:1626-1637</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhongqing">Zhongqing Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a></p>
<p>Abstract:
We present opinion recommendation, a novel task of jointly generating a review with a rating score that a certain user would give to a certain product which is unreviewed by the user, given existing reviews to the product by other users, and the reviews that the user has given to other products. A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning. We use a single neural network to model users and products, generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on Yelp.com data compared with Yelps own ratings. our methods give better results compared to several pipelines baselines.</p>
<p>Keywords:</p>
<h3 id="171. CRF Autoencoder for Unsupervised Dependency Parsing.">171. CRF Autoencoder for Unsupervised Dependency Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1171/">Paper Link</a>    Pages:1638-1643</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cai:Jiong">Jiong Cai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Yong">Yong Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Kewei">Kewei Tu</a></p>
<p>Abstract:
Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.</p>
<p>Keywords:</p>
<h3 id="172. Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence.">172. Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1172/">Paper Link</a>    Pages:1644-1654</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Corro:Caio">Caio Corro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roux:Joseph_Le">Joseph Le Roux</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lacroix:Mathieu">Mathieu Lacroix</a></p>
<p>Abstract:
We present a new method for the joint task of tagging and non-projective dependency parsing. We demonstrate its usefulness with an application to discontinuous phrase-structure parsing where decoding lexicalized spines and syntactic derivations is performed jointly. The main contributions of this paper are (1) a reduction from joint tagging and non-projective dependency parsing to the Generalized Maximum Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem through Lagrangian relaxation. We evaluate this model and obtain state-of-the-art results despite strong independence assumptions.</p>
<p>Keywords:</p>
<h3 id="173. Incremental Graph-based Neural Dependency Parsing.">173. Incremental Graph-based Neural Dependency Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1173/">Paper Link</a>    Pages:1655-1665</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zheng:Xiaoqing">Xiaoqing Zheng</a></p>
<p>Abstract:
Very recently, some studies on neural dependency parsers have shown advantage over the traditional ones on a wide variety of languages. However, for graph-based neural dependency parsing systems, they either count on the long-term memory and attention mechanism to implicitly capture the high-order features or give up the global exhaustive inference algorithms in order to harness the features over a rich history of parsing decisions. The former might miss out the important features for specific headword predictions without the help of the explicit structural information, and the latter may suffer from the error propagation as false early structural constraints are used to create features when making future predictions. We explore the feasibility of explicitly taking high-order features into account while remaining the main advantage of global inference and learning for graph-based parsing. The proposed parser first forms an initial parse tree by head-modifier predictions based on the first-order factorization. High-order features (such as grandparent, sibling, and uncle) then can be defined over the initial tree, and used to refine the parse tree in an iterative fashion. Experimental results showed that our model (called INDP) archived competitive performance to existing benchmark parsers on both English and Chinese datasets.</p>
<p>Keywords:</p>
<h3 id="174. Neural Discontinuous Constituency Parsing.">174. Neural Discontinuous Constituency Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1174/">Paper Link</a>    Pages:1666-1676</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stanojevic:Milos">Milos Stanojevic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Alhama:Raquel_G=">Raquel G. Alhama</a></p>
<p>Abstract:
One of the most pressing issues in discontinuous constituency transition-based parsing is that the relevant information for parsing decisions could be located in any part of the stack or the buffer. In this paper, we propose a solution to this problem by replacing the structured perceptron model with a recursive neural model that computes a global representation of the configuration, therefore allowing even the most remote parts of the configuration to influence the parsing decisions. We also provide a detailed analysis of how this representation should be built out of sub-representations of its core elements (words, trees and stack). Additionally, we investigate how different types of swap oracles influence the results. Our model is the first neural discontinuous constituency parser, and it outperforms all the previously published models on three out of four datasets while on the fourth it obtains second place by a tiny difference.</p>
<p>Keywords:</p>
<h3 id="175. Stack-based Multi-layer Attention for Transition-based Dependency Parsing.">175. Stack-based Multi-layer Attention for Transition-based Dependency Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1175/">Paper Link</a>    Pages:1677-1682</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Zhirui">Zhirui Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Shujie">Shujie Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Mu">Mu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Enhong">Enhong Chen</a></p>
<p>Abstract:
Although sequence-to-sequence (seq2seq) network has achieved significant success in many NLP tasks such as machine translation and text summarization, simply applying this approach to transition-based dependency parsing cannot yield a comparable performance gain as in other state-of-the-art methods, such as stack-LSTM and head selection. In this paper, we propose a stack-based multi-layer attention model for seq2seq learning to better leverage structural linguistics information. In our method, two binary vectors are used to track the decoding stack in transition-based parsing, and multi-layer attention is introduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model.</p>
<p>Keywords:</p>
<h3 id="176. Dependency Grammar Induction with Neural Lexicalization and Big Training Data.">176. Dependency Grammar Induction with Neural Lexicalization and Big Training Data.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1176/">Paper Link</a>    Pages:1683-1688</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Han:Wenjuan">Wenjuan Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Yong">Yong Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Kewei">Kewei Tu</a></p>
<p>Abstract:
We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.</p>
<p>Keywords:</p>
<h3 id="177. Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition.">177. Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1177/">Paper Link</a>    Pages:1689-1694</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Yong">Yong Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Wenjuan">Wenjuan Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Kewei">Kewei Tu</a></p>
<p>Abstract:
Unsupervised dependency parsing aims to learn a dependency parser from unannotated sentences. Existing work focuses on either learning generative models using the expectation-maximization algorithm and its variants, or learning discriminative models using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-of-the-art performance on thirty languages.</p>
<p>Keywords:</p>
<h3 id="178. Effective Inference for Generative Neural Parsing.">178. Effective Inference for Generative Neural Parsing.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1178/">Paper Link</a>    Pages:1695-1700</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stern:Mitchell">Mitchell Stern</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fried:Daniel">Daniel Fried</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klein:Dan">Dan Klein</a></p>
<p>Abstract:
Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.</p>
<p>Keywords:</p>
<h3 id="179. Semi-supervised Structured Prediction with Neural CRF Autoencoder.">179. Semi-supervised Structured Prediction with Neural CRF Autoencoder.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1179/">Paper Link</a>    Pages:1701-1711</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xiao">Xiao Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Yong">Yong Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Hao">Hao Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Kewei">Kewei Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goldwasser:Dan">Dan Goldwasser</a></p>
<p>Abstract:
In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of sequential structured prediction problems. Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep neural networks, and a decoder which is a generative model trying to reconstruct the input. Our model has a unified structure with different loss functions for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the encoder and the decoder simultaneously by decoupling their parameters. Our Experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that our model can outperform competitive systems in both supervised and semi-supervised scenarios.</p>
<p>Keywords:</p>
<h3 id="180. TAG Parsing with Neural Networks and Vector Representations of Supertags.">180. TAG Parsing with Neural Networks and Vector Representations of Supertags.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1180/">Paper Link</a>    Pages:1712-1722</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kasai:Jungo">Jungo Kasai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Frank:Bob">Bob Frank</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McCoy:R=_Thomas">R. Thomas McCoy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rambow:Owen">Owen Rambow</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nasr:Alexis">Alexis Nasr</a></p>
<p>Abstract:
We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (elementary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores. The shift-reduce parsing model eschews lexical information entirely, and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is almost parsing. We demonstrate that the embedding vector representations the parser induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in distributional semantics. This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that TAG is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences.</p>
<p>Keywords:</p>
<h3 id="181. Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification.">181. Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1181/">Paper Link</a>    Pages:1723-1729</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Adel:Heike">Heike Adel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schtze</a></p>
<p>Abstract:
We introduce globally normalized convolutional neural networks for joint entity classification and relation extraction. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset.</p>
<p>Keywords:</p>
<h3 id="182. End-to-End Neural Relation Extraction with Global Optimization.">182. End-to-End Neural Relation Extraction with Global Optimization.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1182/">Paper Link</a>    Pages:1730-1740</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Meishan">Meishan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Guohong">Guohong Fu</a></p>
<p>Abstract:
Neural networks have shown promising results for relation extraction. State-of-the-art models cast the task as an end-to-end problem, solved incrementally using a local classifier. Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks.</p>
<p>Keywords:</p>
<h3 id="183. KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs.">183. KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1183/">Paper Link</a>    Pages:1741-1750</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/o/Ojha:Prakhar">Prakhar Ojha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_P=">Partha P. Talukdar</a></p>
<p>Abstract:
Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention recently. Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity. This important problem has largely been ignored in prior research  we fill this gap and propose KGEval. KGEval uses coupling constraints to bind facts and crowdsources those few that can infer large parts of the graph. We demonstrate that the objective optimized by KGEval is submodular and NP-hard, allowing guarantees for our approximation algorithm. Through experiments on real-world datasets, we demonstrate that KGEval best estimates KG accuracy compared to other baselines, while requiring significantly lesser number of human evaluations.</p>
<p>Keywords:</p>
<h3 id="184. Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short.">184. Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1184/">Paper Link</a>    Pages:1751-1756</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pujara:Jay">Jay Pujara</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Augustine:Eriq">Eriq Augustine</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Getoor:Lise">Lise Getoor</a></p>
<p>Abstract:
Knowledge graph (KG) embedding techniques use structured relationships between entities to learn low-dimensional representations of entities and relations. One prominent goal of these approaches is to improve the quality of knowledge graphs by removing errors and adding missing facts. Surprisingly, most embedding techniques have been evaluated on benchmark datasets consisting of dense and reliable subsets of human-curated KGs, which tend to be fairly complete and have few errors. In this paper, we consider the problem of applying embedding techniques to KGs extracted from text, which are often incomplete and contain errors. We compare the sparsity and unreliability of different KGs and perform empirical experiments demonstrating how embedding approaches degrade as sparsity and unreliability increase.</p>
<p>Keywords:</p>
<h3 id="185. Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations.">185. Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1185/">Paper Link</a>    Pages:1757-1767</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Glavas:Goran">Goran Glavas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Ponzetto:Simone_Paolo">Simone Paolo Ponzetto</a></p>
<p>Abstract:
Detection of lexico-semantic relations is one of the central tasks of computational semantics. Although some fundamental relations (e.g., hypernymy) are asymmetric, most existing models account for asymmetry only implicitly and use the same concept representations to support detection of symmetric and asymmetric relations alike. In this work, we propose the Dual Tensor model, a neural architecture with which we explicitly model the asymmetry and capture the translation between unspecialized and specialized word embeddings via a pair of tensors. Although our Dual Tensor model needs only unspecialized embeddings as input, our experiments on hypernymy and meronymy detection suggest that it can outperform more complex and resource-intensive models. We further demonstrate that the model can account for polysemy and that it exhibits stable performance across languages.</p>
<p>Keywords:</p>
<h3 id="186. Incorporating Relation Paths in Neural Relation Extraction.">186. Incorporating Relation Paths in Neural Relation Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1186/">Paper Link</a>    Pages:1768-1777</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Wenyuan">Wenyuan Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Yankai">Yankai Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a></p>
<p>Abstract:
Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which also provide rich useful information but not yet employed by relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with strong baselines. The source code of this paper can be obtained from <a href="https://github.com/thunlp/PathNRE">https://github.com/thunlp/PathNRE</a>.</p>
<p>Keywords:</p>
<h3 id="187. Adversarial Training for Relation Extraction.">187. Adversarial Training for Relation Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1187/">Paper Link</a>    Pages:1778-1783</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Yi">Yi Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bamman:David">David Bamman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Russell:Stuart_J=">Stuart J. Russell</a></p>
<p>Abstract:
Adversarial training is a mean of regularizing classification algorithms by generating adversarial noise to the training data. We apply adversarial training in relation extraction within the multi-instance multi-label learning framework. We evaluate various neural network architectures on two different datasets. Experimental results demonstrate that adversarial training is generally effective for both CNN and RNN models and significantly improves the precision of predicted relations.</p>
<p>Keywords:</p>
<h3 id="188. Context-Aware Representations for Knowledge Base Relation Extraction.">188. Context-Aware Representations for Knowledge Base Relation Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1188/">Paper Link</a>    Pages:1784-1789</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sorokin:Daniil">Daniil Sorokin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. We combine the context representations with an attention mechanism to make the final prediction. We use the Wikidata knowledge base to construct a dataset of multiple relations per sentence and to evaluate our approach. Compared to a baseline system, our method results in an average error reduction of 24 on a held-out set of relations. The code and the dataset to replicate the experiments are made available at <a href="https://github.com/ukplab/">https://github.com/ukplab/</a>.</p>
<p>Keywords:</p>
<h3 id="189. A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction.">189. A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1189/">Paper Link</a>    Pages:1790-1795</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tianyu">Tianyu Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Kexiang">Kexiang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Baobao">Baobao Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sui:Zhifang">Zhifang Sui</a></p>
<p>Abstract:
Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with knowledge bases. Previous sentence level denoise models dont achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems.</p>
<p>Keywords:</p>
<h3 id="190. A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events.">190. A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1190/">Paper Link</a>    Pages:1796-1802</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Choubey:Prafulla_Kumar">Prafulla Kumar Choubey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Ruihong">Ruihong Huang</a></p>
<p>Abstract:
We present a sequential model for temporal relation classification between intra-sentence events. The key observation is that the overall syntactic structure and compositional meanings of the multi-word context between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The neural nets learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.</p>
<p>Keywords:</p>
<h3 id="191. Deep Residual Learning for Weakly-Supervised Relation Extraction.">191. Deep Residual Learning for Weakly-Supervised Relation Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1191/">Paper Link</a>    Pages:1803-1807</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Yi_Yao">Yi Yao Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>Abstract:
Deep residual learning (ResNet) is a new method for training very deep neural networks using identity mapping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.</p>
<p>Keywords:</p>
<h3 id="192. Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective.">192. Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1192/">Paper Link</a>    Pages:1808-1813</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Qing">Qing Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a></p>
<p>Abstract:
For the task of relation extraction, distant supervision is an efficient approach to generate labeled data by aligning knowledge base with free texts. The essence of it is a challenging incomplete multi-label classification problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top precision improvements over the traditional state-of-the-art approaches.</p>
<p>Keywords:</p>
<h3 id="193. Exploring Vector Spaces for Semantic Relations.">193. Exploring Vector Spaces for Semantic Relations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1193/">Paper Link</a>    Pages:1814-1823</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/G=aacute=bor:Kata">Kata Gbor</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zargayouna:Ha=iuml=fa">Hafa Zargayouna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tellier:Isabelle">Isabelle Tellier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Buscaldi:Davide">Davide Buscaldi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Charnois:Thierry">Thierry Charnois</a></p>
<p>Abstract:
Word embeddings are used with success for a variety of tasks involving lexical semantic similarities between individual words. Using unsupervised methods and just cosine similarity, encouraging results were obtained for analogical similarities. In this paper, we explore the potential of pre-trained word embeddings to identify generic types of semantic relations in an unsupervised experiment. We propose a new relational similarity measure based on the combination of word2vecs CBOW input and output vectors which outperforms concurrent vector representations, when used for unsupervised clustering on SemEval 2010 Relation Classification data.</p>
<p>Keywords:</p>
<h3 id="194. Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants.">194. Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1194/">Paper Link</a>    Pages:1824-1829</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kutuzov:Andrey">Andrey Kutuzov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Velldal:Erik">Erik Velldal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/=/=Oslash=vrelid:Lilja">Lilja vrelid</a></p>
<p>Abstract:
This paper deals with using word embedding models to trace the temporal dynamics of semantic relations between pairs of words. The set-up is similar to the well-known analogies task, but expanded with a time dimension. To this end, we apply incremental updating of the models with new training texts, including incremental vocabulary expansion, coupled with learned transformation matrices that let us map between members of the relation. The proposed approach is evaluated on the task of predicting insurgent armed groups based on geographical locations. The gold standard data for the time span 19942010 is extracted from the UCDP Armed Conflicts dataset. The results show that the method is feasible and outperforms the baselines, but also that important work still remains to be done.</p>
<p>Keywords:</p>
<h3 id="195. Dynamic Entity Representations in Neural Language Models.">195. Dynamic Entity Representations in Neural Language Models.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1195/">Paper Link</a>    Pages:1830-1839</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Yangfeng">Yangfeng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Chenhao">Chenhao Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Martschat:Sebastian">Sebastian Martschat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a></p>
<p>Abstract:
Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.</p>
<p>Keywords:</p>
<h3 id="196. Towards Quantum Language Models.">196. Towards Quantum Language Models.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1196/">Paper Link</a>    Pages:1840-1849</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Basile:Ivano">Ivano Basile</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tamburini:Fabio">Fabio Tamburini</a></p>
<p>Abstract:
This paper presents a new approach for building Language Models using the Quantum Probability Theory, a Quantum Language Model (QLM). It mainly shows that relying on this probability calculus it is possible to build stochastic models able to benefit from quantum correlations due to interference and entanglement. We extensively tested our approach showing its superior performances, both in terms of model perplexity and inserting it into an automatic speech recognition evaluation setting, when compared with state-of-the-art language modelling techniques.</p>
<p>Keywords:</p>
<h3 id="197. Reference-Aware Language Models.">197. Reference-Aware Language Models.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1197/">Paper Link</a>    Pages:1850-1859</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Zichao">Zichao Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blunsom:Phil">Phil Blunsom</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dyer:Chris">Chris Dyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Ling:Wang">Wang Ling</a></p>
<p>Abstract:
We propose a general class of language models that treat reference as discrete stochastic latent variables. This decision allows for the creation of entity mentions by accessing external databases of referents (required by, e.g., dialogue generation) or past internal state (required to explicitly model coreferentiality). Beyond simple copying, our coreference model can additionally refer to a referent using varied mention forms (e.g., a reference to Jane can be realized as she), a characteristic feature of reference in natural languages. Experiments on three representative applications show our model variants outperform models based on deterministic attention and standard language modeling baselines.</p>
<p>Keywords:</p>
<h3 id="198. A Simple Language Model based on PMI Matrix Approximations.">198. A Simple Language Model based on PMI Matrix Approximations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1198/">Paper Link</a>    Pages:1860-1865</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Melamud:Oren">Oren Melamud</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dagan:Ido">Ido Dagan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goldberger:Jacob">Jacob Goldberger</a></p>
<p>Abstract:
In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vecs algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.</p>
<p>Keywords:</p>
<h3 id="199. Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones.">199. Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1199/">Paper Link</a>    Pages:1866-1872</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Assylbekov:Zhenisbek">Zhenisbek Assylbekov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Takhanov:Rustem">Rustem Takhanov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Myrzakhmetov:Bagdat">Bagdat Myrzakhmetov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Washington:Jonathan">Jonathan Washington</a></p>
<p>Abstract:
Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.</p>
<p>Keywords:</p>
<h3 id="200. Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels.">200. Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1200/">Paper Link</a>    Pages:1873-1883</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Frermann:Lea">Lea Frermann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Szarvas:Gy=ouml=rgy">Gyrgy Szarvas</a></p>
<p>Abstract:
Automatically understanding the plot of novels is important both for informing literary scholarship and applications such as summarization or recommendation. Various models have addressed this task, but their evaluation has remained largely intrinsic and qualitative. Here, we propose a principled and scalable framework leveraging expert-provided semantic tags (e.g., mystery, pirates) to evaluate plot representations in an extrinsic fashion, assessing their ability to produce locally coherent groupings of novels (micro-clusters) in model space. We present a deep recurrent autoencoder model that learns richly structured multi-view plot representations, and show that they i) yield better micro-clusters than less structured representations; and ii) are interpretable, and thus useful for further literary analysis or labeling of the emerging micro-clusters.</p>
<p>Keywords:</p>
<h3 id="201. Initializing Convolutional Filters with Semantic Features for Text Classification.">201. Initializing Convolutional Filters with Semantic Features for Text Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1201/">Paper Link</a>    Pages:1884-1889</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Shen">Shen Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Zhe">Zhe Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Tao">Tao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Renfen">Renfen Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Du:Xiaoyong">Xiaoyong Du</a></p>
<p>Abstract:
Convolutional Neural Networks (CNNs) are widely used in NLP tasks. This paper presents a novel weight initialization method to improve the CNNs for text classification. Instead of randomly initializing the convolutional filters, we encode semantic features into them, which helps the model focus on learning useful features at the beginning of the training. Experiments demonstrate the effectiveness of the initialization technique on seven text classification tasks, including sentiment analysis and topic classification.</p>
<p>Keywords:</p>
<h3 id="202. Shortest-Path Graph Kernels for Document Similarity.">202. Shortest-Path Graph Kernels for Document Similarity.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1202/">Paper Link</a>    Pages:1890-1900</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nikolentzos:Giannis">Giannis Nikolentzos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meladianos:Polykarpos">Polykarpos Meladianos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rousseau_0001:Fran=ccedil=ois">Franois Rousseau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stavrakas:Yannis">Yannis Stavrakas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vazirgiannis:Michalis">Michalis Vazirgiannis</a></p>
<p>Abstract:
In this paper, we present a novel document similarity measure based on the definition of a graph kernel between pairs of documents. The proposed measure takes into account both the terms contained in the documents and the relationships between them. By representing each document as a graph-of-words, we are able to model these relationships and then determine how similar two documents are by using a modified shortest-path graph kernel. We evaluate our approach on two tasks and compare it against several baseline approaches using various performance metrics such as DET curves and macro-average F1-score. Experimental results on a range of datasets showed that our proposed approach outperforms traditional techniques and is capable of measuring more accurately the similarity between two documents.</p>
<p>Keywords:</p>
<h3 id="203. Adapting Topic Models using Lexical Associations with Tree Priors.">203. Adapting Topic Models using Lexical Associations with Tree Priors.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1203/">Paper Link</a>    Pages:1901-1906</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Weiwei">Weiwei Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Boyd=Graber:Jordan_L=">Jordan L. Boyd-Graber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Resnik:Philip">Philip Resnik</a></p>
<p>Abstract:
Models work best when they are optimized taking into account the evaluation criteria that people care about. For topic models, people often care about interpretability, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting extrinsic performance.</p>
<p>Keywords:</p>
<h3 id="204. Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation for Crowdsourced Data.">204. Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation for Crowdsourced Data.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1204/">Paper Link</a>    Pages:1907-1912</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Parde:Natalie">Natalie Parde</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nielsen:Rodney_D=">Rodney D. Nielsen</a></p>
<p>Abstract:
Crowdsourcing offers a convenient means of obtaining labeled data quickly and inexpensively. However, crowdsourced labels are often noisier than expert-annotated data, making it difficult to aggregate them meaningfully. We present an aggregation approach that learns a regression model from crowdsourced annotations to predict aggregated labels for instances that have no expert adjudications. The predicted labels achieve a correlation of 0.594 with expert labels on our data, outperforming the best alternative aggregation method by 11.9%. Our approach also outperforms the alternatives on third-party datasets.</p>
<p>Keywords:</p>
<h3 id="205. CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles.">205. CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1205/">Paper Link</a>    Pages:1913-1922</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Chenguang">Chenguang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Akbik:Alan">Alan Akbik</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chiticariu:Laura">Laura Chiticariu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0001:Yunyao">Yunyao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xia:Fei">Fei Xia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Anbang">Anbang Xu</a></p>
<p>Abstract:
Crowdsourcing has proven to be an effective method for generating labeled data for a range of NLP tasks. However, multiple recent attempts of using crowdsourcing to generate gold-labeled training data for semantic role labeling (SRL) reported only modest results, indicating that SRL is perhaps too difficult a task to be effectively crowdsourced. In this paper, we postulate that while producing SRL annotation does require expert involvement in general, a large subset of SRL labeling tasks is in fact appropriate for the crowd. We present a novel workflow in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality.</p>
<p>Keywords:</p>
<h3 id="206. Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon Induction.">206. Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon Induction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1207/">Paper Link</a>    Pages:1934-1945</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Meng">Meng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0005:Yang">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luan:Huanbo">Huanbo Luan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a></p>
<p>Abstract:
Cross-lingual natural language processing hinges on the premise that there exists invariance across languages. At the word level, researchers have identified such invariance in the word embedding semantic spaces of different languages. However, in order to connect the separate spaces, cross-lingual supervision encoded in parallel data is typically required. In this paper, we attempt to establish the cross-lingual connection without relying on any cross-lingual supervision. By viewing word embedding spaces as distributions, we propose to minimize their earth movers distance, a measure of divergence between distributions. We demonstrate the success on the unsupervised bilingual lexicon induction task. In addition, we reveal an interesting finding that the earth movers distance shows potential as a measure of language difference.</p>
<p>Keywords:</p>
<h3 id="207. Unfolding and Shrinking Neural Machine Translation Ensembles.">207. Unfolding and Shrinking Neural Machine Translation Ensembles.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1208/">Paper Link</a>    Pages:1946-1956</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stahlberg:Felix">Felix Stahlberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Byrne:Bill">Bill Byrne</a></p>
<p>Abstract:
Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.</p>
<p>Keywords:</p>
<h3 id="208. Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.">208. Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1209/">Paper Link</a>    Pages:1957-1967</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bastings:Joost">Joost Bastings</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Titov:Ivan">Ivan Titov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aziz:Wilker">Wilker Aziz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marcheggiani:Diego">Diego Marcheggiani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sima=an:Khalil">Khalil Sima&apos;an</a></p>
<p>Abstract:
We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.</p>
<p>Keywords:</p>
<h3 id="209. Trainable Greedy Decoding for Neural Machine Translation.">209. Trainable Greedy Decoding for Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1210/">Paper Link</a>    Pages:1968-1978</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Jiatao">Jiatao Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Victor_O=_K=">Victor O. K. Li</a></p>
<p>Abstract:
Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.</p>
<p>Keywords:</p>
<h3 id="210. Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features.">210. Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1211/">Paper Link</a>    Pages:1979-1989</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Fan">Fan Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mukherjee:Arjun">Arjun Mukherjee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dragut:Eduard_Constantin">Eduard Constantin Dragut</a></p>
<p>Abstract:
Satirical news is considered to be entertainment, but it is potentially deceptive and harmful. Despite the embedded genre in the article, not everyone can recognize the satirical cues and therefore believe the news as true news. We observe that satirical cues are often reflected in certain paragraphs rather than the whole document. Existing works only consider document-level features to detect the satire, which could be limited. We consider paragraph-level linguistic features to unveil the satire by incorporating neural network and attention mechanism. We investigate the difference between paragraph-level features and document-level features, and analyze them on a large satirical news dataset. The evaluation shows that the proposed model detects satirical news effectively and reveals what features are important at which level.</p>
<p>Keywords:</p>
<h3 id="211. Fine Grained Citation Span for References in Wikipedia.">211. Fine Grained Citation Span for References in Wikipedia.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1212/">Paper Link</a>    Pages:1990-1999</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fetahu:Besnik">Besnik Fetahu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Markert:Katja">Katja Markert</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Anand:Avishek">Avishek Anand</a></p>
<p>Abstract:
Verifiability is one of the core editing principles in Wikipedia, where editors are encouraged to provide citations for the added content. For a Wikipedia article determining what content is covered by a citation or the citation span is not trivial, an important aspect for automated citation finding for uncovered content, or fact assessments. We address the problem of determining the citation span in Wikipedia articles. We approach this problem by classifying which textual fragments in an article are covered or hold true given a citation. We propose a sequence classification approach where for a paragraph and a citation, we determine the citation span at a fine-grained level. We provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain, where we show improvement for all evaluation metrics.</p>
<p>Keywords:</p>
<h3 id="212. Identifying Semantic Edit Intentions from Revisions in Wikipedia.">212. Identifying Semantic Edit Intentions from Revisions in Wikipedia.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1213/">Paper Link</a>    Pages:2000-2010</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Diyi">Diyi Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Halfaker:Aaron">Aaron Halfaker</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kraut:Robert_E=">Robert E. Kraut</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>Abstract:
Most studies on human editing focus merely on syntactic revision operations, failing to capture the intentions behind revision changes, which are essential for facilitating the single and collaborative writing process. In this work, we develop in collaboration with Wikipedia editors a 13-category taxonomy of the semantic intention behind edits in Wikipedia articles. Using labeled article edits, we build a computational classifier of intentions that achieved a micro-averaged F1 score of 0.621. We use this model to investigate edit intention effectiveness: how different types of edits predict the retention of newcomers and changes in the quality of articles, two key concerns for Wikipedia today. Our analysis shows that the types of edits that users make in their first session predict their subsequent survival as Wikipedia editors, and articles in different stages need different types of edits.</p>
<p>Keywords:</p>
<h3 id="213. Accurate Supervised and Semi-Supervised Machine Reading for Long Documents.">213. Accurate Supervised and Semi-Supervised Machine Reading for Long Documents.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1214/">Paper Link</a>    Pages:2011-2020</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gur:Izzeddin">Izzeddin Gur</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hewlett:Daniel">Daniel Hewlett</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lacoste:Alexandre">Alexandre Lacoste</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jones:Llion">Llion Jones</a></p>
<p>Abstract:
We introduce a hierarchical architecture for machine reading capable of extracting precise information from long documents. The model divides the document into small, overlapping windows and encodes all windows in parallel with an RNN. It then attends over these window encodings, reducing them to a single encoding, which is decoded into an answer using a sequence decoder. This hierarchical approach allows the model to scale to longer documents without increasing the number of sequential steps. In a supervised setting, our model achieves state of the art accuracy of 76.8 on the WikiReading dataset. We also evaluate the model in a semi-supervised setting by downsampling the WikiReading training set to create increasingly smaller amounts of supervision, while leaving the full unlabeled document corpus to train a sequence autoencoder on document windows. We evaluate models that can reuse autoencoder states and outputs without fine-tuning their weights, allowing for more efficient training and inference.</p>
<p>Keywords:</p>
<h3 id="214. Adversarial Examples for Evaluating Reading Comprehension Systems.">214. Adversarial Examples for Evaluating Reading Comprehension Systems.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1215/">Paper Link</a>    Pages:2021-2031</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jia:Robin">Robin Jia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a></p>
<p>Abstract:
Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.</p>
<p>Keywords:</p>
<h3 id="215. Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension.">215. Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1216/">Paper Link</a>    Pages:2032-2043</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Hongyu">Hongyu Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xianpei">Xianpei Han</a></p>
<p>Abstract:
Reasoning with commonsense knowledge is critical for natural language understanding. Traditional methods for commonsense machine comprehension mostly only focus on one specific kind of knowledge, neglecting the fact that commonsense reasoning requires simultaneously considering different kinds of commonsense knowledge. In this paper, we propose a multi-knowledge reasoning method, which can exploit heterogeneous knowledge for commonsense machine comprehension. Specifically, we first mine different kinds of knowledge (including event narrative knowledge, entity semantic knowledge and sentiment coherent knowledge) and encode them as inference rules with costs. Then we propose a multi-knowledge reasoning model, which selects inference rules for a specific reasoning context using attention mechanism, and reasons by summarizing all valid inference rules. Experiments on RocStories show that our method outperforms traditional models significantly.</p>
<p>Keywords:</p>
<h3 id="216. Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension.">216. Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1217/">Paper Link</a>    Pages:2044-2054</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Yichun">Yichun Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yangqiu">Yangqiu Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0004:Ming">Ming Zhang</a></p>
<p>Abstract:
Document-level multi-aspect sentiment classification is an important task for customer relation management. In this paper, we model the task as a machine comprehension problem where pseudo question-answer pairs are constructed by a small number of aspect-related keywords and aspect ratings. A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions. We adopt a hierarchical architecture to represent both word level and sentence level information, and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism. Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines. We will release our code and data for the method replicability.</p>
<p>Keywords:</p>
<h3 id="217. What is the Essence of a Claim? Cross-Domain Claim Identification.">217. What is the Essence of a Claim? Cross-Domain Claim Identification.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1218/">Paper Link</a>    Pages:2055-2066</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Daxenberger:Johannes">Johannes Daxenberger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Eger:Steffen">Steffen Eger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Habernal:Ivan">Ivan Habernal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stab:Christian">Christian Stab</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.</p>
<p>Keywords:</p>
<h3 id="218. Identifying Where to Focus in Reading Comprehension for Neural Question Generation.">218. Identifying Where to Focus in Reading Comprehension for Neural Question Generation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1219/">Paper Link</a>    Pages:2067-2073</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Du:Xinya">Xinya Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cardie:Claire">Claire Cardie</a></p>
<p>Abstract:
A first step in the task of automatically generating questions for testing reading comprehension is to identify question-worthy sentences, i.e. sentences in a text passage that humans find it worthwhile to ask questions about. We propose a hierarchical neural sentence-level sequence tagging model for this task, which existing approaches to question generation have ignored. The approach is fully data-driven  with no sophisticated NLP pipelines or any hand-crafted rules/features  and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves state-of-the-art performance for paragraph-level question generation for reading comprehension.</p>
<p>Keywords:</p>
<h3 id="219. Break it Down for Me: A Study in Automated Lyric Annotation.">219. Break it Down for Me: A Study in Automated Lyric Annotation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1220/">Paper Link</a>    Pages:2074-2080</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sterckx:Lucas">Lucas Sterckx</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Naradowsky:Jason">Jason Naradowsky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Byrne:Bill">Bill Byrne</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Demeester:Thomas">Thomas Demeester</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Develder:Chris">Chris Develder</a></p>
<p>Abstract:
Comprehending lyrics, as found in songs and poems, can pose a challenge to human and machine readers alike. This motivates the need for systems that can understand the ambiguity and jargon found in such creative texts, and provide commentary to aid readers in reaching the correct interpretation. We introduce the task of automated lyric annotation (ALA). Like text simplification, a goal of ALA is to rephrase the original text in a more easily understandable manner. However, in ALA the system must often include additional information to clarify niche terminology and abstract concepts. To stimulate research on this task, we release a large collection of crowdsourced annotations for song lyrics. We analyze the performance of translation and retrieval models on this task, measuring performance with both automated and human evaluation. We find that each model captures a unique type of information important to the task.</p>
<p>Keywords:</p>
<h3 id="220. Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization.">220. Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1221/">Paper Link</a>    Pages:2081-2090</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Piji">Piji Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lam:Wai">Wai Lam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Weiwei">Weiwei Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Hang">Hang Li</a></p>
<p>Abstract:
When people recall and digest what they have read for writing summaries, the important content is more likely to attract their attention. Inspired by this observation, we propose a cascaded attention based unsupervised model to estimate the salience information from the text for compressive multi-document summarization. The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constraints on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="221. Deep Recurrent Generative Decoder for Abstractive Text Summarization.">221. Deep Recurrent Generative Decoder for Abstractive Text Summarization.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1222/">Paper Link</a>    Pages:2091-2100</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Piji">Piji Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lam:Wai">Wai Lam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zihao">Zihao Wang</a></p>
<p>Abstract:
We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="222. Extractive Summarization Using Multi-Task Learning with Document Classification.">222. Extractive Summarization Using Multi-Task Learning with Document Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1223/">Paper Link</a>    Pages:2101-2110</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/i/Isonuma:Masaru">Masaru Isonuma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fujino:Toru">Toru Fujino</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mori:Junichiro">Junichiro Mori</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Matsuo:Yutaka">Yutaka Matsuo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sakata:Ichiro">Ichiro Sakata</a></p>
<p>Abstract:
The need for automatic document summarization that can be used for practical applications is increasing rapidly. In this paper, we propose a general framework for summarization that extracts sentences from a document using externally related information. Our work is aimed at single document summarization using small amounts of reference summaries. In particular, we address document summarization in the framework of multi-task learning using curriculum learning for sentence extraction and document classification. The proposed framework enables us to obtain better feature representations to extract sentences from documents. We evaluate our proposed summarization method on two datasets: financial report and news corpus. Experimental results demonstrate that our summarizers achieve performance that is comparable to state-of-the-art systems.</p>
<p>Keywords:</p>
<h3 id="223. Towards Automatic Construction of News Overview Articles by News Synthesis.">223. Towards Automatic Construction of News Overview Articles by News Synthesis.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1224/">Paper Link</a>    Pages:2111-2116</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jianmin">Jianmin Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>Abstract:
In this paper we investigate a new task of automatically constructing an overview article from a given set of news articles about a news event. We propose a news synthesis approach to address this task based on passage segmentation, ranking, selection and merging. Our proposed approach is compared with several typical multi-document summarization methods on the Wikinews dataset, and achieves the best performance on both automatic evaluation and manual evaluation.</p>
<p>Keywords:</p>
<h3 id="224. Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank.">224. Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1225/">Paper Link</a>    Pages:2117-2123</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0003:Kai">Kai Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang_0001:Liang">Liang Huang</a></p>
<p>Abstract:
Discourse parsing has long been treated as a stand-alone problem independent from constituency or dependency parsing. Most attempts at this problem rely on annotated text segmentations (Elementary Discourse Units, EDUs) and sophisticated sparse or continuous features to extract syntactic information. In this paper we propose the first end-to-end discourse parser that jointly parses in both syntax and discourse levels, as well as the first syntacto-discourse treebank by integrating the Penn Treebank and the RST Treebank. Built upon our recent span-based constituency parser, this joint syntacto-discourse parser requires no preprocessing efforts such as segmentation or feature extraction, making discourse parsing more convenient. Empirically, our parser achieves the state-of-the-art end-to-end discourse parsing accuracy.</p>
<p>Keywords:</p>
<h3 id="225. Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events.">225. Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1226/">Paper Link</a>    Pages:2124-2133</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Choubey:Prafulla_Kumar">Prafulla Kumar Choubey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Ruihong">Ruihong Huang</a></p>
<p>Abstract:
We introduce a novel iterative approach for event coreference resolution that gradually builds event clusters by exploiting inter-dependencies among event mentions within the same chain as well as across event chains. Among event mentions in the same chain, we distinguish within- and cross-document event coreference links by using two distinct pairwise classifiers, trained separately to capture differences in feature distributions of within- and cross-document event clusters. Our event coreference approach alternates between WD and CD clustering and combines arguments from both event clusters after every merge, continuing till no more merge can be made. And then it performs further merging between event chains that are both closely related to a set of other chains of events. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods in joint task of WD and CD event coreference resolution.</p>
<p>Keywords:</p>
<h3 id="226. When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size).">226. When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size).</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1227/">Paper Link</a>    Pages:2134-2139</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang_0001:Liang">Liang Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0003:Kai">Kai Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Mingbo">Mingbo Ma</a></p>
<p>Abstract:
In neural text generation such as neural machine translation, summarization, and image captioning, beam search is widely used to improve the output text quality. However, in the neural generation setting, hypotheses can finish in different steps, which makes it difficult to decide when to end beam search to ensure optimality. We propose a provably optimal beam search algorithm that will always return the optimal-score complete hypothesis (modulo beam size), and finish as soon as the optimality is established. To counter neural generations tendency for shorter hypotheses, we also introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on neural machine translation demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives.</p>
<p>Keywords:</p>
<h3 id="227. Steering Output Style and Topic in Neural Response Generation.">227. Steering Output Style and Topic in Neural Response Generation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1228/">Paper Link</a>    Pages:2140-2150</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Di">Di Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jojic:Nebojsa">Nebojsa Jojic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brockett:Chris">Chris Brockett</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nyberg:Eric">Eric Nyberg</a></p>
<p>Abstract:
We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to to restrict style and topic without degrading output quality in conversational tasks.</p>
<p>Keywords:</p>
<h3 id="228. Preserving Distributional Information in Dialogue Act Classification.">228. Preserving Distributional Information in Dialogue Act Classification.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1229/">Paper Link</a>    Pages:2151-2156</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tran:Quan_Hung">Quan Hung Tran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zukerman:Ingrid">Ingrid Zukerman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haffari:Gholamreza">Gholamreza Haffari</a></p>
<p>Abstract:
This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural models, achieving state-of-the-art results on the MapTask and Switchboard corpora.</p>
<p>Keywords:</p>
<h3 id="229. Adversarial Learning for Neural Dialogue Generation.">229. Adversarial Learning for Neural Dialogue Generation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1230/">Paper Link</a>    Pages:2157-2169</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jiwei">Jiwei Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Monroe:Will">Will Monroe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Tianlin">Tianlin Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jean:S=eacute=bastien">Sbastien Jean</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ritter:Alan">Alan Ritter</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jurafsky:Dan">Dan Jurafsky</a></p>
<p>Abstract:
We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminatoranalagous to the human evaluator in the Turing test to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines</p>
<p>Keywords:</p>
<h3 id="230. Using Context Information for Dialog Act Classification in DNN Framework.">230. Using Context Information for Dialog Act Classification in DNN Framework.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1231/">Paper Link</a>    Pages:2170-2178</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yang">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Kun">Kun Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Zhao">Zhao Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lei:Yun">Yun Lei</a></p>
<p>Abstract:
Previous work on dialog act (DA) classification has investigated different methods, such as hidden Markov models, maximum entropy, conditional random fields, graphical models, and support vector machines. A few recent studies explored using deep learning neural networks for DA classification, however, it is not clear yet what is the best method for using dialog context or DA sequential information, and how much gain it brings. This paper proposes several ways of using context information for DA classification, all in the deep learning framework. The baseline system classifies each utterance using the convolutional neural networks (CNN). Our proposed methods include using hierarchical models (recurrent neural networks (RNN) or CNN) for DA sequence tagging where the bottom layer takes the sentence CNN representation as input, concatenating predictions from the previous utterances with the CNN vector for classification, and performing sequence decoding based on the predictions from the sentence CNN model. We conduct thorough experiments and comparisons on the Switchboard corpus, demonstrate that incorporating context information significantly improves DA classification, and show that we achieve new state-of-the-art performance for this task.</p>
<p>Keywords:</p>
<h3 id="231. Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences.">231. Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1232/">Paper Link</a>    Pages:2179-2189</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jo:Yohan">Yohan Jo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yoder_0001:Michael">Michael Yoder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jang:Hyeju">Hyeju Jang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ros=eacute=:Carolyn_Penstein">Carolyn Penstein Ros</a></p>
<p>Abstract:
We present an unsupervised model of dialogue act sequences in conversation. By modeling topical themes as transitioning more slowly than dialogue acts in conversation, our model de-emphasizes content-related words in order to focus on conversational function words that signal dialogue acts. We also incorporate speaker tendencies to use some acts more than others as an additional predictor of dialogue act prevalence beyond temporal dependencies. According to the evaluation presented on two dissimilar corpora, the CNET forum and NPS Chat corpus, the effectiveness of each modeling assumption is found to vary depending on characteristics of the data. De-emphasizing content-related words yields improvement on the CNET corpus, while utilizing speaker tendencies is advantageous on the NPS corpus. The components of our model complement one another to achieve robust performance on both corpora and outperform state-of-the-art baseline models.</p>
<p>Keywords:</p>
<h3 id="232. Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems.">232. Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1233/">Paper Link</a>    Pages:2190-2199</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Lili">Lili Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yaoyuan">Yaoyuan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Yansong">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Dongyan">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan_0001:Rui">Rui Yan</a></p>
<p>Abstract:
The study on human-computer conversation systems is a hot research topic nowadays. One of the prevailing methods to build the system is using the generative Sequence-to-Sequence (Seq2Seq) model through neural networks. However, the standard Seq2Seq model is prone to generate trivial responses. In this paper, we aim to generate a more meaningful and informative reply when answering a given question. We propose an implicit content-introducing method which incorporates additional information into the Seq2Seq model in a flexible way. Specifically, we fuse the general decoding and the auxiliary cue word information through our proposed hierarchical gated fusion unit. Experiments on real-life data demonstrate that our model consistently outperforms a set of competitive baselines in terms of BLEU scores and human evaluation.</p>
<p>Keywords:</p>
<h3 id="233. Affordable On-line Dialogue Policy Learning.">233. Affordable On-line Dialogue Policy Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1234/">Paper Link</a>    Pages:2200-2209</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Cheng">Cheng Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Runzhe">Runzhe Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Lu">Lu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Xiang">Xiang Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Kai">Kai Yu</a></p>
<p>Abstract:
The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical. But in reality, due to the scarcity of real interaction data, the dialogue system usually grows slowly. Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is unsustainable. To accurately depict this, two quantitative metrics are proposed to assess safety and efficiency issues. For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher. Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.</p>
<p>Keywords:</p>
<h3 id="234. Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models.">234. Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1235/">Paper Link</a>    Pages:2210-2219</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shao:Yuanlong">Yuanlong Shao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gouws:Stephan">Stephan Gouws</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Britz:Denny">Denny Britz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goldie:Anna">Anna Goldie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Strope:Brian">Brian Strope</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kurzweil:Ray">Ray Kurzweil</a></p>
<p>Abstract:
Sequence-to-sequence models have been applied to the conversation response generation problem where the source sequence is the conversation history and the target sequence is the response. Unlike translation, conversation responding is inherently creative. The generation of long, informative, coherent, and diverse responses remains a hard task. In this work, we focus on the single turn setting. We add self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.</p>
<p>Keywords:</p>
<h3 id="235. Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars.">235. Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1236/">Paper Link</a>    Pages:2220-2230</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Eshghi:Arash">Arash Eshghi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shalyminov:Igor">Igor Shalyminov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lemon:Oliver">Oliver Lemon</a></p>
<p>Abstract:
We investigate an end-to-end method for automatically inducing task-based dialogue systems from small amounts of unannotated dialogue data. It combines an incremental semantic grammar - Dynamic Syntax and Type Theory with Records (DS-TTR) - with Reinforcement Learning (RL), where language generation and dialogue management are a joint decision problem. The systems thus produced are incremental: dialogues are processed word-by-word, shown previously to be essential in supporting natural, spontaneous dialogue. We hypothesised that the rich linguistic knowledge within the grammar should enable a combinatorially large number of dialogue variations to be processed, even when trained on very few dialogues. Our experiments show that our model can process 74% of the Facebook AI bAbI dataset even when trained on only 0.13% of the data (5 dialogues). It can in addition process 65% of bAbI+, a corpus we created by systematically adding incremental dialogue phenomena such as restarts and self-corrections to bAbI. We compare our model with a state-of-the-art retrieval model, MEMN2N. We find that, in terms of semantic accuracy, the MEMN2N model shows very poor robustness to the bAbI+ transformations even when trained on the full bAbI dataset.</p>
<p>Keywords:</p>
<h3 id="236. Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning.">236. Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1237/">Paper Link</a>    Pages:2231-2240</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Baolin">Baolin Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiujun">Xiujun Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0001:Lihong">Lihong Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Jianfeng">Jianfeng Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/=/=Ccedil=elikyilmaz:Asli">Asli elikyilmaz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Sungjin">Sungjin Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wong:Kam=Fai">Kam-Fai Wong</a></p>
<p>Abstract:
Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to collectively complete multiple subtasks. For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in. This paper addresses this challenge by formulating the task in the mathematical framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales. The dialogue manager consists of: (1) a top-level dialogue policy that selects among subtasks or options, (2) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied. Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on handcrafted rules and the other based on flat deep reinforcement learning.</p>
<p>Keywords:</p>
<h3 id="237. Why We Need New Evaluation Metrics for NLG.">237. Why We Need New Evaluation Metrics for NLG.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1238/">Paper Link</a>    Pages:2241-2252</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Novikova:Jekaterina">Jekaterina Novikova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dusek:Ondrej">Ondrej Dusek</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Curry:Amanda_Cercas">Amanda Cercas Curry</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rieser:Verena">Verena Rieser</a></p>
<p>Abstract:
The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.</p>
<p>Keywords:</p>
<h3 id="238. Challenges in Data-to-Document Generation.">238. Challenges in Data-to-Document Generation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1239/">Paper Link</a>    Pages:2253-2263</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wiseman:Sam">Sam Wiseman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shieber:Stuart_M=">Stuart M. Shieber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=">Alexander M. Rush</a></p>
<p>Abstract:
Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.</p>
<p>Keywords:</p>
<h3 id="239. All that is English may be Hindi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media.">239. All that is English may be Hindi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1240/">Paper Link</a>    Pages:2264-2274</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Patro:Jasabanta">Jasabanta Patro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Samanta:Bidisha">Bidisha Samanta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh_0003:Saurabh">Saurabh Singh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Basu:Abhipsa">Abhipsa Basu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mukherjee:Prithwish">Prithwish Mukherjee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choudhury:Monojit">Monojit Choudhury</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mukherjee_0001:Animesh">Animesh Mukherjee</a></p>
<p>Abstract:
n this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from social media. In terms of Spearmans correlation values, our methods perform more than two times better ( 0.62) in predicting the borrowing likeliness compared to the best performing baseline ( 0.26) reported in literature. Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts. In 88% of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems.</p>
<p>Keywords:</p>
<h3 id="240. Multi-View Unsupervised User Feature Embedding for Social Media-based Substance Use Prediction.">240. Multi-View Unsupervised User Feature Embedding for Social Media-based Substance Use Prediction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1241/">Paper Link</a>    Pages:2275-2284</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Tao">Tao Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bickel:Warren_K=">Warren K. Bickel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pan:Shimei">Shimei Pan</a></p>
<p>Abstract:
In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems. Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different unsupervised feature learning methods to take advantage of a large amount of unsupervised social media data. We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as Facebook likes and status updates to enhance system performance. Based on our evaluation, our best models achieved 86% AUC for predicting tobacco use, 81% for alcohol use and 84% for illicit drug use, all of which significantly outperformed existing methods. Our investigation has also uncovered interesting relations between a users social media behavior (e.g., word usage) and substance use.</p>
<p>Keywords:</p>
<h3 id="241. Demographic-aware word associations.">241. Demographic-aware word associations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1242/">Paper Link</a>    Pages:2285-2295</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Garimella:Aparna">Aparna Garimella</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Banea:Carmen">Carmen Banea</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mihalcea:Rada">Rada Mihalcea</a></p>
<p>Abstract:
Variations of word associations across different groups of people can provide insights into peoples psychologies and their world views. To capture these variations, we introduce the task of demographic-aware word associations. We build a new gold standard dataset consisting of word association responses for approximately 300 stimulus words, collected from more than 800 respondents of different gender (male/female) and from different locations (India/United States), and show that there are significant variations in the word associations made by these groups. We also introduce a new demographic-aware word association model based on a neural net skip-gram architecture, and show how computational methods for measuring word associations that specifically account for writer demographics can outperform generic methods that are agnostic to such information.</p>
<p>Keywords:</p>
<h3 id="242. A Factored Neural Network Model for Characterizing Online Discussions in Vector Space.">242. A Factored Neural Network Model for Characterizing Online Discussions in Vector Space.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1243/">Paper Link</a>    Pages:2296-2306</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cheng_0002:Hao">Hao Cheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fang_0002:Hao">Hao Fang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ostendorf:Mari">Mari Ostendorf</a></p>
<p>Abstract:
We develop a novel factored neural model that learns comment embeddings in an unsupervised way leveraging the structure of distributional context in online discussion forums. The model links different context with related language factors in the embedding space, providing a way to interpret the factored embeddings. Evaluated on a community endorsement prediction task using a large collection of topic-varying Reddit discussions, the factored embeddings consistently achieve improvement over other text representations. Qualitative analysis shows that the model captures community style and topic, as well as response trigger patterns.</p>
<p>Keywords:</p>
<h3 id="243. Dimensions of Interpersonal Relationships: Corpus and Experiments.">243. Dimensions of Interpersonal Relationships: Corpus and Experiments.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1244/">Paper Link</a>    Pages:2307-2316</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rashid:Farzana">Farzana Rashid</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blanco_0002:Eduardo">Eduardo Blanco</a></p>
<p>Abstract:
This paper presents a corpus and experiments to determine dimensions of interpersonal relationships. We define a set of dimensions heavily inspired by work in social science. We create a corpus by retrieving pairs of people, and then annotating dimensions for their relationships. A corpus analysis shows that dimensions can be annotated reliably. Experimental results show that given a pair of people, values to dimensions can be assigned automatically.</p>
<p>Keywords:</p>
<h3 id="244. Argument Mining on Twitter: Arguments, Facts and Sources.">244. Argument Mining on Twitter: Arguments, Facts and Sources.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1245/">Paper Link</a>    Pages:2317-2322</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dusmanu:Mihai">Mihai Dusmanu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cabrio:Elena">Elena Cabrio</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Villata:Serena">Serena Villata</a></p>
<p>Abstract:
Social media collect and spread on the Web personal opinions, facts, fake news and all kind of information users may be interested in. Applying argument mining methods to such heterogeneous data sources is a challenging open research issue, in particular considering the peculiarities of the language used to write textual messages on social media. In addition, new issues emerge when dealing with arguments posted on such platforms, such as the need to make a distinction between personal opinions and actual facts, and to detect the source disseminating information about such facts to allow for provenance verification. In this paper, we apply supervised classification to identify arguments on Twitter, and we present two new tasks for argument mining, namely facts recognition and source identification. We study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexit news topics.</p>
<p>Keywords:</p>
<h3 id="245. Distinguishing Japanese Non-standard Usages from Standard Ones.">245. Distinguishing Japanese Non-standard Usages from Standard Ones.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1246/">Paper Link</a>    Pages:2323-2328</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Aoki:Tatsuya">Tatsuya Aoki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sasano:Ryohei">Ryohei Sasano</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Takamura:Hiroya">Hiroya Takamura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Okumura:Manabu">Manabu Okumura</a></p>
<p>Abstract:
We focus on non-standard usages of common words on social media. In the context of social media, words sometimes have other usages that are totally different from their original. In this study, we attempt to distinguish non-standard usages on social media from standard ones in an unsupervised manner. Our basic idea is that non-standardness can be measured by the inconsistency between the expected meaning of the target word and the given context. For this purpose, we use context embeddings derived from word embeddings. Our experimental results show that the model leveraging the context embedding outperforms other methods and provide us with findings, for example, on how to construct context embeddings and which corpus to use.</p>
<p>Keywords:</p>
<h3 id="246. Connotation Frames of Power and Agency in Modern Films.">246. Connotation Frames of Power and Agency in Modern Films.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1247/">Paper Link</a>    Pages:2329-2334</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sap:Maarten">Maarten Sap</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prasettio:Marcella_Cindy">Marcella Cindy Prasettio</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Holtzman:Ari">Ari Holtzman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rashkin:Hannah">Hannah Rashkin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a></p>
<p>Abstract:
The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known Bechdel test. Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.</p>
<p>Keywords:</p>
<h3 id="247. Controlling Human Perception of Basic User Traits.">247. Controlling Human Perception of Basic User Traits.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1248/">Paper Link</a>    Pages:2335-2341</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Preotiuc=Pietro:Daniel">Daniel Preotiuc-Pietro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guntuku:Sharath_Chandra">Sharath Chandra Guntuku</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Ungar:Lyle_H=">Lyle H. Ungar</a></p>
<p>Abstract:
Much of our online communication is text-mediated and, lately, more common with automated agents. Unlike interacting with humans, these agents currently do not tailor their language to the type of person they are communicating to. In this pilot study, we measure the extent to which human perception of basic user trait information  gender and age  is controllable through text. Using automatic models of gender and age prediction, we estimate which tweets posted by a user are more likely to mis-characterize his traits. We perform multiple controlled crowdsourcing experiments in which we show that we can reduce the human prediction accuracy of gender to almost random  an over 20% drop in accuracy. Our experiments show that it is practically feasible for multiple applications such as text generation, text summarization or machine translation to be tailored to specific traits and perceived as such.</p>
<p>Keywords:</p>
<h3 id="248. Topic Signatures in Political Campaign Speeches.">248. Topic Signatures in Political Campaign Speeches.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1249/">Paper Link</a>    Pages:2342-2347</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gautrais:Cl=eacute=ment">Clment Gautrais</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cellier:Peggy">Peggy Cellier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Quiniou:Ren=eacute=">Ren Quiniou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Termier:Alexandre">Alexandre Termier</a></p>
<p>Abstract:
Highlighting the recurrence of topics usage in candidates speeches is a key feature to identify the main ideas of each candidate during a political campaign. In this paper, we present a method combining standard topic modeling with signature mining for analyzing topic recurrence in speeches of Clinton and Trump during the 2016 American presidential campaign. The results show that the method extracts automatically the main ideas of each candidate and, in addition, provides information about the evolution of these topics during the campaign.</p>
<p>Keywords:</p>
<h3 id="249. Assessing Objective Recommendation Quality through Political Forecasting.">249. Assessing Objective Recommendation Quality through Political Forecasting.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1250/">Paper Link</a>    Pages:2348-2357</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:H=_Andrew">H. Andrew Schwartz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rouhizadeh:Masoud">Masoud Rouhizadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bishop:Michael">Michael Bishop</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tetlock:Philip">Philip Tetlock</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mellers:Barbara_A=">Barbara A. Mellers</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Ungar:Lyle_H=">Lyle H. Ungar</a></p>
<p>Abstract:
Recommendations are often rated for their subjective quality, but few researchers have studied comment quality in terms of objective utility. We explore recommendation quality assessment with respect to both subjective (i.e. users ratings) and objective (i.e., did it influence? did it improve decisions?) metrics in a massive online geopolitical forecasting system, ultimately comparing linguistic characteristics of each quality metric. Using a variety of features, we predict all types of quality with better accuracy than the simple yet strong baseline of comment length. Looking at the most predictive content illustrates rater biases; for example, forecasters are subjectively biased in favor of comments mentioning business transactions or dealings as well as material things, even though such comments do not indeed prove any more useful objectively. Additionally, more complex sentence constructions, as evidenced by subordinate conjunctions, are characteristic of comments leading to objective improvements in forecasting.</p>
<p>Keywords:</p>
<h3 id="250. Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs Using Set Cover Problem.">250. Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs Using Set Cover Problem.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1251/">Paper Link</a>    Pages:2358-2367</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shirakawa:Masumi">Masumi Shirakawa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hara:Takahiro">Takahiro Hara</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Maekawa:Takuya">Takuya Maekawa</a></p>
<p>Abstract:
We propose a language-independent data-driven method to exhaustively extract bursty phrases of arbitrary forms (e.g., phrases other than simple noun phrases) from microblogs. The burst (i.e., the rapid increase of the occurrence) of a phrase causes the burst of overlapping N-grams including incomplete ones. In other words, bursty incomplete N-grams inevitably overlap bursty phrases. Thus, the proposed method performs the extraction of bursty phrases as the set cover problem in which all bursty N-grams are covered by a minimum set of bursty phrases. Experimental results using Japanese Twitter data showed that the proposed method outperformed word-based, noun phrase-based, and segmentation-based methods both in terms of accuracy and coverage.</p>
<p>Keywords:</p>
<h3 id="251. Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision.">251. Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1252/">Paper Link</a>    Pages:2368-2378</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Haoruo">Haoruo Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Ming=Wei">Ming-Wei Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yih:Wen=tau">Wen-tau Yih</a></p>
<p>Abstract:
Neural networks have achieved state-of-the-art performance on several structured-output prediction tasks, trained in a fully supervised fashion. However, annotated examples in structured domains are often costly to obtain, which thus limits the applications of neural networks. In this work, we propose Maximum Margin Reward Networks, a neural network-based framework that aims to learn from both explicit (full structures) and implicit supervision signals (delayed feedback on the correctness of the predicted structure). On named entity recognition and semantic parsing, our model outperforms previous systems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.</p>
<p>Keywords:</p>
<h3 id="252. The Impact of Modeling Overall Argumentation with Tree Kernels.">252. The Impact of Modeling Overall Argumentation with Tree Kernels.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1253/">Paper Link</a>    Pages:2379-2389</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wachsmuth:Henning">Henning Wachsmuth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Martino:Giovanni_Da_San">Giovanni Da San Martino</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kiesel:Dora">Dora Kiesel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stein_0001:Benno">Benno Stein</a></p>
<p>Abstract:
Several approaches have been proposed to model either the explicit sequential structure of an argumentative text or its implicit hierarchical structure. So far, the adequacy of these models of overall argumentation remains unclear. This paper asks what type of structure is actually important to tackle downstream tasks in computational argumentation. We analyze patterns in the overall argumentation of texts from three corpora. Then, we adapt the idea of positional tree kernels in order to capture sequential and hierarchical argumentative structure together for the first time. In systematic experiments for three text classification tasks, we find strong evidence for the impact of both types of structure. Our results suggest that either of them is necessary while their combination may be beneficial.</p>
<p>Keywords:</p>
<h3 id="253. Learning Generic Sentence Representations Using Convolutional Neural Networks.">253. Learning Generic Sentence Representations Using Convolutional Neural Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1254/">Paper Link</a>    Pages:2390-2400</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gan:Zhe">Zhe Gan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pu:Yunchen">Yunchen Pu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Henao:Ricardo">Ricardo Henao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Chunyuan">Chunyuan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He_0001:Xiaodong">Xiaodong He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carin:Lawrence">Lawrence Carin</a></p>
<p>Abstract:
We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder. Several tasks are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.</p>
<p>Keywords:</p>
<h3 id="254. Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks.">254. Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1255/">Paper Link</a>    Pages:2401-2410</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Amiri:Hadi">Hadi Amiri</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miller:Timothy_A=">Timothy A. Miller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Savova:Guergana">Guergana Savova</a></p>
<p>Abstract:
We present a novel approach for training artificial neural networks. Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition). We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective algorithm to train neural models. The core part of our algorithm is a cognitively-motivated scheduler according to which training instances and their reviews are spaced over time. Our algorithm uses only 34-50% of data per epoch, is 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines. Our code is available at scholar.harvard.edu/hadi/RbF/.</p>
<p>Keywords:</p>
<h3 id="255. Part-of-Speech Tagging for Twitter with Adversarial Neural Networks.">255. Part-of-Speech Tagging for Twitter with Adversarial Neural Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1256/">Paper Link</a>    Pages:2411-2420</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gui:Tao">Tao Gui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Qi">Qi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Haoran">Haoran Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Minlong">Minlong Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>Abstract:
In this work, we study the problem of part-of-speech tagging for Tweets. In contrast to newswire articles, Tweets are usually informal and contain numerous out-of-vocabulary words. Moreover, there is a lack of large scale labeled datasets for this domain. To tackle these challenges, we propose a novel neural network to make use of out-of-domain labeled data, unlabeled in-domain data, and labeled in-domain data. Inspired by adversarial neural networks, the proposed method tries to learn common features through adversarial discriminator. In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree. Hence, the proposed method adopts a sequence-to-sequence autoencoder to perform this task. Experimental results on three different datasets show that our method achieves better performance than state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="256. Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings.">256. Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1257/">Paper Link</a>    Pages:2421-2431</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Bofang">Bofang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Tao">Tao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Zhe">Zhe Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Buzhou">Buzhou Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Drozd:Aleksandr">Aleksandr Drozd</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rogers:Anna">Anna Rogers</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Du:Xiaoyong">Xiaoyong Du</a></p>
<p>Abstract:
The number of word embedding models is growing every year. Most of them are based on the co-occurrence information of words and their contexts. However, it is still an open question what is the best definition of context. We provide a systematical investigation of 4 different syntactic context types and context representations for learning word embeddings. Comprehensive experiments are conducted to evaluate their effectiveness on 6 extrinsic and intrinsic tasks. We hope that this paper, along with the published code, would be helpful for choosing the best context type and representation for a given task.</p>
<p>Keywords:</p>
<h3 id="257. Does syntax help discourse segmentation? Not so much.">257. Does syntax help discourse segmentation? Not so much.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1258/">Paper Link</a>    Pages:2432-2442</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Braud:Chlo=eacute=">Chlo Braud</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lacroix:Oph=eacute=lie">Ophlie Lacroix</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/S=oslash=gaard:Anders">Anders Sgaard</a></p>
<p>Abstract:
Discourse segmentation is the first step in building discourse parsers. Most work on discourse segmentation does not scale to real-world discourse parsing across languages, for two reasons: (i) models rely on constituent trees, and (ii) experiments have relied on gold standard identification of sentence and token boundaries. We therefore investigate to what extent constituents can be replaced with universal dependencies, or left out completely, as well as how state-of-the-art segmenters fare in the absence of sentence boundaries. Our results show that dependency information is less useful than expected, but we provide a fully scalable, robust model that only relies on part-of-speech information, and show that it performs well across languages in the absence of any gold-standard annotation.</p>
<p>Keywords:</p>
<h3 id="258. Deal or No Deal? End-to-End Learning of Negotiation Dialogues.">258. Deal or No Deal? End-to-End Learning of Negotiation Dialogues.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1259/">Paper Link</a>    Pages:2443-2453</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lewis:Mike">Mike Lewis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yarats:Denis">Denis Yarats</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dauphin:Yann_N=">Yann N. Dauphin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Parikh:Devi">Devi Parikh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Batra:Dhruv">Dhruv Batra</a></p>
<p>Abstract:
Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each others reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.</p>
<p>Keywords:</p>
<h3 id="259. Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning.">259. Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1260/">Paper Link</a>    Pages:2454-2464</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Lu">Lu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Xiang">Xiang Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Cheng">Cheng Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Runzhe">Runzhe Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Kai">Kai Yu</a></p>
<p>Abstract:
Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy. The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance. We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a pre-defined rule-based policy acts as a teacher and guides a data-driven RL system by giving example actions as well as additional rewards. A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teachers experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both safetyand efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.</p>
<p>Keywords:</p>
<h3 id="260. Towards Debate Automation: a Recurrent Model for Predicting Debate Winners.">260. Towards Debate Automation: a Recurrent Model for Predicting Debate Winners.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1261/">Paper Link</a>    Pages:2465-2475</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Potash:Peter">Peter Potash</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rumshisky:Anna">Anna Rumshisky</a></p>
<p>Abstract:
In this paper we introduce a practical first step towards the creation of an automated debate agent: a state-of-the-art recurrent predictive model for predicting debate winners. By having an accurate predictive model, we are able to objectively rate the quality of a statement made at a specific turn in a debate. The model is based on a recurrent neural network architecture with attention, which allows the model to effectively account for the entire debate when making its prediction. Our model achieves state-of-the-art accuracy on a dataset of debate transcripts annotated with audience favorability of the debate teams. Finally, we discuss how future work can leverage our proposed model for the creation of an automated debate agent. We accomplish this by determining the model input that will maximize audience favorability toward a given side of a debate at an arbitrary turn.</p>
<p>Keywords:</p>
<h3 id="261. Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation.">261. Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1262/">Paper Link</a>    Pages:2476-2485</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Qingsong">Qingsong Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Graham:Yvette">Yvette Graham</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldwin:Timothy">Timothy Baldwin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qun">Qun Liu</a></p>
<p>Abstract:
Monolingual evaluation of Machine Translation (MT) aims to simplify human assessment by requiring assessors to compare the meaning of the MT output with a reference translation, opening up the task to a much larger pool of genuinely qualified evaluators. Monolingual evaluation runs the risk, however, of bias in favour of MT systems that happen to produce translations superficially similar to the reference and, consistent with this intuition, previous investigations have concluded monolingual assessment to be strongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results for show no significant evidence of reference bias in monolingual evaluation of MT.</p>
<p>Keywords:</p>
<h3 id="262. A Challenge Set Approach to Evaluating Machine Translation.">262. A Challenge Set Approach to Evaluating Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1263/">Paper Link</a>    Pages:2486-2496</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/i/Isabelle:Pierre">Pierre Isabelle</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cherry:Colin">Colin Cherry</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Foster:George_F=">George F. Foster</a></p>
<p>Abstract:
Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a systems capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.</p>
<p>Keywords:</p>
<h3 id="263. Knowledge Distillation for Bilingual Dictionary Induction.">263. Knowledge Distillation for Bilingual Dictionary Induction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1264/">Paper Link</a>    Pages:2497-2506</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nakashole:Ndapandula">Ndapandula Nakashole</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Flauger:Raphael">Raphael Flauger</a></p>
<p>Abstract:
Leveraging zero-shot learning to learn mapping functions between vector spaces of different languages is a promising approach to bilingual dictionary induction. However, methods using this approach have not yet achieved high accuracy on the task. In this paper, we propose a bridging approach, where our main contribution is a knowledge distillation training objective. As teachers, rich resource translation paths are exploited in this role. And as learners, translation paths involving low resource languages learn from the teachers. Our training objective allows seamless addition of teacher translation paths for any given low resource pair. Since our approach relies on the quality of monolingual word embeddings, we also propose to enhance vector representations of both the source and target language with linguistic information. Our experiments on various languages show large performance gains from our distillation training objective, obtaining as high as 17% accuracy improvements.</p>
<p>Keywords:</p>
<h3 id="264. Machine Translation, it's a question of style, innit? The case of English tag questions.">264. Machine Translation, it's a question of style, innit? The case of English tag questions.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1265/">Paper Link</a>    Pages:2507-2512</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bawden:Rachel">Rachel Bawden</a></p>
<p>Abstract:
In this paper, we address the problem of generating English tag questions (TQs) (e.g. it is, isnt it?) in Machine Translation (MT). We propose a post-edition solution, formulating the problem as a multi-class classification task. We present (i) the automatic annotation of English TQs in a parallel corpus of subtitles and (ii) an approach using a series of classifiers to predict TQ forms, which we use to post-edit state-of-the-art MT outputs. Our method provides significant improvements in English TQ translation when translating from Czech, French and German, in turn improving the fluidity, naturalness, grammatical correctness and pragmatic coherence of MT output.</p>
<p>Keywords:</p>
<h3 id="265. Deciphering Related Languages.">265. Deciphering Related Languages.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1266/">Paper Link</a>    Pages:2513-2518</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pourdamghani:Nima">Nima Pourdamghani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Knight:Kevin">Kevin Knight</a></p>
<p>Abstract:
We present a method for translating texts between close language pairs. The method does not require parallel data, and it does not require the languages to be written in the same script. We show results for six language pairs: Afrikaans/Dutch, Bosnian/Serbian, Danish/Swedish, Macedonian/Bulgarian, Malaysian/Indonesian, and Polish/Belorussian. We report BLEU scores showing our method to outperform others that do not use parallel data.</p>
<p>Keywords:</p>
<h3 id="266. Identifying Cognate Sets Across Dictionaries of Related Languages.">266. Identifying Cognate Sets Across Dictionaries of Related Languages.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1267/">Paper Link</a>    Pages:2519-2528</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Arnaud:Adam_St=">Adam St. Arnaud</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Beck:David">David Beck</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kondrak:Grzegorz">Grzegorz Kondrak</a></p>
<p>Abstract:
We present a system for identifying cognate sets across dictionaries of related languages. The likelihood of a cognate relationship is calculated on the basis of a rich set of features that capture both phonetic and semantic similarity, as well as the presence of regular sound correspondences. The similarity scores are used to cluster words from different languages that may originate from a common proto-word. When tested on the Algonquian language family, our system detects 63% of cognate sets while maintaining cluster purity of 70%.</p>
<p>Keywords:</p>
<h3 id="267. Learning Language Representations for Typology Prediction.">267. Learning Language Representations for Typology Prediction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1268/">Paper Link</a>    Pages:2529-2535</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Malaviya:Chaitanya">Chaitanya Malaviya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Littell:Patrick">Patrick Littell</a></p>
<p>Abstract:
One central mystery of neural NLP is what neural models know about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages? Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one NMT system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages geographic and phylogenetic neighbors.</p>
<p>Keywords:</p>
<h3 id="268. Cheap Translation for Cross-Lingual Named Entity Recognition.">268. Cheap Translation for Cross-Lingual Named Entity Recognition.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1269/">Paper Link</a>    Pages:2536-2545</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mayhew_0001:Stephen">Stephen Mayhew</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsai:Chen=Tse">Chen-Tse Tsai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>Abstract:
Recent work in NLP has attempted to deal with low-resource languages but still assumed a resource level that is not present for most languages, e.g., the availability of Wikipedia in the target language. We propose a simple method for cross-lingual named entity recognition (NER) that works well in settings with very minimal resources. Our approach makes use of a lexicon to translate annotated data available in one or several high resource language(s) into the target language, and learns a standard monolingual NER model there. Further, when Wikipedia is available in the target language, our method can enhance Wikipedia based methods to yield state-of-the-art NER results; we evaluate on 7 diverse languages, improving the state-of-the-art by an average of 5.5% F1 points. With the minimal resources required, this is an extremely portable cross-lingual NER approach, as illustrated using a truly low-resource language, Uyghur.</p>
<p>Keywords:</p>
<h3 id="269. Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation.">269. Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1270/">Paper Link</a>    Pages:2546-2558</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vulic:Ivan">Ivan Vulic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mrksic:Nikola">Nikola Mrksic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Korhonen:Anna">Anna Korhonen</a></p>
<p>Abstract:
Existing approaches to automatic VerbNet-style verb classification are heavily dependent on feature engineering and therefore limited to languages with mature NLP pipelines. In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages. To the best of our knowledge, this is the first study which demonstrates how the architectures for learning word embeddings can be applied to this challenging syntactic-semantic task. Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet. A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using vector dimensions as features for learning verb classes. Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.</p>
<p>Keywords:</p>
<h3 id="270. Classification of telicity using cross-linguistic annotation projection.">270. Classification of telicity using cross-linguistic annotation projection.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1271/">Paper Link</a>    Pages:2559-2565</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Friedrich:Annemarie">Annemarie Friedrich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gateva:Damyana">Damyana Gateva</a></p>
<p>Abstract:
This paper addresses the automatic recognition of telicity, an aspectual notion. A telic event includes a natural endpoint (she walked home), while an atelic event does not (she walked around). Recognizing this difference is a prerequisite for temporal natural language understanding. In English, this classification task is difficult, as telicity is a covert linguistic category. In contrast, in Slavic languages, aspect is part of a verbs meaning and even available in machine-readable dictionaries. Our contributions are as follows. We successfully leverage additional silver standard training data in the form of projected annotations from parallel English-Czech data as well as context information, improving automatic telicity classification for English significantly compared to previous work. We also create a new data set of English texts manually annotated with telicity.</p>
<p>Keywords:</p>
<h3 id="271. Counterfactual Learning from Bandit Feedback under Deterministic Logging : A Case Study in Statistical Machine Translation.">271. Counterfactual Learning from Bandit Feedback under Deterministic Logging : A Case Study in Statistical Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1272/">Paper Link</a>    Pages:2566-2576</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lawrence:Carolin">Carolin Lawrence</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sokolov:Artem">Artem Sokolov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riezler:Stefan">Stefan Riezler</a></p>
<p>Abstract:
The goal of counterfactual learning for statistical machine translation (SMT) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system. A challenge arises by the fact that risk-averse commercial SMT systems deterministically log the most probable translation. The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning. We show that counterfactual learning from deterministic bandit logs is possible nevertheless by smoothing out deterministic components in learning. This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization. Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback.</p>
<p>Keywords:</p>
<h3 id="272. Learning Fine-grained Relations from Chinese User Generated Categories.">272. Learning Fine-grained Relations from Chinese User Generated Categories.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1273/">Paper Link</a>    Pages:2577-2587</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0001:Chengyu">Chengyu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fan:Yan">Yan Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Xiaofeng">Xiaofeng He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Aoying">Aoying Zhou</a></p>
<p>Abstract:
User generated categories (UGCs) are short texts that reflect how people describe and organize entities, expressing rich semantic relations implicitly. While most methods on UGC relation extraction are based on pattern matching in English circumstances, learning relations from Chinese UGCs poses different challenges due to the flexibility of expressions. In this paper, we present a weakly supervised learning framework to harvest relations from Chinese UGCs. We identify is-a relations via word embedding based projection and inference, extract non-taxonomic relations and their category patterns by graph mining. We conduct experiments on Chinese Wikipedia and achieve high accuracy, outperforming state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="273. Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures.">273. Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1274/">Paper Link</a>    Pages:2588-2597</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Lifu">Lifu Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sil:Avirup">Avirup Sil</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Florian:Radu">Radu Florian</a></p>
<p>Abstract:
Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person:cities_of_residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies: (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler; (2). Incorporate two attention mechanisms: local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms state-of-the-art on both relation extraction (16% absolute F-score gain) and slot filling validation for each individual system (up to 8.5% absolute F-score gain).</p>
<p>Keywords:</p>
<h3 id="274. Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation.">274. Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1275/">Paper Link</a>    Pages:2598-2607</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Durrett:Greg">Greg Durrett</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kummerfeld:Jonathan_K=">Jonathan K. Kummerfeld</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berg=Kirkpatrick:Taylor">Taylor Berg-Kirkpatrick</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Portnoff:Rebecca_S=">Rebecca S. Portnoff</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Afroz:Sadia">Sadia Afroz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McCoy:Damon">Damon McCoy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Levchenko:Kirill">Kirill Levchenko</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Paxson:Vern">Vern Paxson</a></p>
<p>Abstract:
One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own fine-grained domain in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.</p>
<p>Keywords:</p>
<h3 id="275. Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention Separators.">275. Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention Separators.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1276/">Paper Link</a>    Pages:2608-2618</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Muis:Aldrian_Obaja">Aldrian Obaja Muis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0011:Wei">Wei Lu</a></p>
<p>Abstract:
In this paper, we propose a new model that is capable of recognizing overlapping mentions. We introduce a novel notion of mention separators that can be effectively used to capture how mentions overlap with one another. On top of a novel multigraph representation that we introduce, we show that efficient and exact inference can still be performed. We present some theoretical analysis on the differences between our model and a recently proposed model for recognizing overlapping mentions, and discuss the possible implications of the differences. Through extensive empirical analysis on standard datasets, we demonstrate the effectiveness of our approach.</p>
<p>Keywords:</p>
<h3 id="276. Deep Joint Entity Disambiguation with Local Neural Attention.">276. Deep Joint Entity Disambiguation with Local Neural Attention.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1277/">Paper Link</a>    Pages:2619-2629</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ganea:Octavian=Eugen">Octavian-Eugen Ganea</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hofmann:Thomas">Thomas Hofmann</a></p>
<p>Abstract:
We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or state-of-the-art accuracy at moderate computational costs.</p>
<p>Keywords:</p>
<h3 id="277. MinIE: Minimizing Facts in Open Information Extraction.">277. MinIE: Minimizing Facts in Open Information Extraction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1278/">Paper Link</a>    Pages:2630-2640</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gashteovski:Kiril">Kiril Gashteovski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gemulla:Rainer">Rainer Gemulla</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Corro:Luciano_Del">Luciano Del Corro</a></p>
<p>Abstract:
The goal of Open Information Extraction (OIE) is to extract surface relations and their arguments from natural-language text in an unsupervised, domain-independent manner. In this paper, we propose MinIE, an OIE system that aims to provide useful, compact extractions with high precision and recall. MinIE approaches these goals by (1) representing information about polarity, modality, attribution, and quantities with semantic annotations instead of in the actual extraction, and (2) identifying and removing parts that are considered overly specific. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher precision and recall than most prior systems, while at the same time producing shorter, semantically enriched extractions.</p>
<p>Keywords:</p>
<h3 id="278. Scientific Information Extraction with Semi-supervised Neural Tagging.">278. Scientific Information Extraction with Semi-supervised Neural Tagging.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1279/">Paper Link</a>    Pages:2641-2651</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luan:Yi">Yi Luan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ostendorf:Mari">Mari Ostendorf</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hajishirzi:Hannaneh">Hannaneh Hajishirzi</a></p>
<p>Abstract:
This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.</p>
<p>Keywords:</p>
<h3 id="279. NITE: A Neural Inductive Teaching Framework for Domain Specific NER.">279. NITE: A Neural Inductive Teaching Framework for Domain Specific NER.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1280/">Paper Link</a>    Pages:2652-2657</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Siliang">Siliang Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Ning">Ning Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jinjian">Jinjian Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0001:Fei">Fei Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhuang:Yueting">Yueting Zhuang</a></p>
<p>Abstract:
In domain-specific NER, due to insufficient labeled training data, deep models usually fail to behave normally. In this paper, we proposed a novel Neural Inductive TEaching framework (NITE) to transfer knowledge from existing domain-specific NER models into an arbitrary deep neural network in a teacher-student training manner. NITE is a general framework that builds upon transfer learning and multiple instance learning, which collaboratively not only transfers knowledge to a deep student network but also reduces the noise from teachers. NITE can help deep learning methods to effectively utilize existing resources (i.e., models, labeled and unlabeled data) in a small domain. The experiment resulted on Disease NER proved that without using any labeled data, NITE can significantly boost the performance of a CNN-bidirectional LSTM-CRF NER neural network nearly over 30% in terms of F1-score.</p>
<p>Keywords:</p>
<h3 id="280. Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods.">280. Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1281/">Paper Link</a>    Pages:2658-2663</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sharma:Aditya">Aditya Sharma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Parekh:Zarana">Zarana Parekh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_P=">Partha P. Talukdar</a></p>
<p>Abstract:
RLIE-DQN is a recently proposed Reinforcement Learning-based Information Extraction (IE) technique which is able to incorporate external evidence during the extraction process. RLIE-DQN trains a single agent sequentially, training on one instance at a time. This results in significant training slowdown which is undesirable. We leverage recent advances in parallel RL training using asynchronous methods and propose RLIE-A3C. RLIE-A3C trains multiple agents in parallel and is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no loss in average accuracy.</p>
<p>Keywords:</p>
<h3 id="281. Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks.">281. Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1282/">Paper Link</a>    Pages:2664-2669</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Peng=Hsuan">Peng-Hsuan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Ruo=Ping">Ruo-Ping Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yu=Siang">Yu-Siang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chou:Ju=Chieh">Ju-Chieh Chou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Wei=Yun">Wei-Yun Ma</a></p>
<p>Abstract:
In this paper, we utilize the linguistic structures of texts to improve named entity recognition by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network. Motivated by the observation that named entities are highly related to linguistic constituents, we propose a constituent-based BRNN-CNN for named entity recognition. In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are linguistic constituents. Then it classifies these chunks with a constituency tree structure by recursively propagating syntactic and semantic information to each constituent node. This method surpasses current state-of-the-art on OntoNotes 5.0 with automatically generated parses.</p>
<p>Keywords:</p>
<h3 id="282. Fast and Accurate Entity Recognition with Iterated Dilated Convolutions.">282. Fast and Accurate Entity Recognition with Iterated Dilated Convolutions.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1283/">Paper Link</a>    Pages:2670-2680</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Strubell:Emma">Emma Strubell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Verga:Patrick">Patrick Verga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Belanger:David">David Belanger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McCallum:Andrew">Andrew McCallum</a></p>
<p>Abstract:
Today when many practitioners run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs. Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF). Though expressive and accurate, these models fail to fully exploit GPU parallelism, limiting their computational efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER: Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are more accurate than Bi-LSTM-CRFs while attaining 8x faster test time speeds.</p>
<p>Keywords:</p>
<h3 id="283. Entity Linking via Joint Encoding of Types, Descriptions, and Context.">283. Entity Linking via Joint Encoding of Types, Descriptions, and Context.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1284/">Paper Link</a>    Pages:2681-2690</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Nitish">Nitish Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh_0001:Sameer">Sameer Singh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>Abstract:
For accurate entity linking, we need to capture various information aspects of an entity, such as its description in a KB, contexts in which it is mentioned, and structured knowledge. Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this work we present a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. We show that the resulting entity linking system is effective at combining these sources, and performs competitively, sometimes out-performing current state-of-the-art systems across datasets, without requiring any domain-specific training data or hand-engineered features. We also show that our model can effectively embed entities that are new to the KB, and is able to link its mentions accurately.</p>
<p>Keywords:</p>
<h3 id="284. An Insight Extraction System on BioMedical Literature with Deep Neural Networks.">284. An Insight Extraction System on BioMedical Literature with Deep Neural Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1285/">Paper Link</a>    Pages:2691-2701</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/He:Hua">Hua He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Ganjam:Kris">Kris Ganjam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jain:Navendu">Navendu Jain</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lundin:Jessica">Jessica Lundin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/White:Ryen">Ryen White</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Jimmy">Jimmy Lin</a></p>
<p>Abstract:
Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them. As new scientific findings appear across a large collection of biomedical publications, our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them. Towards that goal, we develop a system with novel deep neural networks to extract insights on biomedical literature. Evaluation shows our system is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work.</p>
<p>Keywords:</p>
<h3 id="285. Word Etymology as Native Language Interference.">285. Word Etymology as Native Language Interference.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1286/">Paper Link</a>    Pages:2702-2707</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nastase:Vivi">Vivi Nastase</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Strapparava:Carlo">Carlo Strapparava</a></p>
<p>Abstract:
We present experiments that show the influence of native language on lexical choice when producing text in another language  in this particular case English. We start from the premise that non-native English speakers will choose lexical items that are close to words in their native language. This leads us to an etymology-based representation of documents written by people whose mother tongue is an Indo-European language. Based on this representation we grow a language family tree, that matches closely the Indo-European language tree.</p>
<p>Keywords:</p>
<h3 id="286. A Simpler and More Generalizable Story Detector using Verb and Character Features.">286. A Simpler and More Generalizable Story Detector using Verb and Character Features.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1287/">Paper Link</a>    Pages:2708-2715</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/e/Eisenberg:Joshua_D=">Joshua D. Eisenberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Finlayson:Mark_A=">Mark A. Finlayson</a></p>
<p>Abstract:
Story detection is the task of determining whether or not a unit of text contains a story. Prior approaches achieved a maximum performance of 0.66 F1, and did not generalize well across different corpora. We present a new state-of-the-art detector that achieves a maximum performance of 0.75 F1 (a 14% improvement), with significantly greater generalizability than previous work. In particular, our detector achieves performance above 0.70 F1 across a variety of combinations of lexically different corpora for training and testing, as well as dramatic improvements (up to 4,000%) in performance when trained on a small, disfluent data set. The new detector uses two basic types of featuresones related to events, and ones related to characterstotaling 283 specific features overall; previous detectors used tens of thousands of features, and so this detector represents a significant simplification along with increased performance.</p>
<p>Keywords:</p>
<h3 id="287. Multi-modular domain-tailored OCR post-correction.">287. Multi-modular domain-tailored OCR post-correction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1288/">Paper Link</a>    Pages:2716-2726</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schulz:Sarah">Sarah Schulz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kuhn:Jonas">Jonas Kuhn</a></p>
<p>Abstract:
One of the main obstacles for many Digital Humanities projects is the low data availability. Texts have to be digitized in an expensive and time consuming process whereas Optical Character Recognition (OCR) post-correction is one of the time-critical factors. At the example of OCR post-correction, we show the adaptation of a generic system to solve a specific problem with little data. The system accounts for a diversity of errors encountered in OCRed texts coming from different time periods in the domain of literature. We show that the combination of different approaches, such as e.g. Statistical Machine Translation and spell checking, with the help of a ranking mechanism tremendously improves over single-handed approaches. Since we consider the accessibility of the resulting tool as a crucial part of Digital Humanities collaborations, we describe the workflow we suggest for efficient text recognition and subsequent automatic and manual post-correction</p>
<p>Keywords:</p>
<h3 id="288. Learning to Predict Charges for Criminal Cases with Legal Basis.">288. Learning to Predict Charges for Criminal Cases with Legal Basis.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1289/">Paper Link</a>    Pages:2727-2736</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Bingfeng">Bingfeng Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Yansong">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jianbo">Jianbo Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xiang">Xiang Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Dongyan">Dongyan Zhao</a></p>
<p>Abstract:
The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.</p>
<p>Keywords:</p>
<h3 id="289. Quantifying the Effects of Text Duplication on Semantic Models.">289. Quantifying the Effects of Text Duplication on Semantic Models.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1290/">Paper Link</a>    Pages:2737-2747</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schofield:Alexandra">Alexandra Schofield</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thompson:Laure">Laure Thompson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mimno:David_M=">David M. Mimno</a></p>
<p>Abstract:
Duplicate documents are a pervasive problem in text datasets and can have a strong effect on unsupervised models. Methods to remove duplicate texts are typically heuristic or very expensive, so it is vital to know when and why they are needed. We measure the sensitivity of two latent semantic methods to the presence of different levels of document repetition. By artificially creating different forms of duplicate text we confirm several hypotheses about how repeated text impacts models. While a small amount of duplication is tolerable, substantial over-representation of subsets of the text may overwhelm meaningful topical patterns.</p>
<p>Keywords:</p>
<h3 id="290. Identifying Semantically Deviating Outlier Documents.">290. Identifying Semantically Deviating Outlier Documents.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1291/">Paper Link</a>    Pages:2748-2757</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhuang:Honglei">Honglei Zhuang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0001:Chi">Chi Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tao:Fangbo">Fangbo Tao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kaplan:Lance_M=">Lance M. Kaplan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han_0001:Jiawei">Jiawei Han</a></p>
<p>Abstract:
A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many applications, such as screening health records for medical mistakes. In this paper, we study the problem of mining semantically deviating document outliers in a given corpus. We develop a generative model to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to 135% improvement over baselines in terms of recall at top-1% of the outlier ranking.</p>
<p>Keywords:</p>
<h3 id="291. Detecting and Explaining Causes From Text For a Time Series Event.">291. Detecting and Explaining Causes From Text For a Time Series Event.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1292/">Paper Link</a>    Pages:2758-2767</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kang:Dongyeop">Dongyeop Kang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gangal:Varun">Varun Gangal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu:Ang">Ang Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Zheng">Zheng Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>Abstract:
Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between features extracted from text such as N-grams, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient reasoning. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.</p>
<p>Keywords:</p>
<h3 id="292. A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC.">292. A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1293/">Paper Link</a>    Pages:2768-2773</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Zhuoxuan">Zhuoxuan Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Shanshan">Shanshan Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cong:Gao">Gao Cong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miao:Chunyan">Chunyan Miao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiaoming">Xiaoming Li</a></p>
<p>Abstract:
Recent years have witnessed the proliferation of Massive Open Online Courses (MOOCs). With massive learners being offered MOOCs, there is a demand that the forum contents within MOOCs need to be classified in order to facilitate both learners and instructors. Therefore we investigate a significant application, which is to associate forum threads to subtitles of video clips. This task can be regarded as a document ranking problem, and the key is how to learn a distinguishable text representation from word sequences and learners behavior sequences. In this paper, we propose a novel cascade model, which can capture both the latent semantics and latent similarity by modeling MOOC data. Experimental results on two real-world datasets demonstrate that our textual representation outperforms state-of-the-art unsupervised counterparts for the application.</p>
<p>Keywords:</p>
<h3 id="293. Identifying the Provision of Choices in Privacy Policy Text.">293. Identifying the Provision of Choices in Privacy Policy Text.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1294/">Paper Link</a>    Pages:2774-2779</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sathyendra:Kanthashree_Mysore">Kanthashree Mysore Sathyendra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wilson:Shomir">Shomir Wilson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schaub:Florian">Florian Schaub</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zimmeck:Sebastian">Sebastian Zimmeck</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sadeh:Norman_M=">Norman M. Sadeh</a></p>
<p>Abstract:
Websites and mobile apps privacy policies, written in natural language, tend to be long and difficult to understand. Information privacy revolves around the fundamental principle of Notice and choice, namely the idea that users should be able to make informed decisions about what information about them can be collected and how it can be used. Internet users want control over their privacy, but their choices are often hidden in long and convoluted privacy policy texts. Moreover, little (if any) prior work has been done to detect the provision of choices in text. We address this challenge of enabling user choice by automatically identifying and extracting pertinent choice language in privacy policies. In particular, we present a two-stage architecture of classification models to identify opt-out choices in privacy policy text, labelling common varieties of choices with a mean F1 score of 0.735. Our techniques enable the creation of systems to help Internet users to learn about their choices, thereby effectuating notice and choice and improving Internet privacy.</p>
<p>Keywords:</p>
<h3 id="294. An Empirical Analysis of Edit Importance between Document Versions.">294. An Empirical Analysis of Edit Importance between Document Versions.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1295/">Paper Link</a>    Pages:2780-2784</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Goyal:Tanya">Tanya Goyal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kelkar:Sachin">Sachin Kelkar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Agarwal:Manas">Manas Agarwal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grover:Jeenu">Jeenu Grover</a></p>
<p>Abstract:
In this paper, we present a novel approach to infer significance of various textual edits to documents. An author may make several edits to a document; each edit varies in its impact to the content of the document. While some edits are surface changes and introduce negligible change, other edits may change the content/tone of the document significantly. In this paper, we perform an analysis on the human perceptions of edit importance while reviewing documents from one version to the next. We identify linguistic features that influence edit importance and model it in a regression based setting. We show that the predicted importance by our approach is highly correlated with the human perceived importance, established by a Mechanical Turk study.</p>
<p>Keywords:</p>
<h3 id="295. Transition-Based Disfluency Detection using LSTMs.">295. Transition-Based Disfluency Detection using LSTMs.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1296/">Paper Link</a>    Pages:2785-2794</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Shaolei">Shaolei Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Che:Wanxiang">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Meishan">Meishan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>Abstract:
In this paper, we model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our model achieves state-of-the-art f-score of 87.5% on the commonly used English Switchboard test set, and a set of in-house annotated Chinese data.</p>
<p>Keywords:</p>
<h3 id="296. Neural Sequence-Labelling Models for Grammatical Error Correction.">296. Neural Sequence-Labelling Models for Grammatical Error Correction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1297/">Paper Link</a>    Pages:2795-2806</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yannakoudakis:Helen">Helen Yannakoudakis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rei:Marek">Marek Rei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Andersen:=Oslash=istein_E=">istein E. Andersen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yuan:Zheng">Zheng Yuan</a></p>
<p>Abstract:
We propose an approach to N-best list reranking using neural sequence-labelling models. We train a compositional model for error detection that calculates the probability of each token in a sentence being correct or incorrect, utilising the full sentence as context. Using the error detection model, we then re-rank the N best hypotheses generated by statistical machine translation systems. Our approach achieves state-of-the-art results on error correction for three different datasets, and it has the additional advantage of only using a small set of easily computed features that require no linguistic input.</p>
<p>Keywords:</p>
<h3 id="297. Adapting Sequence Models for Sentence Correction.">297. Adapting Sequence Models for Sentence Correction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1298/">Paper Link</a>    Pages:2807-2813</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schmaltz:Allen">Allen Schmaltz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Yoon">Yoon Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=">Alexander M. Rush</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shieber:Stuart_M=">Stuart M. Shieber</a></p>
<p>Abstract:
In a controlled experiment of sequence-to-sequence approaches for the task of sentence correction, we find that character-based models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.</p>
<p>Keywords:</p>
<h3 id="298. A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output.">298. A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1299/">Paper Link</a>    Pages:2814-2819</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Niu:Xing">Xing Niu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Martindale:Marianna_J=">Marianna J. Martindale</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carpuat:Marine">Marine Carpuat</a></p>
<p>Abstract:
Stylistic variations of language, such as formality, carry speakers intention beyond literal meaning and should be conveyed adequately in translation. We propose to use lexical formality models to control the formality level of machine translation output. We demonstrate the effectiveness of our approach in empirical evaluations, as measured by automatic metrics and human assessments.</p>
<p>Keywords:</p>
<h3 id="299. Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU.">299. Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1300/">Paper Link</a>    Pages:2820-2825</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/d/Devlin:Jacob">Jacob Devlin</a></p>
<p>Abstract:
Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems. In this work we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/throughput close to that of a phrasal decoder. We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100 words/sec on single-threaded CPU. We believe this is the best published accuracy/speed trade-off of an NMT system.</p>
<p>Keywords:</p>
<h3 id="300. Exploiting Cross-Sentence Context for Neural Machine Translation.">300. Exploiting Cross-Sentence Context for Neural Machine Translation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1301/">Paper Link</a>    Pages:2826-2831</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Longyue">Longyue Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Zhaopeng">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Way:Andy">Andy Way</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qun">Qun Liu</a></p>
<p>Abstract:
In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.</p>
<p>Keywords:</p>
<h3 id="301. Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources.">301. Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1302/">Paper Link</a>    Pages:2832-2838</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Joo=Kyung">Joo-Kyung Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Young=Bum">Young-Bum Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sarikaya:Ruhi">Ruhi Sarikaya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fosler=Lussier:Eric">Eric Fosler-Lussier</a></p>
<p>Abstract:
Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language. In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora. The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations. The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language. Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.</p>
<p>Keywords:</p>
<h3 id="302. Image Pivoting for Learning Multilingual Multimodal Representations.">302. Image Pivoting for Learning Multilingual Multimodal Representations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1303/">Paper Link</a>    Pages:2839-2845</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gella:Spandana">Spandana Gella</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sennrich:Rico">Rico Sennrich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Keller:Frank">Frank Keller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>Abstract:
In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.</p>
<p>Keywords:</p>
<h3 id="303. Neural Machine Translation with Source Dependency Representation.">303. Neural Machine Translation with Source Dependency Representation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1304/">Paper Link</a>    Pages:2846-2852</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Kehai">Kehai Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0015:Rui">Rui Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Utiyama:Masao">Masao Utiyama</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Lemao">Lemao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tamura:Akihiro">Akihiro Tamura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sumita:Eiichiro">Eiichiro Sumita</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Tiejun">Tiejun Zhao</a></p>
<p>Abstract:
Source dependency information has been successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel NMT with source dependency representation to improve translation performance of NMT, especially long sentences. Empirical results on NIST Chinese-to-English translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system.</p>
<p>Keywords:</p>
<h3 id="304. Visual Denotations for Recognizing Textual Entailment.">304. Visual Denotations for Recognizing Textual Entailment.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1305/">Paper Link</a>    Pages:2853-2859</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/h/Han:Dan">Dan Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mart=iacute=nez=G=oacute=mez:Pascual">Pascual Martnez-Gmez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mineshima:Koji">Koji Mineshima</a></p>
<p>Abstract:
In the logic approach to Recognizing Textual Entailment, identifying phrase-to-phrase semantic relations is still an unsolved problem. Resources such as the Paraphrase Database offer limited coverage despite their large size whereas unsupervised distributional models of meaning often fail to recognize phrasal entailments. We propose to map phrases to their visual denotations and compare their meaning in terms of their images. We show that our approach is effective in the task of Recognizing Textual Entailment when combined with specific linguistic and logic features.</p>
<p>Keywords:</p>
<h3 id="305. Sequence Effects in Crowdsourced Annotations.">305. Sequence Effects in Crowdsourced Annotations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1306/">Paper Link</a>    Pages:2860-2865</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mathur:Nitika">Nitika Mathur</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldwin:Timothy">Timothy Baldwin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a></p>
<p>Abstract:
Manual data annotation is a vital component of NLP research. When designing annotation tasks, properties of the annotation interface can unintentionally lead to artefacts in the resulting dataset, biasing the evaluation. In this paper, we explore sequence effects where annotations of an item are affected by the preceding items. Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next. During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively. We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets. We then recommend a simple way to minimise sequence effects.</p>
<p>Keywords:</p>
<h3 id="306. No Need to Pay Attention: Simple Recurrent Neural Networks Work!">306. No Need to Pay Attention: Simple Recurrent Neural Networks Work!</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1307/">Paper Link</a>    Pages:2866-2872</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/t/T=uuml=re:Ferhan">Ferhan Tre</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jojic:Oliver">Oliver Jojic</a></p>
<p>Abstract:
First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB). While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 65%76% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB. We train a recurrent neural network to solve each problem. On the SimpleQuestions dataset, our approach yields substantial improvements over previously published results  even neural networks based on much more complex architectures. The simplicity of our approach also has practical advantages, such as efficiency and modularity, that are valuable especially in an industry setting. In fact, we present a preliminary analysis of the performance of our model on real queries from Comcasts X1 entertainment platform with millions of users every day.</p>
<p>Keywords:</p>
<h3 id="307. The strange geometry of skip-gram with negative sampling.">307. The strange geometry of skip-gram with negative sampling.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1308/">Paper Link</a>    Pages:2873-2878</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mimno:David_M=">David M. Mimno</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thompson:Laure">Laure Thompson</a></p>
<p>Abstract:
Despite their ubiquity, word embeddings trained with skip-gram negative sampling (SGNS) remain poorly understood. We find that vector positions are not simply determined by semantic similarity, but rather occupy a narrow cone, diametrically opposed to the context vectors. We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.</p>
<p>Keywords:</p>
<h3 id="308. Natural Language Processing with Small Feed-Forward Networks.">308. Natural Language Processing with Small Feed-Forward Networks.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1309/">Paper Link</a>    Pages:2879-2885</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/b/Botha:Jan_A=">Jan A. Botha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pitler:Emily">Emily Pitler</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Ji">Ji Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bakalov:Anton">Anton Bakalov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Salcianu:Alex">Alex Salcianu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weiss:David">David Weiss</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McDonald:Ryan_T=">Ryan T. McDonald</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Petrov:Slav">Slav Petrov</a></p>
<p>Abstract:
We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.</p>
<p>Keywords:</p>
<h3 id="309. Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction.">309. Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1310/">Paper Link</a>    Pages:2886-2892</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xin">Xin Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lam:Wai">Wai Lam</a></p>
<p>Abstract:
We propose a novel LSTM-based deep multi-task learning framework for aspect term extraction from user review sentences. Two LSTMs equipped with extended memories and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions. Sentimental sentence constraint is also added for more accurate prediction via another LSTM. Experiment results over two benchmark datasets demonstrate the effectiveness of our framework.</p>
<p>Keywords:</p>
<h3 id="310. Analogs of Linguistic Structure in Deep Representations.">310. Analogs of Linguistic Structure in Deep Representations.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1311/">Paper Link</a>    Pages:2893-2897</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/a/Andreas:Jacob">Jacob Andreas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klein:Dan">Dan Klein</a></p>
<p>Abstract:
We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a syntax with functional analogues to qualitative properties of natural language.</p>
<p>Keywords:</p>
<h3 id="311. A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings.">311. A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1312/">Paper Link</a>    Pages:2898-2904</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Wei">Wei Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0011:Wei">Wei Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zheng:Vincent_W=">Vincent W. Zheng</a></p>
<p>Abstract:
Learning word embeddings has received a significant amount of attention recently. Often, word embeddings are learned in an unsupervised manner from a large collection of text. The genre of the text typically plays an important role in the effectiveness of the resulting embeddings. How to effectively train word embedding models using data from different domains remains a problem that is less explored. In this paper, we present a simple yet effective method for learning word embeddings based on text from different domains. We demonstrate the effectiveness of our approach through extensive experiments on various down-stream NLP tasks.</p>
<p>Keywords:</p>
<h3 id="312. Learning what to read: Focused machine reading.">312. Learning what to read: Focused machine reading.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1313/">Paper Link</a>    Pages:2905-2910</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/n/Noriega=Atala:Enrique">Enrique Noriega-Atala</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Valenzuela=Esc=aacute=rcega:Marco_Antonio">Marco Antonio Valenzuela-Escrcega</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morrison:Clayton_T=">Clayton T. Morrison</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Surdeanu:Mihai">Mihai Surdeanu</a></p>
<p>Abstract:
Recent efforts in bioinformatics have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at todays scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and processing overhead. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of algorithms for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the baseline, while being more efficient, i.e., reading fewer documents.</p>
<p>Keywords:</p>
<h3 id="313. DOC: Deep Open Classification of Text Documents.">313. DOC: Deep Open Classification of Text Documents.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1314/">Paper Link</a>    Pages:2911-2916</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shu_0004:Lei">Lei Shu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Hu">Hu Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Bing">Bing Liu</a></p>
<p>Abstract:
Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.</p>
<p>Keywords:</p>
<h3 id="314. Charmanteau: Character Embedding Models For Portmanteau Creation.">314. Charmanteau: Character Embedding Models For Portmanteau Creation.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1315/">Paper Link</a>    Pages:2917-2922</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gangal:Varun">Varun Gangal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jhamtani:Harsh">Harsh Jhamtani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nyberg:Eric">Eric Nyberg</a></p>
<p>Abstract:
Portmanteaus are a word formation phenomenon where two words combine into a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.</p>
<p>Keywords:</p>
<h3 id="315. Using Automated Metaphor Identification to Aid in Detection and Prediction of First-Episode Schizophrenia.">315. Using Automated Metaphor Identification to Aid in Detection and Prediction of First-Episode Schizophrenia.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1316/">Paper Link</a>    Pages:2923-2930</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guti=eacute=rrez:E=_Dario">E. Dario Gutirrez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cecchi:Guillermo_A=">Guillermo A. Cecchi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Corcoran:Cheryl">Cheryl Corcoran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Corlett:Philip">Philip Corlett</a></p>
<p>Abstract:
The diagnosis of serious mental health conditions such as schizophrenia is based on the judgment of clinicians whose training takes several years, and cannot be easily formalized into objective measures. However, previous research suggests there are disturbances in aspects of the language use of patients with schizophrenia. Using metaphor-identification and sentiment-analysis algorithms to automatically generate features, we create a classifier, that, with high accuracy, can predict which patients will develop (or currently suffer from) schizophrenia. To our knowledge, this study is the first to demonstrate the utility of automated metaphor identification algorithms for detection or prediction of disease.</p>
<p>Keywords:</p>
<h3 id="316. Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking.">316. Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1317/">Paper Link</a>    Pages:2931-2937</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rashkin:Hannah">Hannah Rashkin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Eunsol">Eunsol Choi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jang:Jin_Yea">Jin Yea Jang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Volkova:Svitlana">Svitlana Volkova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a></p>
<p>Abstract:
We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.</p>
<p>Keywords:</p>
<h3 id="317. Topic-Based Agreement and Disagreement in US Electoral Manifestos.">317. Topic-Based Agreement and Disagreement in US Electoral Manifestos.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1318/">Paper Link</a>    Pages:2938-2944</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/m/Menini:Stefano">Stefano Menini</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nanni:Federico">Federico Nanni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Ponzetto:Simone_Paolo">Simone Paolo Ponzetto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tonelli:Sara">Sara Tonelli</a></p>
<p>Abstract:
We present a topic-based analysis of agreement and disagreement in political manifestos, which relies on a new method for topic detection based on key concept clustering. Our approach outperforms both standard techniques like LDA and a state-of-the-art graph-based method, and provides promising initial results for this new task in computational social science.</p>
<p>Keywords:</p>
<h3 id="318. Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora.">318. Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1319/">Paper Link</a>    Pages:2945-2950</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Hainan">Hainan Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Koehn:Philipp">Philipp Koehn</a></p>
<p>Abstract:
We introduce Zipporah, a fast and scalable data cleaning system. We propose a novel type of bag-of-words translation feature, and train logistic regression models to classify good data and synthetic noisy data in the proposed feature space. The trained model is used to score parallel sentences in the data pool for selection. As shown in experiments, Zipporah selects a high-quality parallel corpus from a large, mixed quality data pool. In particular, for one noisy dataset, Zipporah achieves a 2.1 BLEU score improvement with using 1/5 of the data over using the entire corpus.</p>
<p>Keywords:</p>
<h3 id="319. Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps.">319. Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1320/">Paper Link</a>    Pages:2951-2961</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/f/Falke:Tobias">Tobias Falke</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>Abstract:
Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.</p>
<p>Keywords:</p>
<h3 id="320. Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog.">320. Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1321/">Paper Link</a>    Pages:2962-2967</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kottur:Satwik">Satwik Kottur</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moura:Jos=eacute=_M=_F=">Jos M. F. Moura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Stefan">Stefan Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Batra:Dhruv">Dhruv Batra</a></p>
<p>Abstract:
A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task &amp; Talk reference game between two agents as a testbed, we present a sequence of negative results culminating in a positive one  showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge naturally,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.</p>
<p>Keywords:</p>
<h3 id="321. Depression and Self-Harm Risk Assessment in Online Forums.">321. Depression and Self-Harm Risk Assessment in Online Forums.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1322/">Paper Link</a>    Pages:2968-2978</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yates:Andrew">Andrew Yates</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohan:Arman">Arman Cohan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goharian:Nazli">Nazli Goharian</a></p>
<p>Abstract:
Users suffering from mental health conditions often turn to online resources for support, including specialized online support communities or general communities such as Twitter and Reddit. In this work, we present a framework for supporting and studying users in both types of communities. We propose methods for identifying posts in support communities that may indicate a risk of self-harm, and demonstrate that our approach outperforms strong previously proposed methods for identifying such posts. Self-harm is closely related to depression, which makes identifying depressed users on general forums a crucial related task. We introduce a large-scale general forum dataset consisting of users with self-reported depression diagnoses matched with control users. We show how our method can be applied to effectively identify depressed users from their use of language alone. We demonstrate that our method outperforms strong baselines on this general forum dataset.</p>
<p>Keywords:</p>
<h3 id="322. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints.">322. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints.</h3>
<p><a href="https://www.aclweb.org/anthology/D17-1323/">Paper Link</a>    Pages:2979-2989</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Jieyu">Jieyu Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Tianlu">Tianlu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yatskar:Mark">Mark Yatskar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ordonez:Vicente">Vicente Ordonez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Kai=Wei">Kai-Wei Chang</a></p>
<p>Abstract:
Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively</p>
<p>Keywords:</p>
 

<div class="home">
<i title='' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title=''><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="https://github.com/huntercmd/ccf"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
