 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="cssfile" rel="stylesheet" type="text/css" href="https://rawcdn.githack.com/huntercmd/blog/master/config/css/light.css">
<script src="https://rawcdn.githack.com/huntercmd/blog/d9beff1/config/css/skin.js"></script>
<script src="https://rawcdn.githack.com/huntercmd/blog/master/config/css/classie.js"></script>

<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#EMNLP-IJCNLP 2019:Hong Kong, China">EMNLP-IJCNLP 2019:Hong Kong, China</a><ul>
<li><a href="#Paper Num: 681 || Session Num: 0">Paper Num: 681 || Session Num: 0</a><ul>
<li><a href="#1. Attending to Future Tokens for Bidirectional Sequence Generation.">1. Attending to Future Tokens for Bidirectional Sequence Generation.</a></li>
<li><a href="#2. Attention is not not Explanation.">2. Attention is not not Explanation.</a></li>
<li><a href="#3. Practical Obstacles to Deploying Active Learning.">3. Practical Obstacles to Deploying Active Learning.</a></li>
<li><a href="#4. Transfer Learning Between Related Tasks Using Expected Label Proportions.">4. Transfer Learning Between Related Tasks Using Expected Label Proportions.</a></li>
<li><a href="#5. Knowledge Enhanced Contextual Word Representations.">5. Knowledge Enhanced Contextual Word Representations.</a></li>
<li><a href="#6. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.">6. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.</a></li>
<li><a href="#7. Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings.">7. Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings.</a></li>
<li><a href="#8. Correlations between Word Vector Sets.">8. Correlations between Word Vector Sets.</a></li>
<li><a href="#9. Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation.">9. Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation.</a></li>
<li><a href="#10. Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog.">10. Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog.</a></li>
<li><a href="#11. Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots.">11. Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots.</a></li>
<li><a href="#12. MoEL: Mixture of Empathetic Listeners.">12. MoEL: Mixture of Empathetic Listeners.</a></li>
<li><a href="#13. Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever.">13. Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever.</a></li>
<li><a href="#14. Building Task-Oriented Visual Dialog Systems Through Alternative Optimization Between Dialog Policy and Language Generation.">14. Building Task-Oriented Visual Dialog Systems Through Alternative Optimization Between Dialog Policy and Language Generation.</a></li>
<li><a href="#15. DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation.">15. DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation.</a></li>
<li><a href="#16. Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations.">16. Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations.</a></li>
<li><a href="#17. Interpretable Relevant Emotion Ranking with Event-Driven Attention.">17. Interpretable Relevant Emotion Ranking with Event-Driven Attention.</a></li>
<li><a href="#18. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects.">18. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects.</a></li>
<li><a href="#19. Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning.">19. Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning.</a></li>
<li><a href="#20. Leveraging Dependency Forest for Neural Medical Relation Extraction.">20. Leveraging Dependency Forest for Neural Medical Relation Extraction.</a></li>
<li><a href="#21. Open Relation Extraction: Relational Knowledge Transfer from Supervised Data to Unsupervised Data.">21. Open Relation Extraction: Relational Knowledge Transfer from Supervised Data to Unsupervised Data.</a></li>
<li><a href="#22. Improving Relation Extraction with Knowledge-attention.">22. Improving Relation Extraction with Knowledge-attention.</a></li>
<li><a href="#23. Jointly Learning Entity and Relation Representations for Entity Alignment.">23. Jointly Learning Entity and Relation Representations for Entity Alignment.</a></li>
<li><a href="#24. Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion.">24. Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion.</a></li>
<li><a href="#25. Low-Resource Name Tagging Learned with Weakly Labeled Data.">25. Low-Resource Name Tagging Learned with Weakly Labeled Data.</a></li>
<li><a href="#26. Learning Dynamic Context Augmentation for Global Entity Linking.">26. Learning Dynamic Context Augmentation for Global Entity Linking.</a></li>
<li><a href="#27. Open Event Extraction from Online Text using a Generative Adversarial Network.">27. Open Event Extraction from Online Text using a Generative Adversarial Network.</a></li>
<li><a href="#28. Learning to Bootstrap for Entity Set Expansion.">28. Learning to Bootstrap for Entity Set Expansion.</a></li>
<li><a href="#29. Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Text.">29. Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Text.</a></li>
<li><a href="#30. Cross-lingual Structure Transfer for Relation and Event Extraction.">30. Cross-lingual Structure Transfer for Relation and Event Extraction.</a></li>
<li><a href="#31. Uncover the Ground-Truth Relations in Distant Supervision: A Neural Expectation-Maximization Framework.">31. Uncover the Ground-Truth Relations in Distant Supervision: A Neural Expectation-Maximization Framework.</a></li>
<li><a href="#32. Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction.">32. Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction.</a></li>
<li><a href="#33. Event Detection with Trigger-Aware Lattice Neural Network.">33. Event Detection with Trigger-Aware Lattice Neural Network.</a></li>
<li><a href="#34. A Boundary-aware Neural Model for Nested Named Entity Recognition.">34. A Boundary-aware Neural Model for Nested Named Entity Recognition.</a></li>
<li><a href="#35. Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning.">35. Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning.</a></li>
<li><a href="#36. CaRe: Open Knowledge Graph Embeddings.">36. CaRe: Open Knowledge Graph Embeddings.</a></li>
<li><a href="#37. Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly Supervised Relation Extraction.">37. Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly Supervised Relation Extraction.</a></li>
<li><a href="#38. Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping.">38. Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping.</a></li>
<li><a href="#39. Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation Extraction.">39. Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation Extraction.</a></li>
<li><a href="#40. EntEval: A Holistic Evaluation Benchmark for Entity Representations.">40. EntEval: A Holistic Evaluation Benchmark for Entity Representations.</a></li>
<li><a href="#41. Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction.">41. Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction.</a></li>
<li><a href="#42. Hierarchical Text Classification with Reinforced Label Assignment.">42. Hierarchical Text Classification with Reinforced Label Assignment.</a></li>
<li><a href="#43. Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification.">43. Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification.</a></li>
<li><a href="#44. Label-Specific Document Representation for Multi-Label Text Classification.">44. Label-Specific Document Representation for Multi-Label Text Classification.</a></li>
<li><a href="#45. Hierarchical Attention Prototypical Networks for Few-Shot Text Classification.">45. Hierarchical Attention Prototypical Networks for Few-Shot Text Classification.</a></li>
<li><a href="#46. Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification.">46. Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification.</a></li>
<li><a href="#47. Enhancing Local Feature Extraction with Global Representation for Neural Text Classification.">47. Enhancing Local Feature Extraction with Global Representation for Neural Text Classification.</a></li>
<li><a href="#48. Latent-Variable Generative Models for Data-Efficient Text Classification.">48. Latent-Variable Generative Models for Data-Efficient Text Classification.</a></li>
<li><a href="#49. PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space.">49. PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space.</a></li>
<li><a href="#50. Linking artificial and human neural representations of language.">50. Linking artificial and human neural representations of language.</a></li>
<li><a href="#51. Neural Text Summarization: A Critical Evaluation.">51. Neural Text Summarization: A Critical Evaluation.</a></li>
<li><a href="#52. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures.">52. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures.</a></li>
<li><a href="#53. MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance.">53. MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance.</a></li>
<li><a href="#54. Select and Attend: Towards Controllable Content Selection in Text Generation.">54. Select and Attend: Towards Controllable Content Selection in Text Generation.</a></li>
<li><a href="#55. Sentence-Level Content Planning and Style Specification for Neural Text Generation.">55. Sentence-Level Content Planning and Style Specification for Neural Text Generation.</a></li>
<li><a href="#56. Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling.">56. Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling.</a></li>
<li><a href="#57. Syntax-Enhanced Self-Attention-Based Semantic Role Labeling.">57. Syntax-Enhanced Self-Attention-Based Semantic Role Labeling.</a></li>
<li><a href="#58. VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling.">58. VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling.</a></li>
<li><a href="#59. Parameter-free Sentence Embedding via Orthogonal Basis.">59. Parameter-free Sentence Embedding via Orthogonal Basis.</a></li>
<li><a href="#60. Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations.">60. Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations.</a></li>
<li><a href="#61. Extracting Possessions from Social Media: Images Complement Language.">61. Extracting Possessions from Social Media: Images Complement Language.</a></li>
<li><a href="#62. Learning to Speak and Act in a Fantasy Text Adventure Game.">62. Learning to Speak and Act in a Fantasy Text Adventure Game.</a></li>
<li><a href="#63. Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning.">63. Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning.</a></li>
<li><a href="#64. Incorporating Visual Semantics into Sentence Representations within a Grounded Space.">64. Incorporating Visual Semantics into Sentence Representations within a Grounded Space.</a></li>
<li><a href="#65. Neural Naturalist: Generating Fine-Grained Image Comparisons.">65. Neural Naturalist: Generating Fine-Grained Image Comparisons.</a></li>
<li><a href="#66. Fine-Grained Evaluation for Entity Linking.">66. Fine-Grained Evaluation for Entity Linking.</a></li>
<li><a href="#67. Supervising Unsupervised Open Information Extraction Models.">67. Supervising Unsupervised Open Information Extraction Models.</a></li>
<li><a href="#68. Neural Cross-Lingual Event Detection with Minimal Parallel Resources.">68. Neural Cross-Lingual Event Detection with Minimal Parallel Resources.</a></li>
<li><a href="#69. KnowledgeNet: A Benchmark Dataset for Knowledge Base Population.">69. KnowledgeNet: A Benchmark Dataset for Knowledge Base Population.</a></li>
<li><a href="#70. Effective Use of Transformer Networks for Entity Tracking.">70. Effective Use of Transformer Networks for Entity Tracking.</a></li>
<li><a href="#71. Explicit Cross-lingual Pre-training for Unsupervised Machine Translation.">71. Explicit Cross-lingual Pre-training for Unsupervised Machine Translation.</a></li>
<li><a href="#72. Latent Part-of-Speech Sequences for Neural Machine Translation.">72. Latent Part-of-Speech Sequences for Neural Machine Translation.</a></li>
<li><a href="#73. Improving Back-Translation with Uncertainty-based Confidence Estimation.">73. Improving Back-Translation with Uncertainty-based Confidence Estimation.</a></li>
<li><a href="#74. Towards Linear Time Neural Machine Translation with Capsule Networks.">74. Towards Linear Time Neural Machine Translation with Capsule Networks.</a></li>
<li><a href="#75. Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment.">75. Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment.</a></li>
<li><a href="#76. Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages.">76. Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages.</a></li>
<li><a href="#77. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT.">77. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT.</a></li>
<li><a href="#78. Iterative Dual Domain Adaptation for Neural Machine Translation.">78. Iterative Dual Domain Adaptation for Neural Machine Translation.</a></li>
<li><a href="#79. Multi-agent Learning for Neural Machine Translation.">79. Multi-agent Learning for Neural Machine Translation.</a></li>
<li><a href="#80. Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages.">80. Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages.</a></li>
<li><a href="#81. Context-Aware Monolingual Repair for Neural Machine Translation.">81. Context-Aware Monolingual Repair for Neural Machine Translation.</a></li>
<li><a href="#82. Multi-Granularity Self-Attention for Neural Machine Translation.">82. Multi-Granularity Self-Attention for Neural Machine Translation.</a></li>
<li><a href="#83. Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention.">83. Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention.</a></li>
<li><a href="#84. A Discriminative Neural Model for Cross-Lingual Word Alignment.">84. A Discriminative Neural Model for Cross-Lingual Word Alignment.</a></li>
<li><a href="#85. One Model to Learn Both: Zero Pronoun Prediction and Translation.">85. One Model to Learn Both: Zero Pronoun Prediction and Translation.</a></li>
<li><a href="#86. Dynamic Past and Future for Neural Machine Translation.">86. Dynamic Past and Future for Neural Machine Translation.</a></li>
<li><a href="#87. Revisit Automatic Error Detection for Wrong and Missing Translation - A Supervised Approach.">87. Revisit Automatic Error Detection for Wrong and Missing Translation - A Supervised Approach.</a></li>
<li><a href="#88. Towards Understanding Neural Machine Translation with Word Importance.">88. Towards Understanding Neural Machine Translation with Word Importance.</a></li>
<li><a href="#89. Multilingual Neural Machine Translation with Language Clustering.">89. Multilingual Neural Machine Translation with Language Clustering.</a></li>
<li><a href="#90. Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction.">90. Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction.</a></li>
<li><a href="#91. Pushing the Limits of Low-Resource Morphological Inflection.">91. Pushing the Limits of Low-Resource Morphological Inflection.</a></li>
<li><a href="#92. Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank.">92. Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank.</a></li>
<li><a href="#93. Hierarchical Pointer Net Parsing.">93. Hierarchical Pointer Net Parsing.</a></li>
<li><a href="#94. Semi-Supervised Semantic Role Labeling with Cross-View Training.">94. Semi-Supervised Semantic Role Labeling with Cross-View Training.</a></li>
<li><a href="#95. Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized Representations.">95. Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized Representations.</a></li>
<li><a href="#96. A Lexicon-Based Graph Neural Network for Chinese NER.">96. A Lexicon-Based Graph Neural Network for Chinese NER.</a></li>
<li><a href="#97. CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding.">97. CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding.</a></li>
<li><a href="#98. Tree Transformer: Integrating Tree Structures into Self-Attention.">98. Tree Transformer: Integrating Tree Structures into Self-Attention.</a></li>
<li><a href="#99. Semantic Role Labeling with Iterative Structure Refinement.">99. Semantic Role Labeling with Iterative Structure Refinement.</a></li>
<li><a href="#100. Entity Projection via Machine Translation for Cross-Lingual NER.">100. Entity Projection via Machine Translation for Cross-Lingual NER.</a></li>
<li><a href="#101. A Bayesian Approach for Sequence Tagging with Crowds.">101. A Bayesian Approach for Sequence Tagging with Crowds.</a></li>
<li><a href="#102. A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages.">102. A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages.</a></li>
<li><a href="#103. Target Language-Aware Constrained Inference for Cross-lingual Dependency Parsing.">103. Target Language-Aware Constrained Inference for Cross-lingual Dependency Parsing.</a></li>
<li><a href="#104. Look-up and Adapt: A One-shot Semantic Parser.">104. Look-up and Adapt: A One-shot Semantic Parser.</a></li>
<li><a href="#105. Similarity Based Auxiliary Classifier for Named Entity Recognition.">105. Similarity Based Auxiliary Classifier for Named Entity Recognition.</a></li>
<li><a href="#106. Variable beam search for generative neural parsing and its relevance for the analysis of neuro-imaging signal.">106. Variable beam search for generative neural parsing and its relevance for the analysis of neuro-imaging signal.</a></li>
<li><a href="#107. Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets.">107. Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets.</a></li>
<li><a href="#108. Robust Text Classifier on Test-Time Budgets.">108. Robust Text Classifier on Test-Time Budgets.</a></li>
<li><a href="#109. Commonsense Knowledge Mining from Pretrained Models.">109. Commonsense Knowledge Mining from Pretrained Models.</a></li>
<li><a href="#110. RNN Architecture Learning with Sparse Regularization.">110. RNN Architecture Learning with Sparse Regularization.</a></li>
<li><a href="#111. Analytical Methods for Interpretable Ultradense Word Embeddings.">111. Analytical Methods for Interpretable Ultradense Word Embeddings.</a></li>
<li><a href="#112. Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks.">112. Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks.</a></li>
<li><a href="#113. Retrofitting Contextualized Word Embeddings with Paraphrases.">113. Retrofitting Contextualized Word Embeddings with Paraphrases.</a></li>
<li><a href="#114. Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling.">114. Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling.</a></li>
<li><a href="#115. Neural Linguistic Steganography.">115. Neural Linguistic Steganography.</a></li>
<li><a href="#116. The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization.">116. The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization.</a></li>
<li><a href="#117. Attention Optimization for Abstractive Document Summarization.">117. Attention Optimization for Abstractive Document Summarization.</a></li>
<li><a href="#118. Rewarding Coreference Resolvers for Being Consistent with World Knowledge.">118. Rewarding Coreference Resolvers for Being Consistent with World Knowledge.</a></li>
<li><a href="#119. An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction.">119. An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction.</a></li>
<li><a href="#120. A Multilingual Topic Model for Learning Weighted Topic Links Across Corpora with Low Comparability.">120. A Multilingual Topic Model for Learning Weighted Topic Links Across Corpora with Low Comparability.</a></li>
<li><a href="#121. Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study.">121. Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study.</a></li>
<li><a href="#122. Towards Extracting Medical Family History from Natural Language Interactions: A New Dataset and Baselines.">122. Towards Extracting Medical Family History from Natural Language Interactions: A New Dataset and Baselines.</a></li>
<li><a href="#123. Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue.">123. Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue.</a></li>
<li><a href="#124. Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation.">124. Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation.</a></li>
<li><a href="#125. Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling.">125. Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling.</a></li>
<li><a href="#126. A Progressive Model to Enable Continual Learning for Semantic Slot Filling.">126. A Progressive Model to Enable Continual Learning for Semantic Slot Filling.</a></li>
<li><a href="#127. CASA-NLU: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots.">127. CASA-NLU: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots.</a></li>
<li><a href="#128. Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems.">128. Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems.</a></li>
<li><a href="#129. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables.">129. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables.</a></li>
<li><a href="#130. Modeling Multi-Action Policy for Task-Oriented Dialogues.">130. Modeling Multi-Action Policy for Task-Oriented Dialogues.</a></li>
<li><a href="#131. An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction.">131. An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction.</a></li>
<li><a href="#132. Automatically Learning Data Augmentation Policies for Dialogue Tasks.">132. Automatically Learning Data Augmentation Policies for Dialogue Tasks.</a></li>
<li><a href="#133. uniblock: Scoring and Filtering Corpus with Unicode Block Information.">133. uniblock: Scoring and Filtering Corpus with Unicode Block Information.</a></li>
<li><a href="#134. Multilingual word translation using auxiliary languages.">134. Multilingual word translation using auxiliary languages.</a></li>
<li><a href="#135. Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons.">135. Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons.</a></li>
<li><a href="#136. Vecalign: Improved Sentence Alignment in Linear Time and Space.">136. Vecalign: Improved Sentence Alignment in Linear Time and Space.</a></li>
<li><a href="#137. Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation.">137. Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation.</a></li>
<li><a href="#138. Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER.">138. Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER.</a></li>
<li><a href="#139. Recurrent Positional Embedding for Neural Machine Translation.">139. Recurrent Positional Embedding for Neural Machine Translation.</a></li>
<li><a href="#140. Machine Translation for Machines: the Sentiment Classification Use Case.">140. Machine Translation for Machines: the Sentiment Classification Use Case.</a></li>
<li><a href="#141. Investigating the Effectiveness of BPE: The Power of Shorter Sequences.">141. Investigating the Effectiveness of BPE: The Power of Shorter Sequences.</a></li>
<li><a href="#142. HABLex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation.">142. HABLex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation.</a></li>
<li><a href="#143. Handling Syntactic Divergence in Low-resource Machine Translation.">143. Handling Syntactic Divergence in Low-resource Machine Translation.</a></li>
<li><a href="#144. Speculative Beam Search for Simultaneous Translation.">144. Speculative Beam Search for Simultaneous Translation.</a></li>
<li><a href="#145. Self-Attention with Structural Position Representations.">145. Self-Attention with Structural Position Representations.</a></li>
<li><a href="#146. Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation.">146. Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation.</a></li>
<li><a href="#147. Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings.">147. Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings.</a></li>
<li><a href="#148. A Regularization-based Framework for Bilingual Grammar Induction.">148. A Regularization-based Framework for Bilingual Grammar Induction.</a></li>
<li><a href="#149. Encoders Help You Disambiguate Word Senses in Neural Machine Translation.">149. Encoders Help You Disambiguate Word Senses in Neural Machine Translation.</a></li>
<li><a href="#150. Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model.">150. Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model.</a></li>
<li><a href="#151. Efficient Convolutional Neural Networks for Diacritic Restoration.">151. Efficient Convolutional Neural Networks for Diacritic Restoration.</a></li>
<li><a href="#152. Improving Generative Visual Dialog by Answering Diverse Questions.">152. Improving Generative Visual Dialog by Answering Diverse Questions.</a></li>
<li><a href="#153. Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Understanding.">153. Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Understanding.</a></li>
<li><a href="#154. Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations.">154. Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations.</a></li>
<li><a href="#155. Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering.">155. Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering.</a></li>
<li><a href="#156. REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning.">156. REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning.</a></li>
<li><a href="#157. WSLLN: Weakly Supervised Natural Language Localization Networks.">157. WSLLN: Weakly Supervised Natural Language Localization Networks.</a></li>
<li><a href="#158. Grounding learning of modifier dynamics: An application to color naming.">158. Grounding learning of modifier dynamics: An application to color naming.</a></li>
<li><a href="#159. Robust Navigation with Language Pretraining and Stochastic Sampling.">159. Robust Navigation with Language Pretraining and Stochastic Sampling.</a></li>
<li><a href="#160. Towards Making a Dependency Parser See.">160. Towards Making a Dependency Parser See.</a></li>
<li><a href="#161. Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders.">161. Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders.</a></li>
<li><a href="#162. Dependency Parsing for Spoken Dialog Systems.">162. Dependency Parsing for Spoken Dialog Systems.</a></li>
<li><a href="#163. Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog.">163. Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog.</a></li>
<li><a href="#164. Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation.">164. Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation.</a></li>
<li><a href="#165. Simple, Scalable Adaptation for Neural Machine Translation.">165. Simple, Scalable Adaptation for Neural Machine Translation.</a></li>
<li><a href="#166. Controlling Text Complexity in Neural Machine Translation.">166. Controlling Text Complexity in Neural Machine Translation.</a></li>
<li><a href="#167. Investigating Multilingual NMT Representations at Scale.">167. Investigating Multilingual NMT Representations at Scale.</a></li>
<li><a href="#168. Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation.">168. Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation.</a></li>
<li><a href="#169. Cross-Lingual Machine Reading Comprehension.">169. Cross-Lingual Machine Reading Comprehension.</a></li>
<li><a href="#170. A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning.">170. A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning.</a></li>
<li><a href="#171. Neural Duplicate Question Detection without Labeled Training Data.">171. Neural Duplicate Question Detection without Labeled Training Data.</a></li>
<li><a href="#172. Asking Clarification Questions in Knowledge-Based Question Answering.">172. Asking Clarification Questions in Knowledge-Based Question Answering.</a></li>
<li><a href="#173. Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection.">173. Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection.</a></li>
<li><a href="#174. Multi-label Categorization of Accounts of Sexism using a Neural Framework.">174. Multi-label Categorization of Accounts of Sexism using a Neural Framework.</a></li>
<li><a href="#175. The Trumpiest Trump? Identifying a Subject's Most Characteristic Tweets.">175. The Trumpiest Trump? Identifying a Subject's Most Characteristic Tweets.</a></li>
<li><a href="#176. Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts.">176. Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts.</a></li>
<li><a href="#177. Reinforced Product Metadata Selection for Helpfulness Assessment of Customer Reviews.">177. Reinforced Product Metadata Selection for Helpfulness Assessment of Customer Reviews.</a></li>
<li><a href="#178. Learning Invariant Representations of Social Media Users.">178. Learning Invariant Representations of Social Media Users.</a></li>
<li><a href="#179. (Male, Bachelor">179. (Male, Bachelor) and (Female, Ph.D) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas.</a> and (Female, Ph.D) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas.)</li>
<li><a href="#180. Movie Plot Analysis via Turning Point Identification.">180. Movie Plot Analysis via Turning Point Identification.</a></li>
<li><a href="#181. Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention.">181. Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention.</a></li>
<li><a href="#182. Deep Ordinal Regression for Pledge Specificity Prediction.">182. Deep Ordinal Regression for Pledge Specificity Prediction.</a></li>
<li><a href="#183. Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks.">183. Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks.</a></li>
<li><a href="#184. Multi-Granularity Representations of Dialog.">184. Multi-Granularity Representations of Dialog.</a></li>
<li><a href="#185. Are You for Real? Detecting Identity Fraud via Dialogue Interactions.">185. Are You for Real? Detecting Identity Fraud via Dialogue Interactions.</a></li>
<li><a href="#186. Hierarchy Response Learning for Neural Conversation Generation.">186. Hierarchy Response Learning for Neural Conversation Generation.</a></li>
<li><a href="#187. Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs.">187. Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs.</a></li>
<li><a href="#188. Adaptive Parameterization for Neural Dialogue Generation.">188. Adaptive Parameterization for Neural Dialogue Generation.</a></li>
<li><a href="#189. Towards Knowledge-Based Recommender Dialog System.">189. Towards Knowledge-Based Recommender Dialog System.</a></li>
<li><a href="#190. Structuring Latent Spaces for Stylized Response Generation.">190. Structuring Latent Spaces for Stylized Response Generation.</a></li>
<li><a href="#191. Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration.">191. Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration.</a></li>
<li><a href="#192. Unsupervised Context Rewriting for Open Domain Conversation.">192. Unsupervised Context Rewriting for Open Domain Conversation.</a></li>
<li><a href="#193. Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots.">193. Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots.</a></li>
<li><a href="#194. DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs.">194. DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs.</a></li>
<li><a href="#195. Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework.">195. Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework.</a></li>
<li><a href="#196. Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation.">196. Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation.</a></li>
<li><a href="#197. Low-Resource Response Generation with Template Prior.">197. Low-Resource Response Generation with Template Prior.</a></li>
<li><a href="#198. A Discrete CVAE for Response Generation on Short-Text Conversation.">198. A Discrete CVAE for Response Generation on Short-Text Conversation.</a></li>
<li><a href="#199. Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations.">199. Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations.</a></li>
<li><a href="#200. A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency in Dialogue Generation.">200. A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency in Dialogue Generation.</a></li>
<li><a href="#201. Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders.">201. Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders.</a></li>
<li><a href="#202. Variational Hierarchical User-based Conversation Model.">202. Variational Hierarchical User-based Conversation Model.</a></li>
<li><a href="#203. Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue.">203. Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue.</a></li>
<li><a href="#204. CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases.">204. CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases.</a></li>
<li><a href="#205. A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response Selection.">205. A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response Selection.</a></li>
<li><a href="#206. How to Build User Simulators to Train RL-based Dialog Systems.">206. How to Build User Simulators to Train RL-based Dialog Systems.</a></li>
<li><a href="#207. Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning.">207. Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning.</a></li>
<li><a href="#208. Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach.">208. Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach.</a></li>
<li><a href="#209. Dual Attention Networks for Visual Reference Resolution in Visual Dialog.">209. Dual Attention Networks for Visual Reference Resolution in Visual Dialog.</a></li>
<li><a href="#210. Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents.">210. Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents.</a></li>
<li><a href="#211. UR-FUNNY: A Multimodal Language Dataset for Understanding Humor.">211. UR-FUNNY: A Multimodal Language Dataset for Understanding Humor.</a></li>
<li><a href="#212. Partners in Crime: Multi-view Sequential Inference for Movie Understanding.">212. Partners in Crime: Multi-view Sequential Inference for Movie Understanding.</a></li>
<li><a href="#213. Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag.">213. Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag.</a></li>
<li><a href="#214. A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding.">214. A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding.</a></li>
<li><a href="#215. Talk2Car: Taking Control of Your Self-Driving Car.">215. Talk2Car: Taking Control of Your Self-Driving Car.</a></li>
<li><a href="#216. Fact-Checking Meets Fauxtography: Verifying Claims About Images.">216. Fact-Checking Meets Fauxtography: Verifying Claims About Images.</a></li>
<li><a href="#217. Video Dialog via Progressive Inference and Cross-Transformer.">217. Video Dialog via Progressive Inference and Cross-Transformer.</a></li>
<li><a href="#218. Executing Instructions in Situated Collaborative Interactions.">218. Executing Instructions in Situated Collaborative Interactions.</a></li>
<li><a href="#219. Fusion of Detected Objects in Text for Visual Question Answering.">219. Fusion of Detected Objects in Text for Visual Question Answering.</a></li>
<li><a href="#220. TIGEr: Text-to-Image Grounding for Image Caption Evaluation.">220. TIGEr: Text-to-Image Grounding for Image Caption Evaluation.</a></li>
<li><a href="#221. Universal Adversarial Triggers for Attacking and Analyzing NLP.">221. Universal Adversarial Triggers for Attacking and Analyzing NLP.</a></li>
<li><a href="#222. To Annotate or Not? Predicting Performance Drop under Domain Shift.">222. To Annotate or Not? Predicting Performance Drop under Domain Shift.</a></li>
<li><a href="#223. Adaptively Sparse Transformers.">223. Adaptively Sparse Transformers.</a></li>
<li><a href="#224. Show Your Work: Improved Reporting of Experimental Results.">224. Show Your Work: Improved Reporting of Experimental Results.</a></li>
<li><a href="#225. A Deep Factorization of Style and Structure in Fonts.">225. A Deep Factorization of Style and Structure in Fonts.</a></li>
<li><a href="#226. Cross-lingual Semantic Specialization via Lexical Relation Induction.">226. Cross-lingual Semantic Specialization via Lexical Relation Induction.</a></li>
<li><a href="#227. Modelling the interplay of metaphor and emotion through multitask learning.">227. Modelling the interplay of metaphor and emotion through multitask learning.</a></li>
<li><a href="#228. How well do NLI models capture verb veridicality?">228. How well do NLI models capture verb veridicality?</a></li>
<li><a href="#229. Modeling Color Terminology Across Thousands of Languages.">229. Modeling Color Terminology Across Thousands of Languages.</a></li>
<li><a href="#230. Negative Focus Detection via Contextual Attention Mechanism.">230. Negative Focus Detection via Contextual Attention Mechanism.</a></li>
<li><a href="#231. A Unified Neural Coherence Model.">231. A Unified Neural Coherence Model.</a></li>
<li><a href="#232. Topic-Guided Coherence Modeling for Sentence Ordering by Preserving Global and Local Information.">232. Topic-Guided Coherence Modeling for Sentence Ordering by Preserving Global and Local Information.</a></li>
<li><a href="#233. Neural Generative Rhetorical Structure Parsing.">233. Neural Generative Rhetorical Structure Parsing.</a></li>
<li><a href="#234. Weak Supervision for Learning Discourse Structure.">234. Weak Supervision for Learning Discourse Structure.</a></li>
<li><a href="#235. Predicting Discourse Structure using Distant Supervision from Sentiment.">235. Predicting Discourse Structure using Distant Supervision from Sentiment.</a></li>
<li><a href="#236. The Myth of Double-Blind Review Revisited: ACL vs. EMNLP.">236. The Myth of Double-Blind Review Revisited: ACL vs. EMNLP.</a></li>
<li><a href="#237. Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization.">237. Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization.</a></li>
<li><a href="#238. Identifying Predictive Causal Factors from News Streams.">238. Identifying Predictive Causal Factors from News Streams.</a></li>
<li><a href="#239. Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content.">239. Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content.</a></li>
<li><a href="#240. Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference.">240. Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference.</a></li>
<li><a href="#241. Tree-structured Decoding for Solving Math Word Problems.">241. Tree-structured Decoding for Solving Math Word Problems.</a></li>
<li><a href="#242. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text.">242. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text.</a></li>
<li><a href="#243. Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning.">243. Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning.</a></li>
<li><a href="#244. Finding Generalizable Evidence by Learning to Convince Q&A Models.">244. Finding Generalizable Evidence by Learning to Convince Q&amp;A Models.</a></li>
<li><a href="#245. Ranking and Sampling in Open-Domain Question Answering.">245. Ranking and Sampling in Open-Domain Question Answering.</a></li>
<li><a href="#246. A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs.">246. A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs.</a></li>
<li><a href="#247. Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss.">247. Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss.</a></li>
<li><a href="#248. Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base.">248. Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base.</a></li>
<li><a href="#249. BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels.">249. BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels.</a></li>
<li><a href="#250. Language Models as Knowledge Bases?">250. Language Models as Knowledge Bases?</a></li>
<li><a href="#251. NumNet: Machine Reading Comprehension with Numerical Reasoning.">251. NumNet: Machine Reading Comprehension with Numerical Reasoning.</a></li>
<li><a href="#252. Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks.">252. Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks.</a></li>
<li><a href="#253. Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering.">253. Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering.</a></li>
<li><a href="#254. Adversarial Domain Adaptation for Machine Reading Comprehension.">254. Adversarial Domain Adaptation for Machine Reading Comprehension.</a></li>
<li><a href="#255. Incorporating External Knowledge into Machine Reading for Generative Question Answering.">255. Incorporating External Knowledge into Machine Reading for Generative Question Answering.</a></li>
<li><a href="#256. Answering questions by learning to rank - Learning to rank by answering questions.">256. Answering questions by learning to rank - Learning to rank by answering questions.</a></li>
<li><a href="#257. Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension.">257. Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension.</a></li>
<li><a href="#258. Revealing the Importance of Semantic Retrieval for Machine Reading at Scale.">258. Revealing the Importance of Semantic Retrieval for Machine Reading at Scale.</a></li>
<li><a href="#259. PubMedQA: A Dataset for Biomedical Research Question Answering.">259. PubMedQA: A Dataset for Biomedical Research Question Answering.</a></li>
<li><a href="#260. Quick and (not so">260. Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering.</a> Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering.)</li>
<li><a href="#261. Answering Complex Open-domain Questions Through Iterative Query Generation.">261. Answering Complex Open-domain Questions Through Iterative Query Generation.</a></li>
<li><a href="#262. NL2pSQL: Generating Pseudo-SQL Queries from Under-Specified Natural Language Questions.">262. NL2pSQL: Generating Pseudo-SQL Queries from Under-Specified Natural Language Questions.</a></li>
<li><a href="#263. Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering.">263. Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering.</a></li>
<li><a href="#264. Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning.">264. Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning.</a></li>
<li><a href="#265. Learning to Update Knowledge Graphs by Reading News.">265. Learning to Update Knowledge Graphs by Reading News.</a></li>
<li><a href="#266. DIVINE: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning.">266. DIVINE: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning.</a></li>
<li><a href="#267. Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching.">267. Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching.</a></li>
<li><a href="#268. Representation Learning with Ordered Relation Paths for Knowledge Graph Completion.">268. Representation Learning with Ordered Relation Paths for Knowledge Graph Completion.</a></li>
<li><a href="#269. Collaborative Policy Learning for Open Knowledge Graph Reasoning.">269. Collaborative Policy Learning for Open Knowledge Graph Reasoning.</a></li>
<li><a href="#270. Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder.">270. Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder.</a></li>
<li><a href="#271. Asynchronous Deep Interaction Network for Natural Language Inference.">271. Asynchronous Deep Interaction Network for Natural Language Inference.</a></li>
<li><a href="#272. Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange.">272. Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange.</a></li>
<li><a href="#273. Query-focused Scenario Construction.">273. Query-focused Scenario Construction.</a></li>
<li><a href="#274. Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and Cross-graph Model.">274. Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and Cross-graph Model.</a></li>
<li><a href="#275. Designing and Interpreting Probes with Control Tasks.">275. Designing and Interpreting Probes with Control Tasks.</a></li>
<li><a href="#276. Specializing Word Embeddings (for Parsing">276. Specializing Word Embeddings (for Parsing) by Information Bottleneck.</a> by Information Bottleneck.)</li>
<li><a href="#277. Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing - A Tale of Two Parsers Revisited.">277. Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing - A Tale of Two Parsers Revisited.</a></li>
<li><a href="#278. Semantic graph parsing with recurrent neural network DAG grammars.">278. Semantic graph parsing with recurrent neural network DAG grammars.</a></li>
<li><a href="#279. 75 Languages, 1 Model: Parsing Universal Dependencies Universally.">279. 75 Languages, 1 Model: Parsing Universal Dependencies Universally.</a></li>
<li><a href="#280. Interactive Language Learning by Question Answering.">280. Interactive Language Learning by Question Answering.</a></li>
<li><a href="#281. What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering.">281. What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering.</a></li>
<li><a href="#282. KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning.">282. KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning.</a></li>
<li><a href="#283. Learning with Limited Data for Multilingual Reading Comprehension.">283. Learning with Limited Data for Multilingual Reading Comprehension.</a></li>
<li><a href="#284. A Discrete Hard EM Approach for Weakly Supervised Question Answering.">284. A Discrete Hard EM Approach for Weakly Supervised Question Answering.</a></li>
<li><a href="#285. Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts.">285. Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts.</a></li>
<li><a href="#286. Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs.">286. Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs.</a></li>
<li><a href="#287. Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study.">287. Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study.</a></li>
<li><a href="#288. Towards Zero-shot Language Modeling.">288. Towards Zero-shot Language Modeling.</a></li>
<li><a href="#289. What Gets Echoed? Understanding the "Pointers" in Explanations of Persuasive Arguments.">289. What Gets Echoed? Understanding the "Pointers" in Explanations of Persuasive Arguments.</a></li>
<li><a href="#290. Modeling Frames in Argumentation.">290. Modeling Frames in Argumentation.</a></li>
<li><a href="#291. AMPERSAND: Argument Mining for PERSuAsive oNline Discussions.">291. AMPERSAND: Argument Mining for PERSuAsive oNline Discussions.</a></li>
<li><a href="#292. Evaluating adversarial attacks against multiple fact verification systems.">292. Evaluating adversarial attacks against multiple fact verification systems.</a></li>
<li><a href="#293. Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials.">293. Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials.</a></li>
<li><a href="#294. Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite.">294. Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite.</a></li>
<li><a href="#295. A Regularization Approach for Incorporating Event Knowledge and Coreference Relations into Neural Discourse Parsing.">295. A Regularization Approach for Incorporating Event Knowledge and Coreference Relations into Neural Discourse Parsing.</a></li>
<li><a href="#296. Weakly Supervised Multilingual Causality Extraction from Wikipedia.">296. Weakly Supervised Multilingual Causality Extraction from Wikipedia.</a></li>
<li><a href="#297. Attribute-aware Sequence Network for Review Summarization.">297. Attribute-aware Sequence Network for Review Summarization.</a></li>
<li><a href="#298. Extractive Summarization of Long Documents by Combining Global and Local Context.">298. Extractive Summarization of Long Documents by Combining Global and Local Context.</a></li>
<li><a href="#299. Enhancing Neural Data-To-Text Generation Models with External Background Knowledge.">299. Enhancing Neural Data-To-Text Generation Models with External Background Knowledge.</a></li>
<li><a href="#300. Reading Like HER: Human Reading Inspired Extractive Summarization.">300. Reading Like HER: Human Reading Inspired Extractive Summarization.</a></li>
<li><a href="#301. Contrastive Attention Mechanism for Abstractive Sentence Summarization.">301. Contrastive Attention Mechanism for Abstractive Sentence Summarization.</a></li>
<li><a href="#302. NCLS: Neural Cross-Lingual Summarization.">302. NCLS: Neural Cross-Lingual Summarization.</a></li>
<li><a href="#303. Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning.">303. Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning.</a></li>
<li><a href="#304. Concept Pointer Network for Abstractive Summarization.">304. Concept Pointer Network for Abstractive Summarization.</a></li>
<li><a href="#305. Surface Realisation Using Full Delexicalisation.">305. Surface Realisation Using Full Delexicalisation.</a></li>
<li><a href="#306. IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation.">306. IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation.</a></li>
<li><a href="#307. Better Rewards Yield Better Summaries: Learning to Summarise Without References.">307. Better Rewards Yield Better Summaries: Learning to Summarise Without References.</a></li>
<li><a href="#308. Mixture Content Selection for Diverse Sequence Generation.">308. Mixture Content Selection for Diverse Sequence Generation.</a></li>
<li><a href="#309. An End-to-End Generative Architecture for Paraphrase Generation.">309. An End-to-End Generative Architecture for Paraphrase Generation.</a></li>
<li><a href="#310. Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time">310. Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time).</a>.)</li>
<li><a href="#311. Subtopic-driven Multi-Document Summarization.">311. Subtopic-driven Multi-Document Summarization.</a></li>
<li><a href="#312. Referring Expression Generation Using Entity Profiles.">312. Referring Expression Generation Using Entity Profiles.</a></li>
<li><a href="#313. Exploring Diverse Expressions for Paraphrase Generation.">313. Exploring Diverse Expressions for Paraphrase Generation.</a></li>
<li><a href="#314. Enhancing AMR-to-Text Generation with Dual Graph Representations.">314. Enhancing AMR-to-Text Generation with Dual Graph Representations.</a></li>
<li><a href="#315. Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning.">315. Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning.</a></li>
<li><a href="#316. Toward a Task of Feedback Comment Generation for Writing Learning.">316. Toward a Task of Feedback Comment Generation for Writing Learning.</a></li>
<li><a href="#317. Improving Question Generation With to the Point Context.">317. Improving Question Generation With to the Point Context.</a></li>
<li><a href="#318. Deep Copycat Networks for Text-to-Text Generation.">318. Deep Copycat Networks for Text-to-Text Generation.</a></li>
<li><a href="#319. Towards Controllable and Personalized Review Generation.">319. Towards Controllable and Personalized Review Generation.</a></li>
<li><a href="#320. Answers Unite! Unsupervised Metrics for Reinforced Summarization Models.">320. Answers Unite! Unsupervised Metrics for Reinforced Summarization Models.</a></li>
<li><a href="#321. Long and Diverse Text Generation with Planning-based Hierarchical Variational Model.">321. Long and Diverse Text Generation with Planning-based Hierarchical Variational Model.</a></li>
<li><a href="#322. "Transforming" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer.">322. "Transforming" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer.</a></li>
<li><a href="#323. An Entity-Driven Framework for Abstractive Summarization.">323. An Entity-Driven Framework for Abstractive Summarization.</a></li>
<li><a href="#324. Neural Extractive Text Summarization with Syntactic Compression.">324. Neural Extractive Text Summarization with Syntactic Compression.</a></li>
<li><a href="#325. Domain Adaptive Text Style Transfer.">325. Domain Adaptive Text Style Transfer.</a></li>
<li><a href="#326. Let's Ask Again: Refine Network for Automatic Question Generation.">326. Let's Ask Again: Refine Network for Automatic Question Generation.</a></li>
<li><a href="#327. Earlier Isn't Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization.">327. Earlier Isn't Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization.</a></li>
<li><a href="#328. Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction.">328. Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction.</a></li>
<li><a href="#329. Towards Realistic Practices In Low-Resource Natural Language Processing: The Development Set.">329. Towards Realistic Practices In Low-Resource Natural Language Processing: The Development Set.</a></li>
<li><a href="#330. Synchronously Generating Two Languages with Interactive Decoding.">330. Synchronously Generating Two Languages with Interactive Decoding.</a></li>
<li><a href="#331. On NMT Search Errors and Model Errors: Cat Got Your Tongue?">331. On NMT Search Errors and Model Errors: Cat Got Your Tongue?</a></li>
<li><a href="#332. "Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding.">332. "Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding.</a></li>
<li><a href="#333. QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization.">333. QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization.</a></li>
<li><a href="#334. Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations.">334. Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations.</a></li>
<li><a href="#335. How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the Winograd Schema Challenge and SWAG.">335. How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the Winograd Schema Challenge and SWAG.</a></li>
<li><a href="#336. Pun-GAN: Generative Adversarial Network for Pun Generation.">336. Pun-GAN: Generative Adversarial Network for Pun Generation.</a></li>
<li><a href="#337. Multi-Task Learning with Language Modeling for Question Generation.">337. Multi-Task Learning with Language Modeling for Question Generation.</a></li>
<li><a href="#338. Autoregressive Text Generation Beyond Feedback Loops.">338. Autoregressive Text Generation Beyond Feedback Loops.</a></li>
<li><a href="#339. The Woman Worked as a Babysitter: On Biases in Language Generation.">339. The Woman Worked as a Babysitter: On Biases in Language Generation.</a></li>
<li><a href="#340. On the Importance of Delexicalization for Fact Verification.">340. On the Importance of Delexicalization for Fact Verification.</a></li>
<li><a href="#341. Towards Debiasing Fact Verification Models.">341. Towards Debiasing Fact Verification Models.</a></li>
<li><a href="#342. Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks.">342. Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks.</a></li>
<li><a href="#343. Investigating Dynamic Routing in Tree-Structured LSTM for Sentiment Analysis.">343. Investigating Dynamic Routing in Tree-Structured LSTM for Sentiment Analysis.</a></li>
<li><a href="#344. A Label Informative Wide & Deep Classifier for Patents and Papers.">344. A Label Informative Wide &amp; Deep Classifier for Patents and Papers.</a></li>
<li><a href="#345. Text Level Graph Neural Network for Text Classification.">345. Text Level Graph Neural Network for Text Classification.</a></li>
<li><a href="#346. Semantic Relatedness Based Re-ranker for Text Spotting.">346. Semantic Relatedness Based Re-ranker for Text Spotting.</a></li>
<li><a href="#347. Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings.">347. Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings.</a></li>
<li><a href="#348. Visual Detection with Context for Document Layout Analysis.">348. Visual Detection with Context for Document Layout Analysis.</a></li>
<li><a href="#349. Evaluating Topic Quality with Posterior Variability.">349. Evaluating Topic Quality with Posterior Variability.</a></li>
<li><a href="#350. Neural Topic Model with Reinforcement Learning.">350. Neural Topic Model with Reinforcement Learning.</a></li>
<li><a href="#351. Modelling Stopping Criteria for Search Results using Poisson Processes.">351. Modelling Stopping Criteria for Search Results using Poisson Processes.</a></li>
<li><a href="#352. Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval.">352. Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval.</a></li>
<li><a href="#353. The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval.">353. The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval.</a></li>
<li><a href="#354. Rotate King to get Queen: Word Relationships as Orthogonal Transformations in Embedding Space.">354. Rotate King to get Queen: Word Relationships as Orthogonal Transformations in Embedding Space.</a></li>
<li><a href="#355. GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge.">355. GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge.</a></li>
<li><a href="#356. Leveraging Adjective-Noun Phrasing Knowledge for Comparison Relation Prediction in Text-to-SQL.">356. Leveraging Adjective-Noun Phrasing Knowledge for Comparison Relation Prediction in Text-to-SQL.</a></li>
<li><a href="#357. Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling.">357. Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling.</a></li>
<li><a href="#358. Don't Just Scratch the Surface: Enhancing Word Representations for Korean with Hanja.">358. Don't Just Scratch the Surface: Enhancing Word Representations for Korean with Hanja.</a></li>
<li><a href="#359. SyntagNet: Challenging Supervised Word Sense Disambiguation with Lexical-Semantic Combinations.">359. SyntagNet: Challenging Supervised Word Sense Disambiguation with Lexical-Semantic Combinations.</a></li>
<li><a href="#360. Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition.">360. Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition.</a></li>
<li><a href="#361. Fine-tune BERT with Sparse Self-Attention Mechanism.">361. Fine-tune BERT with Sparse Self-Attention Mechanism.</a></li>
<li><a href="#362. Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels.">362. Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels.</a></li>
<li><a href="#363. A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation.">363. A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation.</a></li>
<li><a href="#364. Out-of-Domain Detection for Low-Resource Text Classification Tasks.">364. Out-of-Domain Detection for Low-Resource Text Classification Tasks.</a></li>
<li><a href="#365. Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer.">365. Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer.</a></li>
<li><a href="#366. Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training.">366. Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training.</a></li>
<li><a href="#367. Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition.">367. Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition.</a></li>
<li><a href="#368. Using Pairwise Occurrence Information to Improve Knowledge Graph Completion on Large-Scale Datasets.">368. Using Pairwise Occurrence Information to Improve Knowledge Graph Completion on Large-Scale Datasets.</a></li>
<li><a href="#369. Single Training Dimension Selection for Word Embedding with PCA.">369. Single Training Dimension Selection for Word Embedding with PCA.</a></li>
<li><a href="#370. A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text.">370. A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text.</a></li>
<li><a href="#371. SciBERT: A Pretrained Language Model for Scientific Text.">371. SciBERT: A Pretrained Language Model for Scientific Text.</a></li>
<li><a href="#372. Humor Detection: A Transformer Gets the Last Laugh.">372. Humor Detection: A Transformer Gets the Last Laugh.</a></li>
<li><a href="#373. Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training.">373. Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training.</a></li>
<li><a href="#374. Small and Practical BERT Models for Sequence Labeling.">374. Small and Practical BERT Models for Sequence Labeling.</a></li>
<li><a href="#375. Data Augmentation with Atomic Templates for Spoken Language Understanding.">375. Data Augmentation with Atomic Templates for Spoken Language Understanding.</a></li>
<li><a href="#376. PaLM: A Hybrid Parser and Language Model.">376. PaLM: A Hybrid Parser and Language Model.</a></li>
<li><a href="#377. A Pilot Study for Chinese SQL Semantic Parsing.">377. A Pilot Study for Chinese SQL Semantic Parsing.</a></li>
<li><a href="#378. Global Reasoning over Database Structures for Text-to-SQL Parsing.">378. Global Reasoning over Database Structures for Text-to-SQL Parsing.</a></li>
<li><a href="#379. Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis.">379. Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis.</a></li>
<li><a href="#380. Efficient Sentence Embedding using Discrete Cosine Transform.">380. Efficient Sentence Embedding using Discrete Cosine Transform.</a></li>
<li><a href="#381. A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection.">381. A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection.</a></li>
<li><a href="#382. PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification.">382. PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification.</a></li>
<li><a href="#383. Pretrained Language Models for Sequential Sentence Classification.">383. Pretrained Language Models for Sequential Sentence Classification.</a></li>
<li><a href="#384. Emergent Linguistic Phenomena in Multi-Agent Communication Games.">384. Emergent Linguistic Phenomena in Multi-Agent Communication Games.</a></li>
<li><a href="#385. TalkDown: A Corpus for Condescension Detection in Context.">385. TalkDown: A Corpus for Condescension Detection in Context.</a></li>
<li><a href="#386. Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization.">386. Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization.</a></li>
<li><a href="#387. Text Summarization with Pretrained Encoders.">387. Text Summarization with Pretrained Encoders.</a></li>
<li><a href="#388. How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing.">388. How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing.</a></li>
<li><a href="#389. BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle.">389. BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle.</a></li>
<li><a href="#390. Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator.">390. Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator.</a></li>
<li><a href="#391. Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs.">391. Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs.</a></li>
<li><a href="#392. Broad-Coverage Semantic Parsing as Transduction.">392. Broad-Coverage Semantic Parsing as Transduction.</a></li>
<li><a href="#393. Core Semantic First: A Top-down Approach for AMR Parsing.">393. Core Semantic First: A Top-down Approach for AMR Parsing.</a></li>
<li><a href="#394. Don't paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing.">394. Don't paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing.</a></li>
<li><a href="#395. Improving Distantly-Supervised Relation Extraction with Joint Label Embedding.">395. Improving Distantly-Supervised Relation Extraction with Joint Label Embedding.</a></li>
<li><a href="#396. Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network.">396. Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network.</a></li>
<li><a href="#397. Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction.">397. Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction.</a></li>
<li><a href="#398. Easy First Relation Extraction with Information Redundancy.">398. Easy First Relation Extraction with Information Redundancy.</a></li>
<li><a href="#399. Dependency-Guided LSTM-CRF for Named Entity Recognition.">399. Dependency-Guided LSTM-CRF for Named Entity Recognition.</a></li>
<li><a href="#400. Cross-Cultural Transfer Learning for Text Classification.">400. Cross-Cultural Transfer Learning for Text Classification.</a></li>
<li><a href="#401. Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification.">401. Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification.</a></li>
<li><a href="#402. ProSeqo: Projection Sequence Networks for On-Device Text Classification.">402. ProSeqo: Projection Sequence Networks for On-Device Text Classification.</a></li>
<li><a href="#403. Induction Networks for Few-Shot Text Classification.">403. Induction Networks for Few-Shot Text Classification.</a></li>
<li><a href="#404. Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach.">404. Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach.</a></li>
<li><a href="#405. A Logic-Driven Framework for Consistency of Neural Models.">405. A Logic-Driven Framework for Consistency of Neural Models.</a></li>
<li><a href="#406. Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites.">406. Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites.</a></li>
<li><a href="#407. Implicit Deep Latent Variable Models for Text Generation.">407. Implicit Deep Latent Variable Models for Text Generation.</a></li>
<li><a href="#408. Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach.">408. Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach.</a></li>
<li><a href="#409. Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.">409. Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.</a></li>
<li><a href="#410. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.">410. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.</a></li>
<li><a href="#411. Learning Only from Relevant Keywords and Unlabeled Documents.">411. Learning Only from Relevant Keywords and Unlabeled Documents.</a></li>
<li><a href="#412. Denoising based Sequence-to-Sequence Pre-training for Text Generation.">412. Denoising based Sequence-to-Sequence Pre-training for Text Generation.</a></li>
<li><a href="#413. Dialog Intent Induction with Deep Multi-View Clustering.">413. Dialog Intent Induction with Deep Multi-View Clustering.</a></li>
<li><a href="#414. Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction.">414. Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction.</a></li>
<li><a href="#415. Auditing Deep Learning processes through Kernel-based Explanatory Models.">415. Auditing Deep Learning processes through Kernel-based Explanatory Models.</a></li>
<li><a href="#416. Enhancing Variational Autoencoders with Mutual Information Neural Estimation for Text Generation.">416. Enhancing Variational Autoencoders with Mutual Information Neural Estimation for Text Generation.</a></li>
<li><a href="#417. Sampling Bias in Deep Active Classification: An Empirical Study.">417. Sampling Bias in Deep Active Classification: An Empirical Study.</a></li>
<li><a href="#418. Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases.">418. Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases.</a></li>
<li><a href="#419. Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation.">419. Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation.</a></li>
<li><a href="#420. Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control.">420. Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control.</a></li>
<li><a href="#421. Experimenting with Power Divergences for Language Modeling.">421. Experimenting with Power Divergences for Language Modeling.</a></li>
<li><a href="#422. Hierarchically-Refined Label Attention Network for Sequence Labeling.">422. Hierarchically-Refined Label Attention Network for Sequence Labeling.</a></li>
<li><a href="#423. Certified Robustness to Adversarial Word Substitutions.">423. Certified Robustness to Adversarial Word Substitutions.</a></li>
<li><a href="#424. Visualizing and Understanding the Effectiveness of BERT.">424. Visualizing and Understanding the Effectiveness of BERT.</a></li>
<li><a href="#425. Topics to Avoid: Demoting Latent Confounds in Text Classification.">425. Topics to Avoid: Demoting Latent Confounds in Text Classification.</a></li>
<li><a href="#426. Learning to Ask for Conversational Machine Learning.">426. Learning to Ask for Conversational Machine Learning.</a></li>
<li><a href="#427. Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training.">427. Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training.</a></li>
<li><a href="#428. Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs.">428. Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs.</a></li>
<li><a href="#429. Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation.">429. Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation.</a></li>
<li><a href="#430. Exploiting Monolingual Data at Scale for Neural Machine Translation.">430. Exploiting Monolingual Data at Scale for Neural Machine Translation.</a></li>
<li><a href="#431. Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs.">431. Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs.</a></li>
<li><a href="#432. Distributionally Robust Language Modeling.">432. Distributionally Robust Language Modeling.</a></li>
<li><a href="#433. Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling.">433. Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling.</a></li>
<li><a href="#434. Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial Crowds.">434. Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial Crowds.</a></li>
<li><a href="#435. Parallel Iterative Edit Models for Local Sequence Transduction.">435. Parallel Iterative Edit Models for Local Sequence Transduction.</a></li>
<li><a href="#436. ARAML: A Stable Adversarial Training Framework for Text Generation.">436. ARAML: A Stable Adversarial Training Framework for Text Generation.</a></li>
<li><a href="#437. FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow.">437. FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow.</a></li>
<li><a href="#438. Compositional Generalization for Primitive Substitutions.">438. Compositional Generalization for Primitive Substitutions.</a></li>
<li><a href="#439. WikiCREM: A Large Unsupervised Corpus for Coreference Resolution.">439. WikiCREM: A Large Unsupervised Corpus for Coreference Resolution.</a></li>
<li><a href="#440. Identifying and Explaining Discriminative Attributes.">440. Identifying and Explaining Discriminative Attributes.</a></li>
<li><a href="#441. Patient Knowledge Distillation for BERT Model Compression.">441. Patient Knowledge Distillation for BERT Model Compression.</a></li>
<li><a href="#442. Neural Gaussian Copula for Variational Autoencoder.">442. Neural Gaussian Copula for Variational Autoencoder.</a></li>
<li><a href="#443. Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel.">443. Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel.</a></li>
<li><a href="#444. Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification.">444. Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification.</a></li>
<li><a href="#445. Revealing the Dark Secrets of BERT.">445. Revealing the Dark Secrets of BERT.</a></li>
<li><a href="#446. Machine Translation With Weakly Paired Documents.">446. Machine Translation With Weakly Paired Documents.</a></li>
<li><a href="#447. Countering Language Drift via Visual Grounding.">447. Countering Language Drift via Visual Grounding.</a></li>
<li><a href="#448. The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.">448. The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.</a></li>
<li><a href="#449. Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?">449. Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?</a></li>
<li><a href="#450. Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings.">450. Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings.</a></li>
<li><a href="#451. Aligning Cross-Lingual Entities with Multi-Aspect Information.">451. Aligning Cross-Lingual Entities with Multi-Aspect Information.</a></li>
<li><a href="#452. Contrastive Language Adaptation for Cross-Lingual Stance Detection.">452. Contrastive Language Adaptation for Cross-Lingual Stance Detection.</a></li>
<li><a href="#453. Jointly Learning to Align and Translate with Transformer Models.">453. Jointly Learning to Align and Translate with Transformer Models.</a></li>
<li><a href="#454. Social IQa: Commonsense Reasoning about Social Interactions.">454. Social IQa: Commonsense Reasoning about Social Interactions.</a></li>
<li><a href="#455. Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.">455. Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.</a></li>
<li><a href="#456. Posing Fair Generalization Tasks for Natural Language Inference.">456. Posing Fair Generalization Tasks for Natural Language Inference.</a></li>
<li><a href="#457. Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text.">457. Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text.</a></li>
<li><a href="#458. CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text.">458. CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text.</a></li>
<li><a href="#459. Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset.">459. Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset.</a></li>
<li><a href="#460. Multi-Domain Goal-Oriented Dialogues (MultiDoGO">460. Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data.</a>: Strategies toward Curating and Annotating Large Scale Dialogue Data.)</li>
<li><a href="#461. Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack.">461. Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack.</a></li>
<li><a href="#462. GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue.">462. GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue.</a></li>
<li><a href="#463. Task-Oriented Conversation Generation Using Heterogeneous Memory Networks.">463. Task-Oriented Conversation Generation Using Heterogeneous Memory Networks.</a></li>
<li><a href="#464. Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks.">464. Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks.</a></li>
<li><a href="#465. Coupling Global and Local Context for Unsupervised Aspect Extraction.">465. Coupling Global and Local Context for Unsupervised Aspect Extraction.</a></li>
<li><a href="#466. Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning.">466. Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning.</a></li>
<li><a href="#467. CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis.">467. CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis.</a></li>
<li><a href="#468. Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training.">468. Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training.</a></li>
<li><a href="#469. Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts.">469. Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts.</a></li>
<li><a href="#470. Neural Conversation Recommendation with Online Interaction Modeling.">470. Neural Conversation Recommendation with Online Interaction Modeling.</a></li>
<li><a href="#471. Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection.">471. Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection.</a></li>
<li><a href="#472. Text-based inference of moral sentiment change.">472. Text-based inference of moral sentiment change.</a></li>
<li><a href="#473. Detecting Causal Language Use in Science Findings.">473. Detecting Causal Language Use in Science Findings.</a></li>
<li><a href="#474. Multilingual and Multi-Aspect Hate Speech Analysis.">474. Multilingual and Multi-Aspect Hate Speech Analysis.</a></li>
<li><a href="#475. MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims.">475. MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims.</a></li>
<li><a href="#476. A Deep Neural Information Fusion Architecture for Textual Network Embeddings.">476. A Deep Neural Information Fusion Architecture for Textual Network Embeddings.</a></li>
<li><a href="#477. You Shall Know a User by the Company It Keeps: Dynamic Representations for Social Media Users in NLP.">477. You Shall Know a User by the Company It Keeps: Dynamic Representations for Social Media Users in NLP.</a></li>
<li><a href="#478. Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis.">478. Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis.</a></li>
<li><a href="#479. A Hierarchical Location Prediction Neural Network for Twitter User Geolocation.">479. A Hierarchical Location Prediction Neural Network for Twitter User Geolocation.</a></li>
<li><a href="#480. Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop.">480. Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop.</a></li>
<li><a href="#481. A Benchmark Dataset for Learning to Intervene in Online Hate Speech.">481. A Benchmark Dataset for Learning to Intervene in Online Hate Speech.</a></li>
<li><a href="#482. Detecting and Reducing Bias in a High Stakes Domain.">482. Detecting and Reducing Bias in a High Stakes Domain.</a></li>
<li><a href="#483. CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums.">483. CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums.</a></li>
<li><a href="#484. Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity.">484. Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity.</a></li>
<li><a href="#485. Reconstructing Capsule Networks for Zero-shot Intent Classification.">485. Reconstructing Capsule Networks for Zero-shot Intent Classification.</a></li>
<li><a href="#486. Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network.">486. Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network.</a></li>
<li><a href="#487. Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification.">487. Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification.</a></li>
<li><a href="#488. Comparing and Developing Tools to Measure the Readability of Domain-Specific Texts.">488. Comparing and Developing Tools to Measure the Readability of Domain-Specific Texts.</a></li>
<li><a href="#489. News2vec: News Network Embedding with Subnode Information.">489. News2vec: News Network Embedding with Subnode Information.</a></li>
<li><a href="#490. Recursive Context-Aware Lexical Simplification.">490. Recursive Context-Aware Lexical Simplification.</a></li>
<li><a href="#491. Leveraging Medical Literature for Section Prediction in Electronic Health Records.">491. Leveraging Medical Literature for Section Prediction in Electronic Health Records.</a></li>
<li><a href="#492. Neural News Recommendation with Heterogeneous User Behavior.">492. Neural News Recommendation with Heterogeneous User Behavior.</a></li>
<li><a href="#493. Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation with Hierarchical Attentive Graph Neural Network.">493. Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation with Hierarchical Attentive Graph Neural Network.</a></li>
<li><a href="#494. Event Representation Learning Enhanced with External Commonsense Knowledge.">494. Event Representation Learning Enhanced with External Commonsense Knowledge.</a></li>
<li><a href="#495. Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification.">495. Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification.</a></li>
<li><a href="#496. A Neural Citation Count Prediction Model based on Peer Review Text.">496. A Neural Citation Count Prediction Model based on Peer Review Text.</a></li>
<li><a href="#497. Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs.">497. Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs.</a></li>
<li><a href="#498. Semi-supervised Text Style Transfer: Cross Projection in Latent Space.">498. Semi-supervised Text Style Transfer: Cross Projection in Latent Space.</a></li>
<li><a href="#499. Question Answering for Privacy Policies: Combining Computational and Legal Perspectives.">499. Question Answering for Privacy Policies: Combining Computational and Legal Perspectives.</a></li>
<li><a href="#500. Stick to the Facts: Learning towards a Fidelity-oriented E-Commerce Product Description Generation.">500. Stick to the Facts: Learning towards a Fidelity-oriented E-Commerce Product Description Generation.</a></li>
<li><a href="#501. Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks.">501. Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks.</a></li>
<li><a href="#502. Learning to Infer Entities, Properties and their Relations from Clinical Conversations.">502. Learning to Infer Entities, Properties and their Relations from Clinical Conversations.</a></li>
<li><a href="#503. Practical Correlated Topic Modeling and Analysis via the Rectified Anchor Word Algorithm.">503. Practical Correlated Topic Modeling and Analysis via the Rectified Anchor Word Algorithm.</a></li>
<li><a href="#504. Modeling the Relationship between User Comments and Edits in Document Revision.">504. Modeling the Relationship between User Comments and Edits in Document Revision.</a></li>
<li><a href="#505. PRADO: Projection Attention Networks for Document Classification On-Device.">505. PRADO: Projection Attention Networks for Document Classification On-Device.</a></li>
<li><a href="#506. Subword Language Model for Query Auto-Completion.">506. Subword Language Model for Query Auto-Completion.</a></li>
<li><a href="#507. Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph.">507. Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph.</a></li>
<li><a href="#508. Counterfactual Story Reasoning and Generation.">508. Counterfactual Story Reasoning and Generation.</a></li>
<li><a href="#509. Encode, Tag, Realize: High-Precision Text Editing.">509. Encode, Tag, Realize: High-Precision Text Editing.</a></li>
<li><a href="#510. Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation.">510. Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation.</a></li>
<li><a href="#511. Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation.">511. Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation.</a></li>
<li><a href="#512. A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features.">512. A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features.</a></li>
<li><a href="#513. LXMERT: Learning Cross-Modality Encoder Representations from Transformers.">513. LXMERT: Learning Cross-Modality Encoder Representations from Transformers.</a></li>
<li><a href="#514. Phrase Grounding by Soft-Label Chain Conditional Random Field.">514. Phrase Grounding by Soft-Label Chain Conditional Random Field.</a></li>
<li><a href="#515. What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues.">515. What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues.</a></li>
<li><a href="#516. YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension.">516. YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension.</a></li>
<li><a href="#517. DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization.">517. DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization.</a></li>
<li><a href="#518. CrossWeigh: Training Named Entity Tagger from Imperfect Annotations.">518. CrossWeigh: Training Named Entity Tagger from Imperfect Annotations.</a></li>
<li><a href="#519. A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers.">519. A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers.</a></li>
<li><a href="#520. Open Domain Web Keyphrase Extraction Beyond Language Modeling.">520. Open Domain Web Keyphrase Extraction Beyond Language Modeling.</a></li>
<li><a href="#521. TuckER: Tensor Factorization for Knowledge Graph Completion.">521. TuckER: Tensor Factorization for Knowledge Graph Completion.</a></li>
<li><a href="#522. Human-grounded Evaluations of Explanation Methods for Text Classification.">522. Human-grounded Evaluations of Explanation Methods for Text Classification.</a></li>
<li><a href="#523. A Context-based Framework for Modeling the Role and Function of On-line Resource Citations in Scientific Literature.">523. A Context-based Framework for Modeling the Role and Function of On-line Resource Citations in Scientific Literature.</a></li>
<li><a href="#524. Adversarial Reprogramming of Text Classification Neural Networks.">524. Adversarial Reprogramming of Text Classification Neural Networks.</a></li>
<li><a href="#525. Document Hashing with Mixture-Prior Generative Models.">525. Document Hashing with Mixture-Prior Generative Models.</a></li>
<li><a href="#526. On Efficient Retrieval of Top Similarity Vectors.">526. On Efficient Retrieval of Top Similarity Vectors.</a></li>
<li><a href="#527. Multiplex Word Embeddings for Selectional Preference Acquisition.">527. Multiplex Word Embeddings for Selectional Preference Acquisition.</a></li>
<li><a href="#528. MulCode: A Multiplicative Multi-way Model for Compressing Neural Language Model.">528. MulCode: A Multiplicative Multi-way Model for Compressing Neural Language Model.</a></li>
<li><a href="#529. It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution.">529. It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution.</a></li>
<li><a href="#530. Examining Gender Bias in Languages with Grammatical Gender.">530. Examining Gender Bias in Languages with Grammatical Gender.</a></li>
<li><a href="#531. Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge Distillation.">531. Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge Distillation.</a></li>
<li><a href="#532. Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations.">532. Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations.</a></li>
<li><a href="#533. Do NLP Models Know Numbers? Probing Numeracy in Embeddings.">533. Do NLP Models Know Numbers? Probing Numeracy in Embeddings.</a></li>
<li><a href="#534. A Split-and-Recombine Approach for Follow-up Query Analysis.">534. A Split-and-Recombine Approach for Follow-up Query Analysis.</a></li>
<li><a href="#535. Text2Math: End-to-end Parsing Text into Math Expressions.">535. Text2Math: End-to-end Parsing Text into Math Expressions.</a></li>
<li><a href="#536. Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions.">536. Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions.</a></li>
<li><a href="#537. Syntax-aware Multilingual Semantic Role Labeling.">537. Syntax-aware Multilingual Semantic Role Labeling.</a></li>
<li><a href="#538. Cloze-driven Pretraining of Self-attention Networks.">538. Cloze-driven Pretraining of Self-attention Networks.</a></li>
<li><a href="#539. Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling.">539. Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling.</a></li>
<li><a href="#540. A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling.">540. A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling.</a></li>
<li><a href="#541. Transfer Fine-Tuning: A BERT Case Study.">541. Transfer Fine-Tuning: A BERT Case Study.</a></li>
<li><a href="#542. Data-Anonymous Encoding for Text-to-SQL Generation.">542. Data-Anonymous Encoding for Text-to-SQL Generation.</a></li>
<li><a href="#543. Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks.">543. Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks.</a></li>
<li><a href="#544. Learning Programmatic Idioms for Scalable Semantic Parsing.">544. Learning Programmatic Idioms for Scalable Semantic Parsing.</a></li>
<li><a href="#545. JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation.">545. JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation.</a></li>
<li><a href="#546. Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study.">546. Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study.</a></li>
<li><a href="#547. Modeling Graph Structure in Transformer for Better AMR-to-Text Generation.">547. Modeling Graph Structure in Transformer for Better AMR-to-Text Generation.</a></li>
<li><a href="#548. Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks.">548. Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks.</a></li>
<li><a href="#549. Learning Explicit and Implicit Structures for Targeted Sentiment Analysis.">549. Learning Explicit and Implicit Structures for Targeted Sentiment Analysis.</a></li>
<li><a href="#550. Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification.">550. Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification.</a></li>
<li><a href="#551. Emotion Detection with Neural Personal Discrimination.">551. Emotion Detection with Neural Personal Discrimination.</a></li>
<li><a href="#552. Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification.">552. Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification.</a></li>
<li><a href="#553. LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification.">553. LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification.</a></li>
<li><a href="#554. Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery.">554. Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery.</a></li>
<li><a href="#555. From the Token to the Review: A Hierarchical Multimodal approach to Opinion Mining.">555. From the Token to the Review: A Hierarchical Multimodal approach to Opinion Mining.</a></li>
<li><a href="#556. Shallow Domain Adaptive Embeddings for Sentiment Analysis.">556. Shallow Domain Adaptive Embeddings for Sentiment Analysis.</a></li>
<li><a href="#557. Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification.">557. Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification.</a></li>
<li><a href="#558. A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis.">558. A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis.</a></li>
<li><a href="#559. Human-Like Decision Making: Document-level Aspect Sentiment Classification via Hierarchical Reinforcement Learning.">559. Human-Like Decision Making: Document-level Aspect Sentiment Classification via Hierarchical Reinforcement Learning.</a></li>
<li><a href="#560. A Dataset of General-Purpose Rebuttal.">560. A Dataset of General-Purpose Rebuttal.</a></li>
<li><a href="#561. Rethinking Attribute Representation and Injection for Sentiment Classification.">561. Rethinking Attribute Representation and Injection for Sentiment Classification.</a></li>
<li><a href="#562. A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis.">562. A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis.</a></li>
<li><a href="#563. Automatic Argument Quality Assessment - New Datasets and Methods.">563. Automatic Argument Quality Assessment - New Datasets and Methods.</a></li>
<li><a href="#564. Fine-Grained Analysis of Propaganda in News Article.">564. Fine-Grained Analysis of Propaganda in News Article.</a></li>
<li><a href="#565. Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis.">565. Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis.</a></li>
<li><a href="#566. Sequential Learning of Convolutional Features for Effective Text Classification.">566. Sequential Learning of Convolutional Features for Effective Text Classification.</a></li>
<li><a href="#567. The Role of Pragmatic and Discourse Context in Determining Argument Impact.">567. The Role of Pragmatic and Discourse Context in Determining Argument Impact.</a></li>
<li><a href="#568. Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree.">568. Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree.</a></li>
<li><a href="#569. Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization.">569. Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization.</a></li>
<li><a href="#570. Simple and Effective Noisy Channel Modeling for Neural Machine Translation.">570. Simple and Effective Noisy Channel Modeling for Neural Machine Translation.</a></li>
<li><a href="#571. MultiFiT: Efficient Multi-lingual Language Model Fine-tuning.">571. MultiFiT: Efficient Multi-lingual Language Model Fine-tuning.</a></li>
<li><a href="#572. Hint-Based Training for Non-Autoregressive Machine Translation.">572. Hint-Based Training for Non-Autoregressive Machine Translation.</a></li>
<li><a href="#573. Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers.">573. Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers.</a></li>
<li><a href="#574. Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing.">574. Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing.</a></li>
<li><a href="#575. Multilingual Grammar Induction with Continuous Language Identification.">575. Multilingual Grammar Induction with Continuous Language Identification.</a></li>
<li><a href="#576. Quantifying the Semantic Core of Gender Systems.">576. Quantifying the Semantic Core of Gender Systems.</a></li>
<li><a href="#577. Perturbation Sensitivity Analysis to Detect Unintended Model Biases.">577. Perturbation Sensitivity Analysis to Detect Unintended Model Biases.</a></li>
<li><a href="#578. Automatically Inferring Gender Associations from Language.">578. Automatically Inferring Gender Associations from Language.</a></li>
<li><a href="#579. Reporting the Unreported: Event Extraction for Analyzing the Local Representation of Hate Crimes.">579. Reporting the Unreported: Event Extraction for Analyzing the Local Representation of Hate Crimes.</a></li>
<li><a href="#580. Minimally Supervised Learning of Affective Events Using Discourse Relations.">580. Minimally Supervised Learning of Affective Events Using Discourse Relations.</a></li>
<li><a href="#581. Event Detection with Multi-Order Graph Convolution and Aggregated Attention.">581. Event Detection with Multi-Order Graph Convolution and Aggregated Attention.</a></li>
<li><a href="#582. Coverage of Information Extraction from Sentences and Paragraphs.">582. Coverage of Information Extraction from Sentences and Paragraphs.</a></li>
<li><a href="#583. HMEAE: Hierarchical Modular Event Argument Extraction.">583. HMEAE: Hierarchical Modular Event Argument Extraction.</a></li>
<li><a href="#584. Entity, Relation, and Event Extraction with Contextualized Span Representations.">584. Entity, Relation, and Event Extraction with Contextualized Span Representations.</a></li>
<li><a href="#585. Next Sentence Prediction helps Implicit Discourse Relation Classification within and across Domains.">585. Next Sentence Prediction helps Implicit Discourse Relation Classification within and across Domains.</a></li>
<li><a href="#586. Split or Merge: Which is Better for Unsupervised RST Parsing?">586. Split or Merge: Which is Better for Unsupervised RST Parsing?</a></li>
<li><a href="#587. BERT for Coreference Resolution: Baselines and Analysis.">587. BERT for Coreference Resolution: Baselines and Analysis.</a></li>
<li><a href="#588. Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs.">588. Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs.</a></li>
<li><a href="#589. Event Causality Recognition Exploiting Multiple Annotators' Judgments and Background Knowledge.">589. Event Causality Recognition Exploiting Multiple Annotators' Judgments and Background Knowledge.</a></li>
<li><a href="#590. What Part of the Neural Network Does This? Understanding LSTMs by Measuring and Dissecting Neurons.">590. What Part of the Neural Network Does This? Understanding LSTMs by Measuring and Dissecting Neurons.</a></li>
<li><a href="#591. Quantity doesn't buy quality syntax with neural language models.">591. Quantity doesn't buy quality syntax with neural language models.</a></li>
<li><a href="#592. Higher-order Comparisons of Sentence Encoder Representations.">592. Higher-order Comparisons of Sentence Encoder Representations.</a></li>
<li><a href="#593. Text Genre and Training Data Size in Human-like Parsing.">593. Text Genre and Training Data Size in Human-like Parsing.</a></li>
<li><a href="#594. Feature2Vec: Distributional semantic modelling of human property knowledge.">594. Feature2Vec: Distributional semantic modelling of human property knowledge.</a></li>
<li><a href="#595. Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation.">595. Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation.</a></li>
<li><a href="#596. GeoSQA: A Benchmark for Scenario-based Question Answering in the Geography Domain at High School Level.">596. GeoSQA: A Benchmark for Scenario-based Question Answering in the Geography Domain at High School Level.</a></li>
<li><a href="#597. Revisiting the Evaluation of Theory of Mind through Question Answering.">597. Revisiting the Evaluation of Theory of Mind through Question Answering.</a></li>
<li><a href="#598. Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering.">598. Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering.</a></li>
<li><a href="#599. A Span-Extraction Dataset for Chinese Machine Reading Comprehension.">599. A Span-Extraction Dataset for Chinese Machine Reading Comprehension.</a></li>
<li><a href="#600. MICRON: Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering.">600. MICRON: Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering.</a></li>
<li><a href="#601. Machine Reading Comprehension Using Structural Knowledge Graph-aware Network.">601. Machine Reading Comprehension Using Structural Knowledge Graph-aware Network.</a></li>
<li><a href="#602. Answering Conversational Questions on Structured Data without Logical Forms.">602. Answering Conversational Questions on Structured Data without Logical Forms.</a></li>
<li><a href="#603. Improving Answer Selection and Answer Triggering using Hard Negatives.">603. Improving Answer Selection and Answer Triggering using Hard Negatives.</a></li>
<li><a href="#604. Can You Unpack That? Learning to Rewrite Questions-in-Context.">604. Can You Unpack That? Learning to Rewrite Questions-in-Context.</a></li>
<li><a href="#605. Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning.">605. Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning.</a></li>
<li><a href="#606. Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model.">606. Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model.</a></li>
<li><a href="#607. QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions.">607. QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions.</a></li>
<li><a href="#608. Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension.">608. Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension.</a></li>
<li><a href="#609. A Gated Self-attention Memory Network for Answer Selection.">609. A Gated Self-attention Memory Network for Answer Selection.</a></li>
<li><a href="#610. Polly Want a Cracker: Analyzing Performance of Parroting on Paraphrase Generation Datasets.">610. Polly Want a Cracker: Analyzing Performance of Parroting on Paraphrase Generation Datasets.</a></li>
<li><a href="#611. Query-focused Sentence Compression in Linear Time.">611. Query-focused Sentence Compression in Linear Time.</a></li>
<li><a href="#612. Generating Personalized Recipes from Historical User Preferences.">612. Generating Personalized Recipes from Historical User Preferences.</a></li>
<li><a href="#613. Generating Highly Relevant Questions.">613. Generating Highly Relevant Questions.</a></li>
<li><a href="#614. Improving Neural Story Generation by Targeted Common Sense Grounding.">614. Improving Neural Story Generation by Targeted Common Sense Grounding.</a></li>
<li><a href="#615. Abstract Text Summarization: A Low Resource Challenge.">615. Abstract Text Summarization: A Low Resource Challenge.</a></li>
<li><a href="#616. Generating Modern Poetry Automatically in Finnish.">616. Generating Modern Poetry Automatically in Finnish.</a></li>
<li><a href="#617. SUM-QE: a BERT-based Summary Quality Estimation Model.">617. SUM-QE: a BERT-based Summary Quality Estimation Model.</a></li>
<li><a href="#618. An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation.">618. An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation.</a></li>
<li><a href="#619. Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses.">619. Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses.</a></li>
<li><a href="#620. Learning Rhyming Constraints using Structured Adversaries.">620. Learning Rhyming Constraints using Structured Adversaries.</a></li>
<li><a href="#621. Question-type Driven Question Generation.">621. Question-type Driven Question Generation.</a></li>
<li><a href="#622. Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization.">622. Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization.</a></li>
<li><a href="#623. Clause-Wise and Recursive Decoding for Complex and Cross-Domain Text-to-SQL Generation.">623. Clause-Wise and Recursive Decoding for Complex and Cross-Domain Text-to-SQL Generation.</a></li>
<li><a href="#624. Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects.">624. Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects.</a></li>
<li><a href="#625. Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching.">625. Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching.</a></li>
<li><a href="#626. What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition.">626. What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition.</a></li>
<li><a href="#627. Pre-Training BERT on Domain Resources for Short Answer Grading.">627. Pre-Training BERT on Domain Resources for Short Answer Grading.</a></li>
<li><a href="#628. WIQA: A dataset for "What if..." reasoning over procedural text.">628. WIQA: A dataset for "What if..." reasoning over procedural text.</a></li>
<li><a href="#629. Evaluating BERT for natural language inference: A case study on the CommitmentBank.">629. Evaluating BERT for natural language inference: A case study on the CommitmentBank.</a></li>
<li><a href="#630. Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs.">630. Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs.</a></li>
<li><a href="#631. The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.">631. The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.</a></li>
<li><a href="#632. Mask-Predict: Parallel Decoding of Conditional Masked Language Models.">632. Mask-Predict: Parallel Decoding of Conditional Masked Language Models.</a></li>
<li><a href="#633. Learning to Copy for Automatic Post-Editing.">633. Learning to Copy for Automatic Post-Editing.</a></li>
<li><a href="#634. Exploring Human Gender Stereotypes with Word Association Test.">634. Exploring Human Gender Stereotypes with Word Association Test.</a></li>
<li><a href="#635. A Modular Architecture for Unsupervised Sarcasm Generation.">635. A Modular Architecture for Unsupervised Sarcasm Generation.</a></li>
<li><a href="#636. Generating Classical Chinese Poems from Vernacular Chinese.">636. Generating Classical Chinese Poems from Vernacular Chinese.</a></li>
<li><a href="#637. Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes.">637. Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes.</a></li>
<li><a href="#638. Constraint-based Learning of Phonological Processes.">638. Constraint-based Learning of Phonological Processes.</a></li>
<li><a href="#639. Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint Embedding for Chinese Character Variation Representation.">639. Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint Embedding for Chinese Character Variation Representation.</a></li>
<li><a href="#640. An Attentive Fine-Grained Entity Typing Model with Latent Type Representation.">640. An Attentive Fine-Grained Entity Typing Model with Latent Type Representation.</a></li>
<li><a href="#641. An Improved Neural Baseline for Temporal Relation Extraction.">641. An Improved Neural Baseline for Temporal Relation Extraction.</a></li>
<li><a href="#642. Improving Fine-grained Entity Typing with Entity Linking.">642. Improving Fine-grained Entity Typing with Entity Linking.</a></li>
<li><a href="#643. Combining Spans into Entities: A Neural Two-Stage Approach for Recognizing Discontiguous Entities.">643. Combining Spans into Entities: A Neural Two-Stage Approach for Recognizing Discontiguous Entities.</a></li>
<li><a href="#644. Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal Schemas.">644. Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal Schemas.</a></li>
<li><a href="#645. Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition.">645. Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition.</a></li>
<li><a href="#646. "A Buster Keaton of Linguistics": First Automated Approaches for the Extraction of Vossian Antonomasia.">646. "A Buster Keaton of Linguistics": First Automated Approaches for the Extraction of Vossian Antonomasia.</a></li>
<li><a href="#647. Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound Paraphrasing.">647. Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound Paraphrasing.</a></li>
<li><a href="#648. FewRel 2.0: Towards More Challenging Few-Shot Relation Classification.">648. FewRel 2.0: Towards More Challenging Few-Shot Relation Classification.</a></li>
<li><a href="#649. ner and pos when nothing is capitalized.">649. ner and pos when nothing is capitalized.</a></li>
<li><a href="#650. CaRB: A Crowdsourced Benchmark for Open IE.">650. CaRB: A Crowdsourced Benchmark for Open IE.</a></li>
<li><a href="#651. Weakly Supervised Attention Networks for Entity Recognition.">651. Weakly Supervised Attention Networks for Entity Recognition.</a></li>
<li><a href="#652. Revealing and Predicting Online Persuasion Strategy with Elementary Units.">652. Revealing and Predicting Online Persuasion Strategy with Elementary Units.</a></li>
<li><a href="#653. A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis.">653. A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis.</a></li>
<li><a href="#654. Learning with Noisy Labels for Sentence-level Sentiment Classification.">654. Learning with Noisy Labels for Sentence-level Sentiment Classification.</a></li>
<li><a href="#655. DENS: A Dataset for Multi-class Emotion Analysis.">655. DENS: A Dataset for Multi-class Emotion Analysis.</a></li>
<li><a href="#656. Multi-Task Stance Detection with Sentiment and Stance Lexicons.">656. Multi-Task Stance Detection with Sentiment and Stance Lexicons.</a></li>
<li><a href="#657. A Robust Self-Learning Framework for Cross-Lingual Text Classification.">657. A Robust Self-Learning Framework for Cross-Lingual Text Classification.</a></li>
<li><a href="#658. Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora.">658. Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora.</a></li>
<li><a href="#659. Label Embedding using Hierarchical Structure of Labels for Twitter Classification.">659. Label Embedding using Hierarchical Structure of Labels for Twitter Classification.</a></li>
<li><a href="#660. Interpretable Word Embeddings via Informative Priors.">660. Interpretable Word Embeddings via Informative Priors.</a></li>
<li><a href="#661. Adversarial Removal of Demographic Attributes Revisited.">661. Adversarial Removal of Demographic Attributes Revisited.</a></li>
<li><a href="#662. A deep-learning framework to detect sarcasm targets.">662. A deep-learning framework to detect sarcasm targets.</a></li>
<li><a href="#663. In Plain Sight: Media Bias Through the Lens of Factual Reporting.">663. In Plain Sight: Media Bias Through the Lens of Factual Reporting.</a></li>
<li><a href="#664. Incorporating Label Dependencies in Multilabel Stance Detection.">664. Incorporating Label Dependencies in Multilabel Stance Detection.</a></li>
<li><a href="#665. Investigating Sports Commentator Bias within a Large Corpus of American Football Broadcasts.">665. Investigating Sports Commentator Bias within a Large Corpus of American Football Broadcasts.</a></li>
<li><a href="#666. Charge-Based Prison Term Prediction with Deep Gating Network.">666. Charge-Based Prison Term Prediction with Deep Gating Network.</a></li>
<li><a href="#667. Restoring ancient text using deep learning: a case study on Greek epigraphy.">667. Restoring ancient text using deep learning: a case study on Greek epigraphy.</a></li>
<li><a href="#668. Embedding Lexical Features via Tensor Decomposition for Small Sample Humor Recognition.">668. Embedding Lexical Features via Tensor Decomposition for Small Sample Humor Recognition.</a></li>
<li><a href="#669. EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.">669. EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.</a></li>
<li><a href="#670. Neural News Recommendation with Multi-Head Self-Attention.">670. Neural News Recommendation with Multi-Head Self-Attention.</a></li>
<li><a href="#671. What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis.">671. What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis.</a></li>
<li><a href="#672. Telling the Whole Story: A Manually Annotated Chinese Dataset for the Analysis of Humor in Jokes.">672. Telling the Whole Story: A Manually Annotated Chinese Dataset for the Analysis of Humor in Jokes.</a></li>
<li><a href="#673. Generating Natural Anagrams: Towards Language Generation Under Hard Combinatorial Constraints.">673. Generating Natural Anagrams: Towards Language Generation Under Hard Combinatorial Constraints.</a></li>
<li><a href="#674. STANCY: Stance Classification Based on Consistency Cues.">674. STANCY: Stance Classification Based on Consistency Cues.</a></li>
<li><a href="#675. Cross-lingual intent classification in a low resource industrial setting.">675. Cross-lingual intent classification in a low resource industrial setting.</a></li>
<li><a href="#676. SoftRegex: Generating Regex from Natural Language Descriptions using Softened Regex Equivalence.">676. SoftRegex: Generating Regex from Natural Language Descriptions using Softened Regex Equivalence.</a></li>
<li><a href="#677. Using Clinical Notes with Time Series Data for ICU Management.">677. Using Clinical Notes with Time Series Data for ICU Management.</a></li>
<li><a href="#678. Spelling-Aware Construction of Macaronic Texts for Teaching Foreign-Language Vocabulary.">678. Spelling-Aware Construction of Macaronic Texts for Teaching Foreign-Language Vocabulary.</a></li>
<li><a href="#679. Towards Machine Reading for Interventions from Humanitarian-Assistance Program Literature.">679. Towards Machine Reading for Interventions from Humanitarian-Assistance Program Literature.</a></li>
<li><a href="#680. RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation.">680. RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation.</a></li>
<li><a href="#681. Context-Aware Conversation Thread Detection in Multi-Party Chat.">681. Context-Aware Conversation Thread Detection in Multi-Party Chat.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="EMNLP-IJCNLP 2019:Hong Kong, China">EMNLP-IJCNLP 2019:Hong Kong, China</h1>
<p><a href="https://aclweb.org/anthology/volumes/D19-1/">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019.</a> Association for Computational Linguistics
<a href="https://dblp.uni-trier.de/db/conf/emnlp/emnlp2019-1.html">DBLP Link</a></p>
<h2 id="Paper Num: 681 || Session Num: 0">Paper Num: 681 || Session Num: 0</h2>
<h3 id="1. Attending to Future Tokens for Bidirectional Sequence Generation.">1. Attending to Future Tokens for Bidirectional Sequence Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1001">Paper Link</a>    Pages:1-10</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/191/6056.html">Carolin Lawrence</a> ; <a href="https://dblp.uni-trier.de/pid/143/7131.html">Bhushan Kotnis</a> ; <a href="https://dblp.uni-trier.de/pid/n/MathiasNiepert.html">Mathias Niepert</a></p>
<p>Abstract:
Neural sequence generation is typically performed token-by-token and left-to-right. Whenever a token is generated only previously produced tokens are taken into consideration. In contrast, for problems such as sequence classification, bidirectional attention, which takes both past and future tokens into consideration, has been shown to perform much better. We propose to make the sequence generation process bidirectional by employing special placeholder tokens. Treated as a node in a fully connected graph, a placeholder token can take past and future tokens into consideration when generating the actual output token. We verify the effectiveness of our approach experimentally on two conversational tasks where the proposed bidirectional model outperforms competitive baselines by a large margin.</p>
<p>Keywords:</p>
<h3 id="2. Attention is not not Explanation.">2. Attention is not not Explanation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1002">Paper Link</a>    Pages:11-20</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/215/4324.html">Sarah Wiegreffe</a> ; <a href="https://dblp.uni-trier.de/pid/153/5384.html">Yuval Pinter</a></p>
<p>Abstract:
Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a models prediction, and consequently reach insights regarding the models decision-making process. A recent paper claims that Attention is not Explanation (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on ones definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they dont perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.</p>
<p>Keywords:</p>
<h3 id="3. Practical Obstacles to Deploying Active Learning.">3. Practical Obstacles to Deploying Active Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1003">Paper Link</a>    Pages:21-30</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/179/4244.html">David Lowell</a> ; <a href="https://dblp.uni-trier.de/pid/142/2839.html">Zachary C. Lipton</a> ; <a href="https://dblp.uni-trier.de/pid/00/8247.html">Byron C. Wallace</a></p>
<p>Abstract:
Active learning (AL) is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. In AL, one iteratively selects training examples for annotation, often those for which the current model is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper, we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice, one does not have the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.</p>
<p>Keywords:</p>
<h3 id="4. Transfer Learning Between Related Tasks Using Expected Label Proportions.">4. Transfer Learning Between Related Tasks Using Expected Label Proportions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1004">Paper Link</a>    Pages:31-42</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7830.html">Matan Ben Noach</a> ; <a href="https://dblp.uni-trier.de/pid/68/5296.html">Yoav Goldberg</a></p>
<p>Abstract:
Deep learning systems thrive on abundance of labeled training data but such data is not always available, calling for alternative methods of supervision. One such method is expectation regularization (XR) (Mann and McCallum, 2007), where models are trained based on expected label proportions. We propose a novel application of the XR framework for transfer learning between related tasks, where knowing the labels of task A provides an estimation of the label proportion of task B. We then use a model trained for A to label a large corpus, and use this corpus with an XR loss to train a model for task B. To make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure. We demonstrate the approach on the task of Aspect-based Sentiment classification, where we effectively use a sentence-level sentiment predictor to train accurate aspect-based predictor. The method improves upon fully supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretraining, as we demonstrate by improving a BERT-based Aspect-based Sentiment model.</p>
<p>Keywords:</p>
<h3 id="5. Knowledge Enhanced Contextual Word Representations.">5. Knowledge Enhanced Contextual Word Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1005">Paper Link</a>    Pages:43-54</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/48/9898.html">Matthew E. Peters</a> ; <a href="https://dblp.uni-trier.de/pid/37/6418.html">Mark Neumann</a> ; <a href="https://dblp.uni-trier.de/pid/210/2652.html">Robert L. Logan IV</a> ; <a href="https://dblp.uni-trier.de/pid/19/376.html">Roy Schwartz</a> ; <a href="https://dblp.uni-trier.de/pid/205/8948.html">Vidur Joshi</a> ; <a href="https://dblp.uni-trier.de/pid/13/3568-1.html">Sameer Singh</a> ; <a href="https://dblp.uni-trier.de/pid/90/5204.html">Noah A. Smith</a></p>
<p>Abstract:
Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBerts runtime is comparable to BERTs and it scales to large KBs.</p>
<p>Keywords:</p>
<h3 id="6. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.">6. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1006">Paper Link</a>    Pages:55-65</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/198/6540.html">Kawin Ethayarajh</a></p>
<p>Abstract:
Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a words contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.</p>
<p>Keywords:</p>
<h3 id="7. Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings.">7. Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1007">Paper Link</a>    Pages:66-76</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/141/5981.html">Philippa Shoemark</a> ; <a href="https://dblp.uni-trier.de/pid/184/5375.html">Farhana Ferdousi Liza</a> ; <a href="https://dblp.uni-trier.de/pid/91/102-2.html">Dong Nguyen</a> ; <a href="https://dblp.uni-trier.de/pid/32/10840.html">Scott A. Hale</a> ; <a href="https://dblp.uni-trier.de/pid/10/8162.html">Barbara McGillivray</a></p>
<p>Abstract:
Word embeddings are increasingly used for the automatic detection of semantic change; yet, a robust evaluation and systematic comparison of the choices involved has been lacking. We propose a new evaluation framework for semantic change detection and find that (i) using the whole time series is preferable over only comparing between the first and last time points; (ii) independently trained and aligned embeddings perform better than continuously trained embeddings for long time periods; and (iii) that the reference point for comparison matters. We also present an analysis of the changes detected on a large Twitter dataset spanning 5.5 years.</p>
<p>Keywords:</p>
<h3 id="8. Correlations between Word Vector Sets.">8. Correlations between Word Vector Sets.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1008">Paper Link</a>    Pages:77-87</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/3062.html">Vitalii Zhelezniak</a> ; <a href="https://dblp.uni-trier.de/pid/220/3024.html">April Shen</a> ; <a href="https://dblp.uni-trier.de/pid/220/3480.html">Daniel Busbridge</a> ; <a href="https://dblp.uni-trier.de/pid/30/8162.html">Aleksandar Savkov</a> ; <a href="https://dblp.uni-trier.de/pid/69/9545.html">Nils Hammerla</a></p>
<p>Abstract:
Similarity measures based purely on word embeddings are comfortably competing with much more sophisticated deep learning and expert-engineered systems on unsupervised semantic textual similarity (STS) tasks. In contrast to commonly used geometric approaches, we treat a single word embedding as e.g. 300 observations from a scalar random variable. Using this paradigm, we first illustrate that similarities derived from elementary pooling operations and classic correlation coefficients yield excellent results on standard STS benchmarks, outperforming many recently proposed methods while being much faster and trivial to implement. Next, we demonstrate how to avoid pooling operations altogether and compare sets of word embeddings directly via correlation operators between reproducing kernel Hilbert spaces. Just like cosine similarity is used to compare individual word vectors, we introduce a novel application of the centered kernel alignment (CKA) as a natural generalisation of squared cosine similarity for sets of word vectors. Likewise, CKA is very easy to implement and enjoys very strong empirical results.</p>
<p>Keywords:</p>
<h3 id="9. Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation.">9. Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1009">Paper Link</a>    Pages:88-99</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/84/10883.html">Rocco Tripodi</a> ; <a href="https://dblp.uni-trier.de/pid/n/RobertoNavigli.html">Roberto Navigli</a></p>
<p>Abstract:
Game-theoretic models, thanks to their intrinsic ability to exploit contextual information, have shown to be particularly suited for the Word Sense Disambiguation task. They represent ambiguous words as the players of a non cooperative game and their senses as the strategies that the players can select in order to play the games. The interaction among the players is modeled with a weighted graph and the payoff as an embedding similarity function, that the players try to maximize. The impact of the word and sense embedding representations in the framework has been tested and analyzed extensively: experiments on standard benchmarks show state-of-art performances and different tests hint at the usefulness of using disambiguation to obtain contextualized word representations.</p>
<p>Keywords:</p>
<h3 id="10. Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog.">10. Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1010">Paper Link</a>    Pages:100-110</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/222/7875.html">Ryuichi Takanobu</a> ; <a href="https://dblp.uni-trier.de/pid/231/1206.html">Hanlin Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/47/6668.html">Minlie Huang</a></p>
<p>Abstract:
Dialog policy decides what and how a task-oriented dialog system will respond, and plays a vital role in delivering effective conversations. Many studies apply Reinforcement Learning to learn a dialog policy with the reward function which requires elaborate design and pre-specified user goals. With the growing needs to handle complex goals across multiple domains, such manually designed reward functions are not affordable to deal with the complexity of real-world tasks. To this end, we propose Guided Dialog Policy Learning, a novel algorithm based on Adversarial Inverse Reinforcement Learning for joint reward estimation and policy optimization in multi-domain task-oriented dialog. The proposed approach estimates the reward signal and infers the user goal in the dialog sessions. The reward estimator evaluates the state-action pairs so that it can guide the dialog policy at each dialog turn. Extensive experiments on a multi-domain dialog dataset show that the dialog policy guided by the learned reward function achieves remarkably higher task success than state-of-the-art baselines.</p>
<p>Keywords:</p>
<h3 id="11. Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots.">11. Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1011">Paper Link</a>    Pages:111-120</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8262.html">Chunyuan Yuan</a> ; <a href="https://dblp.uni-trier.de/pid/69/5011-19.html">Wei Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/60/4522.html">Mingming Li</a> ; <a href="https://dblp.uni-trier.de/pid/207/9988.html">Shangwen Lv</a> ; <a href="https://dblp.uni-trier.de/pid/200/8105.html">Fuqing Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/72/6837.html">Jizhong Han</a> ; <a href="https://dblp.uni-trier.de/pid/67/4108.html">Songlin Hu</a></p>
<p>Abstract:
Multi-turn retrieval-based conversation is an important task for building intelligent dialogue systems. Existing works mainly focus on matching candidate responses with every context utterance on multiple levels of granularity, which ignore the side effect of using excessive context information. Context utterances provide abundant information for extracting more matching features, but it also brings noise signals and unnecessary information. In this paper, we will analyze the side effect of using too many context utterances and propose a multi-hop selector network (MSN) to alleviate the problem. Specifically, MSN firstly utilizes a multi-hop selector to select the relevant utterances as context. Then, the model matches the filtered context with the candidate response and obtains a matching score. Experimental results show that MSN outperforms some state-of-the-art methods on three public multi-turn dialogue datasets.</p>
<p>Keywords:</p>
<h3 id="12. MoEL: Mixture of Empathetic Listeners.">12. MoEL: Mixture of Empathetic Listeners.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1012">Paper Link</a>    Pages:121-132</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/228/9217.html">Zhaojiang Lin</a> ; <a href="https://dblp.uni-trier.de/pid/174/2905.html">Andrea Madotto</a> ; <a href="https://dblp.uni-trier.de/pid/225/5387.html">Jamin Shin</a> ; <a href="https://dblp.uni-trier.de/pid/84/586.html">Peng Xu</a> ; <a href="https://dblp.uni-trier.de/pid/29/4187.html">Pascale Fung</a></p>
<p>Abstract:
Previous research on empathetic dialogue systems has mostly focused on generating responses given certain emotions. However, being empathetic not only requires the ability of generating emotional responses, but more importantly, requires the understanding of user emotions and replying appropriately. In this paper, we propose a novel end-to-end approach for modeling empathy in dialogue systems: Mixture of Empathetic Listeners (MoEL). Our model first captures the user emotions and outputs an emotion distribution. Based on this, MoEL will softly combine the output states of the appropriate Listener(s), which are each optimized to react to certain emotions, and generate an empathetic response. Human evaluations on EMPATHETIC-DIALOGUES dataset confirm that MoEL outperforms multitask training baseline in terms of empathy, relevance, and fluency. Furthermore, the case study on generated responses of different Listeners shows high interpretability of our model.</p>
<p>Keywords:</p>
<h3 id="13. Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever.">13. Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1013">Paper Link</a>    Pages:133-142</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/213/9481.html">Libo Qin</a> ; <a href="https://dblp.uni-trier.de/pid/89/7780.html">Yijia Liu</a> ; <a href="https://dblp.uni-trier.de/pid/98/4640.html">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pid/222/1956.html">Haoyang Wen</a> ; <a href="https://dblp.uni-trier.de/pid/62/8367.html">Yangming Li</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a></p>
<p>Abstract:
Querying the knowledge base (KB) has long been a challenge in the end-to-end task-oriented dialogue system. Previous sequence-to-sequence (Seq2Seq) dialogue generation work treats the KB query as an attention over the entire KB, without the guarantee that the generated entities are consistent with each other. In this paper, we propose a novel framework which queries the KB in two steps to improve the consistency of generated entities. In the first step, inspired by the observation that a response can usually be supported by a single KB row, we introduce a KB retrieval component which explicitly returns the most relevant KB row given a dialogue history. The retrieval result is further used to filter the irrelevant entities in a Seq2Seq response generation model to improve the consistency among the output entities. In the second step, we further perform the attention mechanism to address the most correlated KB column. Two methods are proposed to make the training feasible without labeled retrieval data, which include distant supervision and Gumbel-Softmax technique. Experiments on two publicly available task oriented dialog datasets show the effectiveness of our model by outperforming the baseline systems and producing entity-consistent responses.</p>
<p>Keywords:</p>
<h3 id="14. Building Task-Oriented Visual Dialog Systems Through Alternative Optimization Between Dialog Policy and Language Generation.">14. Building Task-Oriented Visual Dialog Systems Through Alternative Optimization Between Dialog Policy and Language Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1014">Paper Link</a>    Pages:143-153</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/195/5899.html">Mingyang Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/248/8150.html">Josh Arnold</a> ; <a href="https://dblp.uni-trier.de/pid/83/3205.html">Zhou Yu</a></p>
<p>Abstract:
Reinforcement learning (RL) is an effective approach to learn an optimal dialog policy for task-oriented visual dialog systems. A common practice is to apply RL on a neural sequence-to-sequence(seq2seq) framework with the action space being the output vocabulary in the decoder. However, it is difficult to design a reward function that can achieve a balance between learning an effective policy and generating a natural dialog response. This paper proposes a novel framework that alternatively trains a RL policy for image guessing and a supervised seq2seq model to improve dialog generation quality. We evaluate our framework on the GuessWhich task and the framework achieves the state-of-the-art performance in both task completion and dialog quality.</p>
<p>Keywords:</p>
<h3 id="15. DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation.">15. DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1015">Paper Link</a>    Pages:154-164</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9407.html">Deepanway Ghosal</a> ; <a href="https://dblp.uni-trier.de/pid/198/3608.html">Navonil Majumder</a> ; <a href="https://dblp.uni-trier.de/pid/116/4904.html">Soujanya Poria</a> ; <a href="https://dblp.uni-trier.de/pid/83/9923.html">Niyati Chhaya</a> ; <a href="https://dblp.uni-trier.de/pid/g/AlexanderFGelbukh.html">Alexander F. Gelbukh</a></p>
<p>Abstract:
Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets.</p>
<p>Keywords:</p>
<h3 id="16. Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations.">16. Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1016">Paper Link</a>    Pages:165-176</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/159/2744.html">Peixiang Zhong</a> ; <a href="https://dblp.uni-trier.de/pid/18/5410-4.html">Di Wang</a> ; <a href="https://dblp.uni-trier.de/pid/m/ChunyanMiao.html">Chunyan Miao</a></p>
<p>Abstract:
Messages in human conversations inherently convey emotions. The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks. However, enabling machines to analyze emotions in conversations is challenging, partly because humans often rely on the context and commonsense knowledge to express emotions. In this paper, we address these challenges by proposing a Knowledge-Enriched Transformer (KET), where contextual utterances are interpreted using hierarchical self-attention and external commonsense knowledge is dynamically leveraged using a context-aware affective graph attention mechanism. Experiments on multiple textual conversation datasets demonstrate that both context and commonsense knowledge are consistently beneficial to the emotion detection performance. In addition, the experimental results show that our KET model outperforms the state-of-the-art models on most of the tested datasets in F1 score.</p>
<p>Keywords:</p>
<h3 id="17. Interpretable Relevant Emotion Ranking with Event-Driven Attention.">17. Interpretable Relevant Emotion Ranking with Event-Driven Attention.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1017">Paper Link</a>    Pages:177-187</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/48/450.html">Yang Yang</a> ; <a href="https://dblp.uni-trier.de/pid/79/2854.html">Deyu Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/75/5430.html">Yulan He</a> ; <a href="https://dblp.uni-trier.de/pid/04/6901.html">Meng Zhang</a></p>
<p>Abstract:
Multiple emotions with different intensities are often evoked by events described in documents. Oftentimes, such event information is hidden and needs to be discovered from texts. Unveiling the hidden event information can help to understand how the emotions are evoked and provide explainable results. However, existing studies often ignore the latent event information. In this paper, we proposed a novel interpretable relevant emotion ranking model with the event information incorporated into a deep learning architecture using the event-driven attentions. Moreover, corpus-level event embeddings and document-level event distributions are introduced respectively to consider the global events in corpus and the document-specific events simultaneously. Experimental results on three real-world corpora show that the proposed approach performs remarkably better than the state-of-the-art emotion detection approaches and multi-label approaches. Moreover, interpretable results can be obtained to shed light on the events which trigger certain emotions.</p>
<p>Keywords:</p>
<h3 id="18. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects.">18. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1018">Paper Link</a>    Pages:188-197</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/161/2449.html">Jianmo Ni</a> ; <a href="https://dblp.uni-trier.de/pid/18/5576.html">Jiacheng Li</a> ; <a href="https://dblp.uni-trier.de/pid/29/3483.html">Julian J. McAuley</a></p>
<p>Abstract:
Several recent works have considered the problem of generating reviews (or tips) as a form of explanation as to why a recommendation might match a customers interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users decision-making process. We seek to introduce new datasets and methods to address the recommendation justification task. In terms of data, we first propose an extractive approach to identify review segments which justify users intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.</p>
<p>Keywords:</p>
<h3 id="19. Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning.">19. Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1019">Paper Link</a>    Pages:198-207</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/30/11037.html">Kaisong Song</a> ; <a href="https://dblp.uni-trier.de/pid/53/6625.html">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pid/28/2073-1.html">Wei Gao</a> ; <a href="https://dblp.uni-trier.de/pid/55/1226.html">Jun Lin</a> ; <a href="https://dblp.uni-trier.de/pid/222/7936.html">Lujun Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/34/10219.html">Jiancheng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/212/2092.html">Changlong Sun</a> ; <a href="https://dblp.uni-trier.de/pid/11/6389.html">Xiaozhong Liu</a> ; <a href="https://dblp.uni-trier.de/pid/52/323-1.html">Qi Zhang</a></p>
<p>Abstract:
Customers ask questions and customer service staffs answer their questions, which is the basic service model via multi-turn customer service (CS) dialogues on E-commerce platforms. Existing studies fail to provide comprehensive service satisfaction analysis, namely satisfaction polarity classification (e.g., well satisfied, met and unsatisfied) and sentimental utterance identification (e.g., positive, neutral and negative). In this paper, we conduct a pilot study on the task of service satisfaction analysis (SSA) based on multi-turn CS dialogues. We propose an extensible Context-Assisted Multiple Instance Learning (CAMIL) model to predict the sentiments of all the customer utterances and then aggregate those sentiments into service satisfaction polarity. After that, we propose a novel Context Clue Matching Mechanism (CCMM) to enhance the representations of all customer utterances with their matched context clues, i.e., sentiment and reasoning clues. We construct two CS dialogue datasets from a top E-commerce platform. Extensive experimental results are presented and contrasted against a few previous models to demonstrate the efficacy of our model.</p>
<p>Keywords:</p>
<h3 id="20. Leveraging Dependency Forest for Neural Medical Relation Extraction.">20. Leveraging Dependency Forest for Neural Medical Relation Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1020">Paper Link</a>    Pages:208-218</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/136/3610.html">Linfeng Song</a> ; <a href="https://dblp.uni-trier.de/pid/47/722-4.html">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/51/844.html">Daniel Gildea</a> ; <a href="https://dblp.uni-trier.de/pid/32/7445.html">Mo Yu</a> ; <a href="https://dblp.uni-trier.de/pid/80/709.html">Zhiguo Wang</a> ; <a href="https://dblp.uni-trier.de/pid/05/9013.html">Jinsong Su</a></p>
<p>Abstract:
Medical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1-best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain more than one possible decisions and therefore have higher recall but more noise compared with 1-best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature.</p>
<p>Keywords:</p>
<h3 id="21. Open Relation Extraction: Relational Knowledge Transfer from Supervised Data to Unsupervised Data.">21. Open Relation Extraction: Relational Knowledge Transfer from Supervised Data to Unsupervised Data.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1021">Paper Link</a>    Pages:219-228</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/224/4293.html">Ruidong Wu</a> ; <a href="https://dblp.uni-trier.de/pid/25/4120.html">Yuan Yao</a> ; <a href="https://dblp.uni-trier.de/pid/19/3011-7.html">Xu Han</a> ; <a href="https://dblp.uni-trier.de/pid/178/8590.html">Ruobing Xie</a> ; <a href="https://dblp.uni-trier.de/pid/53/3245-1.html">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/62/1512.html">Fen Lin</a> ; <a href="https://dblp.uni-trier.de/pid/218/7323.html">Leyu Lin</a> ; <a href="https://dblp.uni-trier.de/pid/95/3291.html">Maosong Sun</a></p>
<p>Abstract:
Open relation extraction (OpenRE) aims to extract relational facts from the open-domain corpus. To this end, it discovers relation patterns between named entities and then clusters those semantically equivalent patterns into a united relation cluster. Most OpenRE methods typically confine themselves to unsupervised paradigms, without taking advantage of existing relational facts in knowledge bases (KBs) and their high-quality labeled instances. To address this issue, we propose Relational Siamese Networks (RSNs) to learn similarity metrics of relations from labeled data of pre-defined relations, and then transfer the relational knowledge to identify novel relations in unlabeled data. Experiment results on two real-world datasets show that our framework can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at <a href="https://github.com/thunlp/RSN">https://github.com/thunlp/RSN</a>.</p>
<p>Keywords:</p>
<h3 id="22. Improving Relation Extraction with Knowledge-attention.">22. Improving Relation Extraction with Knowledge-attention.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1022">Paper Link</a>    Pages:229-239</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/10/1749.html">Pengfei Li</a> ; <a href="https://dblp.uni-trier.de/pid/m/KezhiMao.html">Kezhi Mao</a> ; <a href="https://dblp.uni-trier.de/pid/90/147.html">Xuefeng Yang</a> ; <a href="https://dblp.uni-trier.de/pid/181/2688.html">Qi Li</a></p>
<p>Abstract:
While attention mechanisms have been proven to be effective in many NLP tasks, majority of them are data-driven. We propose a novel knowledge-attention encoder which incorporates prior knowledge from external lexical resources into deep neural networks for relation extraction task. Furthermore, we present three effective ways of integrating knowledge-attention with self-attention to maximize the utilization of both knowledge and data. The proposed relation extraction system is end-to-end and fully attention-based. Experiment results show that the proposed knowledge-attention mechanism has complementary strengths with self-attention, and our integrated models outperform existing CNN, RNN, and self-attention based models. State-of-the-art performance is achieved on TACRED, a complex and large-scale relation extraction dataset.</p>
<p>Keywords:</p>
<h3 id="23. Jointly Learning Entity and Relation Representations for Entity Alignment.">23. Jointly Learning Entity and Relation Representations for Entity Alignment.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1023">Paper Link</a>    Pages:240-249</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/09/10409.html">Yuting Wu</a> ; <a href="https://dblp.uni-trier.de/pid/82/1364.html">Xiao Liu</a> ; <a href="https://dblp.uni-trier.de/pid/25/2643.html">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pid/w/ZhengWang1.html">Zheng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/63/1870.html">Dongyan Zhao</a></p>
<p>Abstract:
Entity alignment is a viable means for integrating heterogeneous knowledge among different knowledge graphs (KGs). Recent developments in the field often take an embedding-based approach to model the structural information of KGs so that entity alignment can be easily performed in the embedding space. However, most existing works do not explicitly utilize useful relation representations to assist in entity alignment, which, as we will show in the paper, is a simple yet effective way for improving entity alignment. This paper presents a novel joint learning framework for entity alignment. At the core of our approach is a Graph Convolutional Network (GCN) based framework for learning both entity and relation representations. Rather than relying on pre-aligned relation seeds to learn relation representations, we first approximate them using entity embeddings learned by the GCN. We then incorporate the relation approximation into entities to iteratively learn better representations for both. Experiments performed on three real-world cross-lingual datasets show that our approach substantially outperforms state-of-the-art entity alignment methods.</p>
<p>Keywords:</p>
<h3 id="24. Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion.">24. Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1024">Paper Link</a>    Pages:250-260</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/148/9655.html">Zihao Wang</a> ; <a href="https://dblp.uni-trier.de/pid/194/5270.html">Kwun Ping Lai</a> ; <a href="https://dblp.uni-trier.de/pid/77/8278.html">Piji Li</a> ; <a href="https://dblp.uni-trier.de/pid/53/6625.html">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pid/48/1707.html">Wai Lam</a></p>
<p>Abstract:
For large-scale knowledge graphs (KGs), recent research has been focusing on the large proportion of infrequent relations which have been ignored by previous studies. For example few-shot learning paradigm for relations has been investigated. In this work, we further advocate that handling uncommon entities is inevitable when dealing with infrequent relations. Therefore, we propose a meta-learning framework that aims at handling infrequent relations with few-shot learning and uncommon entities by using textual descriptions. We design a novel model to better extract key information from textual descriptions. Besides, we also develop a novel generative model in our framework to enhance the performance by generating extra triplets during the training stage. Experiments are conducted on two datasets from real-world KGs, and the results show that our framework outperforms previous methods when dealing with infrequent relations and their accompanying uncommon entities.</p>
<p>Keywords:</p>
<h3 id="25. Low-Resource Name Tagging Learned with Weakly Labeled Data.">25. Low-Resource Name Tagging Learned with Weakly Labeled Data.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1025">Paper Link</a>    Pages:261-270</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/20/8038-2.html">Yixin Cao</a> ; <a href="https://dblp.uni-trier.de/pid/224/6013.html">Zikun Hu</a> ; <a href="https://dblp.uni-trier.de/pid/24/6606.html">Tat-Seng Chua</a> ; <a href="https://dblp.uni-trier.de/pid/53/3245-1.html">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/61/2408.html">Heng Ji</a></p>
<p>Abstract:
Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively: (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two modules are combined via shared parameters. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6% and 7.8% F1 gains on average) as well as efficiency.</p>
<p>Keywords:</p>
<h3 id="26. Learning Dynamic Context Augmentation for Global Entity Linking.">26. Learning Dynamic Context Augmentation for Global Entity Linking.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1026">Paper Link</a>    Pages:271-281</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/7255.html">Xiyuan Yang</a> ; <a href="https://dblp.uni-trier.de/pid/190/5278.html">Xiaotao Gu</a> ; <a href="https://dblp.uni-trier.de/pid/81/6595.html">Sheng Lin</a> ; <a href="https://dblp.uni-trier.de/pid/44/5693.html">Siliang Tang</a> ; <a href="https://dblp.uni-trier.de/pid/218/7793.html">Yueting Zhuang</a> ; <a href="https://dblp.uni-trier.de/pid/84/3254-1.html">Fei Wu</a> ; <a href="https://dblp.uni-trier.de/pid/96/6090.html">Zhigang Chen</a> ; <a href="https://dblp.uni-trier.de/pid/59/5304.html">Guoping Hu</a> ; <a href="https://dblp.uni-trier.de/pid/36/360.html">Xiang Ren</a></p>
<p>Abstract:
Despite of the recent success of collective entity linking (EL) methods, these global inference methods may yield sub-optimal results when the all-mention coherence assumption breaks, and often suffer from high computational cost at the inference stage, due to the complex search space. In this paper, we propose a simple yet effective solution, called Dynamic Context Augmentation (DCA), for collective EL, which requires only one pass through the mentions in a document. DCA sequentially accumulates context information to make efficient, collective inference, and can cope with different local EL models as a plug-and-enhance module. We explore both supervised and reinforcement learning strategies for learning the DCA model. Extensive experiments show the effectiveness of our model with different learning settings, base models, decision orders and attention mechanisms.</p>
<p>Keywords:</p>
<h3 id="27. Open Event Extraction from Online Text using a Generative Adversarial Network.">27. Open Event Extraction from Online Text using a Generative Adversarial Network.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1027">Paper Link</a>    Pages:282-291</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/06/2293-43.html">Rui Wang</a> ; <a href="https://dblp.uni-trier.de/pid/79/2854.html">Deyu Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/75/5430.html">Yulan He</a></p>
<p>Abstract:
To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15% is observed in F-measure.</p>
<p>Keywords:</p>
<h3 id="28. Learning to Bootstrap for Entity Set Expansion.">28. Learning to Bootstrap for Entity Set Expansion.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1028">Paper Link</a>    Pages:292-301</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8048.html">Lingyong Yan</a> ; <a href="https://dblp.uni-trier.de/pid/57/2368.html">Xianpei Han</a> ; <a href="https://dblp.uni-trier.de/pid/78/5897-1.html">Le Sun</a> ; <a href="https://dblp.uni-trier.de/pid/27/1201.html">Ben He</a></p>
<p>Abstract:
Bootstrapping for Entity Set Expansion (ESE) aims at iteratively acquiring new instances of a specific target category. Traditional bootstrapping methods often suffer from two problems: 1) delayed feedback, i.e., the pattern evaluation relies on both its direct extraction quality and extraction quality in later iterations. 2) sparse supervision, i.e., only few seed entities are used as the supervision. To address the above two problems, we propose a novel bootstrapping method combining the Monte Carlo Tree Search (MCTS) algorithm with a deep similarity network, which can efficiently estimate delayed feedback for pattern evaluation and adaptively score entities given sparse supervision signals. Experimental results confirm the effectiveness of the proposed method.</p>
<p>Keywords:</p>
<h3 id="29. Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Text.">29. Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Text.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1029">Paper Link</a>    Pages:302-312</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/207/5395.html">Tianwen Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/94/6503-3.html">Tong Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/86/5934.html">Bing Qin</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pid/c/NiteshVChawla.html">Nitesh V. Chawla</a> ; <a href="https://dblp.uni-trier.de/pid/69/339-1.html">Meng Jiang</a></p>
<p>Abstract:
Condition is essential in scientific statement. Without the conditions (e.g., equipment, environment) that were precisely specified, facts (e.g., observations) in the statements may no longer be valid. Existing ScienceIE methods, which aim at extracting factual tuples from scientific text, do not consider the conditions. In this work, we propose a new sequence labeling framework (as well as a new tag schema) to jointly extract the fact and condition tuples from statement sentences. The framework has (1) a multi-output module to generate one or multiple tuples and (2) a multi-input module to feed in multiple types of signals as sequences. It improves F1 score relatively by 4.2% on BioNLP2013 and by 6.2% on a new bio-text dataset for tuple extraction.</p>
<p>Keywords:</p>
<h3 id="30. Cross-lingual Structure Transfer for Relation and Event Extraction.">30. Cross-lingual Structure Transfer for Relation and Event Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1030">Paper Link</a>    Pages:313-325</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/179/2367.html">Ananya Subburathinam</a> ; <a href="https://dblp.uni-trier.de/pid/70/5794.html">Di Lu</a> ; <a href="https://dblp.uni-trier.de/pid/61/2408.html">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pid/00/4758.html">Jonathan May</a> ; <a href="https://dblp.uni-trier.de/pid/c/ShihFuChang.html">Shih-Fu Chang</a> ; <a href="https://dblp.uni-trier.de/pid/07/10489.html">Avirup Sil</a> ; <a href="https://dblp.uni-trier.de/pid/41/3792.html">Clare R. Voss</a></p>
<p>Abstract:
The identification of complex semantic structures such as events and entity relations, already a challenging Information Extraction task, is doubly difficult from sources written in under-resourced and under-annotated languages. We investigate the suitability of cross-lingual structure transfer techniques for these tasks. We exploit relation- and event-relevant language-universal features, leveraging both symbolic (including part-of-speech and dependency path) and distributional (including type representation and contextualized representation) information. By representing all entity mentions, event triggers, and contexts into this complex and structured multilingual common space, using graph convolutional networks, we can train a relation or event extractor from source language annotations and apply it to the target language. Extensive experiments on cross-lingual relation and event transfer among English, Chinese, and Arabic demonstrate that our approach achieves performance comparable to state-of-the-art supervised models trained on up to 3,000 manually annotated mentions: up to 62.6% F-score for Relation Extraction, and 63.1% F-score for Event Argument Role Labeling. The event argument role labeling model transferred from English to Chinese achieves similar performance as the model trained from Chinese. We thus find that language-universal symbolic and distributional representations are complementary for cross-lingual structure transfer.</p>
<p>Keywords:</p>
<h3 id="31. Uncover the Ground-Truth Relations in Distant Supervision: A Neural Expectation-Maximization Framework.">31. Uncover the Ground-Truth Relations in Distant Supervision: A Neural Expectation-Maximization Framework.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1031">Paper Link</a>    Pages:326-336</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8207.html">Junfan Chen</a> ; <a href="https://dblp.uni-trier.de/pid/61/1229.html">Richong Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/86/2933.html">Yongyi Mao</a> ; <a href="https://dblp.uni-trier.de/pid/08/3722.html">Hongyu Guo</a> ; <a href="https://dblp.uni-trier.de/pid/37/5126-7.html">Jie Xu</a></p>
<p>Abstract:
Distant supervision for relation extraction enables one to effectively acquire structured relations out of very large text corpora with less human efforts. Nevertheless, most of the prior-art models for such tasks assume that the given text can be noisy, but their corresponding labels are clean. Such unrealistic assumption is contradictory with the fact that the given labels are often noisy as well, thus leading to significant performance degradation of those models on real-world data. To cope with this challenge, we propose a novel label-denoising framework that combines neural network with probabilistic modelling, which naturally takes into account the noisy labels during learning. We empirically demonstrate that our approach significantly improves the current art in uncovering the ground-truth relation labels.</p>
<p>Keywords:</p>
<h3 id="32. Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction.">32. Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1032">Paper Link</a>    Pages:337-346</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/179/2615.html">Shun Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/54/6265.html">Wei Cao</a> ; <a href="https://dblp.uni-trier.de/pid/32/1213.html">Wei Xu</a> ; <a href="https://dblp.uni-trier.de/pid/09/851.html">Jiang Bian</a></p>
<p>Abstract:
Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at <a href="https://github.com/dolphin-zs/Doc2EDAG">https://github.com/dolphin-zs/Doc2EDAG</a>.</p>
<p>Keywords:</p>
<h3 id="33. Event Detection with Trigger-Aware Lattice Neural Network.">33. Event Detection with Trigger-Aware Lattice Neural Network.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1033">Paper Link</a>    Pages:347-356</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/04/4910.html">Ning Ding</a> ; <a href="https://dblp.uni-trier.de/pid/187/3454.html">Ziran Li</a> ; <a href="https://dblp.uni-trier.de/pid/53/3245-1.html">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/20/134.html">Haitao Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/254/8299.html">Zibo Lin</a></p>
<p>Abstract:
Event detection (ED) aims to locate trigger words in raw text and then classify them into correct event types. In this task, neural net- work based models became mainstream in re- cent years. However, two problems arise when it comes to languages without natural delim- iters, such as Chinese. First, word-based mod- els severely suffer from the problem of word- trigger mismatch, limiting the performance of the methods. In addition, even if trigger words could be accurately located, the ambi- guity of polysemy of triggers could still af- fect the trigger classification stage. To ad- dress the two issues simultaneously, we pro- pose the Trigger-aware Lattice Neural Net- work (TLNN). (1) The framework dynami- cally incorporates word and character informa- tion so that the trigger-word mismatch issue can be avoided. (2) Moreover, for polysemous characters and words, we model all senses of them with the help of an external linguistic knowledge base, so as to alleviate the prob- lem of ambiguous triggers. Experiments on two benchmark datasets show that our model could effectively tackle the two issues and outperforms previous state-of-the-art methods significantly, giving the best results. The source code of this paper can be obtained from <a href="https://github.com/thunlp/TLNN">https://github.com/thunlp/TLNN</a>.</p>
<p>Keywords:</p>
<h3 id="34. A Boundary-aware Neural Model for Nested Named Entity Recognition.">34. A Boundary-aware Neural Model for Nested Named Entity Recognition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1034">Paper Link</a>    Pages:357-366</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8208.html">Changmeng Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/58/3467.html">Yi Cai</a> ; <a href="https://dblp.uni-trier.de/pid/195/3062.html">Jingyun Xu</a> ; <a href="https://dblp.uni-trier.de/pid/l/HofungLeung.html">Ho-fung Leung</a> ; <a href="https://dblp.uni-trier.de/pid/59/2340.html">Guandong Xu</a></p>
<p>Abstract:
In natural language processing, it is common that many entities contain other entities inside them. Most existing works on named entity recognition (NER) only deal with flat entities but ignore nested ones. We propose a boundary-aware neural model for nested NER which leverages entity boundaries to predict entity categorical labels. Our model can locate entities precisely by detecting boundaries using sequence labeling models. Based on the detected boundaries, our model utilizes the boundary-relevant regions to predict entity categorical labels, which can decrease computation cost and relieve error propagation problem in layered sequence labeling model. We introduce multitask learning to capture the dependencies of entity boundaries and their categorical labels, which helps to improve the performance of identifying entities. We conduct our experiments on GENIA dataset and the experimental results demonstrate that our model outperforms other state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="35. Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning.">35. Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1035">Paper Link</a>    Pages:367-377</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/135/6112.html">Xiangrong Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/136/8650.html">Shizhu He</a> ; <a href="https://dblp.uni-trier.de/pid/133/1954.html">Daojian Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/42/4903.html">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/21/5679.html">Shengping Liu</a> ; <a href="https://dblp.uni-trier.de/pid/47/2026-1.html">Jun Zhao</a></p>
<p>Abstract:
The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works didnt consider the extraction order of relational facts in a sentence. In this paper we argue that the extraction order is important in this task. To take the extraction order into consideration, we apply the reinforcement learning into a sequence-to-sequence model. The proposed model could generate relational facts freely. Widely conducted experiments on two public datasets demonstrate the efficacy of the proposed method.</p>
<p>Keywords:</p>
<h3 id="36. CaRe: Open Knowledge Graph Embeddings.">36. CaRe: Open Knowledge Graph Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1036">Paper Link</a>    Pages:378-388</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/170/1981.html">Swapnil Gupta</a> ; <a href="https://dblp.uni-trier.de/pid/83/5743.html">Sreyash Kenkre</a> ; <a href="https://dblp.uni-trier.de/pid/86/687.html">Partha P. Talukdar</a></p>
<p>Abstract:
Open Information Extraction (OpenIE) methods are effective at extracting (noun phrase, relation phrase, noun phrase) triples from text, e.g., (Barack Obama, took birth in, Honolulu). Organization of such triples in the form of a graph with noun phrases (NPs) as nodes and relation phrases (RPs) as edges results in the construction of Open Knowledge Graphs (OpenKGs). In order to use such OpenKGs in downstream tasks, it is often desirable to learn embeddings of the NPs and RPs present in the graph. Even though several Knowledge Graph (KG) embedding methods have been recently proposed, all of those methods have targeted Ontological KGs, as opposed to OpenKGs. Straightforward application of existing Ontological KG embedding methods to OpenKGs is challenging, as unlike Ontological KGs, OpenKGs are not canonicalized, i.e., a real-world entity may be represented using multiple nodes in the OpenKG, with each node corresponding to a different NP referring to the entity. For example, nodes with labels Barack Obama, Obama, and President Obama may refer to the same real-world entity Barack Obama. Even though canonicalization of OpenKGs has received some attention lately, output of such methods has not been used to improve OpenKG embed- dings. We fill this gap in the paper and propose Canonicalization-infused Representations (CaRe) for OpenKGs. Through extensive experiments, we observe that CaRe enables existing models to adapt to the challenges in OpenKGs and achieve substantial improvements for the link prediction task.</p>
<p>Keywords:</p>
<h3 id="37. Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly Supervised Relation Extraction.">37. Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly Supervised Relation Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1037">Paper Link</a>    Pages:389-398</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/165/4867.html">Yuyun Huang</a> ; <a href="https://dblp.uni-trier.de/pid/61/2112.html">Jinhua Du</a></p>
<p>Abstract:
Distance supervision is widely used in relation extraction tasks, particularly when large-scale manual annotations are virtually impossible to conduct. Although Distantly Supervised Relation Extraction (DSRE) benefits from automatic labelling, it suffers from serious mislabelling issues, i.e. some or all of the instances for an entity pair (head and tail entities) do not express the labelled relation. In this paper, we propose a novel model that employs a collaborative curriculum learning framework to reduce the effects of mislabelled data. Specifically, we firstly propose an internal self-attention mechanism between the convolution operations in convolutional neural networks (CNNs) to learn a better sentence representation from the noisy inputs. Then we define two sentence selection models as two relation extractors in order to collaboratively learn and regularise each other under a curriculum scheme to alleviate noisy effects, where the curriculum could be constructed by conflicts or small loss. Finally, experiments are conducted on a widely-used public dataset and the results indicate that the proposed model significantly outperforms baselines including the state-of-the-art in terms of P@N and PR curve metrics, thus evidencing its capability of reducing noisy effects for DSRE.</p>
<p>Keywords:</p>
<h3 id="38. Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping.">38. Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1038">Paper Link</a>    Pages:399-409</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/75/2470.html">Jian Ni</a> ; <a href="https://dblp.uni-trier.de/pid/91/663.html">Radu Florian</a></p>
<p>Abstract:
Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-of-the-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor language. In this paper, we propose a new approach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language, so that a well-trained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good performance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs.</p>
<p>Keywords:</p>
<h3 id="39. Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation Extraction.">39. Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1039">Paper Link</a>    Pages:410-420</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/95/4545.html">Xiang Deng</a> ; <a href="https://dblp.uni-trier.de/pid/33/2952.html">Huan Sun</a></p>
<p>Abstract:
Distant supervision (DS) has been widely used to automatically construct (noisy) labeled data for relation extraction (RE). Given two entities, distant supervision exploits sentences that directly mention them for predicting their semantic relation. We refer to this strategy as 1-hop DS, which unfortunately may not work well for long-tail entities with few supporting sentences. In this paper, we introduce a new strategy named 2-hop DS to enhance distantly supervised RE, based on the observation that there exist a large number of relational tables on the Web which contain entity pairs that share common relations. We refer to such entity pairs as anchors for each other, and collect all sentences that mention the anchor entity pairs of a given target entity pair to help relation prediction. We develop a new neural RE method REDS2 in the multi-instance learning paradigm, which adopts a hierarchical model structure to fuse information respectively from 1-hop DS and 2-hop DS. Extensive experimental results on a benchmark dataset show that REDS2 can consistently outperform various baselines across different settings by a substantial margin.</p>
<p>Keywords:</p>
<h3 id="40. EntEval: A Holistic Evaluation Benchmark for Entity Representations.">40. EntEval: A Holistic Evaluation Benchmark for Entity Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1040">Paper Link</a>    Pages:421-433</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/2003.html">Mingda Chen</a> ; <a href="https://dblp.uni-trier.de/pid/169/6786.html">Zewei Chu</a> ; <a href="https://dblp.uni-trier.de/pid/48/4792.html">Yang Chen</a> ; <a href="https://dblp.uni-trier.de/pid/07/11293.html">Karl Stratos</a> ; <a href="https://dblp.uni-trier.de/pid/47/1252.html">Kevin Gimpel</a></p>
<p>Abstract:
Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this work, we propose EntEval: a test suite of diverse tasks that require nontrivial understanding of entities including entity typing, entity similarity, entity relation prediction, and entity disambiguation. In addition, we develop training techniques for learning better entity representations by using natural hyperlink annotations in Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018) and show that they improve strong baselines on multiple EntEval tasks.</p>
<p>Keywords:</p>
<h3 id="41. Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction.">41. Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1041">Paper Link</a>    Pages:434-444</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/228/5627.html">Rujun Han</a> ; <a href="https://dblp.uni-trier.de/pid/127/1927.html">Qiang Ning</a> ; <a href="https://dblp.uni-trier.de/pid/117/4036.html">Nanyun Peng</a></p>
<p>Abstract:
We propose a joint event and temporal relation extraction model with shared representation learning and structured prediction. The proposed method has two advantages over existing work. First, it improves event representation by allowing the event and relation modules to share the same contextualized embeddings and neural representation learner. Second, it avoids error propagation in the conventional pipeline systems by leveraging structured inference and learning methods to assign both the event labels and the temporal relation labels jointly. Experiments show that the proposed method can improve both event extraction and temporal relation extraction over state-of-the-art systems, with the end-to-end F1 improved by 10% and 6.8% on two benchmark datasets respectively.</p>
<p>Keywords:</p>
<h3 id="42. Hierarchical Text Classification with Reinforced Label Assignment.">42. Hierarchical Text Classification with Reinforced Label Assignment.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1042">Paper Link</a>    Pages:445-455</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/178/3692.html">Yuning Mao</a> ; <a href="https://dblp.uni-trier.de/pid/150/2507.html">Jingjing Tian</a> ; <a href="https://dblp.uni-trier.de/pid/h/JiaweiHan.html">Jiawei Han</a> ; <a href="https://dblp.uni-trier.de/pid/36/360.html">Xiang Ren</a></p>
<p>Abstract:
While existing hierarchical text classification (HTC) methods attempt to capture label hierarchies for model training, they either make local decisions regarding each label or completely ignore the hierarchy information during inference. To solve the mismatch between training and inference as well as modeling label dependencies in a more principled way, we formulate HTC as a Markov decision process and propose to learn a Label Assignment Policy via deep reinforcement learning to determine where to place an object and when to stop the assignment process. The proposed method, HiLAP, explores the hierarchy during both training and inference time in a consistent manner and makes inter-dependent decisions. As a general framework, HiLAP can incorporate different neural encoders as base models for end-to-end training. Experiments on five public datasets and four base models show that HiLAP yields an average improvement of 33.4% in Macro-F1 over flat classifiers and outperforms state-of-the-art HTC methods by a large margin. Data and code can be found at <a href="https://github.com/morningmoni/HiLAP">https://github.com/morningmoni/HiLAP</a>.</p>
<p>Keywords:</p>
<h3 id="43. Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification.">43. Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1043">Paper Link</a>    Pages:456-465</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8020.html">Chunning Du</a> ; <a href="https://dblp.uni-trier.de/pid/00/11044.html">Haifeng Sun</a> ; <a href="https://dblp.uni-trier.de/pid/37/2749-1.html">Jingyu Wang</a> ; <a href="https://dblp.uni-trier.de/pid/80/6406-1.html">Qi Qi</a> ; <a href="https://dblp.uni-trier.de/pid/60/4951.html">Jianxin Liao</a> ; <a href="https://dblp.uni-trier.de/pid/42/675.html">Chun Wang</a> ; <a href="https://dblp.uni-trier.de/pid/53/5636.html">Bing Ma</a></p>
<p>Abstract:
As an essential component of natural language processing, text classification relies on deep learning in recent years. Various neural networks are designed for text classification on the basis of word embedding. However, polysemy is a fundamental feature of the natural language, which brings challenges to text classification. One polysemic word contains more than one sense, while the word embedding procedure conflates different senses of a polysemic word into a single vector. Extracting the distinct representation for the specific sense could thus lead to fine-grained models with strong generalization ability. It has been demonstrated that multiple senses of a word actually reside in linear superposition within the word embedding so that specific senses can be extracted from the original word embedding. Therefore, we propose to use capsule networks to construct the vectorized representation of semantics and utilize hyperplanes to decompose each capsule to acquire the specific senses. A novel dynamic routing mechanism named routing-on-hyperplane will select the proper sense for the downstream classification task. Our model is evaluated on 6 different datasets, and the experimental results show that our model is capable of extracting more discriminative semantic features and yields a significant performance gain compared to other baseline methods.</p>
<p>Keywords:</p>
<h3 id="44. Label-Specific Document Representation for Multi-Label Text Classification.">44. Label-Specific Document Representation for Multi-Label Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1044">Paper Link</a>    Pages:466-475</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/98/4025.html">Lin Xiao</a> ; <a href="https://dblp.uni-trier.de/pid/98/5766.html">Xin Huang</a> ; <a href="https://dblp.uni-trier.de/pid/143/5757.html">Boli Chen</a> ; <a href="https://dblp.uni-trier.de/pid/54/2770.html">Liping Jing</a></p>
<p>Abstract:
Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper, we propose a Label-Specific Attention Network (LSAN) to learn a label-specific document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile, the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to seamlessly integrate the above two parts, an adaptive fusion strategy is proposed, which can effectively output the comprehensive label-specific document representation to build multi-label text classifier. Extensive experimental results demonstrate that LSAN consistently outperforms the state-of-the-art methods on four different datasets, especially on the prediction of low-frequency labels. The code and hyper-parameter settings are released to facilitate other researchers.</p>
<p>Keywords:</p>
<h3 id="45. Hierarchical Attention Prototypical Networks for Few-Shot Text Classification.">45. Hierarchical Attention Prototypical Networks for Few-Shot Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1045">Paper Link</a>    Pages:476-485</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/76/8755.html">Shengli Sun</a> ; <a href="https://dblp.uni-trier.de/pid/194/5100.html">Qingfeng Sun</a> ; <a href="https://dblp.uni-trier.de/pid/244/5409.html">Kevin Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/254/8010.html">Tengchao Lv</a></p>
<p>Abstract:
Most of the current effective methods for text classification tasks are based on large-scale labeled data and a great number of parameters, but when the supervised training data are few and difficult to be collected, these models are not available. In this work, we propose a hierarchical attention prototypical networks (HAPN) for few-shot text classification. We design the feature level, word level, and instance level multi cross attention for our model to enhance the expressive ability of semantic space, so it can highlight or weaken the importance of the features, words, and instances separately. We verify the effectiveness of our model on two standard benchmark few-shot text classification datasetsFewRel and CSID, and achieve the state-of-the-art performance. The visualization of hierarchical attention layers illustrates that our model can capture more important features, words, and instances. In addition, our attention mechanism increases support set augmentability and accelerates convergence speed in the training stage.</p>
<p>Keywords:</p>
<h3 id="46. Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification.">46. Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1046">Paper Link</a>    Pages:486-495</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/173/9215.html">Vivian Lai</a> ; <a href="https://dblp.uni-trier.de/pid/44/4184.html">Zheng Cai</a> ; <a href="https://dblp.uni-trier.de/pid/95/8314.html">Chenhao Tan</a></p>
<p>Abstract:
Feature importance is commonly used to explain machine predictions. While feature importance can be derived from a machine learning model with a variety of methods, the consistency of feature importance via different methods remains understudied. In this work, we systematically compare feature importance from built-in mechanisms in a model such as attention values and post-hoc methods that approximate model behavior such as LIME. Using text classification as a testbed, we find that 1) no matter which method we use, important features from traditional models such as SVM and XGBoost are more similar with each other, than with deep learning models; 2) post-hoc methods tend to generate more similar important features for two models than built-in methods. We further demonstrate how such similarity varies across instances. Notably, important features do not always resemble each other better when two models agree on the predicted label than when they disagree.</p>
<p>Keywords:</p>
<h3 id="47. Enhancing Local Feature Extraction with Global Representation for Neural Text Classification.">47. Enhancing Local Feature Extraction with Global Representation for Neural Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1047">Paper Link</a>    Pages:496-506</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/07/10187.html">Guocheng Niu</a> ; <a href="https://dblp.uni-trier.de/pid/225/5297.html">Hengru Xu</a> ; <a href="https://dblp.uni-trier.de/pid/254/8195.html">Bolei He</a> ; <a href="https://dblp.uni-trier.de/pid/87/8177.html">Xinyan Xiao</a> ; <a href="https://dblp.uni-trier.de/pid/27/6045-3.html">Hua Wu</a> ; <a href="https://dblp.uni-trier.de/pid/35/5676.html">Sheng Gao</a></p>
<p>Abstract:
For text classification, traditional local feature driven models learn long dependency by deeply stacking or hybrid modeling. This paper proposes a novel Encoder1-Encoder2 architecture, where global information is incorporated into the procedure of local feature extraction from scratch. In particular, Encoder1 serves as a global information provider, while Encoder2 performs as a local feature extractor and is directly fed into the classifier. Meanwhile, two modes are also designed for their interaction. Thanks to the awareness of global information, our method is able to learn better instance specific local features and thus avoids complicated upper operations. Experiments conducted on eight benchmark datasets demonstrate that our proposed architecture promotes local feature driven models by a substantial margin and outperforms the previous best models in the fully-supervised setting.</p>
<p>Keywords:</p>
<h3 id="48. Latent-Variable Generative Models for Data-Efficient Text Classification.">48. Latent-Variable Generative Models for Data-Efficient Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1048">Paper Link</a>    Pages:507-517</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/71/8596.html">Xiaoan Ding</a> ; <a href="https://dblp.uni-trier.de/pid/47/1252.html">Kevin Gimpel</a></p>
<p>Abstract:
Generative classifiers offer potential advantages over their discriminative counterparts, namely in the areas of data efficiency, robustness to data shift and adversarial examples, and zero-shot learning (Ng and Jordan,2002; Yogatama et al., 2017; Lewis and Fan,2019). In this paper, we improve generative text classifiers by introducing discrete latent variables into the generative story, and explore several graphical model configurations. We parameterize the distributions using standard neural architectures used in conditional language modeling and perform learning by directly maximizing the log marginal likelihood via gradient-based optimization, which avoids the need to do expectation-maximization. We empirically characterize the performance of our models on six text classification datasets. The choice of where to include the latent variable has a significant impact on performance, with the strongest results obtained when using the latent variable as an auxiliary conditioning variable in the generation of the textual input. This model consistently outperforms both the generative and discriminative classifiers in small-data settings. We analyze our model by finding that the latent variable captures interpretable properties of the data, even with very small training sets.</p>
<p>Keywords:</p>
<h3 id="49. PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space.">49. PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1049">Paper Link</a>    Pages:518-528</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/92/9052.html">Omer Anjum</a> ; <a href="https://dblp.uni-trier.de/pid/163/7318.html">Hongyu Gong</a> ; <a href="https://dblp.uni-trier.de/pid/66/9013.html">Suma Bhat</a> ; <a href="https://dblp.uni-trier.de/pid/03/4630.html">Wen-Mei Hwu</a> ; <a href="https://dblp.uni-trier.de/pid/81/1130.html">Jinjun Xiong</a></p>
<p>Abstract:
Finding the right reviewers to assess the quality of conference submissions is a time consuming process for conference organizers. Given the importance of this step, various automated reviewer-paper matching solutions have been proposed to alleviate the burden. Prior approaches including bag-of-words model and probabilistic topic model are less effective to deal with the vocabulary mismatch and partial topic overlap between the submission and reviewer. Our approach, the common topic model, jointly models the topics common to the submission and the reviewers profile while relying on abstract topic vectors. Experiments and insightful evaluations on two datasets demonstrate that the proposed method achieves consistent improvements compared to the state-of-the-art.</p>
<p>Keywords:</p>
<h3 id="50. Linking artificial and human neural representations of language.">50. Linking artificial and human neural representations of language.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1050">Paper Link</a>    Pages:529-539</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/177/8885.html">Jon Gauthier</a> ; <a href="https://dblp.uni-trier.de/pid/23/90.html">Roger Levy</a></p>
<p>Abstract:
What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance. We find that none of the sentence encoding tasks tested yield significant increases in brain decoding performance. Through further task ablations and representational analyses, we find that tasks which produce syntax-light representations yield significant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.</p>
<p>Keywords:</p>
<h3 id="51. Neural Text Summarization: A Critical Evaluation.">51. Neural Text Summarization: A Critical Evaluation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1051">Paper Link</a>    Pages:540-551</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/225/5389.html">Wojciech Kryscinski</a> ; <a href="https://dblp.uni-trier.de/pid/166/6729.html">Nitish Shirish Keskar</a> ; <a href="https://dblp.uni-trier.de/pid/205/2296.html">Bryan McCann</a> ; <a href="https://dblp.uni-trier.de/pid/80/7282.html">Caiming Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/79/128.html">Richard Socher</a></p>
<p>Abstract:
Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings: 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.</p>
<p>Keywords:</p>
<h3 id="52. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures.">52. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1052">Paper Link</a>    Pages:552-562</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/144/6801.html">Thiago Castro Ferreira</a> ; <a href="https://dblp.uni-trier.de/pid/211/7403.html">Chris van der Lee</a> ; <a href="https://dblp.uni-trier.de/pid/161/9943.html">Emiel van Miltenburg</a> ; <a href="https://dblp.uni-trier.de/pid/61/3236.html">Emiel Krahmer</a></p>
<p>Abstract:
Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. By contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of the encoder-decoder Gated-Recurrent Units (GRU) and Transformer, two state-of-the art deep learning methods. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.</p>
<p>Keywords:</p>
<h3 id="53. MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance.">53. MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1053">Paper Link</a>    Pages:563-578</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/z/WeiZhao.html">Wei Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/184/3721.html">Maxime Peyrard</a> ; <a href="https://dblp.uni-trier.de/pid/64/1350-4.html">Fei Liu</a> ; <a href="https://dblp.uni-trier.de/pid/89/4402-21.html">Yang Gao</a> ; <a href="https://dblp.uni-trier.de/pid/18/7930.html">Christian M. Meyer</a> ; <a href="https://dblp.uni-trier.de/pid/69/9271.html">Steffen Eger</a></p>
<p>Abstract:
A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that metrics combining contextualized representations with a distance measure perform the best. Such metrics also demonstrate strong generalization capability across tasks. For ease-of-use we make our metrics available as web service.</p>
<p>Keywords:</p>
<h3 id="54. Select and Attend: Towards Controllable Content Selection in Text Generation.">54. Select and Attend: Towards Controllable Content Selection in Text Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1054">Paper Link</a>    Pages:579-590</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/85/7634-1.html">Xiaoyu Shen</a> ; <a href="https://dblp.uni-trier.de/pid/78/6923.html">Jun Suzuki</a> ; <a href="https://dblp.uni-trier.de/pid/90/3315.html">Kentaro Inui</a> ; <a href="https://dblp.uni-trier.de/pid/89/1838.html">Hui Su</a> ; <a href="https://dblp.uni-trier.de/pid/00/1846.html">Dietrich Klakow</a> ; <a href="https://dblp.uni-trier.de/pid/10/163.html">Satoshi Sekine</a></p>
<p>Abstract:
Many text generation tasks naturally contain two steps: content selection and surface realization. Current neural encoder-decoder models conflate both steps into a black-box architecture. As a result, the content to be described in the text cannot be explicitly controlled. This paper tackles this problem by decoupling content selection from the decoder. The decoupled content selection is human interpretable, whose value can be manually manipulated to control the content of generated text. The model can be trained end-to-end without human annotations by maximizing a lower bound of the marginal likelihood. We further propose an effective way to trade-off between performance and controllability with a single adjustable hyperparameter. In both data-to-text and headline generation tasks, our model achieves promising results, paving the way for controllable content selection in text generation.</p>
<p>Keywords:</p>
<h3 id="55. Sentence-Level Content Planning and Style Specification for Neural Text Generation.">55. Sentence-Level Content Planning and Style Specification for Neural Text Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1055">Paper Link</a>    Pages:591-602</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/200/7911.html">Xinyu Hua</a> ; <a href="https://dblp.uni-trier.de/pid/49/3800-8.html">Lu Wang</a></p>
<p>Abstract:
Building effective text generation systems requires three critical components: content selection, text planning, and surface realization, and traditionally they are tackled as separate problems. Recent all-in-one style neural generation models have made impressive progress, yet they often produce outputs that are incoherent and unfaithful to the input. To address these issues, we present an end-to-end trained two-step generation model, where a sentence-level content planner first decides on the keyphrases to cover as well as a desired language style, followed by a surface realization decoder that generates relevant and coherent text. For experiments, we consider three tasks from domains with diverse topics and varying language styles: persuasive argument construction from Reddit, paragraph generation for normal and simple versions of Wikipedia, and abstract generation for scientific articles. Automatic evaluation shows that our system can significantly outperform competitive comparisons. Human judges further rate our system generated text as more fluent and correct, compared to the generations by its variants that do not consider language style.</p>
<p>Keywords:</p>
<h3 id="56. Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling.">56. Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1056">Paper Link</a>    Pages:603-615</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/185/6803.html">Angel Daza</a> ; <a href="https://dblp.uni-trier.de/pid/82/6572.html">Anette Frank</a></p>
<p>Abstract:
We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.</p>
<p>Keywords:</p>
<h3 id="57. Syntax-Enhanced Self-Attention-Based Semantic Role Labeling.">57. Syntax-Enhanced Self-Attention-Based Semantic Role Labeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1057">Paper Link</a>    Pages:616-626</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/47/722-4.html">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/06/2293-51.html">Rui Wang</a> ; <a href="https://dblp.uni-trier.de/pid/14/1217.html">Luo Si</a></p>
<p>Abstract:
As a fundamental NLP task, semantic role labeling (SRL) aims to discover the semantic roles for each predicate within one sentence. This paper investigates how to incorporate syntactic knowledge into the SRL task effectively. We present different approaches of en- coding the syntactic information derived from dependency trees of different quality and representations; we propose a syntax-enhanced self-attention model and compare it with other two strong baseline methods; and we con- duct experiments with newly published deep contextualized word representations as well. The experiment results demonstrate that with proper incorporation of the high quality syntactic information, our model achieves a new state-of-the-art performance for the Chinese SRL task on the CoNLL-2009 dataset.</p>
<p>Keywords:</p>
<h3 id="58. VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling.">58. VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1058">Paper Link</a>    Pages:627-637</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/7974.html">Andrea Di Fabio</a> ; <a href="https://dblp.uni-trier.de/pid/254/8205.html">Simone Conia</a> ; <a href="https://dblp.uni-trier.de/pid/n/RobertoNavigli.html">Roberto Navigli</a></p>
<p>Abstract:
We present VerbAtlas, a new, hand-crafted lexical-semantic resource whose goal is to bring together all verbal synsets from WordNet into semantically-coherent frames. The frames define a common, prototypical argument structure while at the same time providing new concept-specific information. In contrast to PropBank, which defines enumerative semantic roles, VerbAtlas comes with an explicit, cross-frame set of semantic roles linked to selectional preferences expressed in terms of WordNet synsets, and is the first resource enriched with semantic information about implicit, shadow, and default arguments. We demonstrate the effectiveness of VerbAtlas in the task of dependency-based Semantic Role Labeling and show how its integration into a high-performance system leads to improvements on both the in-domain and out-of-domain test sets of CoNLL-2009. VerbAtlas is available at <a href="http://verbatlas.org">http://verbatlas.org</a>.</p>
<p>Keywords:</p>
<h3 id="59. Parameter-free Sentence Embedding via Orthogonal Basis.">59. Parameter-free Sentence Embedding via Orthogonal Basis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1059">Paper Link</a>    Pages:638-648</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/217/8277.html">Ziyi Yang</a> ; <a href="https://dblp.uni-trier.de/pid/48/7536.html">Chenguang Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/79/2536.html">Weizhu Chen</a></p>
<p>Abstract:
We propose a simple and robust non-parameterized approach for building sentence representations. Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence. We model the semantic meaning of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is the words novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace. Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representations. This approach requires zero parameters, along with efficient inference performance. We evaluate our approach on 11 downstream NLP tasks. Our model shows superior performance compared with non-parameterized alternatives and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.</p>
<p>Keywords:</p>
<h3 id="60. Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations.">60. Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1060">Paper Link</a>    Pages:649-662</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/2003.html">Mingda Chen</a> ; <a href="https://dblp.uni-trier.de/pid/169/6786.html">Zewei Chu</a> ; <a href="https://dblp.uni-trier.de/pid/47/1252.html">Kevin Gimpel</a></p>
<p>Abstract:
Prior work on pretrained sentence embeddings and benchmarks focus on the capabilities of stand-alone sentences. We propose DiscoEval, a test suite of tasks to evaluate whether sentence representations include broader context information. We also propose a variety of training objectives that makes use of natural annotations from Wikipedia to build sentence encoders capable of modeling discourse. We benchmark sentence encoders pretrained with our proposed training objectives, as well as other popular pretrained sentence encoders on DiscoEval and other sentence evaluation tasks. Empirically, we show that these training objectives help to encode different aspects of information in document structures. Moreover, BERT and ELMo demonstrate strong performances over DiscoEval with individual hidden layers showing different characteristics.</p>
<p>Keywords:</p>
<h3 id="61. Extracting Possessions from Social Media: Images Complement Language.">61. Extracting Possessions from Social Media: Images Complement Language.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1061">Paper Link</a>    Pages:663-672</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/2021.html">Dhivya Chinnappa</a> ; <a href="https://dblp.uni-trier.de/pid/254/7907.html">Srikala Murugan</a> ; <a href="https://dblp.uni-trier.de/pid/32/369-2.html">Eduardo Blanco</a></p>
<p>Abstract:
This paper describes a new dataset and experiments to determine whether authors of tweets possess the objects they tweet about. We work with 5,000 tweets and show that both humans and neural networks benefit from images in addition to text. We also introduce a simple yet effective strategy to incorporate visual information into any neural network beyond weights from pretrained networks. Specifically, we consider the tags identified in an image as an additional textual input, and leverage pretrained word embeddings as usually done with regular text. Experimental results show this novel strategy is beneficial.</p>
<p>Keywords:</p>
<h3 id="62. Learning to Speak and Act in a Fantasy Text Adventure Game.">62. Learning to Speak and Act in a Fantasy Text Adventure Game.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1062">Paper Link</a>    Pages:673-683</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/210/1040.html">Jack Urbanek</a> ; <a href="https://dblp.uni-trier.de/pid/192/1872.html">Angela Fan</a> ; <a href="https://dblp.uni-trier.de/pid/199/1922.html">Siddharth Karamcheti</a> ; <a href="https://dblp.uni-trier.de/pid/227/2617.html">Saachi Jain</a> ; <a href="https://dblp.uni-trier.de/pid/139/2584.html">Samuel Humeau</a> ; <a href="https://dblp.uni-trier.de/pid/213/7927.html">Emily Dinan</a> ; <a href="https://dblp.uni-trier.de/pid/43/11537.html">Tim Rocktschel</a> ; <a href="https://dblp.uni-trier.de/pid/136/9140.html">Douwe Kiela</a> ; <a href="https://dblp.uni-trier.de/pid/22/6733.html">Arthur Szlam</a> ; <a href="https://dblp.uni-trier.de/pid/29/6977.html">Jason Weston</a></p>
<p>Abstract:
We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the game. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these models are able to effectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their affordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and dialogue. We analyze the ingredients necessary for successful grounding in this setting, and how each of these factors relate to agents that can talk and act successfully.</p>
<p>Keywords:</p>
<h3 id="63. Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning.">63. Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1063">Paper Link</a>    Pages:684-695</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/53/6791.html">Khanh Nguyen</a> ; <a href="https://dblp.uni-trier.de/pid/77/2856.html">Hal Daum III</a></p>
<p>Abstract:
Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop Help, Anna! (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural language-and-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments.</p>
<p>Keywords:</p>
<h3 id="64. Incorporating Visual Semantics into Sentence Representations within a Grounded Space.">64. Incorporating Visual Semantics into Sentence Representations within a Grounded Space.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1064">Paper Link</a>    Pages:696-707</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/2882.html">Patrick Bordes</a> ; <a href="https://dblp.uni-trier.de/pid/204/2791.html">Eloi Zablocki</a> ; <a href="https://dblp.uni-trier.de/pid/00/11514.html">Laure Soulier</a> ; <a href="https://dblp.uni-trier.de/pid/15/2146.html">Benjamin Piwowarski</a> ; <a href="https://dblp.uni-trier.de/pid/g/PatrickGallinari.html">Patrick Gallinari</a></p>
<p>Abstract:
Language grounding is an active field aiming at enriching textual representations with visual information. Generally, textual and visual elements are embedded in the same representation space, which implicitly assumes a one-to-one correspondence between modalities. This hypothesis does not hold when representing words, and becomes problematic when used to learn sentence representations  the focus of this paper  as a visual scene can be described by a wide variety of sentences. To overcome this limitation, we propose to transfer visual information to textual representations by learning an intermediate representation space: the grounded space. We further propose two new complementary objectives ensuring that (1) sentences associated with the same visual content are close in the grounded space and (2) similarities between related elements are preserved across modalities. We show that this model outperforms the previous state-of-the-art on classification and semantic relatedness tasks.</p>
<p>Keywords:</p>
<h3 id="65. Neural Naturalist: Generating Fine-Grained Image Comparisons.">65. Neural Naturalist: Generating Fine-Grained Image Comparisons.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1065">Paper Link</a>    Pages:708-717</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/151/9292.html">Maxwell Forbes</a> ; <a href="https://dblp.uni-trier.de/pid/211/7622.html">Christine Kaeser-Chen</a> ; <a href="https://dblp.uni-trier.de/pid/91/1508.html">Piyush Sharma</a> ; <a href="https://dblp.uni-trier.de/pid/b/SJBelongie.html">Serge J. Belongie</a></p>
<p>Abstract:
We introduce the new Birds-to-Words dataset of 41k sentences describing fine-grained differences between photographs of birds. The language collected is highly detailed, while remaining understandable to the everyday observer (e.g., heart-shaped face, squat body). Paragraph-length descriptions naturally adapt to varying levels of taxonomic and visual distancedrawn from a novel stratified sampling approachwith the appropriate level of detail. We propose a new model called Neural Naturalist that uses a joint image encoding and comparative module to generate comparative language, and evaluate the results with humans who must use the descriptions to distinguish real images. Our results indicate promising potential for neural models to explain differences in visual embedding space using natural language, as well as a concrete path for machine learning to aid citizen scientists in their effort to preserve biodiversity.</p>
<p>Keywords:</p>
<h3 id="66. Fine-Grained Evaluation for Entity Linking.">66. Fine-Grained Evaluation for Entity Linking.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1066">Paper Link</a>    Pages:718-727</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/137/2308.html">Henry Rosales-Mndez</a> ; <a href="https://dblp.uni-trier.de/pid/h/AidanHogan.html">Aidan Hogan</a> ; <a href="https://dblp.uni-trier.de/pid/83/6509.html">Barbara Poblete</a></p>
<p>Abstract:
The Entity Linking (EL) task identifies entity mentions in a text corpus and associates them with an unambiguous identifier in a Knowledge Base. While much work has been done on the topic, we first present the results of a survey that reveal a lack of consensus in the community regarding what forms of mentions in a text and what forms of links the EL task should consider. We argue that no one definition of the Entity Linking task fits all, and rather propose a fine-grained categorization of different types of entity mentions and links. We then re-annotate three EL benchmark datasets  ACE2004, KORE50, and VoxEL  with respect to these categories. We propose a fuzzy recall metric to address the lack of consensus and conclude with fine-grained evaluation results comparing a selection of online EL systems.</p>
<p>Keywords:</p>
<h3 id="67. Supervising Unsupervised Open Information Extraction Models.">67. Supervising Unsupervised Open Information Extraction Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1067">Paper Link</a>    Pages:728-737</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/126/4826.html">Arpita Roy</a> ; <a href="https://dblp.uni-trier.de/pid/65/6400.html">Youngja Park</a> ; <a href="https://dblp.uni-trier.de/pid/60/10355.html">Taesung Lee</a> ; <a href="https://dblp.uni-trier.de/pid/p/ShimeiPan.html">Shimei Pan</a></p>
<p>Abstract:
We propose a novel supervised open information extraction (Open IE) framework that leverages an ensemble of unsupervised Open IE systems and a small amount of labeled data to improve system performance. It uses the outputs of multiple unsupervised Open IE systems plus a diverse set of lexical and syntactic information such as word embedding, part-of-speech embedding, syntactic role embedding and dependency structure as its input features and produces a sequence of word labels indicating whether the word belongs to a relation, the arguments of the relation or irrelevant. Comparing with existing supervised Open IE systems, our approach leverages the knowledge in existing unsupervised Open IE systems to overcome the problem of insufficient training data. By employing multiple unsupervised Open IE systems, our system learns to combine the strength and avoid the weakness in each individual Open IE system. We have conducted experiments on multiple labeled benchmark data sets. Our evaluation results have demonstrated the superiority of the proposed method over existing supervised and unsupervised models by a significant margin.</p>
<p>Keywords:</p>
<h3 id="68. Neural Cross-Lingual Event Detection with Minimal Parallel Resources.">68. Neural Cross-Lingual Event Detection with Minimal Parallel Resources.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1068">Paper Link</a>    Pages:738-748</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/35/295.html">Jian Liu</a> ; <a href="https://dblp.uni-trier.de/pid/90/7879.html">Yubo Chen</a> ; <a href="https://dblp.uni-trier.de/pid/42/4903.html">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/47/2026-1.html">Jun Zhao</a></p>
<p>Abstract:
The scarcity in annotated data poses a great challenge for event detection (ED). Cross-lingual ED aims to tackle this challenge by transferring knowledge between different languages to boost performance. However, previous cross-lingual methods for ED demonstrated a heavy dependency on parallel resources, which might limit their applicability. In this paper, we propose a new method for cross-lingual ED, demonstrating a minimal dependency on parallel resources. Specifically, to construct a lexical mapping between different languages, we devise a context-dependent translation method; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual co-training. The efficiency of our method is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario.</p>
<p>Keywords:</p>
<h3 id="69. KnowledgeNet: A Benchmark Dataset for Knowledge Base Population.">69. KnowledgeNet: A Benchmark Dataset for Knowledge Base Population.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1069">Paper Link</a>    Pages:749-758</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/26/6421.html">Filipe de S Mesquita</a> ; <a href="https://dblp.uni-trier.de/pid/181/6218.html">Matteo Cannaviccio</a> ; <a href="https://dblp.uni-trier.de/pid/136/9172.html">Jordan Schmidek</a> ; <a href="https://dblp.uni-trier.de/pid/140/3058.html">Paramita Mirza</a> ; <a href="https://dblp.uni-trier.de/pid/b/DBarbosa.html">Denilson Barbosa</a></p>
<p>Abstract:
KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction). We discuss five baseline approaches, where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79% (0.28). However, our best baseline is far from reaching human performance (0.82), indicating our dataset is challenging. The KnowledgeNet dataset and baselines are available at <a href="https://github.com/diffbot/knowledge-net">https://github.com/diffbot/knowledge-net</a></p>
<p>Keywords:</p>
<h3 id="70. Effective Use of Transformer Networks for Entity Tracking.">70. Effective Use of Transformer Networks for Entity Tracking.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1070">Paper Link</a>    Pages:759-769</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/66/841.html">Aditya Gupta</a> ; <a href="https://dblp.uni-trier.de/pid/69/7968.html">Greg Durrett</a></p>
<p>Abstract:
Tracking entities in procedural language requires understanding the transformations arising from actions on entities as well as those entities interactions. While self-attention-based pre-trained language encoders like GPT and BERT have been successfully applied across a range of natural language understanding tasks, their ability to handle the nuances of procedural texts is still unknown. In this paper, we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text. First, we test standard lightweight approaches for prediction with pre-trained transformers, and find that these approaches underperforms even simple baselines. We show that much stronger results can be attained by restructuring the input to guide the model to focus on a particular entity. Second, we assess the degree to which the transformer networks capture the process dynamics, investigating such factors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-of-the-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate process state.</p>
<p>Keywords:</p>
<h3 id="71. Explicit Cross-lingual Pre-training for Unsupervised Machine Translation.">71. Explicit Cross-lingual Pre-training for Unsupervised Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1071">Paper Link</a>    Pages:770-779</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/147/6063.html">Shuo Ren</a> ; <a href="https://dblp.uni-trier.de/pid/22/0-12.html">Yu Wu</a> ; <a href="https://dblp.uni-trier.de/pid/54/2695.html">Shujie Liu</a> ; <a href="https://dblp.uni-trier.de/pid/16/1161-1.html">Ming Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/35/6569.html">Shuai Ma</a></p>
<p>Abstract:
Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the cross-lingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pairs, we propose a new pre-training model called Cross-lingual Masked Language Model (CMLM), which randomly chooses source n-grams in the input text stream and predicts their translation candidates at each time step. Experiments show that our method can incorporate beneficial cross-lingual information into pre-trained models. Taking pre-trained CMLM models as the encoder and decoder, we significantly improve the performance of unsupervised machine translation.</p>
<p>Keywords:</p>
<h3 id="72. Latent Part-of-Speech Sequences for Neural Machine Translation.">72. Latent Part-of-Speech Sequences for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1072">Paper Link</a>    Pages:780-790</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/189/3032.html">Xuewen Yang</a> ; <a href="https://dblp.uni-trier.de/pid/160/6116.html">Yingru Liu</a> ; <a href="https://dblp.uni-trier.de/pid/27/3906.html">Dongliang Xie</a> ; <a href="https://dblp.uni-trier.de/pid/10/5630.html">Xin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/40/1931.html">Niranjan Balasubramanian</a></p>
<p>Abstract:
Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating syntax through latent variables introduces additional complexity in inference, as the models need to marginalize over the latent syntactic structures. To avoid this, models often resort to greedy search which only allows them to explore a limited portion of the latent space. In this work, we introduce a new latent variable model, LaSyn, that captures the co-dependence between syntax and semantics, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.</p>
<p>Keywords:</p>
<h3 id="73. Improving Back-Translation with Uncertainty-based Confidence Estimation.">73. Improving Back-Translation with Uncertainty-based Confidence Estimation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1073">Paper Link</a>    Pages:791-802</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/63/1591.html">Shuo Wang</a> ; <a href="https://dblp.uni-trier.de/pid/51/3710-5.html">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/188/7759-49.html">Chao Wang</a> ; <a href="https://dblp.uni-trier.de/pid/55/5812.html">Huanbo Luan</a> ; <a href="https://dblp.uni-trier.de/pid/95/3291.html">Maosong Sun</a></p>
<p>Abstract:
While back-translation is simple and effective in exploiting abundant monolingual corpora to improve low-resource neural machine translation (NMT), the synthetic bilingual corpora generated by NMT models trained on limited authentic bilingual data are inevitably noisy. In this work, we propose to quantify the confidence of NMT model predictions based on model uncertainty. With word- and sentence-level confidence measures based on uncertainty, it is possible for back-translation to better cope with noise in synthetic bilingual corpora. Experiments on Chinese-English and English-German translation tasks show that uncertainty-based confidence estimation significantly improves the performance of back-translation.</p>
<p>Keywords:</p>
<h3 id="74. Towards Linear Time Neural Machine Translation with Capsule Networks.">74. Towards Linear Time Neural Machine Translation with Capsule Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1074">Paper Link</a>    Pages:803-812</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/43/11214.html">Mingxuan Wang</a></p>
<p>Abstract:
In this study, we first investigate a novel capsule network with dynamic routing for linear time Neural Machine Translation (NMT), referred as CapsNMT. CapsNMT uses an aggregation mechanism to map the source sentence into a matrix with pre-determined size, and then applys a deep LSTM network to decode the target sequence from the source representation. Unlike the previous work (CITATION) to store the source sentence with a passive and bottom-up way, the dynamic routing policy encodes the source sentence with an iterative process to decide the credit attribution between nodes from lower and higher layers. CapsNMT has two core properties: it runs in time that is linear in the length of the sequences and provides a more flexible way to aggregate the part-whole information of the source sentence. On WMT14 English-German task and a larger WMT14 English-French task, CapsNMT achieves comparable results with the Transformer system. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for sequence to sequence problems.</p>
<p>Keywords:</p>
<h3 id="75. Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment.">75. Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1075">Paper Link</a>    Pages:813-822</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/159/1022.html">Xiaofei Shi</a> ; <a href="https://dblp.uni-trier.de/pid/96/999.html">Yanghua Xiao</a></p>
<p>Abstract:
Entity alignment aims to find entities in different knowledge graphs (KGs) that refer to the same real-world object. An effective solution for cross-lingual entity alignment is crucial for many cross-lingual AI and NLP applications. Recently many embedding-based approaches were proposed for cross-lingual entity alignment. However, almost all of them are based on TransE or its variants, which have been demonstrated by many studies to be unsuitable for encoding multi-mapping relations such as 1-N, N-1 and N-N relations, thus these methods obtain low alignment precision. To solve this issue, we propose a new embedding-based framework. Through defining dot product-based functions over embeddings, our model can better capture the semantics of both 1-1 and multi-mapping relations. We calibrate embeddings of different KGs via a small set of pre-aligned seeds. We also propose a weighted negative sampling strategy to generate valuable negative samples during training and we regard prediction as a bidirectional problem in the end. Experimental results (especially with the metric Hits@1) on real-world multilingual datasets show that our approach significantly outperforms many other embedding-based approaches with state-of-the-art performance.</p>
<p>Keywords:</p>
<h3 id="76. Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages.">76. Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1076">Paper Link</a>    Pages:823-832</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/07/8462.html">Masud Moshtaghi</a></p>
<p>Abstract:
Enabling cross-lingual NLP tasks by leveraging multilingual word embedding has recently attracted much attention. An important motivation is to support lower resourced languages, however, most efforts focus on demonstrating the effectiveness of the techniques using embeddings derived from similar languages to English with large parallel content. In this study, we first describe the general requirements for the success of these techniques and then present a noise tolerant piecewise linear technique to learn a non-linear mapping between two monolingual word embedding vector spaces. We evaluate our approach on inferring bilingual dictionaries. We show that our technique outperforms the state-of-the-art in lower resourced settings with an average of 3.7% improvement of precision @10 across 14 mostly low resourced languages.</p>
<p>Keywords:</p>
<h3 id="77. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT.">77. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1077">Paper Link</a>    Pages:833-844</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/225/7822.html">Shijie Wu</a> ; <a href="https://dblp.uni-trier.de/pid/31/5468.html">Mark Dredze</a></p>
<p>Abstract:
Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.</p>
<p>Keywords:</p>
<h3 id="78. Iterative Dual Domain Adaptation for Neural Machine Translation.">78. Iterative Dual Domain Adaptation for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1078">Paper Link</a>    Pages:845-855</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/213/5861.html">Jiali Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/51/3710-5.html">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/05/9013.html">Jinsong Su</a> ; <a href="https://dblp.uni-trier.de/pid/216/4408.html">Yubin Ge</a> ; <a href="https://dblp.uni-trier.de/pid/15/3214-1.html">Yaojie Lu</a> ; <a href="https://dblp.uni-trier.de/pid/228/5564.html">Yongjing Yin</a> ; <a href="https://dblp.uni-trier.de/pid/25/5545.html">Jiebo Luo</a></p>
<p>Abstract:
Previous studies on the domain adaptation for neural machine translation (NMT) mainly focus on the one-pass transferring out-of-domain translation knowledge to in-domain NMT model. In this paper, we argue that such a strategy fails to fully extract the domain-shared translation knowledge, and repeatedly utilizing corpora of different domains can lead to better distillation of domain-shared translation knowledge. To this end, we propose an iterative dual domain adaptation framework for NMT. Specifically, we first pretrain in-domain and out-of-domain NMT models using their own training corpora respectively, and then iteratively perform bidirectional translation knowledge transfer (from in-domain to out-of-domain and then vice versa) based on knowledge distillation until the in-domain NMT model convergences. Furthermore, we extend the proposed framework to the scenario of multiple out-of-domain training corpora, where the above-mentioned transfer is performed sequentially between the in-domain and each out-of-domain NMT models in the ascending order of their domain similarities. Empirical results on Chinese-English and English-German translation tasks demonstrate the effectiveness of our framework.</p>
<p>Keywords:</p>
<h3 id="79. Multi-agent Learning for Neural Machine Translation.">79. Multi-agent Learning for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1079">Paper Link</a>    Pages:856-865</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7465.html">Tianchi Bi</a> ; <a href="https://dblp.uni-trier.de/pid/46/5036.html">Hao Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/50/1726.html">Zhongjun He</a> ; <a href="https://dblp.uni-trier.de/pid/27/6045-3.html">Hua Wu</a> ; <a href="https://dblp.uni-trier.de/pid/10/5209.html">Haifeng Wang</a></p>
<p>Abstract:
Conventional Neural Machine Translation (NMT) models benefit from the training with an additional agent, e.g., dual learning, and bidirectional decoding with one agent decod- ing from left to right and the other decoding in the opposite direction. In this paper, we extend the training framework to the multi-agent sce- nario by introducing diverse agents in an in- teractive updating process. At training time, each agent learns advanced knowledge from others, and they work together to improve translation quality. Experimental results on NIST Chinese-English, IWSLT 2014 German- English, WMT 2014 English-German and large-scale Chinese-English translation tasks indicate that our approach achieves absolute improvements over the strong baseline sys- tems and shows competitive performance on all tasks.</p>
<p>Keywords:</p>
<h3 id="80. Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages.">80. Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1080">Paper Link</a>    Pages:866-876</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/129/4079.html">Yunsu Kim</a> ; <a href="https://dblp.uni-trier.de/pid/249/2766.html">Petre Petrov</a> ; <a href="https://dblp.uni-trier.de/pid/205/2696.html">Pavel Petrushkov</a> ; <a href="https://dblp.uni-trier.de/pid/03/5199.html">Shahram Khadivi</a> ; <a href="https://dblp.uni-trier.de/pid/n/HermannNey.html">Hermann Ney</a></p>
<p>Abstract:
We present effective pre-training strategies for neural machine translation (NMT) using parallel corpora involving a pivot language, i.e., source-pivot and pivot-target, leading to a significant improvement in source-target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training: 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our methods greatly outperform multilingual models up to +2.6% BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements are valid also in zero-shot/zero-resource scenarios.</p>
<p>Keywords:</p>
<h3 id="81. Context-Aware Monolingual Repair for Neural Machine Translation.">81. Context-Aware Monolingual Repair for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1081">Paper Link</a>    Pages:877-886</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/4162.html">Elena Voita</a> ; <a href="https://dblp.uni-trier.de/pid/00/8341.html">Rico Sennrich</a> ; <a href="https://dblp.uni-trier.de/pid/08/5391.html">Ivan Titov</a></p>
<p>Abstract:
Modern sentence-level NMT systems often produce plausible translations of isolated sentences. However, when put in context, these translations may end up being inconsistent with each other. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. For training, the DocRepair model requires only monolingual document-level data in the target language. It is trained as a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. We show that this approach successfully imitates inconsistencies we aim to fix: using contrastive evaluation, we show large improvements in the translation of several contextual phenomena in an English-Russian translation task, as well as improvements in the BLEU score. We also conduct a human evaluation and show a strong preference of the annotators to corrected translations over the baseline ones. Moreover, we analyze which discourse phenomena are hard to capture using monolingual data only.</p>
<p>Keywords:</p>
<h3 id="82. Multi-Granularity Self-Attention for Neural Machine Translation.">82. Multi-Granularity Self-Attention for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1082">Paper Link</a>    Pages:887-897</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/80/4735.html">Jie Hao</a> ; <a href="https://dblp.uni-trier.de/pid/02/3674-7.html">Xing Wang</a> ; <a href="https://dblp.uni-trier.de/pid/s/ShumingShi-1.html">Shuming Shi</a> ; <a href="https://dblp.uni-trier.de/pid/43/661.html">Jinfeng Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/71/9281.html">Zhaopeng Tu</a></p>
<p>Abstract:
Current state-of-the-art neural machine translation (NMT) uses a deep multi-head self-attention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced substantial improvements, suggesting the possibility of improving NMT performance from explicit modeling of phrases. In this work, we present multi-granularity self-attention (Mg-Sa): a neural network that combines multi-head self-attention and phrase modeling. Specifically, we train several attention heads to attend to phrases in either n-gram or syntactic formalisms. Moreover, we exploit interactions among phrases to enhance the strength of structure modeling  a commonly-cited weakness of self-attention. Experimental results on WMT14 English-to-German and NIST Chinese-to-English translation tasks show the proposed approach consistently improves performance. Targeted linguistic analysis reveal that Mg-Sa indeed captures useful phrase information at various levels of granularities.</p>
<p>Keywords:</p>
<h3 id="83. Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention.">83. Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1083">Paper Link</a>    Pages:898-909</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/83/3266.html">Biao Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/08/5391.html">Ivan Titov</a> ; <a href="https://dblp.uni-trier.de/pid/00/8341.html">Rico Sennrich</a></p>
<p>Abstract:
The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connection and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified average-based self-attention sublayer and the encoder-decoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt. Source code for reproduction will be released soon.</p>
<p>Keywords:</p>
<h3 id="84. A Discriminative Neural Model for Cross-Lingual Word Alignment.">84. A Discriminative Neural Model for Cross-Lingual Word Alignment.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1084">Paper Link</a>    Pages:910-920</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/6138.html">Elias Stengel-Eskin</a> ; <a href="https://dblp.uni-trier.de/pid/205/2576.html">Tzu-Ray Su</a> ; <a href="https://dblp.uni-trier.de/pid/51/8151.html">Matt Post</a> ; <a href="https://dblp.uni-trier.de/pid/06/4775.html">Benjamin Van Durme</a></p>
<p>Abstract:
We introduce a novel discriminative word alignment model, which we integrate into a Transformer-based machine translation model. In experiments based on a small number of labeled examples (1.7K5K sentences) we evaluate its performance intrinsically on both English-Chinese and English-Arabic alignment, where we achieve major improvements over unsupervised baselines (1127 F1). We evaluate the model extrinsically on data projection for Chinese NER, showing that our alignments lead to higher performance when used to project NER tags from English to Chinese. Finally, we perform an ablation analysis and an annotation experiment that jointly support the utility and feasibility of future manual alignment elicitation.</p>
<p>Keywords:</p>
<h3 id="85. One Model to Learn Both: Zero Pronoun Prediction and Translation.">85. One Model to Learn Both: Zero Pronoun Prediction and Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1085">Paper Link</a>    Pages:921-930</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/127/3421.html">Longyue Wang</a> ; <a href="https://dblp.uni-trier.de/pid/71/9281.html">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pid/02/3674-7.html">Xing Wang</a> ; <a href="https://dblp.uni-trier.de/pid/s/ShumingShi-1.html">Shuming Shi</a></p>
<p>Abstract:
Zero pronouns (ZPs) are frequently omitted in pro-drop languages, but should be recalled in non-pro-drop languages. This discourse phenomenon poses a significant challenge for machine translation (MT) when translating texts from pro-drop to non-pro-drop languages. In this paper, we propose a unified and discourse-aware ZP translation approach for neural MT models. Specifically, we jointly learn to predict and translate ZPs in an end-to-end manner, allowing both components to interact with each other. In addition, we employ hierarchical neural networks to exploit discourse-level context, which is beneficial for ZP prediction and thus translation. Experimental results on both Chinese-English and Japanese-English data show that our approach significantly and accumulatively improves both translation performance and ZP prediction accuracy over not only baseline but also previous works using external ZP prediction models. Extensive analyses confirm that the performance improvement comes from the alleviation of different kinds of errors especially caused by subjective ZPs.</p>
<p>Keywords:</p>
<h3 id="86. Dynamic Past and Future for Neural Machine Translation.">86. Dynamic Past and Future for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1086">Paper Link</a>    Pages:931-941</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/2769.html">Zaixiang Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/57/8451.html">Shujian Huang</a> ; <a href="https://dblp.uni-trier.de/pid/71/9281.html">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pid/39/5815.html">Xin-Yu Dai</a> ; <a href="https://dblp.uni-trier.de/pid/42/4315.html">Jiajun Chen</a></p>
<p>Abstract:
Previous studies have shown that neural machine translation (NMT) models can benefit from explicitly modeling translated () and untranslated () source contents as recurrent states (CITATION). However, this less interpretable recurrent process hinders its power to model the dynamic updating of and contents during decoding. In this paper, we propose to model the dynamic principles by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (CITATION), namely Guided Dynamic Routing, where the translating status at each decoding step guides the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both Rnmt and Transformer by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.</p>
<p>Keywords:</p>
<h3 id="87. Revisit Automatic Error Detection for Wrong and Missing Translation - A Supervised Approach.">87. Revisit Automatic Error Detection for Wrong and Missing Translation - A Supervised Approach.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1087">Paper Link</a>    Pages:942-952</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/167/9604.html">Wenqiang Lei</a> ; <a href="https://dblp.uni-trier.de/pid/57/4640.html">Weiwen Xu</a> ; <a href="https://dblp.uni-trier.de/pid/40/357.html">Ai Ti Aw</a> ; <a href="https://dblp.uni-trier.de/pid/218/7464.html">Yuanxin Xiang</a> ; <a href="https://dblp.uni-trier.de/pid/24/6606.html">Tat-Seng Chua</a></p>
<p>Abstract:
While achieving great fluency, current machine translation (MT) techniques are bottle-necked by adequacy issues. To have a closer study of these issues and accelerate model development, we propose automatic detecting adequacy errors in MT hypothesis for MT model evaluation. To do that, we annotate missing and wrong translations, the two most prevalent issues for current neural machine translation model, in 15000 Chinese-English translation pairs. We build a supervised alignment model for translation error detection (AlignDet) based on a simple Alignment Triangle strategy to set the benchmark for automatic error detection task. We also discuss the difficulties of this task and the benefits of this task for existing evaluation metrics.</p>
<p>Keywords:</p>
<h3 id="88. Towards Understanding Neural Machine Translation with Word Importance.">88. Towards Understanding Neural Machine Translation with Word Importance.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1088">Paper Link</a>    Pages:953-962</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/187/0843.html">Shilin He</a> ; <a href="https://dblp.uni-trier.de/pid/71/9281.html">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pid/02/3674-7.html">Xing Wang</a> ; <a href="https://dblp.uni-trier.de/pid/127/3421.html">Longyue Wang</a> ; <a href="https://dblp.uni-trier.de/pid/l/MichaelRLyu.html">Michael R. Lyu</a> ; <a href="https://dblp.uni-trier.de/pid/s/ShumingShi-1.html">Shuming Shi</a></p>
<p>Abstract:
Although neural machine translation (NMT) has advanced the state-of-the-art on various language pairs, the interpretability of NMT remains unsatisfactory. In this work, we propose to address this gap by focusing on understanding the input-output behavior of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures, demonstrating its superiority on identifying input words with higher influence on translation performance. Encouragingly, the calculated importance can serve as indicators of input words that are under-translated by NMT models. Furthermore, our analysis reveals that words of certain syntactic categories have higher importance while the categories vary across language pairs, which can inspire better design principles of NMT architectures for multi-lingual translation.</p>
<p>Keywords:</p>
<h3 id="89. Multilingual Neural Machine Translation with Language Clustering.">89. Multilingual Neural Machine Translation with Language Clustering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1089">Paper Link</a>    Pages:963-973</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/96/10484-3.html">Xu Tan</a> ; <a href="https://dblp.uni-trier.de/pid/214/9699.html">Jiale Chen</a> ; <a href="https://dblp.uni-trier.de/pid/74/184.html">Di He</a> ; <a href="https://dblp.uni-trier.de/pid/144/7737.html">Yingce Xia</a> ; <a href="https://dblp.uni-trier.de/pid/14/6841.html">Tao Qin</a> ; <a href="https://dblp.uni-trier.de/pid/l/TieYanLiu.html">Tie-Yan Liu</a></p>
<p>Abstract:
Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods.</p>
<p>Keywords:</p>
<h3 id="90. Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction.">90. Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1090">Paper Link</a>    Pages:974-983</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/243/1769.html">Paula Czarnowska</a> ; <a href="https://dblp.uni-trier.de/pid/186/7066.html">Sebastian Ruder</a> ; <a href="https://dblp.uni-trier.de/pid/50/10261.html">Edouard Grave</a> ; <a href="https://dblp.uni-trier.de/pid/146/4361.html">Ryan Cotterell</a> ; <a href="https://dblp.uni-trier.de/pid/73/5801.html">Ann A. Copestake</a></p>
<p>Abstract:
Human translators routinely have to translate rare inflections of words  due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as hablramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the best performing models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.</p>
<p>Keywords:</p>
<h3 id="91. Pushing the Limits of Low-Resource Morphological Inflection.">91. Pushing the Limits of Low-Resource Morphological Inflection.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1091">Paper Link</a>    Pages:984-996</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/148/9479.html">Antonios Anastasopoulos</a> ; <a href="https://dblp.uni-trier.de/pid/03/8155.html">Graham Neubig</a></p>
<p>Abstract:
Recent years have seen exceptional strides in the task of automatic morphological inflection generation. However, for a long tail of languages the necessary resources are hard to come by, and state-of-the-art neural methods that work well under higher resource settings perform poorly in the face of a paucity of data. In response, we propose a battery of improvements that greatly improve performance under such low-resource conditions. First, we present a novel two-step attention architecture for the inflection decoder. In addition, we investigate the effects of cross-lingual transfer from single and multiple languages, as well as monolingual data hallucination. The macro-averaged accuracy of our models outperforms the state-of-the-art by 15 percentage points. Also, we identify the crucial factors for success with cross-lingual transfer for morphological inflection: typological similarity and a common representation across languages.</p>
<p>Keywords:</p>
<h3 id="92. Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank.">92. Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1092">Paper Link</a>    Pages:997-1006</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/127/0273.html">Meishan Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/47/722-4.html">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/23/5204.html">Guohong Fu</a></p>
<p>Abstract:
Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods.</p>
<p>Keywords:</p>
<h3 id="93. Hierarchical Pointer Net Parsing.">93. Hierarchical Pointer Net Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1093">Paper Link</a>    Pages:1007-1017</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/35/7711.html">Linlin Liu</a> ; <a href="https://dblp.uni-trier.de/pid/29/6347.html">Xiang Lin</a> ; <a href="https://dblp.uni-trier.de/pid/62/2078.html">Shafiq R. Joty</a> ; <a href="https://dblp.uni-trier.de/pid/167/2089.html">Simeng Han</a> ; <a href="https://dblp.uni-trier.de/pid/53/6625.html">Lidong Bing</a></p>
<p>Abstract:
Transition-based top-down parsing with pointer networks has achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose hierarchical pointer network parsers, and apply them to dependency and sentence-level discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art.</p>
<p>Keywords:</p>
<h3 id="94. Semi-Supervised Semantic Role Labeling with Cross-View Training.">94. Semi-Supervised Semantic Role Labeling with Cross-View Training.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1094">Paper Link</a>    Pages:1018-1027</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/83/462.html">Rui Cai</a> ; <a href="https://dblp.uni-trier.de/pid/59/6701.html">Mirella Lapata</a></p>
<p>Abstract:
The successful application of neural networks to a variety of NLP tasks has provided strong impetus to develop end-to-end models for semantic role labeling which forego the need for extensive feature engineering. Recent approaches rely on high-quality annotations which are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains). Our work aims to reduce the annotation effort involved via semi-supervised learning. We propose an end-to-end SRL model and demonstrate it can effectively leverage unlabeled data under the cross-view training modeling paradigm. Our LSTM-based semantic role labeler is jointly trained with a sentence learner, which performs POS tagging, dependency parsing, and predicate identification which we argue are critical to learning directly from unlabeled data without recourse to external pre-processing tools. Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish.</p>
<p>Keywords:</p>
<h3 id="95. Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized Representations.">95. Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1095">Paper Link</a>    Pages:1028-1039</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/211/9466.html">Zuyi Bao</a> ; <a href="https://dblp.uni-trier.de/pid/56/2875.html">Rui Huang</a> ; <a href="https://dblp.uni-trier.de/pid/164/3294.html">Chen Li</a> ; <a href="https://dblp.uni-trier.de/pid/z/KennyQiliZhu.html">Kenny Q. Zhu</a></p>
<p>Abstract:
Previous work on cross-lingual sequence labeling tasks either requires parallel data or bridges the two languages through word-by-word matching. Such requirements and assumptions are infeasible for most languages, especially for languages with large linguistic distances, e.g., English and Chinese. In this work, we propose a Multilingual Language Model with deep semantic Alignment (MLMA) to generate language-independent representations for cross-lingual sequence labeling. Our methods require only monolingual corpora with no bilingual resources at all and take advantage of deep contextualized representations. Experimental results show that our approach achieves new state-of-the-art NER and POS performance across European languages, and is also effective on distant language pairs such as English and Chinese.</p>
<p>Keywords:</p>
<h3 id="96. A Lexicon-Based Graph Neural Network for Chinese NER.">96. A Lexicon-Based Graph Neural Network for Chinese NER.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1096">Paper Link</a>    Pages:1040-1050</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/135/6973.html">Tao Gui</a> ; <a href="https://dblp.uni-trier.de/pid/224/6030.html">Yicheng Zou</a> ; <a href="https://dblp.uni-trier.de/pid/52/323-1.html">Qi Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/205/9010.html">Minlong Peng</a> ; <a href="https://dblp.uni-trier.de/pid/218/7289.html">Jinlan Fu</a> ; <a href="https://dblp.uni-trier.de/pid/31/10489.html">Zhongyu Wei</a> ; <a href="https://dblp.uni-trier.de/pid/05/6735.html">Xuanjing Huang</a></p>
<p>Abstract:
Recurrent neural networks (RNN) used for Chinese named entity recognition (NER) that sequentially track character and word information have achieved great success. However, the characteristic of chain structure and the lack of global semantics determine that RNN-based models are vulnerable to word ambiguities. In this work, we try to alleviate this problem by introducing a lexicon-based graph neural network with global semantics, in which lexicon knowledge is used to connect characters to capture the local composition, while a global relay node can capture global sentence semantics and long-range dependency. Based on the multiple graph-based interactions among characters, potential words, and the whole-sentence semantics, word ambiguities can be effectively tackled. Experiments on four NER datasets show that the proposed model achieves significant improvements against other baseline models.</p>
<p>Keywords:</p>
<h3 id="97. CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding.">97. CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1097">Paper Link</a>    Pages:1051-1060</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/242/7766.html">Yijin Liu</a> ; <a href="https://dblp.uni-trier.de/pid/117/4056.html">Fandong Meng</a> ; <a href="https://dblp.uni-trier.de/pid/127/3143.html">Jinchao Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/00/5012-16.html">Jie Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/64/5715.html">Yufeng Chen</a> ; <a href="https://dblp.uni-trier.de/pid/67/3124.html">Jinan Xu</a></p>
<p>Abstract:
Spoken Language Understanding (SLU) mainly involves two tasks, intent detection and slot filling, which are generally modeled jointly in existing works. However, most existing models fail to fully utilize cooccurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block. The CM-block firstly captures slot-specific and intent-specific features from memories in a collaborative manner, and then uses these enriched features to enhance local context representations, based on which the sequential information flow leads to more specific (slot and intent) global utterance representations. Through stacking multiple CM-blocks, our CM-Net is able to alternately perform information exchange among specific memories, local contexts and the global utterance, and thus incrementally enriches each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a self-collected corpus (CAIS). Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and significantly outperforms the baseline models on the CAIS. Additionally, we make the CAIS dataset publicly available for the research community.</p>
<p>Keywords:</p>
<h3 id="98. Tree Transformer: Integrating Tree Structures into Self-Attention.">98. Tree Transformer: Integrating Tree Structures into Self-Attention.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1098">Paper Link</a>    Pages:1061-1070</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/5536.html">Yau-Shian Wang</a> ; <a href="https://dblp.uni-trier.de/pid/81/8056.html">Hung-yi Lee</a> ; <a href="https://dblp.uni-trier.de/pid/04/9878.html">Yun-Nung Chen</a></p>
<p>Abstract:
Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed Constituent Attention module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.</p>
<p>Keywords:</p>
<h3 id="99. Semantic Role Labeling with Iterative Structure Refinement.">99. Semantic Role Labeling with Iterative Structure Refinement.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1099">Paper Link</a>    Pages:1071-1082</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/172/1054.html">Chunchuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pid/04/5629.html">Shay B. Cohen</a> ; <a href="https://dblp.uni-trier.de/pid/08/5391.html">Ivan Titov</a></p>
<p>Abstract:
Modern state-of-the-art Semantic Role Labeling (SRL) methods rely on expressive sentence encoders (e.g., multi-layer LSTMs) but tend to model only local (if any) interactions between individual argument labeling decisions. This contrasts with earlier work and also with the intuition that the labels of individual arguments are strongly interdependent. We model interactions between argument labeling decisions through iterative refinement. Starting with an output produced by a factorized model, we iteratively refine it using a refinement network. Instead of modeling arbitrary interactions among roles and words, we encode prior knowledge about the SRL problem by designing a restricted network architecture capturing non-local interactions. This modeling choice prevents overfitting and results in an effective model, outperforming strong factorized baseline models on all 7 CoNLL-2009 languages, and achieving state-of-the-art results on 5 of them, including English.</p>
<p>Keywords:</p>
<h3 id="100. Entity Projection via Machine Translation for Cross-Lingual NER.">100. Entity Projection via Machine Translation for Cross-Lingual NER.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1100">Paper Link</a>    Pages:1083-1092</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/185/6686.html">Alankar Jain</a> ; <a href="https://dblp.uni-trier.de/pid/188/5984.html">Bhargavi Paranjape</a> ; <a href="https://dblp.uni-trier.de/pid/142/2839.html">Zachary C. Lipton</a></p>
<p>Abstract:
Although over 100 languages are supported by strong off-the-shelf machine translation systems, only a subset of them possess large annotated corpora for named entity recognition. Motivated by this fact, we leverage machine translation to improve annotation-projection approaches to cross-lingual named entity recognition. We propose a system that improves over prior entity-projection methods by: (a) leveraging machine translation systems twice: first for translating sentences and subsequently for translating entities; (b) matching entities based on orthographic and phonetic similarity; and (c) identifying matches based on distributional statistics derived from the dataset. Our approach improves upon current state-of-the-art methods for cross-lingual named entity recognition on 5 diverse languages by an average of 4.1 points. Further, our method achieves state-of-the-art F_1 scores for Armenian, outperforming even a monolingual model trained on Armenian source data.</p>
<p>Keywords:</p>
<h3 id="101. A Bayesian Approach for Sequence Tagging with Crowds.">101. A Bayesian Approach for Sequence Tagging with Crowds.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1101">Paper Link</a>    Pages:1093-1104</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/79/7584.html">Edwin D. Simpson</a> ; <a href="https://dblp.uni-trier.de/pid/85/6201.html">Iryna Gurevych</a></p>
<p>Abstract:
Current methods for sequence tagging, a core task in NLP, are data hungry, which motivates the use of crowdsourcing as a cheap way to obtain labelled data. However, annotators are often unreliable and current aggregation methods cannot capture common types of span annotation error. To address this, we propose a Bayesian method for aggregating sequence tags that reduces errors by modelling sequential dependencies between the annotations as well as the ground-truth labels. By taking a Bayesian approach, we account for uncertainty in the model due to both annotator errors and the lack of data for modelling annotators who complete few tasks. We evaluate our model on crowdsourced data for named entity recognition, information extraction and argument mining, showing that our sequential model outperforms the previous state of the art, and that Bayesian approaches outperform non-Bayesian alternatives. We also find that our approach can reduce crowdsourcing costs through more effective active learning, as it better captures uncertainty in the sequence labels when there are few annotations.</p>
<p>Keywords:</p>
<h3 id="102. A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages.">102. A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1102">Paper Link</a>    Pages:1105-1116</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/47/8427.html">Clara Vania</a> ; <a href="https://dblp.uni-trier.de/pid/225/7708.html">Yova Kementchedjhieva</a> ; <a href="https://dblp.uni-trier.de/pid/30/2756.html">Anders Sgaard</a> ; <a href="https://dblp.uni-trier.de/pid/65/5274.html">Adam Lopez</a></p>
<p>Abstract:
Parsers are available for only a handful of the worlds languages, since they require lots of training data. How far can we get with just a small amount of training data? We systematically compare a set of simple strategies for improving low-resource parsers: data augmentation, which has not been tested before; cross-lingual training; and transliteration. Experimenting on three typologically diverse low-resource languagesNorth Smi, Galician, and KazahWe find that (1) when only the low-resource treebank is available, data augmentation is very helpful; (2) when a related high-resource treebank is available, cross-lingual training is helpful and complements data augmentation; and (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful.</p>
<p>Keywords:</p>
<h3 id="103. Target Language-Aware Constrained Inference for Cross-lingual Dependency Parsing.">103. Target Language-Aware Constrained Inference for Cross-lingual Dependency Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1103">Paper Link</a>    Pages:1117-1128</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/37/4204.html">Tao Meng</a> ; <a href="https://dblp.uni-trier.de/pid/117/4036.html">Nanyun Peng</a> ; <a href="https://dblp.uni-trier.de/pid/18/2428.html">Kai-Wei Chang</a></p>
<p>Abstract:
Prior work on cross-lingual dependency parsing often focuses on capturing the commonalities between source and target languages and overlook the potential to leverage the linguistic properties of the target languages to facilitate the transfer. In this paper, we show that weak supervisions of linguistic knowledge for the target languages can improve a cross-lingual graph-based dependency parser substantially. Specifically, we explore several types of corpus linguistic statistics and compile them into corpus-statistics constraints to facilitate the inference procedure. We propose new algorithms that adapt two techniques, Lagrangian relaxation and posterior regularization, to conduct inference with corpus-statistics constraints. Experiments show that the Lagrangian relaxation and posterior regularization techniques improve the performances on 15 and 17 out of 19 target languages, respectively. The improvements are especially large for the target languages that have different word order features from the source language.</p>
<p>Keywords:</p>
<h3 id="104. Look-up and Adapt: A One-shot Semantic Parser.">104. Look-up and Adapt: A One-shot Semantic Parser.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1104">Paper Link</a>    Pages:1129-1139</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/251/8646.html">Zhichu Lu</a> ; <a href="https://dblp.uni-trier.de/pid/144/0914.html">Forough Arabshahi</a> ; <a href="https://dblp.uni-trier.de/pid/53/6801.html">Igor Labutov</a> ; <a href="https://dblp.uni-trier.de/pid/81/1460.html">Tom M. Mitchell</a></p>
<p>Abstract:
Computing devices have recently become capable of interacting with their end users via natural language. However, they can only operate within a limited supported domain of discourse and fail drastically when faced with an out-of-domain utterance, mainly due to the limitations of their semantic parser. In this paper, we propose a semantic parser that generalizes to out-of-domain examples by learning a general strategy for parsing an unseen utterance through adapting the logical forms of seen utterances, instead of learning to generate a logical form from scratch. Our parser maintains a memory consisting of a representative subset of the seen utterances paired with their logical forms. Given an unseen utterance, our parser works by looking up a similar utterance from the memory and adapting its logical form until it fits the unseen utterance. Moreover, we present a data generation strategy for constructing utterance-logical form pairs from different domains. Our results show an improvement of up to 68.8% on one-shot parsing under two different evaluation settings compared to the baselines.</p>
<p>Keywords:</p>
<h3 id="105. Similarity Based Auxiliary Classifier for Named Entity Recognition.">105. Similarity Based Auxiliary Classifier for Named Entity Recognition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1105">Paper Link</a>    Pages:1140-1149</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/76/5227.html">Shiyuan Xiao</a> ; <a href="https://dblp.uni-trier.de/pid/12/1640.html">Yuanxin Ouyang</a> ; <a href="https://dblp.uni-trier.de/pid/18/5572.html">Wenge Rong</a> ; <a href="https://dblp.uni-trier.de/pid/242/4275.html">Jianxin Yang</a> ; <a href="https://dblp.uni-trier.de/pid/77/6921.html">Zhang Xiong</a></p>
<p>Abstract:
The segmentation problem is one of the fundamental challenges associated with name entity recognition (NER) tasks that aim to reduce the boundary error when detecting a sequence of entity words. A considerable number of advanced approaches have been proposed and most of them exhibit performance deterioration when entities become longer. Inspired by previous work in which a multi-task strategy is used to solve segmentation problems, we design a similarity based auxiliary classifier (SAC), which can distinguish entity words from non-entity words. Unlike conventional classifiers, SAC uses vectors to indicate tags. Therefore, SAC can calculate the similarities between words and tags, and then compute a weighted sum of the tag vectors, which can be considered a useful feature for NER tasks. Empirical results are used to verify the rationality of the SAC structure and demonstrate the SAC models potential in performance improvement against our baseline approaches.</p>
<p>Keywords:</p>
<h3 id="106. Variable beam search for generative neural parsing and its relevance for the analysis of neuro-imaging signal.">106. Variable beam search for generative neural parsing and its relevance for the analysis of neuro-imaging signal.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1106">Paper Link</a>    Pages:1150-1160</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/95/6.html">Benot Crabb</a> ; <a href="https://dblp.uni-trier.de/pid/225/5863.html">Murielle Fabre</a> ; <a href="https://dblp.uni-trier.de/pid/83/7489.html">Christophe Pallier</a></p>
<p>Abstract:
This paper describes a method of variable beam size inference for Recurrent Neural Network Grammar (rnng) by drawing inspiration from sequential Monte-Carlo methods such as particle filtering. The paper studies the relevance of such methods for speeding up the computations of direct generative parsing for rnng. But it also studies the potential cognitive interpretation of the underlying representations built by the search method (beam activity) through analysis of neuro-imaging signal.</p>
<p>Keywords:</p>
<h3 id="107. Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets.">107. Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1107">Paper Link</a>    Pages:1161-1166</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9159.html">Mor Geva</a> ; <a href="https://dblp.uni-trier.de/pid/68/5296.html">Yoav Goldberg</a> ; <a href="https://dblp.uni-trier.de/pid/31/8178.html">Jonathan Berant</a></p>
<p>Abstract:
Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.</p>
<p>Keywords:</p>
<h3 id="108. Robust Text Classifier on Test-Time Budgets.">108. Robust Text Classifier on Test-Time Budgets.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1108">Paper Link</a>    Pages:1167-1172</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/180/3830.html">Md. Rizwan Parvez</a> ; <a href="https://dblp.uni-trier.de/pid/150/4230.html">Tolga Bolukbasi</a> ; <a href="https://dblp.uni-trier.de/pid/18/2428.html">Kai-Wei Chang</a> ; <a href="https://dblp.uni-trier.de/pid/67/4721.html">Venkatesh Saligrama</a></p>
<p>Abstract:
We design a generic framework for learning a robust text classification model that achieves high accuracy under different selection budgets (a.k.a selection rates) at test-time. We take a different approach from existing methods and learn to dynamically filter a large fraction of unimportant words by a low-complexity selector such that any high-complexity state-of-art classifier only needs to process a small fraction of text, relevant for the target task. To this end, we propose a data aggregation method to train the classifier, allowing it to achieve competitive performance on fractured sentences. On four benchmark text classification tasks, we demonstrate that the framework gains consistent speedup with little degradation in accuracy on various selection budgets.</p>
<p>Keywords:</p>
<h3 id="109. Commonsense Knowledge Mining from Pretrained Models.">109. Commonsense Knowledge Mining from Pretrained Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1109">Paper Link</a>    Pages:1173-1178</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/5513.html">Joe Davison</a> ; <a href="https://dblp.uni-trier.de/pid/205/4387.html">Joshua Feldman</a> ; <a href="https://dblp.uni-trier.de/pid/67/9012.html">Alexander M. Rush</a></p>
<p>Abstract:
Inferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triples validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.</p>
<p>Keywords:</p>
<h3 id="110. RNN Architecture Learning with Sparse Regularization.">110. RNN Architecture Learning with Sparse Regularization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1110">Paper Link</a>    Pages:1179-1184</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/49/11425.html">Jesse Dodge</a> ; <a href="https://dblp.uni-trier.de/pid/19/376.html">Roy Schwartz</a> ; <a href="https://dblp.uni-trier.de/pid/69/7742.html">Hao Peng</a> ; <a href="https://dblp.uni-trier.de/pid/90/5204.html">Noah A. Smith</a></p>
<p>Abstract:
Neural models for NLP typically use large numbers of parameters to reach state-of-the-art performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of models that is closely connected to weighted finite-state automata (WFSAs). We take advantage of rational RNNs natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment analysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parameters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of rational RNNs. We show that sparsifying such models makes them easier to visualize, and we present models that rely exclusively on as few as three WFSAs after pruning more than 90% of the weights. We publicly release our code.</p>
<p>Keywords:</p>
<h3 id="111. Analytical Methods for Interpretable Ultradense Word Embeddings.">111. Analytical Methods for Interpretable Ultradense Word Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1111">Paper Link</a>    Pages:1185-1191</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/213/8070.html">Philipp Dufter</a> ; <a href="https://dblp.uni-trier.de/pid/s/HinrichSchutze.html">Hinrich Schtze</a></p>
<p>Abstract:
Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in closed form, is hyperparameter-free and thus more robust than Densifier. We evaluate the three methods on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing gender bias from embeddings.</p>
<p>Keywords:</p>
<h3 id="112. Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks.">112. Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1112">Paper Link</a>    Pages:1192-1197</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/8985.html">Zi-Yi Dou</a> ; <a href="https://dblp.uni-trier.de/pid/238/2737.html">Keyi Yu</a> ; <a href="https://dblp.uni-trier.de/pid/148/9479.html">Antonios Anastasopoulos</a></p>
<p>Abstract:
Learning general representations of text is a fundamental problem for many natural language understanding (NLU) tasks. Previously, researchers have proposed to use language model pre-training and multi-task learning to learn robust representations. However, these methods can achieve sub-optimal performance in low-resource scenarios. Inspired by the recent success of optimization-based meta-learning algorithms, in this paper, we explore the model-agnostic meta-learning algorithm (MAML) and its variants for low-resource NLU tasks. We validate our methods on the GLUE benchmark and show that our proposed models can outperform several strong baselines. We further empirically demonstrate that the learned representations can be adapted to new tasks efficiently and effectively.</p>
<p>Keywords:</p>
<h3 id="113. Retrofitting Contextualized Word Embeddings with Paraphrases.">113. Retrofitting Contextualized Word Embeddings with Paraphrases.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1113">Paper Link</a>    Pages:1198-1203</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/132/8060.html">Weijia Shi</a> ; <a href="https://dblp.uni-trier.de/pid/173/2608.html">Muhao Chen</a> ; <a href="https://dblp.uni-trier.de/pid/23/8064.html">Pei Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/18/2428.html">Kai-Wei Chang</a></p>
<p>Abstract:
Contextualized word embeddings, such as ELMo, provide meaningful representations for words and their contexts. They have been shown to have a great impact on downstream applications. However, we observe that the contextualized embeddings of a word might change drastically when its contexts are paraphrased. As these embeddings are over-sensitive to the context, the downstream model may make different predictions when the input sentence is paraphrased. To address this issue, we propose a post-processing approach to retrofit the embedding with paraphrases. Our method learns an orthogonal transformation on the input space of the contextualized word embedding model, which seeks to minimize the variance of word representations on paraphrased contexts. Experiments show that the proposed method significantly improves ELMo on various sentence classification and inference tasks.</p>
<p>Keywords:</p>
<h3 id="114. Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling.">114. Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1114">Paper Link</a>    Pages:1204-1209</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/36/7028.html">Linqing Liu</a> ; <a href="https://dblp.uni-trier.de/pid/03/1094-17.html">Wei Yang</a> ; <a href="https://dblp.uni-trier.de/pid/134/5708.html">Jinfeng Rao</a> ; <a href="https://dblp.uni-trier.de/pid/207/7684.html">Raphael Tang</a> ; <a href="https://dblp.uni-trier.de/pid/00/7739.html">Jimmy Lin</a></p>
<p>Abstract:
Semantic similarity modeling is central to many NLP problems such as natural language inference and question answering. Syntactic structures interact closely with semantics in learning compositional representations and alleviating long-range dependency issues. How-ever, such structure priors have not been well exploited in previous work for semantic mod-eling. To examine their effectiveness, we start with the Pairwise Word Interaction Model, one of the best models according to a recent reproducibility study, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs. In addition, we introduce residual connections to the deep convolutional neural network component of the model. Extensive evaluations on eight benchmark datasets show that incorporating structural information contributes to consistent improvements over strong baselines.</p>
<p>Keywords:</p>
<h3 id="115. Neural Linguistic Steganography.">115. Neural Linguistic Steganography.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1115">Paper Link</a>    Pages:1210-1215</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/234/8809.html">Zachary M. Ziegler</a> ; <a href="https://dblp.uni-trier.de/pid/166/1720.html">Yuntian Deng</a> ; <a href="https://dblp.uni-trier.de/pid/67/9012.html">Alexander M. Rush</a></p>
<p>Abstract:
Whereas traditional cryptography encrypts a secret message into an unintelligible form, steganography conceals that communication is taking place by encoding a secret message into a cover signal. Language is a particularly pragmatic cover signal due to its benign occurrence and independence from any one medium. Traditionally, linguistic steganography systems encode secret messages in existing text via synonym substitution or word order rearrangements. Advances in neural language models enable previously impractical generation-based techniques. We propose a steganography technique based on arithmetic coding with large-scale neural language models. We find that our approach can generate realistic looking cover sentences as evaluated by humans, while at the same time preserving security by matching the cover message distribution with the language model distribution.</p>
<p>Keywords:</p>
<h3 id="116. The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization.">116. The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1116">Paper Link</a>    Pages:1216-1221</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/8261.html">Simeng Sun</a> ; <a href="https://dblp.uni-trier.de/pid/58/896.html">Ani Nenkova</a></p>
<p>Abstract:
ROUGE is widely used to automatically evaluate summarization systems. However, ROUGE measures semantic overlap between a system summary and a human reference on word-string level, much at odds with the contemporary treatment of semantic meaning. Here we present a suite of experiments on using distributed representations for evaluating summarizers, both in reference-based and in reference-free setting. Our experimental results show that the max value over each dimension of the summary ELMo word embeddings is a good representation that results in high correlation with human ratings. Averaging the cosine similarity of all encoders we tested yields high correlation with manual scores in reference-free setting. The distributed representations outperform ROUGE in recent corpora for abstractive news summarization but are less good on test data used in past evaluations.</p>
<p>Keywords:</p>
<h3 id="117. Attention Optimization for Abstractive Document Summarization.">117. Attention Optimization for Abstractive Document Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1117">Paper Link</a>    Pages:1222-1228</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/34/1462.html">Min Gui</a> ; <a href="https://dblp.uni-trier.de/pid/93/1076.html">Junfeng Tian</a> ; <a href="https://dblp.uni-trier.de/pid/06/2293.html">Rui Wang</a> ; <a href="https://dblp.uni-trier.de/pid/43/5146.html">Zhenglu Yang</a></p>
<p>Abstract:
Attention plays a key role in the improvement of sequence-to-sequence-based document summarization models. To obtain a powerful attention helping with reproducing the most salient information and avoiding repetitions, we augment the vanilla attention model from both local and global aspects. We propose attention refinement unit paired with local variance loss to impose supervision on the attention model at each decoding step, and we also propose a global variance loss to optimize the attention distributions of all decoding steps from the global perspective. The performances on CNN/Daily Mail dataset verify the effectiveness of our methods.</p>
<p>Keywords:</p>
<h3 id="118. Rewarding Coreference Resolvers for Being Consistent with World Knowledge.">118. Rewarding Coreference Resolvers for Being Consistent with World Knowledge.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1118">Paper Link</a>    Pages:1229-1235</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/202/8441.html">Rahul Aralikatte</a> ; <a href="https://dblp.uni-trier.de/pid/242/7101.html">Heather Lent</a> ; <a href="https://dblp.uni-trier.de/pid/218/7022.html">Ana Valeria Gonzlez-Garduo</a> ; <a href="https://dblp.uni-trier.de/pid/145/9324.html">Daniel Hershcovich</a> ; <a href="https://dblp.uni-trier.de/pid/93/10695.html">Chen Qiu</a> ; <a href="https://dblp.uni-trier.de/pid/33/2327.html">Anders Sandholm</a> ; <a href="https://dblp.uni-trier.de/pid/248/7556.html">Michael Ringaard</a> ; <a href="https://dblp.uni-trier.de/pid/30/2756.html">Anders Sgaard</a></p>
<p>Abstract:
Unresolved coreference is a bottleneck for relation extraction, and high-quality coreference resolvers may produce an output that makes it a lot easier to extract knowledge triples. We show how to improve coreference resolvers by forwarding their input to a relation extraction system and reward the resolvers for producing triples that are found in knowledge bases. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning.</p>
<p>Keywords:</p>
<h3 id="119. An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction.">119. An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1119">Paper Link</a>    Pages:1236-1242</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/211/7611.html">Shun Kiyono</a> ; <a href="https://dblp.uni-trier.de/pid/78/6923.html">Jun Suzuki</a> ; <a href="https://dblp.uni-trier.de/pid/213/1183.html">Masato Mita</a> ; <a href="https://dblp.uni-trier.de/pid/67/9254.html">Tomoya Mizumoto</a> ; <a href="https://dblp.uni-trier.de/pid/90/3315.html">Kentaro Inui</a></p>
<p>Abstract:
The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set (F0.5=65.0) and the official test set of the BEA-2019 shared task (F0.5=70.2) without making any modifications to the model architecture.</p>
<p>Keywords:</p>
<h3 id="120. A Multilingual Topic Model for Learning Weighted Topic Links Across Corpora with Low Comparability.">120. A Multilingual Topic Model for Learning Weighted Topic Links Across Corpora with Low Comparability.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1120">Paper Link</a>    Pages:1243-1248</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/09/6743.html">Weiwei Yang</a> ; <a href="https://dblp.uni-trier.de/pid/57/5950.html">Jordan L. Boyd-Graber</a> ; <a href="https://dblp.uni-trier.de/pid/p/PhilipResnik.html">Philip Resnik</a></p>
<p>Abstract:
Multilingual topic models (MTMs) learn topics on documents in multiple languages. Past models align topics across languages by implicitly assuming the documents in different languages are highly comparable, often a false assumption. We introduce a new model that does not rely on this assumption, particularly useful in important low-resource language scenarios. Our MTM learns weighted topic links and connects cross-lingual topics only when the dominant words defining them are similar, outperforming LDA and previous MTMs in classification tasks using documents topic posteriors as features. It also learns coherent topics on documents with low comparability.</p>
<p>Keywords:</p>
<h3 id="121. Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study.">121. Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1121">Paper Link</a>    Pages:1249-1254</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/5238.html">Bonan Min</a> ; <a href="https://dblp.uni-trier.de/pid/254/8042.html">Xiaoxi Zhao</a></p>
<p>Abstract:
Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the unemployment rate, an indicator widely used by economists and policymakers. We argue that events reported in streaming news can be used as micro-sensors for measuring socio-economic conditions. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic indicators with events. We empirically demonstrate strong correlation between ECIM values to several representative indicators in socio-economic research.</p>
<p>Keywords:</p>
<h3 id="122. Towards Extracting Medical Family History from Natural Language Interactions: A New Dataset and Baselines.">122. Towards Extracting Medical Family History from Natural Language Interactions: A New Dataset and Baselines.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1122">Paper Link</a>    Pages:1255-1260</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/136/9228.html">Mahmoud Azab</a> ; <a href="https://dblp.uni-trier.de/pid/254/8098.html">Stephane Dadian</a> ; <a href="https://dblp.uni-trier.de/pid/05/2290.html">Vivi Nastase</a> ; <a href="https://dblp.uni-trier.de/pid/254/8088.html">Larry An</a> ; <a href="https://dblp.uni-trier.de/pid/m/RadaMihalcea.html">Rada Mihalcea</a></p>
<p>Abstract:
We introduce a new dataset consisting of natural language interactions annotated with medical family histories, obtained during interactions with a genetic counselor and through crowdsourcing, following a questionnaire created by experts in the domain. We describe the data collection process and the annotations performed by medical professionals, including illness and personal attributes (name, age, gender, family relationships) for the patient and their family members. An initial system that performs argument identification and relation extraction shows promising results  average F-score of 0.87 on complex sentences on the targeted relations.</p>
<p>Keywords:</p>
<h3 id="123. Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue.">123. Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1123">Paper Link</a>    Pages:1261-1266</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/48/7536.html">Chenguang Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/232/1866.html">Michael Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/41/4753.html">Xuedong Huang</a></p>
<p>Abstract:
In task-oriented dialogues, Natural Language Generation (NLG) is the final yet crucial step to produce user-facing system utterances. The result of NLG is directly related to the perceived quality and usability of a dialogue system. While most existing systems provide semantically correct responses given goals to present, they struggle to match the variation and fluency in the human language. In this paper, we propose a novel multi-task learning framework, NLG-LM, for natural language generation. In addition to generating high-quality responses conveying the required information, it also explicitly targets for naturalness in generated responses via an unconditioned language model. This can significantly improve the learning of style and variation in human language. Empirical results show that this multi-task learning framework outperforms previous models across multiple datasets. For example, it improves the previous best BLEU score on the E2E-NLG dataset by 2.2%, and on the Laptop dataset by 6.1%.</p>
<p>Keywords:</p>
<h3 id="124. Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation.">124. Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1124">Paper Link</a>    Pages:1267-1272</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/71/425.html">Min Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/172/1346-1.html">Yisen Wang</a> ; <a href="https://dblp.uni-trier.de/pid/90/6959-3.html">Yuan Luo</a></p>
<p>Abstract:
Variational encoder-decoders have achieved well-recognized performance in the dialogue generation task. Existing works simply assume the Gaussian priors of the latent variable, which are incapable of representing complex latent variables effectively. To address the issues, we propose to use the Dirichlet distribution with flexible structures to characterize the latent variables in place of the traditional Gaussian distribution, called Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder model (Dir-VHRED). Based on which, we further find that there is redundancy among the dimensions of latent variable, and the lengths and sentence patterns of the responses can be strongly correlated to each dimension of the latent variable. Therefore, controllable responses can be generated through specifying the value of each dimension of the latent variable. Experimental results on benchmarks show that our proposed Dir-VHRED yields substantial improvements on negative log-likelihood, word-embedding-based and human evaluations.</p>
<p>Keywords:</p>
<h3 id="125. Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling.">125. Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1125">Paper Link</a>    Pages:1273-1278</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/164/5802.html">Bo-Hsiang Tseng</a> ; <a href="https://dblp.uni-trier.de/pid/136/9233.html">Marek Rei</a> ; <a href="https://dblp.uni-trier.de/pid/202/1739.html">Pawel Budzianowski</a> ; <a href="https://dblp.uni-trier.de/pid/40/5352.html">Richard E. Turner</a> ; <a href="https://dblp.uni-trier.de/pid/b/WilliamJByrne.html">Bill Byrne</a> ; <a href="https://dblp.uni-trier.de/pid/14/6532.html">Anna Korhonen</a></p>
<p>Abstract:
Dialogue systems benefit greatly from optimizing on detailed annotations, such as transcribed utterances, internal dialogue state representations and dialogue act labels. However, collecting these annotations is expensive and time-consuming, holding back development in the area of dialogue modelling. In this paper, we investigate semi-supervised learning methods that are able to reduce the amount of required intermediate labelling. We find that by leveraging un-annotated data instead, the amount of turn-level annotations of dialogue state can be significantly reduced when building a neural dialogue system. Our analysis on the MultiWOZ corpus, covering a range of domains and topics, finds that annotations can be reduced by up to 30% while maintaining equivalent system performance. We also describe and evaluate the first end-to-end dialogue model created for the MultiWOZ corpus.</p>
<p>Keywords:</p>
<h3 id="126. A Progressive Model to Enable Continual Learning for Semantic Slot Filling.">126. A Progressive Model to Enable Continual Learning for Semantic Slot Filling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1126">Paper Link</a>    Pages:1279-1284</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/30/383.html">Yilin Shen</a> ; <a href="https://dblp.uni-trier.de/pid/06/8405.html">Xiangyu Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/55/2789.html">Hongxia Jin</a></p>
<p>Abstract:
Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on precollected data, it is crucial to continually improve the model after deployment to learn users new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the model on all data or fine tune the model only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component; and meanwhile enables this new component to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24% and 3.03% on two benchmark datasets.</p>
<p>Keywords:</p>
<h3 id="127. CASA-NLU: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots.">127. CASA-NLU: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1127">Paper Link</a>    Pages:1285-1290</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/238/0361.html">Arshit Gupta</a> ; <a href="https://dblp.uni-trier.de/pid/21/1048.html">Peng Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/249/2859.html">Garima Lalwani</a> ; <a href="https://dblp.uni-trier.de/pid/15/4305.html">Mona T. Diab</a></p>
<p>Abstract:
Natural Language Understanding (NLU) is a core component of dialog systems. It typically involves two tasks - Intent Classification (IC) and Slot Labeling (SL), which are then followed by a dialogue management (DM) component. Such NLU systems cater to utterances in isolation, thus pushing the problem of context management to DM. However, contextual information is critical to the correct prediction of intents in a conversation. Prior work on contextual NLU has been limited in terms of the types of contextual signals used and the understanding of their impact on the model. In this work, we propose a context-aware self-attentive NLU (CASA-NLU) model that uses multiple signals over a variable context window, such as previous intents, slots, dialog acts and utterances, in addition to the current user utterance. CASA-NLU outperforms a recurrent contextual NLU baseline on two conversational datasets, yielding a gain of up to 7% on the IC task. Moreover, a non-contextual variant of CASA-NLU achieves state-of-the-art performance on standard public datasets - SNIPS and ATIS.</p>
<p>Keywords:</p>
<h3 id="128. Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems.">128. Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1128">Paper Link</a>    Pages:1291-1296</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/23/6950.html">Jia Li</a> ; <a href="https://dblp.uni-trier.de/pid/194/2515.html">Chongyang Tao</a> ; <a href="https://dblp.uni-trier.de/pid/95/6985-14.html">Wei Wu</a> ; <a href="https://dblp.uni-trier.de/pid/25/2643.html">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pid/63/1870.html">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/19/2405-1.html">Rui Yan</a></p>
<p>Abstract:
We study how to sample negative examples to automatically construct a training set for effective model learning in retrieval-based dialogue systems. Following an idea of dynamically adapting negative examples to matching models in learning, we consider four strategies including minimum sampling, maximum sampling, semi-hard sampling, and decay-hard sampling. Empirical studies on two benchmarks with three matching models indicate that compared with the widely used random sampling strategy, although the first two strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks.</p>
<p>Keywords:</p>
<h3 id="129. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables.">129. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1129">Paper Link</a>    Pages:1297-1303</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/46/9231.html">Zihan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/225/5387.html">Jamin Shin</a> ; <a href="https://dblp.uni-trier.de/pid/03/4702-12.html">Yan Xu</a> ; <a href="https://dblp.uni-trier.de/pid/212/6318.html">Genta Indra Winata</a> ; <a href="https://dblp.uni-trier.de/pid/84/586.html">Peng Xu</a> ; <a href="https://dblp.uni-trier.de/pid/174/2905.html">Andrea Madotto</a> ; <a href="https://dblp.uni-trier.de/pid/29/4187.html">Pascale Fung</a></p>
<p>Abstract:
Despite the surging demands for multilingual task-oriented dialog systems (e.g., Alexa, Google Home), there has been less research done in multilingual or cross-lingual scenarios. Hence, we propose a zero-shot adaptation of task-oriented dialogue system to low-resource languages. To tackle this challenge, we first use a set of very few parallel word pairs to refine the aligned cross-lingual word-level representations. We then employ a latent variable model to cope with the variance of similar sentences across different languages, which is induced by imperfect cross-lingual alignments and inherent differences in languages. Finally, the experimental results show that even though we utilize much less external resources, our model achieves better adaptation performance for natural language understanding task (i.e., the intent detection and slot filling) compared to the current state-of-the-art model in the zero-shot scenario.</p>
<p>Keywords:</p>
<h3 id="130. Modeling Multi-Action Policy for Task-Oriented Dialogues.">130. Modeling Multi-Action Policy for Task-Oriented Dialogues.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1130">Paper Link</a>    Pages:1304-1310</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/19/2932-4.html">Lei Shu</a> ; <a href="https://dblp.uni-trier.de/pid/11/6234.html">Hu Xu</a> ; <a href="https://dblp.uni-trier.de/pid/l/BingLiu1.html">Bing Liu</a> ; <a href="https://dblp.uni-trier.de/pid/77/7837.html">Piero Molino</a></p>
<p>Abstract:
Dialogue management (DM) plays a key role in the quality of the interaction with the user in a task-oriented dialogue system. In most existing approaches, the agent predicts only one DM policy action per turn. This significantly limits the expressive power of the conversational agent and introduces unwanted turns of interactions that may challenge users patience. Longer conversations also lead to more errors and the system needs to be more robust to handle them. In this paper, we compare the performance of several models on the task of predicting multiple acts for each turn. A novel policy model is proposed based on a recurrent cell called gated Continue-Act-Slots (gCAS) that overcomes the limitations of the existing models. Experimental results show that gCAS outperforms other approaches. The datasets and code are available at <a href="https://leishu02.github.io/">https://leishu02.github.io/</a>.</p>
<p>Keywords:</p>
<h3 id="131. An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction.">131. An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1131">Paper Link</a>    Pages:1311-1316</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/239/4267.html">Stefan Larson</a> ; <a href="https://dblp.uni-trier.de/pid/239/4253.html">Anish Mahendran</a> ; <a href="https://dblp.uni-trier.de/pid/228/8552.html">Joseph J. Peper</a> ; <a href="https://dblp.uni-trier.de/pid/84/6475.html">Christopher Clarke</a> ; <a href="https://dblp.uni-trier.de/pid/57/1584.html">Andrew Lee</a> ; <a href="https://dblp.uni-trier.de/pid/180/8211.html">Parker Hill</a> ; <a href="https://dblp.uni-trier.de/pid/84/9011.html">Jonathan K. Kummerfeld</a> ; <a href="https://dblp.uni-trier.de/pid/133/3698.html">Kevin Leach</a> ; <a href="https://dblp.uni-trier.de/pid/29/5459.html">Michael A. Laurenzano</a> ; <a href="https://dblp.uni-trier.de/pid/35/1464.html">Lingjia Tang</a> ; <a href="https://dblp.uni-trier.de/pid/48/6796.html">Jason Mars</a></p>
<p>Abstract:
Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scopei.e., queries that do not fall into any of the systems supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.</p>
<p>Keywords:</p>
<h3 id="132. Automatically Learning Data Augmentation Policies for Dialogue Tasks.">132. Automatically Learning Data Augmentation Policies for Dialogue Tasks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1132">Paper Link</a>    Pages:1317-1323</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/88/7733.html">Tong Niu</a> ; <a href="https://dblp.uni-trier.de/pid/32/5243.html">Mohit Bansal</a></p>
<p>Abstract:
Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an images semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategys required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.</p>
<p>Keywords:</p>
<h3 id="133. uniblock: Scoring and Filtering Corpus with Unicode Block Information.">133. uniblock: Scoring and Filtering Corpus with Unicode Block Information.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1133">Paper Link</a>    Pages:1324-1329</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/226/5635.html">Yingbo Gao</a> ; <a href="https://dblp.uni-trier.de/pid/150/4206.html">Weiyue Wang</a> ; <a href="https://dblp.uni-trier.de/pid/n/HermannNey.html">Hermann Ney</a></p>
<p>Abstract:
The preprocessing pipelines in Natural Language Processing usually involve a step of removing sentences consisted of illegal characters. The definition of illegal characters and the specific removal strategy depend on the task, language, domain, etc, which often lead to tiresome and repetitive scripting of rules. In this paper, we introduce a simple statistical method, uniblock, to overcome this problem. For each sentence, uniblock generates a fixed-size feature vector using Unicode block information of the characters. A Gaussian mixture model is then estimated on some clean corpus using variational inference. The learned model can then be used to score sentences and filter corpus. We present experimental results on Sentiment Analysis, Language Modeling and Machine Translation, and show the simplicity and effectiveness of our method.</p>
<p>Keywords:</p>
<h3 id="134. Multilingual word translation using auxiliary languages.">134. Multilingual word translation using auxiliary languages.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1134">Paper Link</a>    Pages:1330-1335</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/226/2020.html">Hagai Taitelbaum</a> ; <a href="https://dblp.uni-trier.de/pid/c/GalChechik.html">Gal Chechik</a> ; <a href="https://dblp.uni-trier.de/pid/65/6574.html">Jacob Goldberger</a></p>
<p>Abstract:
Current multilingual word translation methods are focused on jointly learning mappings from each language to a shared space. The actual translation, however, is still performed as an isolated bilingual task. In this study we propose a multilingual translation procedure that uses all the learned mappings to translate a word from one language to another. For each source word, we first search for the most relevant auxiliary languages. We then use the translations to these languages to form an improved representation of the source word. Finally, this representation is used for the actual translation to the target language. Experiments on a standard multilingual word translation benchmark demonstrate that our model outperforms state of the art results.</p>
<p>Keywords:</p>
<h3 id="135. Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons.">135. Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1135">Paper Link</a>    Pages:1336-1341</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/80/4735.html">Jie Hao</a> ; <a href="https://dblp.uni-trier.de/pid/02/3674-7.html">Xing Wang</a> ; <a href="https://dblp.uni-trier.de/pid/s/ShumingShi-1.html">Shuming Shi</a> ; <a href="https://dblp.uni-trier.de/pid/43/661.html">Jinfeng Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/71/9281.html">Zhaopeng Tu</a></p>
<p>Abstract:
Recent studies have shown that a hybrid of self-attention networks (SANs) and recurrent neural networks RNNs outperforms both individual architectures, while not much is known about why the hybrid models work. With the belief that modeling hierarchical structure is an essential complementary between SANs and RNNs, we propose to further enhance the strength of hybrid models with an advanced variant of RNNs  Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented inductive bias to perform tree-like composition. Experimental results on the benchmark machine translation task show that the proposed approach outperforms both individual architectures and a standard hybrid model. Further analyses on targeted linguistic evaluation and logical inference tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure.</p>
<p>Keywords:</p>
<h3 id="136. Vecalign: Improved Sentence Alignment in Linear Time and Space.">136. Vecalign: Improved Sentence Alignment in Linear Time and Space.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1136">Paper Link</a>    Pages:1342-1348</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/08/3723.html">Brian Thompson</a> ; <a href="https://dblp.uni-trier.de/pid/84/4538.html">Philipp Koehn</a></p>
<p>Abstract:
We introduce Vecalign, a novel bilingual sentence alignment method which is linear in time and space with respect to the number of sentences being aligned and which requires only bilingual sentence embeddings. On a standard GermanFrench test set, Vecalign outperforms the previous state-of-the-art method (which has quadratic time complexity and requires a machine translation system) by 5 F1 points. It substantially outperforms the popular Hunalign toolkit at recovering Bible verse alignments in medium- to low-resource language pairs, and it improves downstream MT quality by 1.7 and 1.6 BLEU in Sinhala-English and Nepali-English, respectively, compared to the Hunalign-based Paracrawl pipeline.</p>
<p>Keywords:</p>
<h3 id="137. Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation.">137. Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1137">Paper Link</a>    Pages:1349-1354</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/122/2999.html">Baigong Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/196/9053.html">Renjie Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/04/10068.html">Mingbo Ma</a> ; <a href="https://dblp.uni-trier.de/pid/03/5847-1.html">Liang Huang</a></p>
<p>Abstract:
Simultaneous translation is widely useful but remains challenging. Previous work falls into two main categories: (a) fixed-latency policies such as Ma et al. (2019) and (b) adaptive policies such as Gu et al. (2017). The former are simple and effective, but have to aggressively predict future content due to diverging source-target word order; the latter do not anticipate, but suffer from unstable and inefficient training. To combine the merits of both approaches, we propose a simple supervised-learning framework to learn an adaptive policy from oracle READ/WRITE sequences generated from parallel text. At each step, such an oracle sequence chooses to WRITE the next target word if the available source sentence context provides enough information to do so, otherwise READ the next source word. Experiments on German&lt;=&gt;English show that our method, without retraining the underlying NMT model, can learn flexible policies with better BLEU scores and similar latencies compared to previous work.</p>
<p>Keywords:</p>
<h3 id="138. Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER.">138. Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1138">Paper Link</a>    Pages:1355-1360</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/198/0534.html">Phillip Keung</a> ; <a href="https://dblp.uni-trier.de/pid/68/10509.html">Yichao Lu</a> ; <a href="https://dblp.uni-trier.de/pid/16/8158.html">Vikas Bhardwaj</a></p>
<p>Abstract:
Contextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated state-of-the-art performance on various NLP tasks. Recent work with the multilingual version of BERT has shown that the model performs surprisingly well in cross-lingual settings, even when only labeled English data is used to finetune the model. We improve upon multilingual BERTs zero-resource cross-lingual performance via adversarial learning. We report the magnitude of the improvement on the multilingual MLDoc text classification and CoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that language-adversarial training encourages BERT to align the embeddings of English documents and their translations, which may be the cause of the observed performance gains.</p>
<p>Keywords:</p>
<h3 id="139. Recurrent Positional Embedding for Neural Machine Translation.">139. Recurrent Positional Embedding for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1139">Paper Link</a>    Pages:1361-1367</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/78/9623.html">Kehai Chen</a> ; <a href="https://dblp.uni-trier.de/pid/w/RuiWang15.html">Rui Wang</a> ; <a href="https://dblp.uni-trier.de/pid/76/5745.html">Masao Utiyama</a> ; <a href="https://dblp.uni-trier.de/pid/95/5465.html">Eiichiro Sumita</a></p>
<p>Abstract:
In the Transformer network architecture, positional embeddings are used to encode order dependencies into the input representation. However, this input representation only involves static order dependencies based on discrete numerical information, that is, are independent of word content. To address this issue, this work proposes a recurrent positional embedding approach based on word vector. In this approach, these recurrent positional embeddings are learned by a recurrent neural network, encoding word content-based order dependencies into the input representation. They are then integrated into the existing multi-head self-attention model as independent heads or part of each head. The experimental results revealed that the proposed approach improved translation performance over that of the state-of-the-art Transformer baseline in WMT14 English-to-German and NIST Chinese-to-English translation tasks.</p>
<p>Keywords:</p>
<h3 id="140. Machine Translation for Machines: the Sentiment Classification Use Case.">140. Machine Translation for Machines: the Sentiment Classification Use Case.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1140">Paper Link</a>    Pages:1368-1374</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/180/5739.html">Amirhossein Tebbifakhr</a> ; <a href="https://dblp.uni-trier.de/pid/50/1445.html">Luisa Bentivogli</a> ; <a href="https://dblp.uni-trier.de/pid/95/3678.html">Matteo Negri</a> ; <a href="https://dblp.uni-trier.de/pid/96/4886.html">Marco Turchi</a></p>
<p>Abstract:
We propose a neural machine translation (NMT) approach that, instead of pursuing adequacy and fluency (human-oriented quality criteria), aims to generate translations that are best suited as input to a natural language processing component designed for a specific downstream task (a machine-oriented criterion). Towards this objective, we present a reinforcement learning technique based on a new candidate sampling strategy, which exploits the results obtained on the downstream task as weak feedback. Experiments in sentiment classification of Twitter data in German and Italian show that feeding an English classifier with machine-oriented translations significantly improves its performance. Classification results outperform those obtained with translations produced by general-purpose NMT models as well as by an approach based on reinforcement learning. Moreover, our results on both languages approximate the classification accuracy computed on gold standard English tweets.</p>
<p>Keywords:</p>
<h3 id="141. Investigating the Effectiveness of BPE: The Power of Shorter Sequences.">141. Investigating the Effectiveness of BPE: The Power of Shorter Sequences.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1141">Paper Link</a>    Pages:1375-1381</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/99/7907.html">Matthias Gall</a></p>
<p>Abstract:
Byte-Pair Encoding (BPE) is an unsupervised sub-word tokenization technique, commonly used in neural machine translation and other NLP tasks. Its effectiveness makes it a de facto standard, but the reasons for this are not well understood. We link BPE to the broader family of dictionary-based compression algorithms and compare it with other members of this family. Our experiments across datasets, language pairs, translation models, and vocabulary size show that - given a fixed vocabulary size budget - the fewer tokens an algorithm needs to cover the test set, the better the translation (as measured by BLEU).</p>
<p>Keywords:</p>
<h3 id="142. HABLex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation.">142. HABLex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1142">Paper Link</a>    Pages:1382-1387</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/08/3723.html">Brian Thompson</a> ; <a href="https://dblp.uni-trier.de/pid/136/9234.html">Rebecca Knowles</a> ; <a href="https://dblp.uni-trier.de/pid/36/31.html">Xuan Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/132/8648.html">Huda Khayrallah</a> ; <a href="https://dblp.uni-trier.de/pid/58/3217.html">Kevin Duh</a> ; <a href="https://dblp.uni-trier.de/pid/84/4538.html">Philipp Koehn</a></p>
<p>Abstract:
Bilingual lexicons are valuable resources used by professional human translators. While these resources can be easily incorporated in statistical machine translation, it is unclear how to best do so in the neural framework. In this work, we present the HABLex dataset, designed to test methods for bilingual lexicon integration into neural machine translation. Our data consists of human generated alignments of words and phrases in machine translation test sets in three language pairs (Russian-English, Chinese-English, and Korean-English), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines - constrained decoding and continued training - and an improvement to continued training to address overfitting.</p>
<p>Keywords:</p>
<h3 id="143. Handling Syntactic Divergence in Low-resource Machine Translation.">143. Handling Syntactic Divergence in Low-resource Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1143">Paper Link</a>    Pages:1388-1394</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/161/2679.html">Chunting Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/127/0230.html">Xuezhe Ma</a> ; <a href="https://dblp.uni-trier.de/pid/123/0773.html">Junjie Hu</a> ; <a href="https://dblp.uni-trier.de/pid/03/8155.html">Graham Neubig</a></p>
<p>Abstract:
Despite impressive empirical successes of neural machine translation (NMT) on standard benchmarks, limited parallel data impedes the application of NMT models to many language pairs. Data augmentation methods such as back-translation make it possible to use monolingual data to help alleviate these issues, but back-translation itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of training-time supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives.</p>
<p>Keywords:</p>
<h3 id="144. Speculative Beam Search for Simultaneous Translation.">144. Speculative Beam Search for Simultaneous Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1144">Paper Link</a>    Pages:1395-1402</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/196/9053.html">Renjie Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/04/10068.html">Mingbo Ma</a> ; <a href="https://dblp.uni-trier.de/pid/122/2999.html">Baigong Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/03/5847-1.html">Liang Huang</a></p>
<p>Abstract:
Beam search is universally used in (full-sentence) machine translation but its application to simultaneous translation remains highly non-trivial, where output words are committed on the fly. In particular, the recently proposed wait-k policy (Ma et al., 2018) is a simple and effective method that (after an initial wait) commits one output word on receiving each input word, making beam search seemingly inapplicable. To address this challenge, we propose a new speculative beam search algorithm that hallucinates several steps into the future in order to reach a more accurate decision by implicitly benefiting from a target language model. This idea makes beam search applicable for the first time to the generation of a single word in each step. Experiments over diverse language pairs show large improvement compared to previous work.</p>
<p>Keywords:</p>
<h3 id="145. Self-Attention with Structural Position Representations.">145. Self-Attention with Structural Position Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1145">Paper Link</a>    Pages:1403-1409</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/02/3674-7.html">Xing Wang</a> ; <a href="https://dblp.uni-trier.de/pid/71/9281.html">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pid/127/3421.html">Longyue Wang</a> ; <a href="https://dblp.uni-trier.de/pid/s/ShumingShi-1.html">Shuming Shi</a></p>
<p>Abstract:
Although self-attention networks (SANs) have advanced the state-of-the-art on various NLP tasks, one criticism of SANs is their ability of encoding positions of input words (Shaw et al., 2018). In this work, we propose to augment SANs with structural position representations to model the latent structure of the input sentence, which is complementary to the standard sequential positional representations. Specifically, we use dependency tree to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese-to-English and WMT14 English-to-German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations.</p>
<p>Keywords:</p>
<h3 id="146. Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation.">146. Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1146">Paper Link</a>    Pages:1410-1416</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/127/0168.html">Raj Dabre</a> ; <a href="https://dblp.uni-trier.de/pid/44/2827.html">Atsushi Fujita</a> ; <a href="https://dblp.uni-trier.de/pid/126/8755.html">Chenhui Chu</a></p>
<p>Abstract:
This paper highlights the impressive utility of multi-parallel corpora for transfer learning in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pre-training on an external large (209k440k) parallel corpus for English and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage fine-tuning can give 39 BLEU score gains over a simple one-to-one model.</p>
<p>Keywords:</p>
<h3 id="147. Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings.">147. Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1147">Paper Link</a>    Pages:1417-1422</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/8985.html">Zi-Yi Dou</a> ; <a href="https://dblp.uni-trier.de/pid/123/0773.html">Junjie Hu</a> ; <a href="https://dblp.uni-trier.de/pid/148/9479.html">Antonios Anastasopoulos</a> ; <a href="https://dblp.uni-trier.de/pid/03/8155.html">Graham Neubig</a></p>
<p>Abstract:
The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.</p>
<p>Keywords:</p>
<h3 id="148. A Regularization-based Framework for Bilingual Grammar Induction.">148. A Regularization-based Framework for Bilingual Grammar Induction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1148">Paper Link</a>    Pages:1423-1428</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/74/1552.html">Yong Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/188/9071.html">Wenjuan Han</a> ; <a href="https://dblp.uni-trier.de/pid/22/918.html">Kewei Tu</a></p>
<p>Abstract:
Grammar induction aims to discover syntactic structures from unannotated sentences. In this paper, we propose a framework in which the learning process of the grammar model of one language is influenced by knowledge from the model of another language. Unlike previous work on multilingual grammar induction, our approach does not rely on any external resource, such as parallel corpora, word alignments or linguistic phylogenetic trees. We propose three regularization methods that encourage similarity between model parameters, dependency edge scores, and parse trees respectively. We deploy our methods on a state-of-the-art unsupervised discriminative parser and evaluate it on both transfer grammar induction and bilingual grammar induction. Empirical results on multiple languages show that our methods outperform strong baselines.</p>
<p>Keywords:</p>
<h3 id="149. Encoders Help You Disambiguate Word Senses in Neural Machine Translation.">149. Encoders Help You Disambiguate Word Senses in Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1149">Paper Link</a>    Pages:1429-1435</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/169/1561.html">Gongbo Tang</a> ; <a href="https://dblp.uni-trier.de/pid/00/8341.html">Rico Sennrich</a> ; <a href="https://dblp.uni-trier.de/pid/n/JoakimNivre.html">Joakim Nivre</a></p>
<p>Abstract:
Neural machine translation (NMT) has achieved new state-of-the-art performance in translating ambiguous words. However, it is still unclear which component dominates the process of disambiguation. In this paper, we explore the ability of NMT encoders and decoders to disambiguate word senses by evaluating hidden states and investigating the distributions of self-attention. We train a classifier to predict whether a translation is correct given the representation of an ambiguous noun. We find that encoder hidden states outperform word embeddings significantly which indicates that encoders adequately encode relevant information for disambiguation into hidden states. In contrast to encoders, the effect of decoder is different in models with different architectures. Moreover, the attention weights and attention entropy show that self-attention can detect ambiguous nouns and distribute more attention to the context.</p>
<p>Keywords:</p>
<h3 id="150. Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model.">150. Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1150">Paper Link</a>    Pages:1436-1441</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/35/4598.html">Hyun-Je Song</a> ; <a href="https://dblp.uni-trier.de/pid/10/1464.html">Seong-Bae Park</a></p>
<p>Abstract:
Korean morphological analysis has been considered as a sequence of morpheme processing and POS tagging. Thus, a pipeline model of the tasks has been adopted widely by previous studies. However, the model has a problem that it cannot utilize interactions among the tasks. This paper formulates Korean morphological analysis as a combination of the tasks and presents a tied sequence-to-sequence multi-task model for training the two tasks simultaneously without any explicit regularization. The experiments prove the proposed model achieves the state-of-the-art performance.</p>
<p>Keywords:</p>
<h3 id="151. Efficient Convolutional Neural Networks for Diacritic Restoration.">151. Efficient Convolutional Neural Networks for Diacritic Restoration.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1151">Paper Link</a>    Pages:1442-1448</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/184/8757.html">Sawsan Alqahtani</a> ; <a href="https://dblp.uni-trier.de/pid/139/5743.html">Ajay Mishra</a> ; <a href="https://dblp.uni-trier.de/pid/15/4305.html">Mona T. Diab</a></p>
<p>Abstract:
Diacritic restoration has gained importance with the growing need for machines to understand written texts. The task is typically modeled as a sequence labeling problem and currently Bidirectional Long Short Term Memory (BiLSTM) models provide state-of-the-art results. Recently, Bai et al. (2018) show the advantages of Temporal Convolutional Neural Networks (TCN) over Recurrent Neural Networks (RNN) for sequence modeling in terms of performance and computational resources. As diacritic restoration benefits from both previous as well as subsequent timesteps, we further apply and evaluate a variant of TCN, Acausal TCN (A-TCN), which incorporates context from both directions (previous and future) rather than strictly incorporating previous context as in the case of TCN. A-TCN yields significant improvement over TCN for diacritization in three different languages: Arabic, Yoruba, and Vietnamese. Furthermore, A-TCN and BiLSTM have comparable performance, making A-TCN an efficient alternative over BiLSTM since convolutions can be trained in parallel. A-TCN is significantly faster than BiLSTM at inference time (270% 334% improvement in the amount of text diacritized per minute).</p>
<p>Keywords:</p>
<h3 id="152. Improving Generative Visual Dialog by Answering Diverse Questions.">152. Improving Generative Visual Dialog by Answering Diverse Questions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1152">Paper Link</a>    Pages:1449-1454</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/249/5621.html">Vishvak Murahari</a> ; <a href="https://dblp.uni-trier.de/pid/179/2452.html">Prithvijit Chattopadhyay</a> ; <a href="https://dblp.uni-trier.de/pid/67/6586.html">Dhruv Batra</a> ; <a href="https://dblp.uni-trier.de/pid/64/2121.html">Devi Parikh</a> ; <a href="https://dblp.uni-trier.de/pid/40/5262.html">Abhishek Das</a></p>
<p>Abstract:
Prior work on training generative Visual Dialog models with reinforcement learning ((Das et al., ICCV 2017) has explored a Q-Bot-A-Bot image-guessing game and shown that this self-talk approach can lead to improved performance at the downstream dialog-conditioned image-guessing task. However, this improvement saturates and starts degrading after a few rounds of interaction, and does not lead to a better Visual Dialog model. We find that this is due in part to repeated interactions between Q-Bot and A-BOT during self-talk, which are not informative with respect to the image. To improve this, we devise a simple auxiliary objective that incentivizes Q-Bot to ask diverse questions, thus reducing repetitions and in turn enabling A-Bot to explore a larger state space during RL i.e. be exposed to more visual concepts to talk about, and varied questions to answer. We evaluate our approach via a host of automatic metrics and human studies, and demonstrate that it leads to better dialog, i.e. dialog that is more diverse (i.e. less repetitive), consistent (i.e. has fewer conflicting exchanges), fluent (i.e., more human-like), and detailed, while still being comparably image-relevant as prior work and ablations.</p>
<p>Keywords:</p>
<h3 id="153. Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Understanding.">153. Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Understanding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1153">Paper Link</a>    Pages:1455-1460</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/152/6185.html">Quynh Ngoc Thi Do</a> ; <a href="https://dblp.uni-trier.de/pid/25/11105.html">Judith Gaspers</a></p>
<p>Abstract:
A typical cross-lingual transfer learning approach boosting model performance on a language is to pre-train the model on all available supervised data from another language. However, in large-scale systems this leads to high training times and computational requirements. In addition, characteristic differences between the source and target languages raise a natural question of whether source data selection can improve the knowledge transfer. In this paper, we address this question and propose a simple but effective language model based source-language data selection method for cross-lingual transfer learning in large-scale spoken language understanding. The experimental results show that with data selection i) source data and hence training speed is reduced significantly and ii) model performance is improved.</p>
<p>Keywords:</p>
<h3 id="154. Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations.">154. Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1154">Paper Link</a>    Pages:1461-1467</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/154/3943-1.html">Po-Yao Huang</a> ; <a href="https://dblp.uni-trier.de/pid/116/8412.html">Xiaojun Chang</a> ; <a href="https://dblp.uni-trier.de/pid/h/AlexanderGHauptmann.html">Alexander G. Hauptmann</a></p>
<p>Abstract:
With the aim of promoting and understanding the multilingual version of image search, we leverage visual object detection and propose a model with diverse multi-head attention to learn grounded multilingual multimodal representations. Specifically, our model attends to different types of textual semantics in two languages and visual objects for fine-grained alignments between sentences and images. We introduce a new objective function which explicitly encourages attention diversity to learn an improved visual-semantic embedding space. We evaluate our model in the German-Image and English-Image matching tasks on the Multi30K dataset, and in the Semantic Textual Similarity task with the English descriptions of visual content. Results show that our model yields a significant performance gain over other methods in all of the three tasks.</p>
<p>Keywords:</p>
<h3 id="155. Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering.">155. Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1155">Paper Link</a>    Pages:1468-1474</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/139/1319.html">Soravit Changpinyo</a> ; <a href="https://dblp.uni-trier.de/pid/16/6344.html">Bo Pang</a> ; <a href="https://dblp.uni-trier.de/pid/91/1508.html">Piyush Sharma</a> ; <a href="https://dblp.uni-trier.de/pid/83/3497.html">Radu Soricut</a></p>
<p>Abstract:
Object detection plays an important role in current solutions to vision and language tasks like image captioning and visual question answering. However, popular models like Faster R-CNN rely on a costly process of annotating ground-truths for both the bounding boxes and their corresponding semantic labels, making it less amenable as a primitive task for transfer learning. In this paper, we examine the effect of decoupling box proposal and featurization for down-stream tasks. The key insight is that this allows us to leverage a large amount of labeled annotations that were previously unavailable for standard object detection benchmarks. Empirically, we demonstrate that this leads to effective transfer learning and improved image captioning and visual question answering models, as measured on publicly-available benchmarks.</p>
<p>Keywords:</p>
<h3 id="156. REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning.">156. REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1156">Paper Link</a>    Pages:1475-1480</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/02/10377.html">Ming Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/123/0773.html">Junjie Hu</a> ; <a href="https://dblp.uni-trier.de/pid/56/11343.html">Qiuyuan Huang</a> ; <a href="https://dblp.uni-trier.de/pid/97/8704.html">Lei Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/40/3892.html">Jana Diesner</a> ; <a href="https://dblp.uni-trier.de/pid/92/5339.html">Jianfeng Gao</a></p>
<p>Abstract:
Popular metrics used for evaluating image captioning systems, such as BLEU and CIDEr, provide a single score to gauge the systems overall effectiveness. This score is often not informative enough to indicate what specific errors are made by a given system. In this study, we present a fine-grained evaluation method REO for automatically measuring the performance of image captioning systems. REO assesses the quality of captions from three perspectives: 1) Relevance to the ground truth, 2) Extraness of the content that is irrelevant to the ground truth, and 3) Omission of the elements in the images and human references. Experiments on three benchmark datasets demonstrate that our method achieves a higher consistency with human judgments and provides more intuitive evaluation results than alternative metrics.</p>
<p>Keywords:</p>
<h3 id="157. WSLLN: Weakly Supervised Natural Language Localization Networks.">157. WSLLN: Weakly Supervised Natural Language Localization Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1157">Paper Link</a>    Pages:1481-1487</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/67/6825.html">Mingfei Gao</a> ; <a href="https://dblp.uni-trier.de/pid/d/LarrySDavis.html">Larry Davis</a> ; <a href="https://dblp.uni-trier.de/pid/79/128.html">Richard Socher</a> ; <a href="https://dblp.uni-trier.de/pid/80/7282.html">Caiming Xiong</a></p>
<p>Abstract:
We propose weakly supervised language localization networks (WSLLN) to detect events in long, untrimmed videos given language queries. To learn the correspondence between visual segments and texts, most previous methods require temporal coordinates (start and end times) of events for training, which leads to high costs of annotation. WSLLN relieves the annotation burden by training with only video-sentence pairs without accessing to temporal locations of events. With a simple end-to-end structure, WSLLN measures segment-text consistency and conducts segment selection (conditioned on the text) simultaneously. Results from both are merged and optimized as a video-sentence matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate that WSLLN achieves state-of-the-art performance.</p>
<p>Keywords:</p>
<h3 id="158. Grounding learning of modifier dynamics: An application to color naming.">158. Grounding learning of modifier dynamics: An application to color naming.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1158">Paper Link</a>    Pages:1488-1493</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/22/10151.html">Xudong Han</a> ; <a href="https://dblp.uni-trier.de/pid/184/3773.html">Philip Schulz</a> ; <a href="https://dblp.uni-trier.de/pid/66/4613.html">Trevor Cohn</a></p>
<p>Abstract:
Grounding is crucial for natural language understanding. An important subtask is to understand modified color expressions, such as light blue. We present a model of color modifiers that, compared with previous additive models in RGB space, learns more complex transformations. In addition, we present a model that operates in the HSV color space. We show that certain adjectives are better modeled in that space. To account for all modifiers, we train a hard ensemble model that selects a color space depending on the modifier-color pair. Experimental results show significant and consistent improvements compared to the state-of-the-art baseline model.</p>
<p>Keywords:</p>
<h3 id="159. Robust Navigation with Language Pretraining and Stochastic Sampling.">159. Robust Navigation with Language Pretraining and Stochastic Sampling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1159">Paper Link</a>    Pages:1494-1499</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/30/9646.html">Xiujun Li</a> ; <a href="https://dblp.uni-trier.de/pid/64/9590.html">Chunyuan Li</a> ; <a href="https://dblp.uni-trier.de/pid/203/9731.html">Qiaolin Xia</a> ; <a href="https://dblp.uni-trier.de/pid/38/9282.html">Yonatan Bisk</a> ; <a href="https://dblp.uni-trier.de/pid/15/3724.html">Asli elikyilmaz</a> ; <a href="https://dblp.uni-trier.de/pid/92/5339.html">Jianfeng Gao</a> ; <a href="https://dblp.uni-trier.de/pid/90/5204.html">Noah A. Smith</a> ; <a href="https://dblp.uni-trier.de/pid/89/579.html">Yejin Choi</a></p>
<p>Abstract:
Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-to-Room benchmark with 6% absolute gain over the previous best result (47% -&gt; 53%) on the Success Rate weighted by Path Length metric.</p>
<p>Keywords:</p>
<h3 id="160. Towards Making a Dependency Parser See.">160. Towards Making a Dependency Parser See.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1160">Paper Link</a>    Pages:1500-1506</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/236/5680.html">Michalina Strzyz</a> ; <a href="https://dblp.uni-trier.de/pid/128/2835.html">David Vilares</a> ; <a href="https://dblp.uni-trier.de/pid/95/3319.html">Carlos Gmez-Rodrguez</a></p>
<p>Abstract:
We explore whether it is possible to leverage eye-tracking data in an RNN dependency parser (for English) when such information is only available during training - i.e. no aggregated or token-level gaze features are used at inference time. To do so, we train a multitask learning model that parses sentences as sequence labeling and leverages gaze features as auxiliary tasks. Our method also learns to train from disjoint datasets, i.e. it can be used to test whether already collected gaze features are useful to improve the performance on new non-gazed annotated treebanks. Accuracy gains are modest but positive, showing the feasibility of the approach. It can serve as a first step towards architectures that can better leverage eye-tracking data or other complementary information available only for training sentences, possibly leading to improvements in syntactic parsing.</p>
<p>Keywords:</p>
<h3 id="161. Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders.">161. Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1161">Paper Link</a>    Pages:1507-1512</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/200/8508.html">Andrew Drozdov</a> ; <a href="https://dblp.uni-trier.de/pid/160/8356.html">Patrick Verga</a> ; <a href="https://dblp.uni-trier.de/pid/202/0873.html">Yi-Pei Chen</a> ; <a href="https://dblp.uni-trier.de/pid/148/9178.html">Mohit Iyyer</a> ; <a href="https://dblp.uni-trier.de/pid/m/AndrewMcCallum.html">Andrew McCallum</a></p>
<p>Abstract:
Understanding text often requires identifying meaningful constituent spans such as noun phrases and verb phrases. In this work, we show that we can effectively recover these types of labels using the learned phrase vectors from deep inside-outside recursive autoencoders (DIORA). Specifically, we cluster span representations to induce span labels. Additionally, we improve the models labeling accuracy by integrating latent code learning into the training procedure. We evaluate this approach empirically through unsupervised labeled constituency parsing. Our method outperforms ELMo and BERT on two versions of the Wall Street Journal (WSJ) dataset and is competitive to prior work that requires additional human annotations, improving over a previous state-of-the-art system that depends on ground-truth part-of-speech tags by 5 absolute F1 points (19% relative error reduction).</p>
<p>Keywords:</p>
<h3 id="162. Dependency Parsing for Spoken Dialog Systems.">162. Dependency Parsing for Spoken Dialog Systems.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1162">Paper Link</a>    Pages:1513-1519</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8292.html">Sam Davidson</a> ; <a href="https://dblp.uni-trier.de/pid/136/8648.html">Dian Yu</a> ; <a href="https://dblp.uni-trier.de/pid/83/3205.html">Zhou Yu</a></p>
<p>Abstract:
Dependency parsing of conversational input can play an important role in language understanding for dialog systems by identifying the relationships between entities extracted from user utterances. Additionally, effective dependency parsing can elucidate differences in language structure and usage for discourse analysis of human-human versus human-machine dialogs. However, models trained on datasets based on news articles and web data do not perform well on spoken human-machine dialog, and currently available annotation schemes do not adapt well to dialog data. Therefore, we propose the Spoken Conversation Universal Dependencies (SCUD) annotation scheme that extends the Universal Dependencies (UD) (Nivre et al., 2016) guidelines to spoken human-machine dialogs. We also provide ConvBank, a conversation dataset between humans and an open-domain conversational dialog system with SCUD annotation. Finally, to demonstrate the utility of the dataset, we train a dependency parser on the ConvBank dataset. We demonstrate that by pre-training a dependency parser on a set of larger public datasets and fine-tuning on ConvBank data, we achieved the best result, 85.05% unlabeled and 77.82% labeled attachment accuracy.</p>
<p>Keywords:</p>
<h3 id="163. Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog.">163. Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1163">Paper Link</a>    Pages:1520-1526</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/124/9178.html">Panupong Pasupat</a> ; <a href="https://dblp.uni-trier.de/pid/41/3406.html">Sonal Gupta</a> ; <a href="https://dblp.uni-trier.de/pid/254/8226.html">Karishma Mandyam</a> ; <a href="https://dblp.uni-trier.de/pid/39/9013.html">Rushin Shah</a> ; <a href="https://dblp.uni-trier.de/pid/19/6214.html">Mike Lewis</a> ; <a href="https://dblp.uni-trier.de/pid/21/6793.html">Luke Zettlemoyer</a></p>
<p>Abstract:
We propose a semantic parser for parsing compositional utterances into Task Oriented Parse (TOP), a tree representation that has intents and slots as labels of nesting tree nodes. Our parser is span-based: it scores labels of the tree nodes covering each token span independently, but then decodes a valid tree globally. In contrast to previous sequence decoding approaches and other span-based parsers, we (1) improve the training speed by removing the need to run the decoder at training time; and (2) introduce edge scores, which model relations between parent and child labels, to mitigate the independence assumption between node labels and improve accuracy. Our best parser outperforms previous methods on the TOP dataset of mixed-domain task-oriented utterances in both accuracy and training speed.</p>
<p>Keywords:</p>
<h3 id="164. Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation.">164. Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1164">Paper Link</a>    Pages:1527-1537</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/196/4034.html">Zhengxin Yang</a> ; <a href="https://dblp.uni-trier.de/pid/127/3143.html">Jinchao Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/117/4056.html">Fandong Meng</a> ; <a href="https://dblp.uni-trier.de/pid/239/5079.html">Shuhao Gu</a> ; <a href="https://dblp.uni-trier.de/pid/07/6095-4.html">Yang Feng</a> ; <a href="https://dblp.uni-trier.de/pid/00/5012-16.html">Jie Zhou</a></p>
<p>Abstract:
Context modeling is essential to generate coherent and consistent translation for Document-level Neural Machine Translations. The widely used method for document-level translation usually compresses the context information into a representation via hierarchical attention networks. However, this method neither considers the relationship between context words nor distinguishes the roles of context words. To address this problem, we propose a query-guided capsule networks to cluster context information into different perspectives from which the target translation may concern. Experiment results show that our method can significantly outperform strong baselines on multiple data sets of different domains.</p>
<p>Keywords:</p>
<h3 id="165. Simple, Scalable Adaptation for Neural Machine Translation.">165. Simple, Scalable Adaptation for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1165">Paper Link</a>    Pages:1538-1548</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/200/8008.html">Ankur Bapna</a> ; <a href="https://dblp.uni-trier.de/pid/120/2225.html">Orhan Firat</a></p>
<p>Abstract:
Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.</p>
<p>Keywords:</p>
<h3 id="166. Controlling Text Complexity in Neural Machine Translation.">166. Controlling Text Complexity in Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1166">Paper Link</a>    Pages:1549-1564</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/210/7863.html">Sweta Agrawal</a> ; <a href="https://dblp.uni-trier.de/pid/71/1827.html">Marine Carpuat</a></p>
<p>Abstract:
This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of news articles available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting dataset makes it possible to train multi-task sequence to sequence models that can translate and simplify text jointly. We show that these multi-task models outperform pipeline approaches that translate and simplify text independently.</p>
<p>Keywords:</p>
<h3 id="167. Investigating Multilingual NMT Representations at Scale.">167. Investigating Multilingual NMT Representations at Scale.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1167">Paper Link</a>    Pages:1565-1575</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7500.html">Sneha Reddy Kudugunta</a> ; <a href="https://dblp.uni-trier.de/pid/200/8008.html">Ankur Bapna</a> ; <a href="https://dblp.uni-trier.de/pid/236/5919.html">Isaac Caswell</a> ; <a href="https://dblp.uni-trier.de/pid/120/2225.html">Orhan Firat</a></p>
<p>Abstract:
Multilingual Neural Machine Translation (NMT) models have yielded large empirical success in transfer learning settings. However, these black-box representations are poorly understood, and their mode of transfer remains elusive. In this work, we attempt to understand massively multilingual NMT representations (with 103 languages) using Singular Value Canonical Correlation Analysis (SVCCA), a representation similarity framework that allows us to compare representations across different languages, layers and models. Our analysis validates several empirical results and long-standing intuitions, and unveils new observations regarding how representations evolve in a multilingual translation model. We draw three major results from our analysis, with implications on cross-lingual transfer learning: (i) Encoder representations of different languages cluster based on linguistic similarity, (ii) Representations of a source language learned by the encoder are dependent on the target language, and vice-versa, and (iii) Representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. We further connect our findings with existing empirical observations in multilingual NMT and transfer learning.</p>
<p>Keywords:</p>
<h3 id="168. Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation.">168. Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1168">Paper Link</a>    Pages:1576-1585</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/89/6413.html">Xin Tan</a> ; <a href="https://dblp.uni-trier.de/pid/249/7525.html">Longyin Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/55/6548.html">Deyi Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/42/6620.html">Guodong Zhou</a></p>
<p>Abstract:
Document-level machine translation (MT) remains challenging due to the difficulty in efficiently using document context for translation. In this paper, we propose a hierarchical model to learn the global context for document-level neural machine translation (NMT). This is done through a sentence encoder to capture intra-sentence dependencies and a document encoder to model document-level inter-sentence consistency and coherence. With this hierarchical architecture, we feedback the extracted global document context to each word in a top-down fashion to distinguish different translations of a word according to its specific surrounding context. In addition, since large-scale in-domain document-level parallel corpora are usually unavailable, we use a two-step training strategy to take advantage of a large-scale corpus with out-of-domain parallel sentence pairs and a small-scale corpus with in-domain parallel document pairs to achieve the domain adaptability. Experimental results on several benchmark corpora show that our proposed model can significantly improve document-level translation performance over several strong NMT baselines.</p>
<p>Keywords:</p>
<h3 id="169. Cross-Lingual Machine Reading Comprehension.">169. Cross-Lingual Machine Reading Comprehension.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1169">Paper Link</a>    Pages:1586-1595</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/130/6308.html">Yiming Cui</a> ; <a href="https://dblp.uni-trier.de/pid/98/4640.html">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pid/86/5934.html">Bing Qin</a> ; <a href="https://dblp.uni-trier.de/pid/74/5750-1.html">Shijin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/59/5304.html">Guoping Hu</a></p>
<p>Abstract:
Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data.In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task which is straightforward to adopt. However, to exactly align the answer into source language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in bilingual context, and then utilize the learned knowledge to improve reading comprehension performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. Resources available: <a href="https://github.com/ymcui/Cross-Lingual-MRC">https://github.com/ymcui/Cross-Lingual-MRC</a></p>
<p>Keywords:</p>
<h3 id="170. A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning.">170. A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1170">Paper Link</a>    Pages:1596-1606</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/89/3994.html">Minghao Hu</a> ; <a href="https://dblp.uni-trier.de/pid/42/2857.html">Yuxing Peng</a> ; <a href="https://dblp.uni-trier.de/pid/22/3870-6.html">Zhen Huang</a> ; <a href="https://dblp.uni-trier.de/pid/254/0830-1.html">Dongsheng Li</a></p>
<p>Abstract:
Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings. However, the performance of these models degrades significantly when they are applied to more realistic scenarios, such as answers involve various types, multiple text strings are correct answers, or discrete reasoning abilities are required. In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans. In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction. Experiments show that our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results. Source code (<a href="https://github.com/huminghao16/MTMSN">https://github.com/huminghao16/MTMSN</a>) is released to facilitate future work.</p>
<p>Keywords:</p>
<h3 id="171. Neural Duplicate Question Detection without Labeled Training Data.">171. Neural Duplicate Question Detection without Labeled Training Data.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1171">Paper Link</a>    Pages:1607-1617</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9599.html">Andreas Rckl</a> ; <a href="https://dblp.uni-trier.de/pid/127/3977.html">Nafise Sadat Moosavi</a> ; <a href="https://dblp.uni-trier.de/pid/85/6201.html">Iryna Gurevych</a></p>
<p>Abstract:
Supervised training of neural models to duplicate question detection in community Question Answering (CQA) requires large amounts of labeled question pairs, which can be costly to obtain. To minimize this cost, recent works thus often used alternative methods, e.g., adversarial domain adaptation. In this work, we propose two novel methodsweak supervision using the title and body of a question, and the automatic generation of duplicate questionsand show that both can achieve improved performances even though they do not require any labeled data. We provide a comparison of popular training strategies and show that our proposed approaches are more effective in many cases because they can utilize larger amounts of data from the CQA forums. Finally, we show that weak supervision with question title and body information is also an effective method to train CQA answer selection models without direct answer supervision.</p>
<p>Keywords:</p>
<h3 id="172. Asking Clarification Questions in Knowledge-Based Question Answering.">172. Asking Clarification Questions in Knowledge-Based Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1172">Paper Link</a>    Pages:1618-1629</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/25/624.html">Jingjing Xu</a> ; <a href="https://dblp.uni-trier.de/pid/233/7798.html">Yuechen Wang</a> ; <a href="https://dblp.uni-trier.de/pid/135/6318.html">Duyu Tang</a> ; <a href="https://dblp.uni-trier.de/pid/30/8160.html">Nan Duan</a> ; <a href="https://dblp.uni-trier.de/pid/140/6685.html">Pengcheng Yang</a> ; <a href="https://dblp.uni-trier.de/pid/39/7992.html">Qi Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/16/1161-1.html">Ming Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/37/1971-1.html">Xu Sun</a></p>
<p>Abstract:
The ability to ask clarification questions is essential for knowledge-based question answering (KBQA) systems, especially for handling ambiguous phenomena. Despite its importance, clarification has not been well explored in current KBQA systems. Further progress requires supervised resources for training and evaluation, and powerful models for clarification-related text understanding and generation. In this paper, we construct a new clarification dataset, CLAQUA, with nearly 40K open-domain examples. The dataset supports three serial tasks: given a question, identify whether clarification is needed; if yes, generate a clarification question; then predict answers base on external user feedback. We provide representative baselines for these tasks and further introduce a coarse-to-fine model for clarification question generation. Experiments show that the proposed model achieves better performance than strong baselines. The further analysis demonstrates that our dataset brings new challenges and there still remain several unsolved problems, like reasonable automatic evaluation metrics for clarification question generation and powerful models for handling entity sparsity.</p>
<p>Keywords:</p>
<h3 id="173. Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection.">173. Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1173">Paper Link</a>    Pages:1630-1641</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/173/6441.html">Nina Prner</a> ; <a href="https://dblp.uni-trier.de/pid/s/HinrichSchutze.html">Hinrich Schtze</a></p>
<p>Abstract:
We address the problem of Duplicate Question Detection (DQD) in low-resource domain-specific Community Question Answering forums. Our multi-view framework MV-DASE combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis, using unlabeled data only. In our experiments, the ensemble includes generic and domain-specific averaged word embeddings, domain-finetuned BERT and the Universal Sentence Encoder. We evaluate MV-DASE on the CQADupStack corpus and on additional low-resource Stack Exchange forums. Combining the strengths of different encoders, we significantly outperform BM25, all single-view systems as well as a recent supervised domain-adversarial DQD method.</p>
<p>Keywords:</p>
<h3 id="174. Multi-label Categorization of Accounts of Sexism using a Neural Framework.">174. Multi-label Categorization of Accounts of Sexism using a Neural Framework.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1174">Paper Link</a>    Pages:1642-1652</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/72/7629.html">Pulkit Parikh</a> ; <a href="https://dblp.uni-trier.de/pid/184/2150.html">Harika Abburi</a> ; <a href="https://dblp.uni-trier.de/pid/198/5418.html">Pinkesh Badjatiya</a> ; <a href="https://dblp.uni-trier.de/pid/250/9759.html">Radhika Krishnan</a> ; <a href="https://dblp.uni-trier.de/pid/83/9923.html">Niyati Chhaya</a> ; <a href="https://dblp.uni-trier.de/pid/g/ManishGupta1.html">Manish Gupta</a> ; <a href="https://dblp.uni-trier.de/pid/03/4045.html">Vasudeva Varma</a></p>
<p>Abstract:
Sexism, an injustice that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of sexism on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policy makers in utilizing such data to study and counter sexism better. The existing work on sexism classification, which is different from sexism detection, has certain limitations in terms of the categories of sexism used and/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s), and we contribute the largest dataset for sexism categorization. We develop a neural solution for this multi-label classification that can combine sentence representations obtained using models such as BERT with distributional and linguistic word embeddings using a flexible, hierarchical architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our framework. The best proposed method outperforms several deep learning as well as traditional machine learning baselines by an appreciable margin.</p>
<p>Keywords:</p>
<h3 id="175. The Trumpiest Trump? Identifying a Subject's Most Characteristic Tweets.">175. The Trumpiest Trump? Identifying a Subject's Most Characteristic Tweets.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1175">Paper Link</a>    Pages:1653-1663</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8095.html">Charuta Pethe</a> ; <a href="https://dblp.uni-trier.de/pid/s/StevenSkiena.html">Steven Skiena</a></p>
<p>Abstract:
The sequence of documents produced by any given author varies in style and content, but some documents are more typical or representative of the source than others. We quantify the extent to which a given short text is characteristic of a specific person, using a dataset of tweets from fifteen celebrities. Such analysis is useful for generating excerpts of high-volume Twitter profiles, and understanding how representativeness relates to tweet popularity. We first consider the related task of binary author detection (is x the author of text T?), and report a test accuracy of 90.37% for the best of five approaches to this problem. We then use these models to compute characterization scores among all of an authors texts. A user study shows human evaluators agree with our characterization model for all 15 celebrities in our dataset, each with p-value &lt; 0.05. We use these classifiers to show surprisingly strong correlations between characterization scores and the popularity of the associated texts. Indeed, we demonstrate a statistically significant correlation between this score and tweet popularity (likes/replies/retweets) for 13 of the 15 celebrities in our study.</p>
<p>Keywords:</p>
<h3 id="176. Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts.">176. Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1176">Paper Link</a>    Pages:1664-1674</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/241/5167.html">Luke Breitfeller</a> ; <a href="https://dblp.uni-trier.de/pid/188/9199.html">Emily Ahn</a> ; <a href="https://dblp.uni-trier.de/pid/48/4613.html">David Jurgens</a> ; <a href="https://dblp.uni-trier.de/pid/75/8157.html">Yulia Tsvetkov</a></p>
<p>Abstract:
Microaggressions are subtle, often veiled, manifestations of human biases. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of microaggressions in communication has made it difficult for researchers to analyze their exact nature, and to quantify and extract microaggressions automatically. Specifically, the lack of a corpus of real-world microaggressions and objective criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of microaggressions based on a small subset of data that we have. We then create two datasets: one with examples of diverse types of microaggressions recollected by their targets, and another with gender-based microaggressions in public conversations on social media. We introduce a new, more objective, criterion for annotation and an active-learning based procedure that increases the likelihood of surfacing posts containing microaggressions. Finally, we analyze the trends that emerge from these new datasets.</p>
<p>Keywords:</p>
<h3 id="177. Reinforced Product Metadata Selection for Helpfulness Assessment of Customer Reviews.">177. Reinforced Product Metadata Selection for Helpfulness Assessment of Customer Reviews.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1177">Paper Link</a>    Pages:1675-1683</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/130/8157.html">Miao Fan</a> ; <a href="https://dblp.uni-trier.de/pid/97/164.html">Chao Feng</a> ; <a href="https://dblp.uni-trier.de/pid/87/8665.html">Mingming Sun</a> ; <a href="https://dblp.uni-trier.de/pid/62/5860-1.html">Ping Li</a></p>
<p>Abstract:
To automatically assess the helpfulness of a customer review online, conventional approaches generally acquire various linguistic and neural embedding features solely from the textual content of the review itself as the evidence. We, however, find out that a helpful review is largely concerned with the metadata (such as the name, the brand, the category, etc.) of its target product. It leaves us with a challenge of how to choose the correct key-value product metadata to help appraise the helpfulness of free-text reviews more precisely. To address this problem, we propose a novel framework composed of two mutual-benefit modules. Given a product, a selector (agent) learns from both the keys in the product metadata and one of its reviews to take an action that selects the correct value, and a successive predictor (network) makes the free-text review attend to this value to obtain better neural representations for helpfulness assessment. The predictor is directly optimized by SGD with the loss of helpfulness prediction, and the selector could be updated via policy gradient rewarded with the performance of the predictor. We use two real-world datasets from Amazon.com and Yelp.com, respectively, to compare the performance of our framework with other mainstream methods under two application scenarios: helpfulness identification and regression of customer reviews. Extensive results demonstrate that our framework can achieve state-of-the-art performance with substantial improvements.</p>
<p>Keywords:</p>
<h3 id="178. Learning Invariant Representations of Social Media Users.">178. Learning Invariant Representations of Social Media Users.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1178">Paper Link</a>    Pages:1684-1695</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/05/1147.html">Nicholas Andrews</a> ; <a href="https://dblp.uni-trier.de/pid/124/6413.html">Marcus Bishop</a></p>
<p>Abstract:
The evolution of social media users behavior over time complicates user-level comparison tasks such as verification, classification, clustering, and ranking. As a result, naive approaches may fail to generalize to new users or even to future observations of previously known users. In this paper, we propose a novel procedure to learn a mapping from short episodes of user activity on social media to a vector space in which the distance between points captures the similarity of the corresponding users invariant features. We fit the model by optimizing a surrogate metric learning objective over a large corpus of unlabeled social media content. Once learned, the mapping may be applied to users not seen at training time and enables efficient comparisons of users in the resulting vector space. We present a comprehensive evaluation to validate the benefits of the proposed approach using data from Reddit, Twitter, and Wikipedia.</p>
<p>Keywords:</p>
<h3 id="179. (Male, Bachelor) and (Female, Ph.D) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas.">179. (Male, Bachelor) and (Female, Ph.D) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1179">Paper Link</a>    Pages:1696-1706</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/9056.html">Dongyeop Kang</a> ; <a href="https://dblp.uni-trier.de/pid/178/8576.html">Varun Gangal</a> ; <a href="https://dblp.uni-trier.de/pid/47/2454.html">Eduard H. Hovy</a></p>
<p>Abstract:
Stylistic variation in text needs to be studied with different aspects including the writers personal traits, interpersonal relations, rhetoric, and more. Despite recent attempts on computational modeling of the variation, the lack of parallel corpora of style language makes it difficult to systematically control the stylistic change as well as evaluate such models. We release PASTEL, the parallel and annotated stylistic language dataset, that contains ~41K parallel sentences (8.3K parallel stories) annotated across different personas. Each persona has different styles in conjunction: gender, age, country, political view, education, ethnic, and time-of-writing. The dataset is collected from human annotators with solid control of input denotation: not only preserving original meaning between text, but promoting stylistic diversity to annotators. We test the dataset on two interesting applications of style language, where PASTEL helps design appropriate experiment and evaluation. First, in predicting a target style (e.g., male or female in gender) given a text, multiple styles of PASTEL make other external style variables controlled (or fixed), which is a more accurate experimental design. Second, a simple supervised model with our parallel text outperforms the unsupervised models using nonparallel text in style transfer. Our dataset is publicly available.</p>
<p>Keywords:</p>
<h3 id="180. Movie Plot Analysis via Turning Point Identification.">180. Movie Plot Analysis via Turning Point Identification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1180">Paper Link</a>    Pages:1707-1717</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9741.html">Pinelopi Papalampidi</a> ; <a href="https://dblp.uni-trier.de/pid/30/4872.html">Frank Keller</a> ; <a href="https://dblp.uni-trier.de/pid/59/6701.html">Mirella Lapata</a></p>
<p>Abstract:
According to screenwriting theory, turning points (e.g., change of plans, major setback, climax) are crucial narrative moments within a screenplay: they define the plot structure, determine its progression and segment the screenplay into thematic units (e.g., setup, complications, aftermath). We propose the task of turning point identification in movies as a means of analyzing their narrative structure. We argue that turning points and the segmentation they provide can facilitate processing long, complex narratives, such as screenplays, for summarization and question answering. We introduce a dataset consisting of screenplays and plot synopses annotated with turning points and present an end-to-end neural network model that identifies turning points in plot synopses and projects them onto scenes in screenplays. Our model outperforms strong baselines based on state-of-the-art sentence representations and the expected position of turning points.</p>
<p>Keywords:</p>
<h3 id="181. Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention.">181. Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1181">Paper Link</a>    Pages:1718-1728</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/17/6060.html">Lei Cao</a> ; <a href="https://dblp.uni-trier.de/pid/00/3215.html">Huijun Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/58/4464.html">Ling Feng</a> ; <a href="https://dblp.uni-trier.de/pid/251/8651.html">Zihan Wei</a> ; <a href="https://dblp.uni-trier.de/pid/10/5630.html">Xin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/249/8369.html">Ningyun Li</a> ; <a href="https://dblp.uni-trier.de/pid/219/4446.html">Xiaohao He</a></p>
<p>Abstract:
Despite detection of suicidal ideation on social media has made great progress in recent years, peoples implicitly and anti-real contrarily expressed posts still remain as an obstacle, constraining the detectors to acquire higher satisfactory performance. Enlightened by the hidden tree holes phenomenon on microblog, where people at suicide risk tend to disclose their inner real feelings and thoughts to the microblog space whose authors have committed suicide, we explore the use of tree holes to enhance microblog-based suicide risk detection from the following two perspectives. (1) We build suicide-oriented word embeddings based on tree hole contents to strength the sensibility of suicide-related lexicons and context based on tree hole contents. (2) A two-layered attention mechanism is deployed to grasp intermittently changing points from individuals open blog streams, revealing ones inner emotional world more or less. Our experimental results show that with suicide-oriented word embeddings and attention, microblog-based suicide risk detection can achieve over 91% accuracy. A large-scale well-labelled suicide data set is also reported in the paper.</p>
<p>Keywords:</p>
<h3 id="182. Deep Ordinal Regression for Pledge Specificity Prediction.">182. Deep Ordinal Regression for Pledge Specificity Prediction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1182">Paper Link</a>    Pages:1729-1740</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/153/2817.html">Shivashankar Subramanian</a> ; <a href="https://dblp.uni-trier.de/pid/66/4613.html">Trevor Cohn</a> ; <a href="https://dblp.uni-trier.de/pid/65/4863.html">Timothy Baldwin</a></p>
<p>Abstract:
Many pledges are made in the course of an election campaign, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual annotations. In this paper we collate a novel dataset of manifestos from eleven Australian federal election cycles, with over 12,000 sentences annotated with specificity (e.g., rhetorical vs detailed pledge) on a fine-grained scale. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.</p>
<p>Keywords:</p>
<h3 id="183. Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks.">183. Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1183">Paper Link</a>    Pages:1741-1751</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/8962.html">Igor Shalyminov</a> ; <a href="https://dblp.uni-trier.de/pid/29/3671.html">Sungjin Lee</a> ; <a href="https://dblp.uni-trier.de/pid/58/3222.html">Arash Eshghi</a> ; <a href="https://dblp.uni-trier.de/pid/36/6352.html">Oliver Lemon</a></p>
<p>Abstract:
Goal-oriented dialogue systems are now being widely adopted in industry where it is of key importance to maintain a rapid prototyping cycle for new products and domains. Data-driven dialogue system development has to be adapted to meet this requirement  therefore, reducing the amount of data and annotations necessary for training such systems is a central research problem. In this paper, we present the Dialogue Knowledge Transfer Network (DiKTNet), a state-of-the-art approach to goal-oriented dialogue generation which only uses a few example dialogues (i.e. few-shot learning), none of which has to be annotated. We achieve this by performing a 2-stage training. Firstly, we perform unsupervised dialogue representation pre-training on a large source of goal-oriented dialogues in multiple domains, the MetaLWOz corpus. Secondly, at the transfer stage, we train DiKTNet using this representation together with 2 other textual knowledge sources with different levels of generality: ELMo encoder and the main datasets source domains. Our main dataset is the Stanford Multi-Domain dialogue corpus. We evaluate our model on it in terms of BLEU and Entity F1 scores, and show that our approach significantly and consistently improves upon a series of baseline models as well as over the previous state-of-the-art dialogue generation model, ZSDG. The improvement upon the latter  up to 10% in Entity F1 and the average of 3% in BLEU score  is achieved using only 10% equivalent of ZSDGs in-domain training data.</p>
<p>Keywords:</p>
<h3 id="184. Multi-Granularity Representations of Dialog.">184. Multi-Granularity Representations of Dialog.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1184">Paper Link</a>    Pages:1752-1761</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/0069.html">Shikib Mehri</a> ; <a href="https://dblp.uni-trier.de/pid/88/2001.html">Maxine Esknazi</a></p>
<p>Abstract:
Neural models of dialog rely on generalized latent representations of language. This paper introduces a novel training procedure which explicitly learns multiple representations of language at several levels of granularity. The multi-granularity training algorithm modifies the mechanism by which negative candidate responses are sampled in order to control the granularity of learned latent representations. Strong performance gains are observed on the next utterance retrieval task using both the MultiWOZ dataset and the Ubuntu dialog corpus. Analysis significantly demonstrates that multiple granularities of representation are being learned, and that multi-granularity training facilitates better transfer to downstream tasks.</p>
<p>Keywords:</p>
<h3 id="185. Are You for Real? Detecting Identity Fraud via Dialogue Interactions.">185. Are You for Real? Detecting Identity Fraud via Dialogue Interactions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1185">Paper Link</a>    Pages:1762-1771</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/195/9132.html">Weikang Wang</a> ; <a href="https://dblp.uni-trier.de/pid/71/6950.html">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/69/5902.html">Qian Li</a> ; <a href="https://dblp.uni-trier.de/pid/38/6093.html">Chengqing Zong</a> ; <a href="https://dblp.uni-trier.de/pid/l/ZhifeiLi-1.html">Zhifei Li</a></p>
<p>Abstract:
Identity fraud detection is of great importance in many real-world scenarios such as the financial industry. However, few studies addressed this problem before. In this paper, we focus on identity fraud detection in loan applications and propose to solve this problem with a novel interactive dialogue system which consists of two modules. One is the knowledge graph (KG) constructor organizing the personal information for each loan applicant. The other is structured dialogue management that can dynamically generate a series of questions based on the personal KG to ask the applicants and determine their identity states. We also present a heuristic user simulator based on problem analysis to evaluate our method. Experiments have shown that the trainable dialogue system can effectively detect fraudsters, and achieve higher recognition accuracy compared with rule-based systems. Furthermore, our learned dialogue strategies are interpretable and flexible, which can help promote real-world applications.</p>
<p>Keywords:</p>
<h3 id="186. Hierarchy Response Learning for Neural Conversation Generation.">186. Hierarchy Response Learning for Neural Conversation Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1186">Paper Link</a>    Pages:1772-1781</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/36/2259.html">Bo Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/86/2120.html">Xiaoming Zhang</a></p>
<p>Abstract:
The neural encoder-decoder models have shown great promise in neural conversation generation. However, they cannot perceive and express the intention effectively, and hence often generate dull and generic responses. Unlike past work that has focused on diversifying the output at word-level or discourse-level with a flat model to alleviate this problem, we propose a hierarchical generation model to capture the different levels of diversity using the conditional variational autoencoders. Specifically, a hierarchical response generation (HRG) framework is proposed to capture the conversation intention in a natural and coherent way. It has two modules, namely, an expression reconstruction model to capture the hierarchical correlation between expression and intention, and an expression attention model to effectively combine the expressions with contents. Finally, the training procedure of HRG is improved by introducing reconstruction loss. Experiment results show that our model can generate the responses with more appropriate content and expression.</p>
<p>Keywords:</p>
<h3 id="187. Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs.">187. Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1187">Paper Link</a>    Pages:1782-1792</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/99/498.html">Zhibin Liu</a> ; <a href="https://dblp.uni-trier.de/pid/15/1158.html">Zheng-Yu Niu</a> ; <a href="https://dblp.uni-trier.de/pid/27/6045-3.html">Hua Wu</a> ; <a href="https://dblp.uni-trier.de/pid/10/5209.html">Haifeng Wang</a></p>
<p>Abstract:
Two types of knowledge, triples from knowledge graphs and texts from documents, have been studied for knowledge aware open domain conversation generation, in which graph paths can narrow down vertex candidates for knowledge selection decision, and texts can provide rich information for response generation. Fusion of a knowledge graph and texts might yield mutually reinforcing advantages, but there is less study on that. To address this challenge, we propose a knowledge aware chatting machine with three components, an augmented knowledge graph with both triples and texts, knowledge selector, and knowledge aware response generator. For knowledge selection on the graph, we formulate it as a problem of multi-hop graph reasoning to effectively capture conversation flow, which is more explainable and flexible in comparison with previous works. To fully leverage long text information that differentiates our graph from others, we improve a state of the art reasoning algorithm with machine reading comprehension technology. We demonstrate the effectiveness of our system on two datasets in comparison with state-of-the-art models.</p>
<p>Keywords:</p>
<h3 id="188. Adaptive Parameterization for Neural Dialogue Generation.">188. Adaptive Parameterization for Neural Dialogue Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1188">Paper Link</a>    Pages:1793-1802</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/2493.html">Hengyi Cai</a> ; <a href="https://dblp.uni-trier.de/pid/151/8462.html">Hongshen Chen</a> ; <a href="https://dblp.uni-trier.de/pid/82/6384.html">Cheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/169/7096.html">Yonghao Song</a> ; <a href="https://dblp.uni-trier.de/pid/88/47.html">Xiaofang Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/91/4572.html">Dawei Yin</a></p>
<p>Abstract:
Neural conversation systems generate responses based on the sequence-to-sequence (SEQ2SEQ) paradigm. Typically, the model is equipped with a single set of learned parameters to generate responses for given input contexts. When confronting diverse conversations, its adaptability is rather limited and the model is hence prone to generate generic responses. In this work, we propose an Adaptive Neural Dialogue generation model, AdaND, which manages various conversations with conversation-specific parameterization. For each conversation, the model generates parameters of the encoder-decoder by referring to the input context. In particular, we propose two adaptive parameterization mechanisms: a context-aware and a topic-aware parameterization mechanism. The context-aware parameterization directly generates the parameters by capturing local semantics of the given context. The topic-aware parameterization enables parameter sharing among conversations with similar topics by first inferring the latent topics of the given context and then generating the parameters with respect to the distributional topics. Extensive experiments conducted on a large-scale real-world conversational dataset show that our model achieves superior performance in terms of both quantitative metrics and human evaluations.</p>
<p>Keywords:</p>
<h3 id="189. Towards Knowledge-Based Recommender Dialog System.">189. Towards Knowledge-Based Recommender Dialog System.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1189">Paper Link</a>    Pages:1803-1813</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/238/0932.html">Qibin Chen</a> ; <a href="https://dblp.uni-trier.de/pid/215/3823.html">Junyang Lin</a> ; <a href="https://dblp.uni-trier.de/pid/165/9507.html">Yichang Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/48/3462.html">Ming Ding</a> ; <a href="https://dblp.uni-trier.de/pid/241/4994.html">Yukuo Cen</a> ; <a href="https://dblp.uni-trier.de/pid/69/3036.html">Hongxia Yang</a> ; <a href="https://dblp.uni-trier.de/pid/t/JieTang.html">Jie Tang</a></p>
<p>Abstract:
In this paper, we propose a novel end-to-end framework called KBRD, which stands for Knowledge-Based Recommender Dialog System. It integrates the recommender system and the dialog generation system. The dialog generation system can enhance the performance of the recommendation system by introducing information about users preferences, and the recommender system can improve that of the dialog generation system by providing recommendation-aware vocabulary bias. Experimental results demonstrate that our proposed model has significant advantages over the baselines in both the evaluation of dialog generation and recommendation. A series of analyses show that the two systems can bring mutual benefits to each other, and the introduced knowledge contributes to both their performances.</p>
<p>Keywords:</p>
<h3 id="190. Structuring Latent Spaces for Stylized Response Generation.">190. Structuring Latent Spaces for Stylized Response Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1190">Paper Link</a>    Pages:1814-1823</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/14/3881.html">Xiang Gao</a> ; <a href="https://dblp.uni-trier.de/pid/132/4966.html">Yizhe Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/29/3671.html">Sungjin Lee</a> ; <a href="https://dblp.uni-trier.de/pid/05/3289.html">Michel Galley</a> ; <a href="https://dblp.uni-trier.de/pid/95/6500.html">Chris Brockett</a> ; <a href="https://dblp.uni-trier.de/pid/92/5339.html">Jianfeng Gao</a> ; <a href="https://dblp.uni-trier.de/pid/13/486.html">Bill Dolan</a></p>
<p>Abstract:
Generating responses in a targeted style is a useful yet challenging task, especially in the absence of parallel data. With limited data, existing methods tend to generate responses that are either less stylized or less context-relevant. We propose StyleFusion, which bridges conversation modeling and non-parallel style transfer by sharing a structured latent space. This structure allows the system to generate stylized relevant responses by sampling in the neighborhood of the conversation model prediction, and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines.</p>
<p>Keywords:</p>
<h3 id="191. Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration.">191. Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1191">Paper Link</a>    Pages:1824-1833</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/2524.html">Zhu Feng Pan</a> ; <a href="https://dblp.uni-trier.de/pid/56/2984.html">Kun Bai</a> ; <a href="https://dblp.uni-trier.de/pid/59/2227-60.html">Yan Wang</a> ; <a href="https://dblp.uni-trier.de/pid/185/0719.html">Lianqiang Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/21/4323.html">Xiaojiang Liu</a></p>
<p>Abstract:
In multi-turn dialogue, utterances do not always take the full form of sentences. These incomplete utterances will greatly reduce the performance of open-domain dialogue systems. Restoring more incomplete utterances from context could potentially help the systems generate more relevant responses. To facilitate the study of incomplete utterance restoration for open-domain dialogue systems, a large-scale multi-turn dataset Restoration-200K is collected and manually labeled with the explicit relation between an utterance and its context. We also propose a pick-and-combine model to restore the incomplete utterance from its context. Experimental results demonstrate that the annotated dataset and the proposed approach significantly boost the response quality of both single-turn and multi-turn dialogue systems.</p>
<p>Keywords:</p>
<h3 id="192. Unsupervised Context Rewriting for Open Domain Conversation.">192. Unsupervised Context Rewriting for Open Domain Conversation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1192">Paper Link</a>    Pages:1834-1844</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/48/3927.html">Kun Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/55/957.html">Kai Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/22/0-12.html">Yu Wu</a> ; <a href="https://dblp.uni-trier.de/pid/54/2695.html">Shujie Liu</a> ; <a href="https://dblp.uni-trier.de/pid/204/9938.html">Jingsong Yu</a></p>
<p>Abstract:
Context modeling has a pivotal role in open domain conversation. Existing works either use heuristic methods or jointly learn context modeling and response generation with an encoder-decoder framework. This paper proposes an explicit context rewriting method, which rewrites the last utterance by considering context history. We leverage pseudo-parallel data and elaborate a context rewriting network, which is built upon the CopyNet with the reinforcement learning method. The rewritten utterance is beneficial to candidate retrieval, explainable context modeling, as well as enabling to employ a single-turn framework to the multi-turn scenario. The empirical results show that our model outperforms baselines in terms of the rewriting quality, the multi-turn response generation, and the end-to-end retrieval-based chatbots.</p>
<p>Keywords:</p>
<h3 id="193. Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots.">193. Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1193">Paper Link</a>    Pages:1845-1854</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/93/3604.html">Jia-Chen Gu</a> ; <a href="https://dblp.uni-trier.de/pid/70/5210.html">Zhen-Hua Ling</a> ; <a href="https://dblp.uni-trier.de/pid/93/310.html">Xiaodan Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/67/6917.html">Quan Liu</a></p>
<p>Abstract:
This paper proposes a dually interactive matching network (DIM) for presenting the personalities of dialogue agents in retrieval-based chatbots. This model develops from the interactive matching network (IMN) which models the matching degree between a context composed of multiple utterances and a response candidate. Compared with previous persona fusion approach which enhances the representation of a context by calculating its similarity with a given persona, the DIM model adopts a dual matching architecture, which performs interactive matching between responses and contexts and between responses and personas respectively for ranking response candidates. Experimental results on PERSONA-CHAT dataset show that the DIM model outperforms its baseline model, i.e., IMN with persona fusion, by a margin of 14.5% and outperforms the present state-of-the-art model by a margin of 27.7% in terms of top-1 accuracy hits@1.</p>
<p>Keywords:</p>
<h3 id="194. DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs.">194. DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1194">Paper Link</a>    Pages:1855-1865</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/5940.html">Yi-Lin Tuan</a> ; <a href="https://dblp.uni-trier.de/pid/04/9878.html">Yun-Nung Chen</a> ; <a href="https://dblp.uni-trier.de/pid/81/8056.html">Hung-yi Lee</a></p>
<p>Abstract:
Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions.</p>
<p>Keywords:</p>
<h3 id="195. Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework.">195. Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1195">Paper Link</a>    Pages:1866-1875</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/c/DCai.html">Deng Cai</a> ; <a href="https://dblp.uni-trier.de/pid/59/2227-60.html">Yan Wang</a> ; <a href="https://dblp.uni-trier.de/pid/38/1163.html">Wei Bi</a> ; <a href="https://dblp.uni-trier.de/pid/71/9281.html">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pid/21/4323.html">Xiaojiang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/s/ShumingShi-1.html">Shuming Shi</a></p>
<p>Abstract:
End-to-end sequence generation is a popular technique for developing open domain dialogue systems, though they suffer from the safe response problem. Researchers have attempted to tackle this problem by incorporating generative models with the returns of retrieval systems. Recently, a skeleton-then-response framework has been shown promising results for this task. Nevertheless, how to precisely extract a skeleton and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.</p>
<p>Keywords:</p>
<h3 id="196. Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation.">196. Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1196">Paper Link</a>    Pages:1876-1885</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/68/7844.html">Liliang Ren</a> ; <a href="https://dblp.uni-trier.de/pid/161/2449.html">Jianmo Ni</a> ; <a href="https://dblp.uni-trier.de/pid/29/3483.html">Julian J. McAuley</a></p>
<p>Abstract:
Existing approaches to dialogue state tracking rely on pre-defined ontologies consisting of a set of all possible slot types and values. Though such approaches exhibit promising performance on single-domain benchmarks, they suffer from computational complexity that increases proportionally to the number of pre-defined slots that need tracking. This issue becomes more severe when it comes to multi-domain dialogues which include larger numbers of slots. In this paper, we investigate how to approach DST using a generation framework without the pre-defined ontology list. Given each turn of user utterance and system response, we directly generate a sequence of belief states by applying a hierarchical encoder-decoder structure. In this way, the computational complexity of our model will be a constant regardless of the number of pre-defined slots. Experiments on both the multi-domain and the single domain dialogue state tracking dataset show that our model not only scales easily with the increasing number of pre-defined domains and slots but also reaches the state-of-the-art performance.</p>
<p>Keywords:</p>
<h3 id="197. Low-Resource Response Generation with Template Prior.">197. Low-Resource Response Generation with Template Prior.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1197">Paper Link</a>    Pages:1886-1897</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/158/1178.html">Ze Yang</a> ; <a href="https://dblp.uni-trier.de/pid/95/6985-14.html">Wei Wu</a> ; <a href="https://dblp.uni-trier.de/pid/y/JianYang.html">Jian Yang</a> ; <a href="https://dblp.uni-trier.de/pid/33/965.html">Can Xu</a> ; <a href="https://dblp.uni-trier.de/pid/76/2866.html">Zhoujun Li</a></p>
<p>Abstract:
We study open domain response generation with limited message-response pairs. The problem exists in real-world applications but is less explored by the existing work. Since the paired data now is no longer enough to train a neural generation model, we consider leveraging the large scale of unpaired data that are much easier to obtain, and propose response generation with both paired and unpaired data. The generation model is defined by an encoder-decoder architecture with templates as prior, where the templates are estimated from the unpaired data as a neural hidden semi-markov model. By this means, response generation learned from the small paired data can be aided by the semantic and syntactic knowledge in the large unpaired data. To balance the effect of the prior and the input message to response generation, we propose learning the whole generation model with an adversarial approach. Empirical studies on question response generation and sentiment response generation indicate that when only a few pairs are available, our model can significantly outperform several state-of-the-art response generation models in terms of both automatic and human evaluation.</p>
<p>Keywords:</p>
<h3 id="198. A Discrete CVAE for Response Generation on Short-Text Conversation.">198. A Discrete CVAE for Response Generation on Short-Text Conversation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1198">Paper Link</a>    Pages:1898-1908</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/82/4977.html">Jun Gao</a> ; <a href="https://dblp.uni-trier.de/pid/38/1163.html">Wei Bi</a> ; <a href="https://dblp.uni-trier.de/pid/21/4323.html">Xiaojiang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/08/500.html">Junhui Li</a> ; <a href="https://dblp.uni-trier.de/pid/42/6620.html">Guodong Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/s/ShumingShi-1.html">Shuming Shi</a></p>
<p>Abstract:
Neural conversation models such as encoder-decoder models are easy to generate bland and generic responses. Some researchers propose to use the conditional variational autoencoder (CVAE) which maximizes the lower bound on the conditional log-likelihood on a continuous latent variable. With different sampled latent variables, the model is expected to generate diverse responses. Although the CVAE-based models have shown tremendous potential, their improvement of generating high-quality responses is still unsatisfactory. In this paper, we introduce a discrete latent variable with an explicit semantic meaning to improve the CVAE on short-text conversation. A major advantage of our model is that we can exploit the semantic distance between the latent variables to maintain good diversity between the sampled latent variables. Accordingly, we propose a two-stage sampling approach to enable efficient diverse variable selection from a large latent space assumed in the short-text conversation task. Experimental results indicate that our model outperforms various kinds of generation models under both automatic and human evaluations and generates more diverse and informative responses.</p>
<p>Keywords:</p>
<h3 id="199. Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations.">199. Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1199">Paper Link</a>    Pages:1909-1919</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/245/6042.html">Ran Le</a> ; <a href="https://dblp.uni-trier.de/pid/191/6009.html">Wenpeng Hu</a> ; <a href="https://dblp.uni-trier.de/pid/220/3102.html">Mingyue Shang</a> ; <a href="https://dblp.uni-trier.de/pid/254/8059.html">Zhenjun You</a> ; <a href="https://dblp.uni-trier.de/pid/53/6625.html">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pid/63/1870.html">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/19/2405-1.html">Rui Yan</a></p>
<p>Abstract:
Previous research on dialogue systems generally focuses on the conversation between two participants, yet multi-party conversations which involve more than two participants within one session bring up a more complicated but realistic scenario. In real multi- party conversations, we can observe who is speaking, but the addressee information is not always explicit. In this paper, we aim to tackle the challenge of identifying all the miss- ing addressees in a conversation session. To this end, we introduce a novel who-to-whom (W2W) model which models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements.</p>
<p>Keywords:</p>
<h3 id="200. A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency in Dialogue Generation.">200. A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency in Dialogue Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1200">Paper Link</a>    Pages:1920-1930</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8005.html">Jinxin Chang</a> ; <a href="https://dblp.uni-trier.de/pid/96/9018.html">Ruifang He</a> ; <a href="https://dblp.uni-trier.de/pid/74/3148.html">Longbiao Wang</a> ; <a href="https://dblp.uni-trier.de/pid/08/890.html">Xiangyu Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/88/3999.html">Ting Yang</a> ; <a href="https://dblp.uni-trier.de/pid/35/8027.html">Ruifang Wang</a></p>
<p>Abstract:
Neural sequence-to-sequence models for dialog systems suffer from the problem of favoring uninformative and non replier-specific responses due to lack of the global and relevant information guidance. The existing methods model the generation process by leveraging the neural variational network with simple Gaussian. However, the sampled information from latent space usually becomes useless due to the KL divergence vanishing issue, and the highly abstractive global variables easily dilute the personal features of replier, leading to a non replier-specific response. Therefore, a novel Semi-Supervised Stable Variational Network (SSVN) is proposed to address these issues. We use a unit hypersperical distribution, namely the von Mises-Fisher (vMF), as the latent space of a semi-supervised model, which can obtain the stable KL performance by setting a fixed variance and hence enhance the global information representation. Meanwhile, an unsupervised extractor is introduced to automatically distill the replier-tailored feature which is then injected into a supervised generator to encourage the replier-consistency. Experimental results on two large conversation datasets show that our model outperforms the competitive baseline models significantly, and can generate diverse and replier-specific responses.</p>
<p>Keywords:</p>
<h3 id="201. Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders.">201. Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1201">Paper Link</a>    Pages:1931-1940</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/224/5573.html">Zhangming Chan</a> ; <a href="https://dblp.uni-trier.de/pid/32/971.html">Juntao Li</a> ; <a href="https://dblp.uni-trier.de/pid/50/9561.html">Xiaopeng Yang</a> ; <a href="https://dblp.uni-trier.de/pid/33/11343.html">Xiuying Chen</a> ; <a href="https://dblp.uni-trier.de/pid/191/6009.html">Wenpeng Hu</a> ; <a href="https://dblp.uni-trier.de/pid/63/1870.html">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/19/2405-1.html">Rui Yan</a></p>
<p>Abstract:
Variational autoencoders (VAEs) and Wasserstein autoencoders (WAEs) have achieved noticeable progress in open-domain response generation. Through introducing latent variables in continuous space, these models are capable of capturing utterance-level semantics, e.g., topic, syntactic properties, and thus can generate informative and diversified responses. In this work, we improve the WAE for response generation. In addition to the utterance-level information, we also model user-level information in latent continue space. Specifically, we embed user-level and utterance-level information into two multimodal distributions, and combine these two multimodal distributions into a mixed distribution. This mixed distribution will be used as the prior distribution of WAE in our proposed model, named as PersonaWAE. Experimental results on a large-scale real-world dataset confirm the superiority of our model for generating informative and personalized responses, where both automatic and human evaluations outperform state-of-the-art models.</p>
<p>Keywords:</p>
<h3 id="202. Variational Hierarchical User-based Conversation Model.">202. Variational Hierarchical User-based Conversation Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1202">Paper Link</a>    Pages:1941-1950</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/22/11519.html">JinYeong Bak</a> ; <a href="https://dblp.uni-trier.de/pid/50/7562.html">Alice Oh</a></p>
<p>Abstract:
Generating appropriate conversation responses requires careful modeling of the utterances and speakers together. Some recent approaches to response generation model both the utterances and the speakers, but these approaches tend to generate responses that are overly tailored to the speakers. To overcome this limitation, we propose a new model with a stochastic variable designed to capture the speaker information and deliver it to the conversational context. An important part of this model is the network of speakers in which each speaker is connected to one or more conversational partner, and this network is then used to model the speakers better. To test whether our model generates more appropriate conversation responses, we build a new conversation corpus containing approximately 27,000 speakers and 770,000 conversations. With this corpus, we run experiments of generating conversational responses and compare our model with other state-of-the-art models. By automatic evaluation metrics and human evaluation, we show that our model outperforms other models in generating appropriate responses. An additional advantage of our model is that it generates better responses for various new user scenarios, for example when one of the speakers is a known user in our corpus but the partner is a new user. For replicability, we make available all our code and data.</p>
<p>Keywords:</p>
<h3 id="203. Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue.">203. Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1203">Paper Link</a>    Pages:1951-1961</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/9056.html">Dongyeop Kang</a> ; <a href="https://dblp.uni-trier.de/pid/148/4470.html">Anusha Balakrishnan</a> ; <a href="https://dblp.uni-trier.de/pid/151/3076.html">Pararth Shah</a> ; <a href="https://dblp.uni-trier.de/pid/119/3357.html">Paul Crook</a> ; <a href="https://dblp.uni-trier.de/pid/60/5680.html">Y-Lan Boureau</a> ; <a href="https://dblp.uni-trier.de/pid/29/6977.html">Jason Weston</a></p>
<p>Abstract:
Traditional recommendation systems produce static rather than interactive recommendations invariant to a users specific requests, clarifications, or current mood, and can suffer from the cold-start problem if their tastes are unknown. These issues can be alleviated by treating recommendation as an interactive dialogue task instead, where an expert recommender can sequentially ask about someones preferences, react to their requests, and recommend more appropriate items. In this work, we collect a goal-driven recommendation dialogue dataset (GoRecDial), which consists of 9,125 dialogue games and 81,260 conversation turns between pairs of human workers recommending movies to each other. The task is specifically designed as a cooperative game between two players working towards a quantifiable common goal. We leverage the dataset to develop an end-to-end dialogue system that can simultaneously converse and recommend. Models are first trained to imitate the behavior of human players without considering the task goal itself (supervised training). We then finetune our models on simulated bot-bot conversations between two paired pre-trained models (bot-play), in order to achieve the dialogue goal. Our experiments show that models finetuned with bot-play learn improved dialogue strategies, reach the dialogue goal more often when paired with a human, and are rated as more consistent by humans compared to models trained without bot-play. The dataset and code are publicly available through the ParlAI framework.</p>
<p>Keywords:</p>
<h3 id="204. CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases.">204. CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1204">Paper Link</a>    Pages:1962-1979</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/67/1014.html">Tao Yu</a> ; <a href="https://dblp.uni-trier.de/pid/60/2536-37.html">Rui Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/242/8249.html">Heyang Er</a> ; <a href="https://dblp.uni-trier.de/pid/98/7732.html">Suyi Li</a> ; <a href="https://dblp.uni-trier.de/pid/248/7781.html">Eric Xue</a> ; <a href="https://dblp.uni-trier.de/pid/16/6344.html">Bo Pang</a> ; <a href="https://dblp.uni-trier.de/pid/215/5264.html">Xi Victoria Lin</a> ; <a href="https://dblp.uni-trier.de/pid/242/8043.html">Yi Chern Tan</a> ; <a href="https://dblp.uni-trier.de/pid/154/6550.html">Tianze Shi</a> ; <a href="https://dblp.uni-trier.de/pid/233/6392.html">Zihan Li</a> ; <a href="https://dblp.uni-trier.de/pid/165/5324.html">Youxuan Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/202/1809.html">Michihiro Yasunaga</a> ; <a href="https://dblp.uni-trier.de/pid/242/7898.html">Sungrok Shim</a> ; <a href="https://dblp.uni-trier.de/pid/69/510.html">Tao Chen</a> ; <a href="https://dblp.uni-trier.de/pid/203/8539.html">Alexander R. Fabbri</a> ; <a href="https://dblp.uni-trier.de/pid/125/3616.html">Zifan Li</a> ; <a href="https://dblp.uni-trier.de/pid/64/2464.html">Luyao Chen</a> ; <a href="https://dblp.uni-trier.de/pid/90/3868.html">Yuwen Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/242/8242.html">Shreya Dixit</a> ; <a href="https://dblp.uni-trier.de/pid/242/8103.html">Vincent Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/80/7282.html">Caiming Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/79/128.html">Richard Socher</a> ; <a href="https://dblp.uni-trier.de/pid/04/10300.html">Walter S. Lasecki</a> ; <a href="https://dblp.uni-trier.de/pid/r/DragomirRRadev.html">Dragomir R. Radev</a></p>
<p>Abstract:
We present CoSQL, a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by SQL, the expert describes the SQL and execution results to the user, hence maintaining a natural interaction flow. CoSQL introduces new challenges compared to existing task-oriented dialogue datasets: (1) the dialogue states are grounded in SQL, a domain-independent executable representation, instead of domain-specific slot value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that CoSQL presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at <a href="https://yale-lily.github.io/cosql">https://yale-lily.github.io/cosql</a>.</p>
<p>Keywords:</p>
<h3 id="205. A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response Selection.">205. A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response Selection.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1205">Paper Link</a>    Pages:1980-1989</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/79/4208.html">Harshit Kumar</a> ; <a href="https://dblp.uni-trier.de/pid/18/7442.html">Arvind Agarwal</a> ; <a href="https://dblp.uni-trier.de/pid/96/2418.html">Sachindra Joshi</a></p>
<p>Abstract:
Dialogue Acts play an important role in conversation modeling. Research has shown the utility of dialogue acts for the response selection task, however, the underlying assumption is that the dialogue acts are readily available, which is impractical, as dialogue acts are rarely available for new conversations. This paper proposes an end-to-end multi-task model for conversation modeling, which is optimized for two tasks, dialogue act prediction and response selection, with the latter being the task of interest. It proposes a novel way of combining the predicted dialogue acts of context and response with the context (previous utterances) and response (follow-up utterance) in a crossway fashion, such that, it achieves at par performance for the response selection task compared to the model that uses actual dialogue acts. Through experiments on two well known datasets, we demonstrate that the multi-task model not only improves the accuracy of the dialogue act prediction task but also improves the MRR for the response selection task. Also, the cross-stitching of dialogue acts of context and response with the context and response is better than using either one of them individually.</p>
<p>Keywords:</p>
<h3 id="206. How to Build User Simulators to Train RL-based Dialog Systems.">206. How to Build User Simulators to Train RL-based Dialog Systems.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1206">Paper Link</a>    Pages:1990-2000</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/5722.html">Weiyan Shi</a> ; <a href="https://dblp.uni-trier.de/pid/77/2062.html">Kun Qian</a> ; <a href="https://dblp.uni-trier.de/pid/56/4632.html">Xuewei Wang</a> ; <a href="https://dblp.uni-trier.de/pid/83/3205.html">Zhou Yu</a></p>
<p>Abstract:
User simulators are essential for training reinforcement learning (RL) based dialog models. The performance of the simulator directly impacts the RL policy. However, building a good user simulator that models real user behaviors is challenging. We propose a method of standardizing user simulator building that can be used by the community to compare dialog system quality using the same set of user simulators fairly. We present implementations of six user simulators trained with different dialog planning and generation methods. We then calculate a set of automatic metrics to evaluate the quality of these simulators both directly and indirectly. We also ask human users to assess the simulators directly and indirectly by rating the simulated dialogs and interacting with the trained systems. This paper presents a comprehensive evaluation framework for user simulator study and provides a better understanding of the pros and cons of different user simulators, as well as their impacts on the trained systems.</p>
<p>Keywords:</p>
<h3 id="207. Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning.">207. Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1207">Paper Link</a>    Pages:2001-2011</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/88/4850.html">Tao Jin</a> ; <a href="https://dblp.uni-trier.de/pid/146/9031.html">Siyu Huang</a> ; <a href="https://dblp.uni-trier.de/pid/119/1901.html">Yingming Li</a> ; <a href="https://dblp.uni-trier.de/pid/z/ZhongfeiMarkZhang.html">Zhongfei Zhang</a></p>
<p>Abstract:
This paper addresses the challenging task of video captioning which aims to generate descriptions for video data. Recently, the attention-based encoder-decoder structures have been widely used in video captioning. In existing literature, the attention weights are often built from the information of an individual modality, while, the association relationships between multiple modalities are neglected. Motivated by this, we propose a video captioning model with High-Order Cross-Modal Attention (HOCA) where the attention weights are calculated based on the high-order correlation tensor to capture the frame-level cross-modal interaction of different modalities sufficiently. Furthermore, we novelly introduce Low-Rank HOCA which adopts tensor decomposition to reduce the extremely large space requirement of HOCA, leading to a practical and efficient implementation in real-world applications. Experimental results on two benchmark datasets, MSVD and MSR-VTT, show that Low-rank HOCA establishes a new state-of-the-art.</p>
<p>Keywords:</p>
<h3 id="208. Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach.">208. Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1208">Paper Link</a>    Pages:2012-2023</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/16/9611-3.html">Dong-Jin Kim</a> ; <a href="https://dblp.uni-trier.de/pid/138/0075.html">Jinsoo Choi</a> ; <a href="https://dblp.uni-trier.de/pid/119/1450.html">Tae-Hyun Oh</a> ; <a href="https://dblp.uni-trier.de/pid/74/4917.html">In So Kweon</a></p>
<p>Abstract:
Constructing an organized dataset comprised of a large number of images and several captions for each image is a laborious task, which requires vast human effort. On the other hand, collecting a large number of images and sentences separately may be immensely easier. In this paper, we develop a novel data-efficient semi-supervised framework for training an image captioning model. We leverage massive unpaired image and caption data by learning to associate them. To this end, our proposed semi-supervised learning method assigns pseudo-labels to unpaired samples via Generative Adversarial Networks to learn the joint distribution of image and caption. To evaluate, we construct scarcely-paired COCO dataset, a modified version of MS COCO caption dataset. The empirical results show the effectiveness of our method compared to several strong baselines, especially when the amount of the paired samples are scarce.</p>
<p>Keywords:</p>
<h3 id="209. Dual Attention Networks for Visual Reference Resolution in Visual Dialog.">209. Dual Attention Networks for Visual Reference Resolution in Visual Dialog.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1209">Paper Link</a>    Pages:2024-2033</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/236/6256.html">Gi-Cheon Kang</a> ; <a href="https://dblp.uni-trier.de/pid/236/6240.html">Jaeseo Lim</a> ; <a href="https://dblp.uni-trier.de/pid/09/5682.html">Byoung-Tak Zhang</a></p>
<p>Abstract:
Visual dialog (VisDial) is a task which requires a dialog agent to answer a series of questions grounded in an image. Unlike in visual question answering (VQA), the series of questions should be able to capture a temporal context from a dialog history and utilizes visually-grounded information. Visual reference resolution is a problem that addresses these challenges, requiring the agent to resolve ambiguous references in a given question and to find the references in a given image. In this paper, we propose Dual Attention Networks (DAN) for visual reference resolution in VisDial. DAN consists of two kinds of attention modules, REFER and FIND. Specifically, REFER module learns latent relationships between a given question and a dialog history by employing a multi-head attention mechanism. FIND module takes image features and reference-aware representations (i.e., the output of REFER module) as input, and performs visual grounding via bottom-up attention mechanism. We qualitatively and quantitatively evaluate our model on VisDial v1.0 and v0.9 datasets, showing that DAN outperforms the previous state-of-the-art model by a significant margin.</p>
<p>Keywords:</p>
<h3 id="210. Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents.">210. Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1210">Paper Link</a>    Pages:2034-2045</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/132/5250.html">Jack Hessel</a> ; <a href="https://dblp.uni-trier.de/pid/l/LillianLee.html">Lillian Lee</a> ; <a href="https://dblp.uni-trier.de/pid/39/5487.html">David Mimno</a></p>
<p>Abstract:
Images and text co-occur constantly on the web, but explicit links between images and sentences (or other intra-document textual units) are often not present. We present algorithms that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difficulty, ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We find that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can suffice to predict links between specific sentences and specific images within the same document at test time.</p>
<p>Keywords:</p>
<h3 id="211. UR-FUNNY: A Multimodal Language Dataset for Understanding Humor.">211. UR-FUNNY: A Multimodal Language Dataset for Understanding Humor.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1211">Paper Link</a>    Pages:2046-2056</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/64/2529.html">Md. Kamrul Hasan</a> ; <a href="https://dblp.uni-trier.de/pid/239/5132.html">Wasifur Rahman</a> ; <a href="https://dblp.uni-trier.de/pid/150/5010.html">AmirAli Bagher Zadeh</a> ; <a href="https://dblp.uni-trier.de/pid/239/5133.html">Jianyuan Zhong</a> ; <a href="https://dblp.uni-trier.de/pid/48/10270.html">Md. Iftekhar Tanveer</a> ; <a href="https://dblp.uni-trier.de/pid/31/739.html">Louis-Philippe Morency</a> ; <a href="https://dblp.uni-trier.de/pid/26/1236.html">Mohammed (Ehsan) Hoque</a></p>
<p>Abstract:
Humor is a unique and creative communicative behavior often displayed during social interactions. It is produced in a multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding humor from these three modalities falls within boundaries of multimodal language; a recent research trend in natural language processing that models natural language as it happens in face-to-face communication. Although humor detection is an established research area in NLP, in a multimodal context it has been understudied. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The dataset and accompanying studies, present a framework in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.</p>
<p>Keywords:</p>
<h3 id="212. Partners in Crime: Multi-view Sequential Inference for Movie Understanding.">212. Partners in Crime: Multi-view Sequential Inference for Movie Understanding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1212">Paper Link</a>    Pages:2057-2067</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/185/0552.html">Nikos Papasarantopoulos</a> ; <a href="https://dblp.uni-trier.de/pid/117/4041.html">Lea Frermann</a> ; <a href="https://dblp.uni-trier.de/pid/59/6701.html">Mirella Lapata</a> ; <a href="https://dblp.uni-trier.de/pid/04/5629.html">Shay B. Cohen</a></p>
<p>Abstract:
Multi-view learning algorithms are powerful representation learning tools, often exploited in the context of multimodal problems. However, for problems requiring inference at the token-level of a sequence (that is, a separate prediction must be made for every time step), it is often the case that single-view systems are used, or that more than one views are fused in a simple manner. We describe an incremental neural architecture paired with a novel training objective for incremental inference. The network operates on multi-view data. We demonstrate the effectiveness of our approach on the problem of predicting perpetrators in crime drama series, for which our model significantly outperforms previous work and strong baselines. Moreover, we introduce two tasks, crime case and speaker type tagging, that contribute to movie understanding and demonstrate the effectiveness of our model on them.</p>
<p>Keywords:</p>
<h3 id="213. Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag.">213. Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1213">Paper Link</a>    Pages:2068-2077</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/1913.html">Xinyu Xiao</a> ; <a href="https://dblp.uni-trier.de/pid/06/4250.html">Lingfeng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/60/105.html">Bin Fan</a> ; <a href="https://dblp.uni-trier.de/pid/81/6575.html">Shiming Xiang</a> ; <a href="https://dblp.uni-trier.de/pid/26/3810.html">Chunhong Pan</a></p>
<p>Abstract:
In the current video captioning models, the video frames are collected in one network and the semantics are mixed into one feature, which not only increase the difficulty of the caption decoding, but also decrease the interpretability of the captioning models. To address these problems, we propose an Adaptive Semantic Guidance Network (ASGN), which instantiates the whole video semantics to different POS-aware semantics with the supervision of part of speech (POS) tag. In the encoding process, the POS tag activates the related neurons and parses the whole semantic information into corresponding encoded video representations. Furthermore, the potential of the model is stimulated by the POS-aware video features. In the decoding process, the related video features of noun and verb are used as the supervision to construct a new adaptive attention model which can decide whether to attend to the video feature or not. With the explicit improving of the interpretability of the network, the learning process is more transparent and the results are more predictable. Extensive experiments demonstrate the effectiveness of our model when compared with state-of-the-art models.</p>
<p>Keywords:</p>
<h3 id="214. A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding.">214. A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1214">Paper Link</a>    Pages:2078-2087</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/213/9481.html">Libo Qin</a> ; <a href="https://dblp.uni-trier.de/pid/98/4640.html">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pid/62/8367.html">Yangming Li</a> ; <a href="https://dblp.uni-trier.de/pid/222/1956.html">Haoyang Wen</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a></p>
<p>Abstract:
Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel framework for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the error propagation, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our model achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.</p>
<p>Keywords:</p>
<h3 id="215. Talk2Car: Taking Control of Your Self-Driving Car.">215. Talk2Car: Taking Control of Your Self-Driving Car.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1215">Paper Link</a>    Pages:2088-2098</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/249/5327.html">Thierry Deruyttere</a> ; <a href="https://dblp.uni-trier.de/pid/237/9993.html">Simon Vandenhende</a> ; <a href="https://dblp.uni-trier.de/pid/249/5335.html">Dusan Grujicic</a> ; <a href="https://dblp.uni-trier.de/pid/61/5017.html">Luc Van Gool</a> ; <a href="https://dblp.uni-trier.de/pid/m/MarieFrancineMoens.html">Marie-Francine Moens</a></p>
<p>Abstract:
A long-term goal of artificial intelligence is to have an agent execute commands communicated through natural language. In many cases the commands are grounded in a visual environment shared by the human who gives the command and the agent. Execution of the command then requires mapping the command into the physical visual space, after which the appropriate action can be taken. In this paper we consider the former. Or more specifically, we consider the problem in an autonomous driving setting, where a passenger requests an action that can be associated with an object found in a street scene. Our work presents the Talk2Car dataset, which is the first object referral dataset that contains commands written in natural language for self-driving cars. We provide a detailed comparison with related datasets such as ReferIt, RefCOCO, RefCOCO+, RefCOCOg, Cityscape-Ref and CLEVR-Ref. Additionally, we include a performance analysis using strong state-of-the-art models. The results show that the proposed object referral task is a challenging one for which the models show promising results but still require additional research in natural language processing, computer vision and the intersection of these fields. The dataset can be found on our website: <a href="http://macchina-ai.eu/">http://macchina-ai.eu/</a></p>
<p>Keywords:</p>
<h3 id="216. Fact-Checking Meets Fauxtography: Verifying Claims About Images.">216. Fact-Checking Meets Fauxtography: Verifying Claims About Images.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1216">Paper Link</a>    Pages:2099-2108</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/0888.html">Dimitrina Zlatkova</a> ; <a href="https://dblp.uni-trier.de/pid/19/1947.html">Preslav Nakov</a> ; <a href="https://dblp.uni-trier.de/pid/88/2617.html">Ivan Koychev</a></p>
<p>Abstract:
The recent explosion of false claims in social media and on the Web in general has given rise to a lot of manual fact-checking initiatives. Unfortunately, the number of claims that need to be fact-checked is several orders of magnitude larger than what humans can handle manually. Thus, there has been a lot of research aiming at automating the process. Interestingly, previous work has largely ignored the growing number of claims about images. This is despite the fact that visual imagery is more influential than text and naturally appears alongside fake news. Here we aim at bridging this gap. In particular, we create a new dataset for this problem, and we explore a variety of features modeling the claim, the image, and the relationship between the claim and the image. The evaluation results show sizable improvements over the baseline. We release our dataset, hoping to enable further research on fact-checking claims about images.</p>
<p>Keywords:</p>
<h3 id="217. Video Dialog via Progressive Inference and Cross-Transformer.">217. Video Dialog via Progressive Inference and Cross-Transformer.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1217">Paper Link</a>    Pages:2109-2118</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/245/1858.html">Weike Jin</a> ; <a href="https://dblp.uni-trier.de/pid/75/7785.html">Zhou Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/245/1837.html">Mao Gu</a> ; <a href="https://dblp.uni-trier.de/pid/71/2308-1.html">Jun Xiao</a> ; <a href="https://dblp.uni-trier.de/pid/72/5870.html">Furu Wei</a> ; <a href="https://dblp.uni-trier.de/pid/218/7793.html">Yueting Zhuang</a></p>
<p>Abstract:
Video dialog is a new and challenging task, which requires the agent to answer questions combining video information with dialog history. And different from single-turn video question answering, the additional dialog history is important for video dialog, which often includes contextual information for the question. Existing visual dialog methods mainly use RNN to encode the dialog history as a single vector representation, which might be rough and straightforward. Some more advanced methods utilize hierarchical structure, attention and memory mechanisms, which still lack an explicit reasoning process. In this paper, we introduce a novel progressive inference mechanism for video dialog, which progressively updates query information based on dialog history and video content until the agent think the information is sufficient and unambiguous. In order to tackle the multi-modal fusion problem, we propose a cross-transformer module, which could learn more fine-grained and comprehensive interactions both inside and between the modalities. And besides answer generation, we also consider question generation, which is more challenging but significant for a complete video dialog system. We evaluate our method on two large-scale datasets, and the extensive experiments show the effectiveness of our method.</p>
<p>Keywords:</p>
<h3 id="218. Executing Instructions in Situated Collaborative Interactions.">218. Executing Instructions in Situated Collaborative Interactions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1218">Paper Link</a>    Pages:2119-2130</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9306.html">Alane Suhr</a> ; <a href="https://dblp.uni-trier.de/pid/213/8160.html">Claudia Yan</a> ; <a href="https://dblp.uni-trier.de/pid/250/9639.html">Jacob Schluger</a> ; <a href="https://dblp.uni-trier.de/pid/250/9447.html">Stanley Yu</a> ; <a href="https://dblp.uni-trier.de/pid/250/9486.html">Hadi Khader</a> ; <a href="https://dblp.uni-trier.de/pid/250/9478.html">Marwa Mouallem</a> ; <a href="https://dblp.uni-trier.de/pid/230/3729.html">Iris Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/32/10489.html">Yoav Artzi</a></p>
<p>Abstract:
We study a collaborative scenario where a user not only instructs a system to complete tasks, but also acts alongside it. This allows the user to adapt to the system abilities by changing their language or deciding to simply accomplish some tasks themselves, and requires the system to effectively recover from errors as the user strategically assigns it new goals. We build a game environment to study this scenario, and learn to map user instructions to system actions. We introduce a learning approach focused on recovery from cascading errors between instructions, and modeling methods to explicitly reason about instructions with multiple goals. We evaluate with a new evaluation protocol using recorded interactions and online games with human users, and observe how users adapt to the system abilities.</p>
<p>Keywords:</p>
<h3 id="219. Fusion of Detected Objects in Text for Visual Question Answering.">219. Fusion of Detected Objects in Text for Visual Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1219">Paper Link</a>    Pages:2131-2140</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/79/6205.html">Chris Alberti</a> ; <a href="https://dblp.uni-trier.de/pid/157/8170.html">Jeffrey Ling</a> ; <a href="https://dblp.uni-trier.de/pid/29/1340-1.html">Michael Collins</a> ; <a href="https://dblp.uni-trier.de/pid/68/1814.html">David Reitter</a></p>
<p>Abstract:
To advance models of multimodal context, we introduce a simple yet powerful neural architecture for data that combines vision and natural language. The Bounding Boxes in Text Transformer (B2T2) also leverages referential information binding words to portions of the image in a single unified architecture. B2T2 is highly effective on the Visual Commonsense Reasoning benchmark, achieving a new state-of-the-art with a 25% relative reduction in error rate compared to published baselines and obtaining the best performance to date on the public leaderboard (as of May 22, 2019). A detailed ablation analysis shows that the early integration of the visual features into the text analysis is key to the effectiveness of the new architecture. A reference implementation of our models is provided.</p>
<p>Keywords:</p>
<h3 id="220. TIGEr: Text-to-Image Grounding for Image Caption Evaluation.">220. TIGEr: Text-to-Image Grounding for Image Caption Evaluation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1220">Paper Link</a>    Pages:2141-2152</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/02/10377.html">Ming Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/56/11343.html">Qiuyuan Huang</a> ; <a href="https://dblp.uni-trier.de/pid/97/8704.html">Lei Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/10/5630-61.html">Xin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/207/1300.html">Pengchuan Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/41/7845.html">Zhe Gan</a> ; <a href="https://dblp.uni-trier.de/pid/40/3892.html">Jana Diesner</a> ; <a href="https://dblp.uni-trier.de/pid/92/5339.html">Jianfeng Gao</a></p>
<p>Abstract:
This paper presents a new metric called TIGEr for the automatic evaluation of image captioning systems. Popular metrics, such as BLEU and CIDEr, are based solely on text matching between reference captions and machine-generated captions, potentially leading to biased evaluations because references may not fully cover the image content and natural language is inherently ambiguous. Building upon a machine-learned text-image grounding model, TIGEr allows to evaluate caption quality not only based on how well a caption represents image content, but also on how well machine-generated captions match human-generated captions. Our empirical tests show that TIGEr has a higher consistency with human judgments than alternative existing metrics. We also comprehensively assess the metrics effectiveness in caption evaluation by measuring the correlation between human judgments and metric scores.</p>
<p>Keywords:</p>
<h3 id="221. Universal Adversarial Triggers for Attacking and Analyzing NLP.">221. Universal Adversarial Triggers for Attacking and Analyzing NLP.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1221">Paper Link</a>    Pages:2153-2162</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/6165.html">Eric Wallace</a> ; <a href="https://dblp.uni-trier.de/pid/97/1374.html">Shi Feng</a> ; <a href="https://dblp.uni-trier.de/pid/247/5935.html">Nikhil Kandpal</a> ; <a href="https://dblp.uni-trier.de/pid/00/8046.html">Matt Gardner</a> ; <a href="https://dblp.uni-trier.de/pid/13/3568-1.html">Sameer Singh</a></p>
<p>Abstract:
Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of why questions in SQuAD to be answered to kill american people, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.</p>
<p>Keywords:</p>
<h3 id="222. To Annotate or Not? Predicting Performance Drop under Domain Shift.">222. To Annotate or Not? Predicting Performance Drop under Domain Shift.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1222">Paper Link</a>    Pages:2163-2173</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/144/6739.html">Hady ElSahar</a> ; <a href="https://dblp.uni-trier.de/pid/99/7907.html">Matthias Gall</a></p>
<p>Abstract:
Performance drop due to domain-shift is an endemic problem for NLP models in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the model performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods (-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.</p>
<p>Keywords:</p>
<h3 id="223. Adaptively Sparse Transformers.">223. Adaptively Sparse Transformers.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1223">Paper Link</a>    Pages:2174-2184</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/241/9580.html">Gonalo M. Correia</a> ; <a href="https://dblp.uni-trier.de/pid/40/10489.html">Vlad Niculae</a> ; <a href="https://dblp.uni-trier.de/pid/m/AndreFTMartins.html">Andr F. T. Martins</a></p>
<p>Abstract:
Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter  which controls the shape and sparsity of alpha-entmax  allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.</p>
<p>Keywords:</p>
<h3 id="224. Show Your Work: Improved Reporting of Experimental Results.">224. Show Your Work: Improved Reporting of Experimental Results.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1224">Paper Link</a>    Pages:2185-2194</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/49/11425.html">Jesse Dodge</a> ; <a href="https://dblp.uni-trier.de/pid/217/1570.html">Suchin Gururangan</a> ; <a href="https://dblp.uni-trier.de/pid/125/5045.html">Dallas Card</a> ; <a href="https://dblp.uni-trier.de/pid/19/376.html">Roy Schwartz</a> ; <a href="https://dblp.uni-trier.de/pid/90/5204.html">Noah A. Smith</a></p>
<p>Abstract:
Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.</p>
<p>Keywords:</p>
<h3 id="225. A Deep Factorization of Style and Structure in Fonts.">225. A Deep Factorization of Style and Structure in Fonts.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1225">Paper Link</a>    Pages:2195-2205</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/3475.html">Akshay Srivatsan</a> ; <a href="https://dblp.uni-trier.de/pid/30/9988.html">Jonathan T. Barron</a> ; <a href="https://dblp.uni-trier.de/pid/22/1139.html">Dan Klein</a> ; <a href="https://dblp.uni-trier.de/pid/22/8160.html">Taylor Berg-Kirkpatrick</a></p>
<p>Abstract:
We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying generative model combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our model learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our model outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.</p>
<p>Keywords:</p>
<h3 id="226. Cross-lingual Semantic Specialization via Lexical Relation Induction.">226. Cross-lingual Semantic Specialization via Lexical Relation Induction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1226">Paper Link</a>    Pages:2206-2217</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/178/8829.html">Edoardo Maria Ponti</a> ; <a href="https://dblp.uni-trier.de/pid/77/9768.html">Ivan Vulic</a> ; <a href="https://dblp.uni-trier.de/pid/50/11059.html">Goran Glavas</a> ; <a href="https://dblp.uni-trier.de/pid/96/5429.html">Roi Reichart</a> ; <a href="https://dblp.uni-trier.de/pid/14/6532.html">Anna Korhonen</a></p>
<p>Abstract:
Semantic specialization integrates structured linguistic knowledge from external resources (such as lexical relations in WordNet) into pretrained distributional vectors in the form of constraints. However, this technique cannot be leveraged in many languages, because their structured external resources are typically incomplete or non-existent. To bridge this gap, we propose a novel method that transfers specialization from a resource-rich source language (English) to virtually any target language. Our specialization transfer comprises two crucial steps: 1) Inducing noisy constraints in the target language through automatic word translation; and 2) Filtering the noisy constraints via a state-of-the-art relation prediction model trained on the source language constraints. This allows us to specialize any set of distributional vectors in the target language with the refined constraints. We prove the effectiveness of our method through intrinsic word similarity evaluation in 8 languages, and with 3 downstream tasks in 5 languages: lexical simplification, dialog state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages.</p>
<p>Keywords:</p>
<h3 id="227. Modelling the interplay of metaphor and emotion through multitask learning.">227. Modelling the interplay of metaphor and emotion through multitask learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1227">Paper Link</a>    Pages:2218-2229</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/242/7711.html">Verna Dankers</a> ; <a href="https://dblp.uni-trier.de/pid/136/9233.html">Marek Rei</a> ; <a href="https://dblp.uni-trier.de/pid/133/2000.html">Martha Lewis</a> ; <a href="https://dblp.uni-trier.de/pid/33/8156.html">Ekaterina Shutova</a></p>
<p>Abstract:
Metaphors allow us to convey emotion by connecting physical experiences and abstract concepts. The results of previous research in linguistics and psychology suggest that metaphorical phrases tend to be more emotionally evocative than their literal counterparts. In this paper, we investigate the relationship between metaphor and emotion within a computational framework, by proposing the first joint model of these phenomena. We experiment with several multitask learning architectures for this purpose, involving both hard and soft parameter sharing. Our results demonstrate that metaphor identification and emotion prediction mutually benefit from joint learning and our models advance the state of the art in both of these tasks.</p>
<p>Keywords:</p>
<h3 id="228. How well do NLI models capture verb veridicality?">228. How well do NLI models capture verb veridicality?</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1228">Paper Link</a>    Pages:2230-2240</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/239/8686.html">Alexis Ross</a> ; <a href="https://dblp.uni-trier.de/pid/141/4059.html">Ellie Pavlick</a></p>
<p>Abstract:
In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about veridicality in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridicala bias which is amplified in BERT. We further show that, encouragingly, BERTs inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the complement clause (to- vs. that-complements), and negation.</p>
<p>Keywords:</p>
<h3 id="229. Modeling Color Terminology Across Thousands of Languages.">229. Modeling Color Terminology Across Thousands of Languages.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1229">Paper Link</a>    Pages:2241-2250</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/5712.html">Arya D. McCarthy</a> ; <a href="https://dblp.uni-trier.de/pid/219/5637.html">Winston Wu</a> ; <a href="https://dblp.uni-trier.de/pid/248/7949.html">Aaron Mueller</a> ; <a href="https://dblp.uni-trier.de/pid/39/11087.html">Bill Watson</a> ; <a href="https://dblp.uni-trier.de/pid/25/3805.html">David Yarowsky</a></p>
<p>Abstract:
There is an extensive history of scholarship into what constitutes a basic color term, as well as a broadly attested acquisition sequence of basic color terms across many languages, as articulated in the seminal work of Berlin and Kay (1969). This paper employs a set of diverse measures on massively cross-linguistic data to operationalize and critique the Berlin and Kay color term hypotheses. Collectively, the 14 empirically-grounded computational linguistic metrics we designas well as their aggregationcorrelate strongly with both the Berlin and Kay basic/secondary color term partition ( = 0.96) and their hypothesized universal acquisition sequence. The measures and result provide further empirical evidence from computational linguistics in support of their claims, as well as additional nuance: they suggest treating the partition as a spectrum instead of a dichotomy.</p>
<p>Keywords:</p>
<h3 id="230. Negative Focus Detection via Contextual Attention Mechanism.">230. Negative Focus Detection via Contextual Attention Mechanism.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1230">Paper Link</a>    Pages:2251-2261</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8200.html">Longxiang Shen</a> ; <a href="https://dblp.uni-trier.de/pid/136/9191.html">Bowei Zou</a> ; <a href="https://dblp.uni-trier.de/pid/66/5306.html">Yu Hong</a> ; <a href="https://dblp.uni-trier.de/pid/42/6620.html">Guodong Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/28/1279.html">Qiaoming Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/40/357.html">AiTi Aw</a></p>
<p>Abstract:
Negation is a universal but complicated linguistic phenomenon, which has received considerable attention from the NLP community over the last decade, since a negated statement often carries both an explicit negative focus and implicit positive meanings. For the sake of understanding a negated statement, it is critical to precisely detect the negative focus in context. However, how to capture contextual information for negative focus detection is still an open challenge. To well address this, we come up with an attention-based neural network to model contextual information. In particular, we introduce a framework which consists of a Bidirectional Long Short-Term Memory (BiLSTM) neural network and a Conditional Random Fields (CRF) layer to effectively encode the order information and the long-range context dependency in a sentence. Moreover, we design two types of attention mechanisms, word-level contextual attention and topic-level contextual attention, to take advantage of contextual information across sentences from both the word perspective and the topic perspective, respectively. Experimental results on the SEM12 shared task corpus show that our approach achieves the best performance on negative focus detection, yielding an absolute improvement of 2.11% over the state-of-the-art. This demonstrates the great effectiveness of the two types of contextual attention mechanisms.</p>
<p>Keywords:</p>
<h3 id="231. A Unified Neural Coherence Model.">231. A Unified Neural Coherence Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1231">Paper Link</a>    Pages:2262-2272</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7605.html">Han Cheol Moon</a> ; <a href="https://dblp.uni-trier.de/pid/220/3296.html">Tasnim Mohiuddin</a> ; <a href="https://dblp.uni-trier.de/pid/62/2078.html">Shafiq R. Joty</a> ; <a href="https://dblp.uni-trier.de/pid/115/6188.html">Xu Chi</a></p>
<p>Abstract:
Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these models often fail on harder tasks with more realistic application scenarios. In particular, the existing models underperform on tasks that require the model to be sensitive to local contexts such as candidate ranking in conversational dialogue and in machine translation. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art.</p>
<p>Keywords:</p>
<h3 id="232. Topic-Guided Coherence Modeling for Sentence Ordering by Preserving Global and Local Information.">232. Topic-Guided Coherence Modeling for Sentence Ordering by Preserving Global and Local Information.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1232">Paper Link</a>    Pages:2273-2283</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/170/0446.html">Byungkook Oh</a> ; <a href="https://dblp.uni-trier.de/pid/170/0544.html">Seungmin Seo</a> ; <a href="https://dblp.uni-trier.de/pid/254/8067.html">Cheolheon Shin</a> ; <a href="https://dblp.uni-trier.de/pid/251/0147.html">Eunju Jo</a> ; <a href="https://dblp.uni-trier.de/pid/27/5926.html">Kyong-Ho Lee</a></p>
<p>Abstract:
We propose a novel topic-guided coherence modeling (TGCM) for sentence ordering. Our attention based pointer decoder directly utilize sentence vectors in a permutation-invariant manner, without being compressed into a single fixed-length vector as the paragraph representation. Thus, TGCM can improve global dependencies among sentences and preserve relatively informative paragraph-level semantics. Moreover, to predict the next sentence, we capture topic-enhanced sentence-pair interactions between the current predicted sentence and each next-sentence candidate. With the coherent topical context matching, we promote local dependencies that help identify the tight semantic connections for sentence ordering. The experimental results show that TGCM outperforms state-of-the-art models from various perspectives.</p>
<p>Keywords:</p>
<h3 id="233. Neural Generative Rhetorical Structure Parsing.">233. Neural Generative Rhetorical Structure Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1233">Paper Link</a>    Pages:2284-2295</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/0965.html">Amandla Mabona</a> ; <a href="https://dblp.uni-trier.de/pid/29/2006.html">Laura Rimell</a> ; <a href="https://dblp.uni-trier.de/pid/12/336.html">Stephen Clark</a> ; <a href="https://dblp.uni-trier.de/pid/18/1071.html">Andreas Vlachos</a></p>
<p>Abstract:
Rhetorical structure trees have been shown to be useful for several document-level tasks including summarization and document classification. Previous approaches to RST parsing have used discriminative models; however, these are less sample efficient than generative models, and RST parsing datasets are typically small. In this paper, we present the first generative model for RST parsing. Our model is a document-level RNN grammar (RNNG) with a bottom-up traversal order. We show that, for our parsers traversal order, previous beam search algorithms for RNNGs have a left-branching bias which is ill-suited for RST parsing.We develop a novel beam search algorithm that keeps track of both structure-and word-generating actions without exhibit-ing this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data</p>
<p>Keywords:</p>
<h3 id="234. Weak Supervision for Learning Discourse Structure.">234. Weak Supervision for Learning Discourse Structure.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1234">Paper Link</a>    Pages:2296-2305</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/245/8683.html">Sonia Badene</a> ; <a href="https://dblp.uni-trier.de/pid/76/2370.html">Kate Thompson</a> ; <a href="https://dblp.uni-trier.de/pid/37/6937.html">Jean-Pierre Lorr</a> ; <a href="https://dblp.uni-trier.de/pid/33/3380.html">Nicholas Asher</a></p>
<p>Abstract:
This paper provides a detailed comparison of a data programming approach with (i) off-the-shelf, state-of-the-art deep learning architectures that optimize their representations (BERT) and (ii) handcrafted-feature approaches previously used in the discourse analysis literature. We compare these approaches on the task of learning discourse structure for multi-party dialogue. The data programming paradigm offered by the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the generative step into probability distributions of the class labels given the data. We show that on our task the generative model outperforms both deep learning architectures as well as more traditional ML approaches when learning discourse structureit even outperforms the combination of deep learning methods and hand-crafted features. We also implement several strategies for decoding our generative model output in order to improve our results. We conclude that weak supervision methods hold great promise as a means for creating and improving data sets for discourse structure.</p>
<p>Keywords:</p>
<h3 id="235. Predicting Discourse Structure using Distant Supervision from Sentiment.">235. Predicting Discourse Structure using Distant Supervision from Sentiment.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1235">Paper Link</a>    Pages:2306-2316</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/56/4367.html">Patrick Huber</a> ; <a href="https://dblp.uni-trier.de/pid/30/3994.html">Giuseppe Carenini</a></p>
<p>Abstract:
Discourse parsing could not yet take full advantage of the neural NLP revolution, mostly due to the lack of annotated datasets. We propose a novel approach that uses distant supervision on an auxiliary task (sentiment classification), to generate abundant data for RST-style discourse structure prediction. Our approach combines a neural variant of multiple-instance learning, using document-level supervision, with an optimal CKY-style tree generation algorithm. In a series of experiments, we train a discourse parser (for only structure prediction) on our automatically generated dataset and compare it with parsers trained on human-annotated corpora (news domain RST-DT and Instructional domain). Results indicate that while our parser does not yet match the performance of a parser trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the parser is trained on one domain and tested/applied on another one.</p>
<p>Keywords:</p>
<h3 id="236. The Myth of Double-Blind Review Revisited: ACL vs. EMNLP.">236. The Myth of Double-Blind Review Revisited: ACL vs. EMNLP.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1236">Paper Link</a>    Pages:2317-2327</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/6680.html">Cornelia Caragea</a> ; <a href="https://dblp.uni-trier.de/pid/174/7148.html">Ana Sabina Uban</a> ; <a href="https://dblp.uni-trier.de/pid/50/3644.html">Liviu P. Dinu</a></p>
<p>Abstract:
The review and selection process for scientific paper publication is essential for the quality of scholarly publications in a scientific field. The double-blind review system, which enforces author anonymity during the review period, is widely used by prestigious conferences and journals to ensure the integrity of this process. Although the notion of anonymity in the double-blind review has been questioned before, the availability of full text paper collections brings new opportunities for exploring the question: Is the double-blind review process really double-blind? We study this question on the ACL and EMNLP paper collections and present an analysis on how well deep learning techniques can infer the authors of a paper. Specifically, we explore Convolutional Neural Networks trained on various aspects of a paper, e.g., content, style features, and references, to understand the extent to which we can infer the authors of a paper and what aspects contribute the most. Our results show that the authors of a paper can be inferred with accuracy as high as 87% on ACL and 78% on EMNLP for the top 100 most prolific authors.</p>
<p>Keywords:</p>
<h3 id="237. Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization.">237. Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1237">Paper Link</a>    Pages:2328-2337</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/0998.html">Yingchi Liu</a> ; <a href="https://dblp.uni-trier.de/pid/26/3756.html">Quanzhi Li</a> ; <a href="https://dblp.uni-trier.de/pid/222/0089.html">Marika Cifor</a> ; <a href="https://dblp.uni-trier.de/pid/11/6389.html">Xiaozhong Liu</a> ; <a href="https://dblp.uni-trier.de/pid/06/3351.html">Qiong Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/14/1217.html">Luo Si</a></p>
<p>Abstract:
The number of personal stories about sexual harassment shared online has increased exponentially in recent years. This is in part inspired by the #MeToo and #TimesUp movements. Safecity is an online forum for people who experienced or witnessed sexual harassment to share their personal experiences. It has collected &gt;10,000 stories so far. Sexual harassment occurred in a variety of situations, and categorization of the stories and extraction of their key elements will provide great help for the related parties to understand and address sexual harassment. In this study, we manually annotated those stories with labels in the dimensions of location, time, and harassers characteristics, and marked the key elements related to these dimensions. Furthermore, we applied natural language processing technologies with joint learning schemes to automatically categorize these stories in those dimensions and extract key elements at the same time. We also uncovered significant patterns from the categorized sexual harassment stories. We believe our annotated data set, proposed algorithms, and analysis will help people who have been harassed, authorities, researchers and other related parties in various ways, such as automatically filling reports, enlightening the public in order to prevent future harassment, and enabling more effective, faster action to be taken.</p>
<p>Keywords:</p>
<h3 id="238. Identifying Predictive Causal Factors from News Streams.">238. Identifying Predictive Causal Factors from News Streams.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1238">Paper Link</a>    Pages:2338-2348</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/141/2020.html">Ananth Balashankar</a> ; <a href="https://dblp.uni-trier.de/pid/61/9376.html">Sunandan Chakraborty</a> ; <a href="https://dblp.uni-trier.de/pid/245/6087.html">Samuel Fraiberger</a> ; <a href="https://dblp.uni-trier.de/pid/85/5401.html">Lakshminarayanan Subramanian</a></p>
<p>Abstract:
We propose a new framework to uncover the relationship between news events and real world phenomena. We present the Predictive Causal Graph (PCG) which allows to detect latent relationships between events mentioned in news streams. This graph is constructed by measuring how the occurrence of a word in the news influences the occurrence of another (set of) word(s) in the future. We show that PCG can be used to extract latent features from news streams, outperforming other graph-based methods in prediction error of 10 stock price time series for 12 months. We then extended PCG to be applicable for longer time windows by allowing time-varying factors, leading to stock price prediction error rates between 1.5% and 5% for about 4 years. We then manually validated PCG, finding that 67% of the causation semantic frame arguments present in the news corpus were directly connected in the PCG, the remaining being connected through a semantically relevant intermediate node.</p>
<p>Keywords:</p>
<h3 id="239. Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content.">239. Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1239">Paper Link</a>    Pages:2349-2359</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/168/5561.html">Sepideh Mesbah</a> ; <a href="https://dblp.uni-trier.de/pid/12/1198-28.html">Jie Yang</a> ; <a href="https://dblp.uni-trier.de/pid/78/7364.html">Robert-Jan Sips</a> ; <a href="https://dblp.uni-trier.de/pid/225/2471.html">Manuel Valle Torre</a> ; <a href="https://dblp.uni-trier.de/pid/52/980.html">Christoph Lofi</a> ; <a href="https://dblp.uni-trier.de/pid/12/2920.html">Alessandro Bozzon</a> ; <a href="https://dblp.uni-trier.de/pid/h/GeertJanHouben.html">Geert-Jan Houben</a></p>
<p>Abstract:
Social media provides a timely yet challenging data source for adverse drug reaction (ADR) detection. Existing dictionary-based, semi-supervised learning approaches are intrinsically limited by the coverage and maintainability of laymen health vocabularies. In this paper, we introduce a data augmentation approach that leverages variational autoencoders to learn high-quality data distributions from a large unlabeled dataset, and subsequently, to automatically generate a large labeled training set from a small set of labeled samples. This allows for efficient social-media ADR detection with low training and re-training costs to adapt to the changes and emergence of informal medical laymen terms. An extensive evaluation performed on Twitter and Reddit data shows that our approach matches the performance of fully-supervised approaches while requiring only 25% of training data.</p>
<p>Keywords:</p>
<h3 id="240. Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference.">240. Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1240">Paper Link</a>    Pages:2360-2369</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/1951.html">Ahmadreza Mosallanezhad</a> ; <a href="https://dblp.uni-trier.de/pid/156/3879.html">Ghazaleh Beigi</a> ; <a href="https://dblp.uni-trier.de/pid/92/309-1.html">Huan Liu</a></p>
<p>Abstract:
User-generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private-attribute information that they may not want to disclose such as age and location. Users privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning-based Text Anonymizor, RLTA, which addresses the problem of private-attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text w.r.t. a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations w.r.t. the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility.</p>
<p>Keywords:</p>
<h3 id="241. Tree-structured Decoding for Solving Math Word Problems.">241. Tree-structured Decoding for Solving Math Word Problems.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1241">Paper Link</a>    Pages:2370-2379</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/6808.html">Qianying Liu</a> ; <a href="https://dblp.uni-trier.de/pid/254/7971.html">Wenyv Guan</a> ; <a href="https://dblp.uni-trier.de/pid/05/4288.html">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pid/72/4981.html">Daisuke Kawahara</a></p>
<p>Abstract:
Automatically solving math word problems is an interesting research topic that needs to bridge natural language descriptions and formal math equations. Previous studies introduced end-to-end neural network methods, but these approaches did not efficiently consider an important characteristic of the equation, i.e., an abstract syntax tree. To address this problem, we propose a tree-structured decoding method that generates the abstract syntax tree of the equation in a top-down manner. In addition, our approach can automatically stop during decoding without a redundant stop token. The experimental results show that our method achieves single model state-of-the-art performance on Math23K, which is the largest dataset on this task.</p>
<p>Keywords:</p>
<h3 id="242. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text.">242. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1242">Paper Link</a>    Pages:2380-2390</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/185/6000.html">Haitian Sun</a> ; <a href="https://dblp.uni-trier.de/pid/48/6510.html">Tania Bedrax-Weiss</a> ; <a href="https://dblp.uni-trier.de/pid/c/WWCohen.html">William W. Cohen</a></p>
<p>Abstract:
We consider open-domain question answering (QA) where answers are drawn from either a corpus, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a corpus is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., multi-hop) reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or pull) operations on the corpus and/or KB. After the subgraph is complete, another graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.</p>
<p>Keywords:</p>
<h3 id="243. Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning.">243. Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1243">Paper Link</a>    Pages:2391-2401</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/127/0072.html">Lifu Huang</a> ; <a href="https://dblp.uni-trier.de/pid/63/7554.html">Ronan Le Bras</a> ; <a href="https://dblp.uni-trier.de/pid/151/3093.html">Chandra Bhagavatula</a> ; <a href="https://dblp.uni-trier.de/pid/89/579.html">Yejin Choi</a></p>
<p>Abstract:
Understanding narratives requires reading between the lines, which in turn, requires interpreting the likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset of 35,600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. In stark contrast to most existing reading comprehension datasets where the questions focus on factual and literal understanding of the context paragraph, our dataset focuses on reading between the lines over a diverse collection of peoples everyday narratives, asking such questions as what might be the possible reason of ...?", or what would have happened if ..." that require reasoning beyond the exact text spans in the context. To establish baseline performances on Cosmos QA, we experiment with several state-of-the-art neural architectures for reading comprehension, and also propose a new architecture that improves over the competitive baselines. Experimental results demonstrate a significant gap between machine (68.4%) and human performance (94%), pointing to avenues for future research on commonsense machine comprehension. Dataset, code and leaderboard is publicly available at <a href="https://wilburone.github.io/cosmos">https://wilburone.github.io/cosmos</a>.</p>
<p>Keywords:</p>
<h3 id="244. Finding Generalizable Evidence by Learning to Convince Q&A Models.">244. Finding Generalizable Evidence by Learning to Convince Q&amp;A Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1244">Paper Link</a>    Pages:2402-2411</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/192/1812.html">Ethan Perez</a> ; <a href="https://dblp.uni-trier.de/pid/199/1922.html">Siddharth Karamcheti</a> ; <a href="https://dblp.uni-trier.de/pid/77/3763.html">Rob Fergus</a> ; <a href="https://dblp.uni-trier.de/pid/29/6977.html">Jason Weston</a> ; <a href="https://dblp.uni-trier.de/pid/136/9140.html">Douwe Kiela</a> ; <a href="https://dblp.uni-trier.de/pid/41/9736.html">Kyunghyun Cho</a></p>
<p>Abstract:
We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only ~20% of the full passage and (ii) QA models can generalize to longer passages and harder questions.</p>
<p>Keywords:</p>
<h3 id="245. Ranking and Sampling in Open-Domain Question Answering.">245. Ranking and Sampling in Open-Domain Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1245">Paper Link</a>    Pages:2412-2421</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/7997.html">Yanfu Xu</a> ; <a href="https://dblp.uni-trier.de/pid/51/3740.html">Zheng Lin</a> ; <a href="https://dblp.uni-trier.de/pid/55/5877.html">Yuanxin Liu</a> ; <a href="https://dblp.uni-trier.de/pid/42/469.html">Rui Liu</a> ; <a href="https://dblp.uni-trier.de/pid/72/4134-5.html">Weiping Wang</a> ; <a href="https://dblp.uni-trier.de/pid/01/2538.html">Dan Meng</a></p>
<p>Abstract:
Open-domain question answering (OpenQA) aims to answer questions based on a number of unlabeled paragraphs. Existing approaches always follow the distantly supervised setup where some of the paragraphs are wrong-labeled (noisy), and mainly utilize the paragraph-question relevance to denoise. However, the paragraph-paragraph relevance, which may aggregate the evidence among relevant paragraphs, can also be utilized to discover more useful paragraphs. Moreover, current approaches mainly focus on the positive paragraphs which are known to contain the answer during training. This will affect the generalization ability of the model and make it be disturbed by the similar but irrelevant (distracting) paragraphs during testing. In this paper, we first introduce a ranking model leveraging the paragraph-question and the paragraph-paragraph relevance to compute a confidence score for each paragraph. Furthermore, based on the scores, we design a modified weighted sampling strategy for training to mitigate the influence of the noisy and distracting paragraphs. Experiments on three public datasets (Quasar-T, SearchQA and TriviaQA) show that our model advances the state of the art.</p>
<p>Keywords:</p>
<h3 id="246. A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs.">246. A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1246">Paper Link</a>    Pages:2422-2430</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/23/9282.html">Katsuhiko Hayashi</a> ; <a href="https://dblp.uni-trier.de/pid/92/5032.html">Masashi Shimbo</a></p>
<p>Abstract:
Bilinear diagonal models for knowledge graph embedding (KGE), such as DistMult and ComplEx, balance expressiveness and computational efficiency by representing relations as diagonal matrices. Although they perform well in predicting atomic relations, composite relations (relation paths) cannot be modeled naturally by the product of relation matrices, as the product of diagonal matrices is commutative and hence invariant with the order of relations. In this paper, we propose a new bilinear KGE model, called BlockHolE, based on block circulant matrices. In BlockHolE, relation matrices can be non-commutative, allowing composite relations to be modeled by matrix product. The model is parameterized in a way that covers a spectrum ranging from diagonal to full relation matrices. A fast computation technique can be developed on the basis of the duality of the Fourier transform of circulant matrices.</p>
<p>Keywords:</p>
<h3 id="247. Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss.">247. Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1247">Paper Link</a>    Pages:2431-2441</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/26/6730.html">Cao Liu</a> ; <a href="https://dblp.uni-trier.de/pid/42/4903.html">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/136/8650.html">Shizhu He</a> ; <a href="https://dblp.uni-trier.de/pid/n/ZaiqingNie.html">Zaiqing Nie</a> ; <a href="https://dblp.uni-trier.de/pid/47/2026-1.html">Jun Zhao</a></p>
<p>Abstract:
We tackle the task of question generation over knowledge bases. Conventional methods for this task neglect two crucial research issues: 1) the given predicate needs to be expressed; 2) the answer to the generated question needs to be definitive. In this paper, we strive toward the above two issues via incorporating diversified contexts and answer-aware loss. Specifically, we propose a neural encoder-decoder model with multi-level copy mechanisms to generate such questions. Furthermore, the answer aware loss is introduced to make generated questions corresponding to more definitive answers. Experiments demonstrate that our model achieves state-of-the-art performance. Meanwhile, such generated question is able to express the given predicate and correspond to a definitive answer.</p>
<p>Keywords:</p>
<h3 id="248. Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base.">248. Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1248">Paper Link</a>    Pages:2442-2451</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/95/4097.html">Tao Shen</a> ; <a href="https://dblp.uni-trier.de/pid/19/189.html">Xiubo Geng</a> ; <a href="https://dblp.uni-trier.de/pid/14/6841.html">Tao Qin</a> ; <a href="https://dblp.uni-trier.de/pid/225/5494.html">Daya Guo</a> ; <a href="https://dblp.uni-trier.de/pid/135/6318.html">Duyu Tang</a> ; <a href="https://dblp.uni-trier.de/pid/30/8160.html">Nan Duan</a> ; <a href="https://dblp.uni-trier.de/pid/34/10089.html">Guodong Long</a> ; <a href="https://dblp.uni-trier.de/pid/77/5094.html">Daxin Jiang</a></p>
<p>Abstract:
We consider the problem of conversational question answering over a large-scale knowledge base. To handle huge entity vocabulary of a large-scale knowledge base, recent neural semantic parsing based approaches usually decompose the task into several subtasks and then solve them sequentially, which leads to following issues: 1) errors in earlier subtasks will be propagated and negatively affect downstream ones; and 2) each subtask cannot naturally share supervision signals with others. To tackle these issues, we propose an innovative multi-task learning framework where a pointer-equipped semantic parsing model is designed to resolve coreference in conversations, and naturally empower joint learning with a novel type-aware entity detection model. The proposed framework thus enables shared supervisions and alleviates the effect of error propagation. Experiments on a large-scale conversational question answering dataset containing 1.6M question answering pairs over 12.8M entities show that the proposed framework improves overall F1 score from 67% to 79% compared with previous state-of-the-art work.</p>
<p>Keywords:</p>
<h3 id="249. BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels.">249. BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1249">Paper Link</a>    Pages:2452-2462</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/58/4986.html">Yimin Jing</a> ; <a href="https://dblp.uni-trier.de/pid/55/6548.html">Deyi Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/28/955.html">Yan Zhen</a></p>
<p>Abstract:
This paper presents BiPaR, a bilingual parallel novel-style machine reading comprehension (MRC) dataset, developed to support multilingual and cross-lingual reading comprehension. The biggest difference between BiPaR and existing reading comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR is written parallelly in two languages. We collect 3,667 bilingual parallel paragraphs from Chinese and English novels, from which we construct 14,668 parallel question-answer pairs via crowdsourced workers following a strict quality control procedure. We analyze BiPaR in depth and find that BiPaR offers good diversification in prefixes of questions, answer types and relationships between questions and passages. We also observe that answering questions of novels requires reading comprehension skills of coreference resolution, multi-sentence reasoning, and understanding of implicit causality, etc. With BiPaR, we build monolingual, multilingual, and cross-lingual MRC baseline models. Even for the relatively simple monolingual MRC on this dataset, experiments show that a strong BERT baseline is over 30 points behind human in terms of both EM and F1 score, indicating that BiPaR provides a challenging testbed for monolingual, multilingual and cross-lingual MRC on novels. The dataset is available at <a href="https://multinlp.github.io/BiPaR/">https://multinlp.github.io/BiPaR/</a>.</p>
<p>Keywords:</p>
<h3 id="250. Language Models as Knowledge Bases?">250. Language Models as Knowledge Bases?</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1250">Paper Link</a>    Pages:2463-2473</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/118/5349.html">Fabio Petroni</a> ; <a href="https://dblp.uni-trier.de/pid/43/11537.html">Tim Rocktschel</a> ; <a href="https://dblp.uni-trier.de/pid/18/3348-1.html">Sebastian Riedel</a> ; <a href="https://dblp.uni-trier.de/pid/227/3197.html">Patrick S. H. Lewis</a> ; <a href="https://dblp.uni-trier.de/pid/136/7890.html">Anton Bakhtin</a> ; <a href="https://dblp.uni-trier.de/pid/11/4867.html">Yuxiang Wu</a> ; <a href="https://dblp.uni-trier.de/pid/190/7117.html">Alexander H. Miller</a></p>
<p>Abstract:
Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as fill-in-the-blank cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at <a href="https://github.com/facebookresearch/LAMA">https://github.com/facebookresearch/LAMA</a>.</p>
<p>Keywords:</p>
<h3 id="251. NumNet: Machine Reading Comprehension with Numerical Reasoning.">251. NumNet: Machine Reading Comprehension with Numerical Reasoning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1251">Paper Link</a>    Pages:2474-2484</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/237/9718.html">Qiu Ran</a> ; <a href="https://dblp.uni-trier.de/pid/161/0001.html">Yankai Lin</a> ; <a href="https://dblp.uni-trier.de/pid/83/6353-30.html">Peng Li</a> ; <a href="https://dblp.uni-trier.de/pid/00/5012-16.html">Jie Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/53/3245-1.html">Zhiyuan Liu</a></p>
<p>Abstract:
Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in humans reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming all existing machine reading comprehension models by considering the numerical relations among numbers.</p>
<p>Keywords:</p>
<h3 id="252. Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks.">252. Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1252">Paper Link</a>    Pages:2485-2494</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7736.html">Haoyang Huang</a> ; <a href="https://dblp.uni-trier.de/pid/245/8600.html">Yaobo Liang</a> ; <a href="https://dblp.uni-trier.de/pid/30/8160.html">Nan Duan</a> ; <a href="https://dblp.uni-trier.de/pid/34/4521.html">Ming Gong</a> ; <a href="https://dblp.uni-trier.de/pid/239/5572.html">Linjun Shou</a> ; <a href="https://dblp.uni-trier.de/pid/77/5094.html">Daxin Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/16/1161-1.html">Ming Zhou</a></p>
<p>Abstract:
We present Unicoder, a universal language encoder that is insensitive to different languages. Given an arbitrary NLP task, a model can be trained with Unicoder using training data in one language and directly applied to inputs of the same task in other languages. Comparing to similar efforts such as Multilingual BERT and XLM , three new cross-lingual pre-training tasks are proposed, including cross-lingual word recovery, cross-lingual paraphrase classification and cross-lingual masked language model. These tasks help Unicoder learn the mappings among different languages from more perspectives. We also find that doing fine-tuning on multiple languages together can bring further improvement. Experiments are performed on two tasks: cross-lingual natural language inference (XNLI) and cross-lingual question answering (XQA), where XLM is our baseline. On XNLI, 1.8% averaged accuracy improvement (on 15 languages) is obtained. On XQA, which is a new cross-lingual dataset built by us, 5.5% averaged accuracy improvement (on French and German) is obtained.</p>
<p>Keywords:</p>
<h3 id="253. Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering.">253. Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1253">Paper Link</a>    Pages:2495-2509</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/186/8393.html">Shiyue Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/32/5243.html">Mohit Bansal</a></p>
<p>Abstract:
Text-based Question Generation (QG) aims at generating natural and relevant questions that can be answered by a given answer in some context. Existing QG models suffer from a semantic drift problem, i.e., the semantics of the model-generated question drifts away from the given context and answer. In this paper, we first propose two semantics-enhanced rewards obtained from downstream question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions. Second, since the traditional evaluation metrics (e.g., BLEU) often fall short in evaluating the quality of generated questions, we propose a QA-based evaluation method which measures the QG models ability to mimic human annotators in generating QA training data. Experiments show that our method achieves the new state-of-the-art performance w.r.t. traditional metrics, and also performs best on our QA-based evaluation metrics. Further, we investigate how to use our QG model to augment QA datasets and enable semi-supervised QA. We propose two ways to generate synthetic QA pairs: generate new questions from existing articles or collect QA pairs from new articles. We also propose two empirically effective strategies, a data filter and mixing mini-batch training, to properly use the QG-generated data for QA. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.</p>
<p>Keywords:</p>
<h3 id="254. Adversarial Domain Adaptation for Machine Reading Comprehension.">254. Adversarial Domain Adaptation for Machine Reading Comprehension.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1254">Paper Link</a>    Pages:2510-2520</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/163/2233.html">Huazheng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/41/7845.html">Zhe Gan</a> ; <a href="https://dblp.uni-trier.de/pid/65/622.html">Xiaodong Liu</a> ; <a href="https://dblp.uni-trier.de/pid/30/3008-1.html">Jingjing Liu</a> ; <a href="https://dblp.uni-trier.de/pid/92/5339.html">Jianfeng Gao</a> ; <a href="https://dblp.uni-trier.de/pid/05/6545.html">Hongning Wang</a></p>
<p>Abstract:
In this paper, we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain. To this end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i) pseudo questions are first generated for unlabeled passages in the target domain, and then (ii) a domain classifier is incorporated into an MRC model to predict which domain a given passage-question pair comes from. The classifier and the passage-question encoder are jointly trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semi-supervised learning.</p>
<p>Keywords:</p>
<h3 id="255. Incorporating External Knowledge into Machine Reading for Generative Question Answering.">255. Incorporating External Knowledge into Machine Reading for Generative Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1255">Paper Link</a>    Pages:2521-2530</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/28/3252.html">Bin Bi</a> ; <a href="https://dblp.uni-trier.de/pid/78/3213.html">Chen Wu</a> ; <a href="https://dblp.uni-trier.de/pid/51/5332.html">Ming Yan</a> ; <a href="https://dblp.uni-trier.de/pid/35/7092-225.html">Wei Wang</a> ; <a href="https://dblp.uni-trier.de/pid/155/6708.html">Jiangnan Xia</a> ; <a href="https://dblp.uni-trier.de/pid/52/9457.html">Chenliang Li</a></p>
<p>Abstract:
Commonsense and background knowledge is required for a QA model to answer many nontrivial questions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with context. In this paper, we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. During the process of answer generation, KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the model to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over models without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging knowledge.</p>
<p>Keywords:</p>
<h3 id="256. Answering questions by learning to rank - Learning to rank by answering questions.">256. Answering questions by learning to rank - Learning to rank by answering questions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1256">Paper Link</a>    Pages:2531-2540</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/232/1660.html">George-Sebastian Pirtoaca</a> ; <a href="https://dblp.uni-trier.de/pid/16/856.html">Traian Rebedea</a> ; <a href="https://dblp.uni-trier.de/pid/118/3624.html">Stefan Ruseti</a></p>
<p>Abstract:
Answering multiple-choice questions in a setting in which no supporting documents are explicitly provided continues to stand as a core problem in natural language processing. The contribution of this article is two-fold. First, it describes a method which can be used to semantically rank documents extracted from Wikipedia or similar natural language corpora. Second, we propose a model employing the semantic ranking that holds the first place in two of the most popular leaderboards for answering multiple-choice questions: ARC Easy and Challenge. To achieve this, we introduce a self-attention based neural network that latently learns to rank documents by their importance related to a given question, whilst optimizing the objective of predicting the correct answer. These documents are considered relevant contexts for the underlying question. We have published the ranked documents so that they can be used off-the-shelf to improve downstream decision models.</p>
<p>Keywords:</p>
<h3 id="257. Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension.">257. Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1257">Paper Link</a>    Pages:2541-2552</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/166/2019.html">Todor Mihaylov</a> ; <a href="https://dblp.uni-trier.de/pid/82/6572.html">Anette Frank</a></p>
<p>Abstract:
In this work, we propose to use linguistic annotations as a basis for a Discourse-Aware Semantic Self-Attention encoder that we employ for reading comprehension on narrative texts. We extract relations between discourse units, events, and their arguments as well as coreferring mentions, using available annotation tools. Our empirical evaluation shows that the investigated structures improve the overall performance (up to +3.4 Rouge-L), especially intra-sentential and cross-sentential discourse relations, sentence-internal semantic role relations, and long-distance coreference relations. We show that dedicating self-attention heads to intra-sentential relations and relations connecting neighboring sentences is beneficial for finding answers to questions in longer contexts. Our findings encourage the use of discourse-semantic annotations to enhance the generalization capacity of self-attention models for reading comprehension.</p>
<p>Keywords:</p>
<h3 id="258. Revealing the Importance of Semantic Retrieval for Machine Reading at Scale.">258. Revealing the Importance of Semantic Retrieval for Machine Reading at Scale.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1258">Paper Link</a>    Pages:2553-2566</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/2725.html">Yixin Nie</a> ; <a href="https://dblp.uni-trier.de/pid/147/1402.html">Songhe Wang</a> ; <a href="https://dblp.uni-trier.de/pid/32/5243.html">Mohit Bansal</a></p>
<p>Abstract:
Machine Reading at Scale (MRS) is a challenging task in which a system is given an input query and is asked to produce a precise output by reading information from a large knowledge base. The task has gained popularity with its natural combination of information retrieval (IR) and machine comprehension (MC). Advancements in representation learning have led to separated progress in both IR and MC; however, very few studies have examined the relationship and combined design of retrieval and comprehension at different levels of granularity, for development of MRS systems. In this work, we give general guidelines on system design for MRS by proposing a simple yet effective pipeline system with special consideration on hierarchical semantic retrieval at both paragraph and sentence level, and their potential effects on the downstream task. The system is evaluated on both fact verification and open-domain multihop QA, achieving state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of semantic retrieval, we present ablation and analysis studies to quantify the contribution of neural retrieval modules at both paragraph-level and sentence-level, and illustrate that intermediate semantic retrieval modules are vital for not only effectively filtering upstream information and thus saving downstream computation, but also for shaping upstream data distribution and providing better data for downstream modeling.</p>
<p>Keywords:</p>
<h3 id="259. PubMedQA: A Dataset for Biomedical Research Question Answering.">259. PubMedQA: A Dataset for Biomedical Research Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1259">Paper Link</a>    Pages:2567-2577</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/96/5382.html">Qiao Jin</a> ; <a href="https://dblp.uni-trier.de/pid/180/5692.html">Bhuwan Dhingra</a> ; <a href="https://dblp.uni-trier.de/pid/35/9542.html">Zhengping Liu</a> ; <a href="https://dblp.uni-trier.de/pid/c/WWCohen.html">William W. Cohen</a> ; <a href="https://dblp.uni-trier.de/pid/64/3270.html">Xinghua Lu</a></p>
<p>Abstract:
We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at <a href="https://pubmedqa.github.io">https://pubmedqa.github.io</a>.</p>
<p>Keywords:</p>
<h3 id="260. Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering.">260. Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1260">Paper Link</a>    Pages:2578-2589</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/62/5346.html">Vikas Yadav</a> ; <a href="https://dblp.uni-trier.de/pid/52/5246.html">Steven Bethard</a> ; <a href="https://dblp.uni-trier.de/pid/18/3479.html">Mihai Surdeanu</a></p>
<p>Abstract:
We propose an unsupervised strategy for the selection of justification sentences for multi-hop question answering (QA) that (a) maximizes the relevance of the selected sentences, (b) minimizes the overlap between the selected facts, and (c) maximizes the coverage of both question and answer. This unsupervised sentence selection can be coupled with any supervised QA model. We show that the sentences selected by our method improve the performance of a state-of-the-art supervised QA model on two multi-hop QA datasets: AI2s Reasoning Challenge (ARC) and Multi-Sentence Reading Comprehension (MultiRC). We obtain new state-of-the-art performance on both datasets among systems that do not use external resources for training the QA system: 56.82% F1 on ARC (41.24% on Challenge and 64.49% on Easy) and 26.1% EM0 on MultiRC. Our justification sentences have higher quality than the justifications selected by a strong information retrieval baseline, e.g., by 5.4% F1 in MultiRC. We also show that our unsupervised selection of justification sentences is more stable across domains than a state-of-the-art supervised sentence selection method.</p>
<p>Keywords:</p>
<h3 id="261. Answering Complex Open-domain Questions Through Iterative Query Generation.">261. Answering Complex Open-domain Questions Through Iterative Query Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1261">Paper Link</a>    Pages:2590-2602</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/59/9474-3.html">Peng Qi</a> ; <a href="https://dblp.uni-trier.de/pid/210/0844.html">Xiaowen Lin</a> ; <a href="https://dblp.uni-trier.de/pid/185/6795.html">Leo Mehr</a> ; <a href="https://dblp.uni-trier.de/pid/03/4540-2.html">Zijian Wang</a> ; <a href="https://dblp.uni-trier.de/pid/m/ChristopherDManning.html">Christopher D. Manning</a></p>
<p>Abstract:
It is challenging for current one-step retrieve-and-read question answering (QA) systems to answer questions like Which novel by the author of Armada will be adapted as a feature film by Steven Spielberg? because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GoldEn (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GoldEn Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GoldEn Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GoldEn Retriever on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT.</p>
<p>Keywords:</p>
<h3 id="262. NL2pSQL: Generating Pseudo-SQL Queries from Under-Specified Natural Language Questions.">262. NL2pSQL: Generating Pseudo-SQL Queries from Under-Specified Natural Language Questions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1262">Paper Link</a>    Pages:2603-2613</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/167/7868.html">Fuxiang Chen</a> ; <a href="https://dblp.uni-trier.de/pid/h/SeungwonHwang.html">Seung-won Hwang</a> ; <a href="https://dblp.uni-trier.de/pid/07/2074.html">Jaegul Choo</a> ; <a href="https://dblp.uni-trier.de/pid/66/867.html">Jung-Woo Ha</a> ; <a href="https://dblp.uni-trier.de/pid/k/SunghunKim.html">Sunghun Kim</a></p>
<p>Abstract:
Generating SQL codes from natural language questions (NL2SQL) is an emerging research area. Existing studies have mainly focused on clear scenarios where specified information is fully given to generate a SQL query. However, in developer forums such as Stack Overflow, questions cover more diverse tasks including table manipulation or performance issues, where a table is not specified. The SQL query posted in Stack Overflow, Pseudo-SQL (pSQL), does not usually contain table schemas and is not necessarily executable, is sufficient to guide developers. Here we describe a new NL2pSQL task to generate pSQL codes from natural language questions on under-specified database issues, NL2pSQL. In addition, we define two new metrics suitable for the proposed NL2pSQL task, Canonical-BLEU and SQL-BLEU, instead of the conventional BLEU. With a baseline model using sequence-to-sequence architecture integrated by denoising autoencoder, we confirm the validity of our task. Experiments show that the proposed NL2pSQL approach yields well-formed queries (up to 43% more than a standard Seq2Seq model). Our code and datasets will be publicly released.</p>
<p>Keywords:</p>
<h3 id="263. Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering.">263. Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1263">Paper Link</a>    Pages:2614-2622</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/150/3497.html">Jiwei Ding</a> ; <a href="https://dblp.uni-trier.de/pid/52/173-7.html">Wei Hu</a> ; <a href="https://dblp.uni-trier.de/pid/201/6573.html">Qixin Xu</a> ; <a href="https://dblp.uni-trier.de/pid/05/1694.html">Yuzhong Qu</a></p>
<p>Abstract:
Formal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for complex questions. Also, it achieves promising performance with limited training data and noisy entity/relation linking results.</p>
<p>Keywords:</p>
<h3 id="264. Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning.">264. Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1264">Paper Link</a>    Pages:2623-2631</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/61/5618.html">Heng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/133/1966.html">Shuangyin Li</a> ; <a href="https://dblp.uni-trier.de/pid/94/1243.html">Rong Pan</a> ; <a href="https://dblp.uni-trier.de/pid/64/2306.html">Mingzhi Mao</a></p>
<p>Abstract:
Knowledge Graph (KG) reasoning aims at finding reasoning paths for relations, in order to solve the problem of incompleteness in KG. Many previous path-based methods like PRA and DeepPath suffer from lacking memory components, or stuck in training. Therefore, their performances always rely on well-pretraining. In this paper, we present a deep reinforcement learning based model named by AttnPath, which incorporates LSTM and Graph Attention Mechanism as the memory components. We define two metrics, Mean Selection Rate (MSR) and Mean Replacement Rate (MRR), to quantitatively measure how difficult it is to learn the query relations, and take advantages of them to fine-tune the model under the framework of reinforcement learning. Meanwhile, a novel mechanism of reinforcement learning is proposed by forcing an agent to walk forward every step to avoid the agent stalling at the same entity node constantly. Based on this operation, the proposed model not only can get rid of the pretraining process, but also achieves state-of-the-art performance comparing with the other models. We test our model on FB15K-237 and NELL-995 datasets with different tasks. Extensive experiments show that our model is effective and competitive with many current state-of-the-art methods, and also performs well in practice.</p>
<p>Keywords:</p>
<h3 id="265. Learning to Update Knowledge Graphs by Reading News.">265. Learning to Update Knowledge Graphs by Reading News.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1265">Paper Link</a>    Pages:2632-2641</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/223/1853.html">Jizhi Tang</a> ; <a href="https://dblp.uni-trier.de/pid/25/2643.html">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pid/63/1870.html">Dongyan Zhao</a></p>
<p>Abstract:
News streams contain rich up-to-date information which can be used to update knowledge graphs (KGs). Most current text-based KG updating methods rely on elaborately designed information extraction (IE) systems and carefully crafted rules, which are often domain-specific and hard to maintain. Besides, such methods often hardly pay enough attention to the implicit information that lies underneath texts. In this paper, we propose a novel neural network method, GUpdater, to tackle these problems. GUpdater is built upon graph neural networks (GNNs) with a text-based attention mechanism to guide the updating message passing through the KG structures. Experiments on a real-world KG updating dataset show that our model can effectively broadcast the news information to the KG structures and perform necessary link-adding or link-deleting operations to ensure the KG up-to-date according to news snippets.</p>
<p>Keywords:</p>
<h3 id="266. DIVINE: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning.">266. DIVINE: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1266">Paper Link</a>    Pages:2642-2651</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/05/8842.html">Ruiping Li</a> ; <a href="https://dblp.uni-trier.de/pid/29/1059.html">Xiang Cheng</a></p>
<p>Abstract:
Knowledge graphs (KGs) often suffer from sparseness and incompleteness. Knowledge graph reasoning provides a feasible way to address such problems. Recent studies on knowledge graph reasoning have shown that reinforcement learning (RL) based methods can provide state-of-the-art performance. However, existing RL-based methods require numerous trials for path-finding and rely heavily on meticulous reward engineering to fit specific dataset, which is inefficient and laborious to apply to fast-evolving KGs. To this end, in this paper, we present DIVINE, a novel plug-and-play framework based on generative adversarial imitation learning for enhancing existing RL-based methods. DIVINE guides the path-finding process, and learns reasoning policies and reward functions self-adaptively through imitating the demonstrations automatically sampled from KGs. Experimental results on two benchmark datasets show that our framework improves the performance of existing RL-based methods while eliminating extra reward engineering.</p>
<p>Keywords:</p>
<h3 id="267. Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching.">267. Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1267">Paper Link</a>    Pages:2652-2661</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/224/5544.html">Mingtong Liu</a> ; <a href="https://dblp.uni-trier.de/pid/33/1685.html">Yujie Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/67/3124.html">Jinan Xu</a> ; <a href="https://dblp.uni-trier.de/pid/64/5715.html">Yufeng Chen</a></p>
<p>Abstract:
Sentence matching is a key issue in natural language inference and paraphrase identification. Despite the recent progress on multi-layered neural network with cross sentence attention, one sentence learns attention to the intermediate representations of another sentence, which are propagated from preceding layers and therefore are uncertain and unstable for matching, particularly at the risk of error propagation. In this paper, we present an original semantics-oriented attention and deep fusion network (OSOA-DFN) for sentence matching. Unlike existing models, each attention layer of OSOA-DFN is oriented to the original semantic representation of another sentence, which captures the relevant information from a fixed matching target. The multiple attention layers allow one sentence to repeatedly read the important information of another sentence for better matching. We then additionally design deep fusion to propagate the attention information at each matching layer. At last, we introduce a self-attention mechanism to capture global context to enhance attention-aware representation within each sentence. Experiment results on three sentence matching benchmark datasets SNLI, SciTail and Quora show that OSOA-DFN has the ability to model sentence matching more precisely.</p>
<p>Keywords:</p>
<h3 id="268. Representation Learning with Ordered Relation Paths for Knowledge Graph Completion.">268. Representation Learning with Ordered Relation Paths for Knowledge Graph Completion.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1268">Paper Link</a>    Pages:2662-2671</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/79/4629-2.html">Yao Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/85/3862-1.html">Hongzhi Liu</a> ; <a href="https://dblp.uni-trier.de/pid/01/868.html">Zhonghai Wu</a> ; <a href="https://dblp.uni-trier.de/pid/24/4470.html">Yang Song</a> ; <a href="https://dblp.uni-trier.de/pid/15/4777.html">Tao Zhang</a></p>
<p>Abstract:
Incompleteness is a common problem for existing knowledge graphs (KGs), and the completion of KG which aims to predict links between entities is challenging. Most existing KG completion methods only consider the direct relation between nodes and ignore the relation paths which contain useful information for link prediction. Recently, a few methods take relation paths into consideration but pay less attention to the order of relations in paths which is important for reasoning. In addition, these path-based models always ignore nonlinear contributions of path features for link prediction. To solve these problems, we propose a novel KG completion method named OPTransE. Instead of embedding both entities of a relation into the same latent space as in previous methods, we project the head entity and the tail entity of each relation into different spaces to guarantee the order of relations in the path. Meanwhile, we adopt a pooling strategy to extract nonlinear and complex features of different paths to further improve the performance of link prediction. Experimental results on two benchmark datasets show that the proposed model OPTransE performs better than state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="269. Collaborative Policy Learning for Open Knowledge Graph Reasoning.">269. Collaborative Policy Learning for Open Knowledge Graph Reasoning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1269">Paper Link</a>    Pages:2672-2681</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/45/3990.html">Cong Fu</a> ; <a href="https://dblp.uni-trier.de/pid/22/1512.html">Tong Chen</a> ; <a href="https://dblp.uni-trier.de/pid/14/8543.html">Meng Qu</a> ; <a href="https://dblp.uni-trier.de/pid/194/4234.html">Woojeong Jin</a> ; <a href="https://dblp.uni-trier.de/pid/36/360.html">Xiang Ren</a></p>
<p>Abstract:
In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these models often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoninga task that aims to reason for missing facts over a graph augmented by a background text corpus. A key challenge of the task is to filter out irrelevant facts extracted from corpus, in order to maintain an effective search space during path inference. We propose a novel reinforcement learning framework to train two collaborative agents jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach.</p>
<p>Keywords:</p>
<h3 id="270. Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder.">270. Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1270">Paper Link</a>    Pages:2682-2691</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/33/2225.html">Li Du</a> ; <a href="https://dblp.uni-trier.de/pid/44/10861.html">Xiao Ding</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pid/128/6689.html">Zhongyang Li</a></p>
<p>Abstract:
Understanding event and event-centered commonsense reasoning are crucial for natural language processing (NLP). Given an observed event, it is trivial for human to infer its intents and effects, while this type of If-Then reasoning still remains challenging for NLP systems. To facilitate this, a If-Then commonsense reasoning dataset Atomic is proposed, together with an RNN-based Seq2Seq model to conduct such reasoning. However, two fundamental problems still need to be addressed: first, the intents of an event may be multiple, while the generations of RNN-based Seq2Seq models are always semantically close; second, external knowledge of the event background may be necessary for understanding events and conducting the If-Then reasoning. To address these issues, we propose a novel context-aware variational autoencoder effectively learning event background information to guide the If-Then reasoning. Experimental results show that our approach improves the accuracy and diversity of inferences compared with state-of-the-art baseline methods.</p>
<p>Keywords:</p>
<h3 id="271. Asynchronous Deep Interaction Network for Natural Language Inference.">271. Asynchronous Deep Interaction Network for Natural Language Inference.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1271">Paper Link</a>    Pages:2692-2700</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/165/2145.html">Di Liang</a> ; <a href="https://dblp.uni-trier.de/pid/198/6963.html">Fubao Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/52/323-1.html">Qi Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/05/6735.html">Xuanjing Huang</a></p>
<p>Abstract:
Natural language inference aims to predict whether a premise sentence can infer another hypothesis sentence. Existing methods typically have framed the reasoning problem as a semantic matching task. The both sentences are encoded and interacted symmetrically and in parallel. However, in the process of reasoning, the role of the two sentences is obviously different, and the sentence pairs for NLI are asymmetrical corpora. In this paper, we propose an asynchronous deep interaction network (ADIN) to complete the task. ADIN is a neural network structure stacked with multiple inference sub-layers, and each sub-layer consists of two local inference modules in an asymmetrical manner. Different from previous methods, this model deconstructs the reasoning process and implements the asynchronous and multi-step reasoning. Experiment results show that ADIN achieves competitive performance and outperforms strong baselines on three popular benchmarks: SNLI, MultiNLI, and SciTail.</p>
<p>Keywords:</p>
<h3 id="272. Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange.">272. Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1272">Paper Link</a>    Pages:2701-2711</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7959.html">Steven Y. Feng</a> ; <a href="https://dblp.uni-trier.de/pid/208/6524.html">Aaron W. Li</a> ; <a href="https://dblp.uni-trier.de/pid/35/1222.html">Jesse Hoey</a></p>
<p>Abstract:
In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by chatbots and virtual assistants. We introduce a pipeline called SMERTI that combines entity replacement, similarity masking, and text infilling. We measure our pipelines success by its Semantic Text Exchange Score (STES): the ability to preserve the original texts sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on Yelp reviews, Amazon reviews, and news headlines.</p>
<p>Keywords:</p>
<h3 id="273. Query-focused Scenario Construction.">273. Query-focused Scenario Construction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1273">Paper Link</a>    Pages:2712-2722</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/37/5976.html">Su Wang</a> ; <a href="https://dblp.uni-trier.de/pid/69/7968.html">Greg Durrett</a> ; <a href="https://dblp.uni-trier.de/pid/23/556.html">Katrin Erk</a></p>
<p>Abstract:
The news coverage of events often contains not one but multiple incompatible accounts of what happened. We develop a query-based system that extracts compatible sets of events (scenarios) from such data, formulated as one-class clustering. Our system incrementally evaluates each events compatibility with already selected events, taking order into account. We use synthetic data consisting of article mixtures for scalable training and evaluate our model on a new human-curated dataset of scenarios about real-world news topics. Stronger neural network models and harder synthetic training settings are both important to achieve high performance, and our final scenario construction system substantially outperforms baselines based on prior work.</p>
<p>Keywords:</p>
<h3 id="274. Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and Cross-graph Model.">274. Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and Cross-graph Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1274">Paper Link</a>    Pages:2723-2732</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9704.html">Chengjiang Li</a> ; <a href="https://dblp.uni-trier.de/pid/20/8038-2.html">Yixin Cao</a> ; <a href="https://dblp.uni-trier.de/pid/32/5685-1.html">Lei Hou</a> ; <a href="https://dblp.uni-trier.de/pid/151/7509.html">Jiaxin Shi</a> ; <a href="https://dblp.uni-trier.de/pid/l/JuanZiLi.html">Juanzi Li</a> ; <a href="https://dblp.uni-trier.de/pid/24/6606.html">Tat-Seng Chua</a></p>
<p>Abstract:
Entity alignment aims at integrating complementary knowledge graphs (KGs) from different sources or languages, which may benefit many knowledge-driven applications. It is challenging due to the heterogeneity of KGs and limited seed alignments. In this paper, we propose a semi-supervised entity alignment method by joint Knowledge Embedding model and Cross-Graph model (KECG). It can make better use of seed alignments to propagate over the entire graphs with KG-based constraints. Specifically, as for the knowledge embedding model, we utilize TransE to implicitly complete two KGs towards consistency and learn relational constraints between entities. As for the cross-graph model, we extend Graph Attention Network (GAT) with projection constraint to robustly encode graphs, and two KGs share the same GAT to transfer structural knowledge as well as to ignore unimportant neighbors for alignment via attention mechanism. Results on publicly available datasets as well as further analysis demonstrate the effectiveness of KECG. Our codes can be found in https: //github.com/THU-KEG/KECG.</p>
<p>Keywords:</p>
<h3 id="275. Designing and Interpreting Probes with Control Tasks.">275. Designing and Interpreting Probes with Control Tasks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1275">Paper Link</a>    Pages:2733-2743</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/9025.html">John Hewitt</a> ; <a href="https://dblp.uni-trier.de/pid/04/1701.html">Percy Liang</a></p>
<p>Abstract:
Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probes capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.</p>
<p>Keywords:</p>
<h3 id="276. Specializing Word Embeddings (for Parsing) by Information Bottleneck.">276. Specializing Word Embeddings (for Parsing) by Information Bottleneck.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1276">Paper Link</a>    Pages:2744-2754</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/40/1491-63.html">Xiang Lisa Li</a> ; <a href="https://dblp.uni-trier.de/pid/37/3263.html">Jason Eisner</a></p>
<p>Abstract:
Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional POS tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction.</p>
<p>Keywords:</p>
<h3 id="277. Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing - A Tale of Two Parsers Revisited.">277. Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing - A Tale of Two Parsers Revisited.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1277">Paper Link</a>    Pages:2755-2768</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/9236.html">Artur Kulmizev</a> ; <a href="https://dblp.uni-trier.de/pid/163/1873.html">Miryam de Lhoneux</a> ; <a href="https://dblp.uni-trier.de/pid/204/1052.html">Johannes Gontrum</a> ; <a href="https://dblp.uni-trier.de/pid/245/4263.html">Elena Fano</a> ; <a href="https://dblp.uni-trier.de/pid/n/JoakimNivre.html">Joakim Nivre</a></p>
<p>Abstract:
Transition-based and graph-based dependency parsers have previously been shown to have complementary strengths and weaknesses: transition-based parsers exploit rich structural features but suffer from error propagation, while graph-based parsers benefit from global optimization but have restricted feature scope. In this paper, we show that, even though some details of the picture have changed after the switch to neural networks and continuous representations, the basic trade-off between rich features and global optimization remains essentially the same. Moreover, we show that deep contextualized word embeddings, which allow parsers to pack information about global sentence structure into local feature representations, benefit transition-based parsers more than graph-based parsers, making the two approaches virtually equivalent in terms of both accuracy and error profile. We argue that the reason is that these representations help prevent search errors and thereby allow transition-based parsers to better exploit their inherent strength of making accurate local decisions. We support this explanation by an error analysis of parsing experiments on 13 languages.</p>
<p>Keywords:</p>
<h3 id="278. Semantic graph parsing with recurrent neural network DAG grammars.">278. Semantic graph parsing with recurrent neural network DAG grammars.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1278">Paper Link</a>    Pages:2769-2778</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/148/4578.html">Federico Fancellu</a> ; <a href="https://dblp.uni-trier.de/pid/203/0362.html">Sorcha Gilroy</a> ; <a href="https://dblp.uni-trier.de/pid/65/5274.html">Adam Lopez</a> ; <a href="https://dblp.uni-trier.de/pid/59/6701.html">Mirella Lapata</a></p>
<p>Abstract:
Semantic parses are directed acyclic graphs (DAGs), so semantic parsing should be modeled as graph prediction. But predicting graphs presents difficult technical challenges, so it is simpler and more common to predict the <em>linearized</em> graphs found in semantic parsing datasets using well-understood sequence models. The cost of this simplicity is that the predicted strings may not be well-formed graphs. We present recurrent neural network DAG grammars, a graph-aware sequence model that generates only well-formed graphs while sidestepping many difficulties in graph prediction. We test our model on the Parallel Meaning Banka multilingual semantic graphbank. Our approach yields competitive results in English and establishes the first results for German, Italian and Dutch.</p>
<p>Keywords:</p>
<h3 id="279. 75 Languages, 1 Model: Parsing Universal Dependencies Universally.">279. 75 Languages, 1 Model: Parsing Universal Dependencies Universally.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1279">Paper Link</a>    Pages:2779-2795</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/225/4606.html">Daniel Kondratyuk</a> ; <a href="https://dblp.uni-trier.de/pid/18/1522.html">Milan Straka</a></p>
<p>Abstract:
We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on.</p>
<p>Keywords:</p>
<h3 id="280. Interactive Language Learning by Question Answering.">280. Interactive Language Learning by Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1280">Paper Link</a>    Pages:2796-2813</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/40/10147.html">Xingdi Yuan</a> ; <a href="https://dblp.uni-trier.de/pid/118/9636.html">Marc-Alexandre Ct</a> ; <a href="https://dblp.uni-trier.de/pid/16/7565.html">Jie Fu</a> ; <a href="https://dblp.uni-trier.de/pid/121/7919.html">Zhouhan Lin</a> ; <a href="https://dblp.uni-trier.de/pid/45/1217.html">Chris Pal</a> ; <a href="https://dblp.uni-trier.de/pid/56/953.html">Yoshua Bengio</a> ; <a href="https://dblp.uni-trier.de/pid/177/9137.html">Adam Trischler</a></p>
<p>Abstract:
Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching. We address this problem by formulating a novel text-based question answering task: Question Answering with Interactive Text (QAit). In QAit, an agent must interact with a partially observable text-based environment to gather information required to answer questions. QAit poses questions about the existence, location, and attributes of objects found in the environment. The data is built using a text-based game generator that defines the underlying dynamics of interaction with the environment. We propose and evaluate a set of baseline models for the QAit task that includes deep reinforcement learning agents. Experiments show that the task presents a major challenge for machine reading systems, while humans solve it with relative ease.</p>
<p>Keywords:</p>
<h3 id="281. What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering.">281. What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1281">Paper Link</a>    Pages:2814-2828</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/83/8117.html">Tushar Khot</a> ; <a href="https://dblp.uni-trier.de/pid/13/154.html">Ashish Sabharwal</a> ; <a href="https://dblp.uni-trier.de/pid/34/1184.html">Peter Clark</a></p>
<p>Abstract:
Multi-hop textual question answering requires combining information from multiple sentences. We focus on a natural setting where, unlike typical reading comprehension, only partial information is provided with each question. The model must retrieve and use additional knowledge to correctly answer the question. To tackle this challenge, we develop a novel approach that explicitly identifies the knowledge gap between a key span in the provided knowledge and the answer choices. The model, GapQA, learns to fill this gap by determining the relationship between the span and an answer choice, based on retrieved knowledge targeting this gap. We propose jointly training a model to simultaneously fill this knowledge gap and compose it with the provided partial knowledge. On the OpenBookQA dataset, given partial knowledge, explicitly identifying whats missing substantially outperforms previous approaches.</p>
<p>Keywords:</p>
<h3 id="282. KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning.">282. KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1282">Paper Link</a>    Pages:2829-2839</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/190/4518.html">Bill Yuchen Lin</a> ; <a href="https://dblp.uni-trier.de/pid/124/5261.html">Xinyue Chen</a> ; <a href="https://dblp.uni-trier.de/pid/248/7563.html">Jamin Chen</a> ; <a href="https://dblp.uni-trier.de/pid/36/360.html">Xiang Ren</a></p>
<p>Abstract:
Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.</p>
<p>Keywords:</p>
<h3 id="283. Learning with Limited Data for Multilingual Reading Comprehension.">283. Learning with Limited Data for Multilingual Reading Comprehension.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1283">Paper Link</a>    Pages:2840-2850</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/13/7265.html">Kyungjae Lee</a> ; <a href="https://dblp.uni-trier.de/pid/84/9068-5.html">Sunghyun Park</a> ; <a href="https://dblp.uni-trier.de/pid/254/8084.html">Hojae Han</a> ; <a href="https://dblp.uni-trier.de/pid/121/4335.html">Jinyoung Yeo</a> ; <a href="https://dblp.uni-trier.de/pid/h/SeungwonHwang.html">Seung-won Hwang</a> ; <a href="https://dblp.uni-trier.de/pid/55/3410.html">Juho Lee</a></p>
<p>Abstract:
This paper studies the problem of supporting question answering in a new language with limited training resources. As an extreme scenario, when no such resource exists, one can (1) transfer labels from another language, and (2) generate labels from unlabeled data, using translator and automatic labeling function respectively. However, these approaches inevitably introduce noises to the training data, due to translation or generation errors, which require a judicious use of data with varying confidence. To address this challenge, we propose a weakly-supervised framework that quantifies such noises from automatically generated labels, to deemphasize or fix noisy data in training. On reading comprehension task, we demonstrate the effectiveness of our model on low-resource languages with varying similarity to English, namely, Korean and French.</p>
<p>Keywords:</p>
<h3 id="284. A Discrete Hard EM Approach for Weakly Supervised Question Answering.">284. A Discrete Hard EM Approach for Weakly Supervised Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1284">Paper Link</a>    Pages:2851-2864</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9401.html">Sewon Min</a> ; <a href="https://dblp.uni-trier.de/pid/87/7949.html">Danqi Chen</a> ; <a href="https://dblp.uni-trier.de/pid/52/1296.html">Hannaneh Hajishirzi</a> ; <a href="https://dblp.uni-trier.de/pid/21/6793.html">Luke Zettlemoyer</a></p>
<p>Abstract:
Many question answering (QA) tasks only provide weak supervision for how the answer should be computed. For example, TriviaQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text. In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible solutions (e.g. different mentions or equations) that contains one correct option. We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update. Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 210%, and achieves the state-of-the-art on five of them. Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.</p>
<p>Keywords:</p>
<h3 id="285. Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts.">285. Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1285">Paper Link</a>    Pages:2865-2876</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/182/2260.html">Sandro Pezzelle</a> ; <a href="https://dblp.uni-trier.de/pid/02/5384.html">Raquel Fernndez</a></p>
<p>Abstract:
This work aims at modeling how the meaning of gradable adjectives of size (big, small) can be learned from visually-grounded contexts. Inspired by cognitive and linguistic evidence showing that the use of these expressions relies on setting a threshold that is dependent on a specific context, we investigate the ability of multi-modal models in assessing whether an object is big or small in a given visual scene. In contrast with the standard computational approach that simplistically treats gradable adjectives as fixed attributes, we pose the problem as relational: to be successful, a model has to consider the full visual context. By means of four main tasks, we show that state-of-the-art models (but not a relatively strong baseline) can learn the function subtending the meaning of size adjectives, though their performance is found to decrease while moving from simple to more complex tasks. Crucially, models fail in developing abstract representations of gradable adjectives that can be used compositionally.</p>
<p>Keywords:</p>
<h3 id="286. Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs.">286. Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1286">Paper Link</a>    Pages:2877-2887</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/5281.html">Alex Warstadt</a> ; <a href="https://dblp.uni-trier.de/pid/68/6563.html">Yu Cao</a> ; <a href="https://dblp.uni-trier.de/pid/248/7896.html">Ioana Grosu</a> ; <a href="https://dblp.uni-trier.de/pid/16/5560.html">Wei Peng</a> ; <a href="https://dblp.uni-trier.de/pid/248/7673.html">Hagen Blix</a> ; <a href="https://dblp.uni-trier.de/pid/248/7942.html">Yining Nie</a> ; <a href="https://dblp.uni-trier.de/pid/248/7844.html">Anna Alsop</a> ; <a href="https://dblp.uni-trier.de/pid/238/0553.html">Shikha Bordia</a> ; <a href="https://dblp.uni-trier.de/pid/169/0460.html">Haokun Liu</a> ; <a href="https://dblp.uni-trier.de/pid/248/7544.html">Alicia Parrish</a> ; <a href="https://dblp.uni-trier.de/pid/92/2603.html">Sheng-Fu Wang</a> ; <a href="https://dblp.uni-trier.de/pid/227/3174.html">Jason Phang</a> ; <a href="https://dblp.uni-trier.de/pid/225/6840.html">Anhad Mohananey</a> ; <a href="https://dblp.uni-trier.de/pid/218/5488.html">Phu Mon Htut</a> ; <a href="https://dblp.uni-trier.de/pid/248/7486.html">Paloma Jeretic</a> ; <a href="https://dblp.uni-trier.de/pid/116/0502.html">Samuel R. Bowman</a></p>
<p>Abstract:
Though state-of-the-art sentence representation models can perform tasks requiring significant knowledge of grammar, it is an open question how best to evaluate their grammatical knowledge. We explore five experimental methods inspired by prior work evaluating pretrained sentence representation models. We use a single linguistic phenomenon, negative polarity item (NPI) licensing, as a case study for our experiments. NPIs like any are grammatical only if they appear in a licensing environment like negation (Sue doesnt have any cats vs. *Sue has any cats). This phenomenon is challenging because of the variety of NPI licensing environments that exist. We introduce an artificially generated dataset that manipulates key features of NPI licensing for the experiments. We find that BERT has significant knowledge of these features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a models grammatical knowledge in a given domain.</p>
<p>Keywords:</p>
<h3 id="287. Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study.">287. Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1287">Paper Link</a>    Pages:2888-2899</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8179.html">Aixiu An</a> ; <a href="https://dblp.uni-trier.de/pid/74/8662.html">Peng Qian</a> ; <a href="https://dblp.uni-trier.de/pid/227/3505.html">Ethan Wilcox</a> ; <a href="https://dblp.uni-trier.de/pid/23/90.html">Roger Levy</a></p>
<p>Abstract:
Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.</p>
<p>Keywords:</p>
<h3 id="288. Towards Zero-shot Language Modeling.">288. Towards Zero-shot Language Modeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1288">Paper Link</a>    Pages:2900-2910</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/178/8829.html">Edoardo Maria Ponti</a> ; <a href="https://dblp.uni-trier.de/pid/77/9768.html">Ivan Vulic</a> ; <a href="https://dblp.uni-trier.de/pid/146/4361.html">Ryan Cotterell</a> ; <a href="https://dblp.uni-trier.de/pid/96/5429.html">Roi Reichart</a> ; <a href="https://dblp.uni-trier.de/pid/14/6532.html">Anna Korhonen</a></p>
<p>Abstract:
Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplaces method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the worlds languages, we hope that these insights will broaden the scope of applications for language technology.</p>
<p>Keywords:</p>
<h3 id="289. What Gets Echoed? Understanding the "Pointers" in Explanations of Persuasive Arguments.">289. What Gets Echoed? Understanding the "Pointers" in Explanations of Persuasive Arguments.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1289">Paper Link</a>    Pages:2911-2921</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/45/6663.html">David Atkinson</a> ; <a href="https://dblp.uni-trier.de/pid/251/5449.html">Kumar Bhargav Srinivasan</a> ; <a href="https://dblp.uni-trier.de/pid/95/8314.html">Chenhao Tan</a></p>
<p>Abstract:
Explanations are central to everyday life, and are a topic of growing interest in the AI community. To investigate the process of providing natural language explanations, we leverage the dynamics of the /r/ChangeMyView subreddit to build a dataset with 36K naturally occurring explanations of why an argument is persuasive. We propose a novel word-level prediction task to investigate how explanations selectively reuse, or echo, information from what is being explained (henceforth, explanandum). We develop features to capture the properties of a word in the explanandum, and show that our proposed features not only have relatively strong predictive power on the echoing of a word in an explanation, but also enhance neural methods of generating explanations. In particular, while the non-contextual properties of a word itself are more valuable for stopwords, the interaction between the constituent parts of an explanandum is crucial in predicting the echoing of content words. We also find intriguing patterns of a word being echoed. For example, although nouns are generally less likely to be echoed, subjects and objects can, depending on their source, be more likely to be echoed in the explanations.</p>
<p>Keywords:</p>
<h3 id="290. Modeling Frames in Argumentation.">290. Modeling Frames in Argumentation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1290">Paper Link</a>    Pages:2922-2932</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/1270.html">Yamen Ajjour</a> ; <a href="https://dblp.uni-trier.de/pid/160/8727.html">Milad Alshomary</a> ; <a href="https://dblp.uni-trier.de/pid/73/9281.html">Henning Wachsmuth</a> ; <a href="https://dblp.uni-trier.de/pid/69/4806-1.html">Benno Stein</a></p>
<p>Abstract:
In argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. When talking about legalizing drugs, for instance, its economical aspect may be emphasized. In general, we call a set of arguments that focus on the same aspect a frame. An argumentative text has to serve the right frame(s) to convince the audience to adopt the authors stance (e.g., being pro or con legalizing drugs). More specifically, an author has to choose frames that fit the audiences cultural background and interests. This paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. We present a fully unsupervised approach to this task, which first removes topical information and then identifies frames using clustering. For evaluation purposes, we provide a corpus with 12, 326 debate-portal arguments, organized along the frames of the debates topics. On this corpus, our approach outperforms different strong baselines, achieving an F1-score of 0.28.</p>
<p>Keywords:</p>
<h3 id="291. AMPERSAND: Argument Mining for PERSuAsive oNline Discussions.">291. AMPERSAND: Argument Mining for PERSuAsive oNline Discussions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1291">Paper Link</a>    Pages:2933-2943</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/2812.html">Tuhin Chakrabarty</a> ; <a href="https://dblp.uni-trier.de/pid/184/3749.html">Christopher Hidey</a> ; <a href="https://dblp.uni-trier.de/pid/44/70.html">Smaranda Muresan</a> ; <a href="https://dblp.uni-trier.de/pid/m/KathleenMcKeown.html">Kathy McKeown</a> ; <a href="https://dblp.uni-trier.de/pid/205/9377.html">Alyssa Hwang</a></p>
<p>Abstract:
Argumentation is a type of discourse where speakers try to persuade their audience about the reasonableness of a claim by presenting supportive arguments. Most work in argument mining has focused on modeling arguments in monologues. We propose a computational model for argument mining in online persuasive discussion forums that brings together the micro-level (argument as product) and macro-level (argument as process) models of argumentation. Fundamentally, this approach relies on identifying relations between components of arguments in a discussion thread. Our approach for relation prediction uses contextual information in terms of fine-tuning a pre-trained language model and leveraging discourse relations based on Rhetorical Structure Theory. We additionally propose a candidate selection method to automatically predict what parts of ones argument will be targeted by other participants in the discussion. Our models obtain significant improvements compared to recent state-of-the-art approaches using pointer networks and a pre-trained language model.</p>
<p>Keywords:</p>
<h3 id="292. Evaluating adversarial attacks against multiple fact verification systems.">292. Evaluating adversarial attacks against multiple fact verification systems.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1292">Paper Link</a>    Pages:2944-2953</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/1380.html">James Thorne</a> ; <a href="https://dblp.uni-trier.de/pid/18/1071.html">Andreas Vlachos</a> ; <a href="https://dblp.uni-trier.de/pid/76/1581.html">Christos Christodoulopoulos</a> ; <a href="https://dblp.uni-trier.de/pid/20/6147.html">Arpit Mittal</a></p>
<p>Abstract:
Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. Due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. We introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge: the four best-scoring ones and two baselines. We evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. We find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.</p>
<p>Keywords:</p>
<h3 id="293. Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials.">293. Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1293">Paper Link</a>    Pages:2954-2963</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/2973.html">Wonsuk Yang</a> ; <a href="https://dblp.uni-trier.de/pid/254/8016.html">Seungwon Yoon</a> ; <a href="https://dblp.uni-trier.de/pid/254/8124.html">Ada Carpenter</a> ; <a href="https://dblp.uni-trier.de/pid/62/432.html">Jong Park</a></p>
<p>Abstract:
Annotation quality control is a critical aspect for building reliable corpora through linguistic annotation. In this study, we present a simple but powerful quality control method using two-step reason selection. We gathered sentential annotations of local acceptance and three related attributes through a crowdsourcing platform. For each attribute, the reason for the choice of the attribute value is selected in a two-step manner. The options given for reason selection were designed to facilitate the detection of a nonsensical reason selection. We assume that a sentential annotation that contains a nonsensical reason is less reliable than the one without such reason. Our method, based solely on this assumption, is found to retain the annotations with satisfactory quality out of the entire annotations mixed with those of low quality.</p>
<p>Keywords:</p>
<h3 id="294. Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite.">294. Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1294">Paper Link</a>    Pages:2964-2973</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/214/9948.html">Prathyusha Jwalapuram</a> ; <a href="https://dblp.uni-trier.de/pid/62/2078.html">Shafiq R. Joty</a> ; <a href="https://dblp.uni-trier.de/pid/09/4621.html">Irina P. Temnikova</a> ; <a href="https://dblp.uni-trier.de/pid/19/1947.html">Preslav Nakov</a></p>
<p>Abstract:
The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a user study to report correlations with human judgments.</p>
<p>Keywords:</p>
<h3 id="295. A Regularization Approach for Incorporating Event Knowledge and Coreference Relations into Neural Discourse Parsing.">295. A Regularization Approach for Incorporating Event Knowledge and Coreference Relations into Neural Discourse Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1295">Paper Link</a>    Pages:2974-2985</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9131.html">Zeyu Dai</a> ; <a href="https://dblp.uni-trier.de/pid/42/4811.html">Ruihong Huang</a></p>
<p>Abstract:
We argue that external commonsense knowledge and linguistic constraints need to be incorporated into neural network models for mitigating data sparsity issues and further improving the performance of discourse parsing. Realizing that external knowledge and linguistic constraints may not always apply in understanding a particular context, we propose a regularization approach that tightly integrates these constraints with contexts for deriving word representations. Meanwhile, it balances attentions over contexts and constraints through adding a regularization term into the objective function. Experiments show that our knowledge regularization approach outperforms all previous systems on the benchmark dataset PDTB for discourse parsing.</p>
<p>Keywords:</p>
<h3 id="296. Weakly Supervised Multilingual Causality Extraction from Wikipedia.">296. Weakly Supervised Multilingual Causality Extraction from Wikipedia.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1296">Paper Link</a>    Pages:2986-2997</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/01/2636.html">Chikara Hashimoto</a></p>
<p>Abstract:
We present a method for extracting causality knowledge from Wikipedia, such as Protectionism -&gt; Trade war, where the cause and effect entities correspond to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding Wikipedia articles, to translate to multiple languages through Wikidata, and to connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia article sections that describe causality and the redundancy stemming from the multilinguality of Wikipedia. Experiments showed that our method achieved precision and recall above 98% and 64%, respectively. In particular, it could extract causalities whose cause and effect were written distantly in a Wikipedia article. We have released the code and data for further research.</p>
<p>Keywords:</p>
<h3 id="297. Attribute-aware Sequence Network for Review Summarization.">297. Attribute-aware Sequence Network for Review Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1297">Paper Link</a>    Pages:2998-3008</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/83/5144.html">Junjie Li</a> ; <a href="https://dblp.uni-trier.de/pid/85/8260.html">Xuepeng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/91/4572.html">Dawei Yin</a> ; <a href="https://dblp.uni-trier.de/pid/38/6093.html">Chengqing Zong</a></p>
<p>Abstract:
Review summarization aims to generate a condensed summary for a review or multiple reviews. Existing review summarization systems mainly generate summary only based on review content and neglect the authors attributes (e.g., gender, age, and occupation). In fact, when summarizing a review, users with different attributes usually pay attention to specific aspects and have their own word-using habits or writing styles. Therefore, we propose an Attribute-aware Sequence Network (ASN) to take the aforementioned users characteristics into account, which includes three modules: an attribute encoder encodes the attribute preferences over the words; an attribute-aware review encoder adopts an attribute-based selective mechanism to select the important information of a review; and an attribute-aware summary decoder incorporates attribute embedding and attribute-specific word-using habits into word prediction. To validate our model, we collect a new dataset TripAtt, comprising 495,440 attribute-review-summary triplets with three kinds of attribute information: gender, age, and travel status. Extensive experiments show that ASN achieves state-of-the-art performance on review summarization in both auto-metric ROUGE and human evaluation.</p>
<p>Keywords:</p>
<h3 id="298. Extractive Summarization of Long Documents by Combining Global and Local Context.">298. Extractive Summarization of Long Documents by Combining Global and Local Context.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1298">Paper Link</a>    Pages:3009-3019</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/71/2208.html">Wen Xiao</a> ; <a href="https://dblp.uni-trier.de/pid/30/3994.html">Giuseppe Carenini</a></p>
<p>Abstract:
In this paper, we propose a novel neural single-document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers , Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our goal, the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly, an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.</p>
<p>Keywords:</p>
<h3 id="299. Enhancing Neural Data-To-Text Generation Models with External Background Knowledge.">299. Enhancing Neural Data-To-Text Generation Models with External Background Knowledge.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1299">Paper Link</a>    Pages:3020-3030</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/07/6433-3.html">Shuang Chen</a> ; <a href="https://dblp.uni-trier.de/pid/74/113.html">Jinpeng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/173/5091.html">Xiaocheng Feng</a> ; <a href="https://dblp.uni-trier.de/pid/75/1693.html">Feng Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/86/5934.html">Bing Qin</a> ; <a href="https://dblp.uni-trier.de/pid/64/6843.html">Chin-Yew Lin</a></p>
<p>Abstract:
Recent neural models for data-to-text generation rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that writing knowledge can be acquired from the training data alone. However, when people are writing, they not only rely on the data but also consider related knowledge. In this paper, we enhance neural data-to-text models with external knowledge in a simple but effective way to improve the fidelity of generated text. Besides relying on parallel data and text as in previous work, our model attends to relevant external knowledge, encoded as a temporary memory, and combines this knowledge with the context representation of data before generating words. This allows the model to infer relevant facts which are not explicitly stated in the data table from an external knowledge source. Experimental results on twenty-one Wikipedia infobox-to-text datasets show our model, KBAtt, consistently improves a state-of-the-art model on most of the datasets. In addition, to quantify when and why external knowledge is effective, we design a metric, KBGain, which shows a strong correlation with the observed performance boost. This result demonstrates the relevance of external knowledge and sparseness of original data are the main factors affecting system performance.</p>
<p>Keywords:</p>
<h3 id="300. Reading Like HER: Human Reading Inspired Extractive Summarization.">300. Reading Like HER: Human Reading Inspired Extractive Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1300">Paper Link</a>    Pages:3031-3041</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/00/1811.html">Ling Luo</a> ; <a href="https://dblp.uni-trier.de/pid/71/1982.html">Xiang Ao</a> ; <a href="https://dblp.uni-trier.de/pid/09/1398.html">Yan Song</a> ; <a href="https://dblp.uni-trier.de/pid/215/3837.html">Feiyang Pan</a> ; <a href="https://dblp.uni-trier.de/pid/02/1640.html">Min Yang</a> ; <a href="https://dblp.uni-trier.de/pid/14/3700-3.html">Qing He</a></p>
<p>Abstract:
In this work, we re-examine the problem of extractive text summarization for long documents. We observe that the process of extracting summarization of human can be divided into two stages: 1) a rough reading stage to look for sketched information, and 2) a subsequent careful reading stage to select key sentences to form the summary. By simulating such a two-stage process, we propose a novel approach for extractive summarization. We formulate the problem as a contextual-bandit problem and solve it with policy gradient. We adopt a convolutional neural network to encode gist of paragraphs for rough reading, and a decision making policy with an adapted termination mechanism for careful reading. Experiments on the CNN and DailyMail datasets show that our proposed method can provide high-quality summaries with varied length, and significantly outperform the state-of-the-art extractive methods in terms of ROUGE metrics.</p>
<p>Keywords:</p>
<h3 id="301. Contrastive Attention Mechanism for Abstractive Sentence Summarization.">301. Contrastive Attention Mechanism for Abstractive Sentence Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1301">Paper Link</a>    Pages:3042-3051</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/09/5412.html">Xiangyu Duan</a> ; <a href="https://dblp.uni-trier.de/pid/79/7950.html">Hongfei Yu</a> ; <a href="https://dblp.uni-trier.de/pid/245/8643.html">Mingming Yin</a> ; <a href="https://dblp.uni-trier.de/pid/83/5342-5.html">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/22/1116.html">Weihua Luo</a> ; <a href="https://dblp.uni-trier.de/pid/47/722-4.html">Yue Zhang</a></p>
<p>Abstract:
We propose a contrastive attention mechanism to extend the sequence-to-sequence framework for abstractive sentence summarization task, which aims to generate a brief summary of a given source sentence. The proposed contrastive attention mechanism accommodates two categories of attention: one is the conventional attention that attends to relevant parts of the source sentence, the other is the opponent attention that attends to irrelevant or less relevant parts of the source sentence. Both attentions are trained in an opposite way so that the contribution from the conventional attention is encouraged and the contribution from the opponent attention is discouraged through a novel softmax and softmin functionality. Experiments on benchmark datasets show that, the proposed contrastive attention mechanism is more focused on the relevant parts for the summary than the conventional attention mechanism, and greatly advances the state-of-the-art performance on the abstractive sentence summarization task. We release the code at <a href="https://github.com/travel-go/">https://github.com/travel-go/</a> Abstractive-Text-Summarization.</p>
<p>Keywords:</p>
<h3 id="302. NCLS: Neural Cross-Lingual Summarization.">302. NCLS: Neural Cross-Lingual Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1302">Paper Link</a>    Pages:3052-3062</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/8977.html">Junnan Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/75/5723.html">Qian Wang</a> ; <a href="https://dblp.uni-trier.de/pid/04/7235.html">Yining Wang</a> ; <a href="https://dblp.uni-trier.de/pid/36/2728-1.html">Yu Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/71/6950.html">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/29/8236.html">Shaonan Wang</a> ; <a href="https://dblp.uni-trier.de/pid/38/6093.html">Chengqing Zong</a></p>
<p>Abstract:
Cross-lingual summarization (CLS) is the task to produce a summary in one particular language for a source document in a different language. Existing methods simply divide this task into two steps: summarization and translation, leading to the problem of error propagation. To handle that, we present an end-to-end CLS framework, which we refer to as Neural Cross-Lingual Summarization (NCLS), for the first time. Moreover, we propose to further improve NCLS by incorporating two related tasks, monolingual summarization and machine translation, into the training process of CLS under multi-task learning. Due to the lack of supervised CLS data, we propose a round-trip translation strategy to acquire two high-quality large-scale CLS datasets based on existing monolingual summarization datasets. Experimental results have shown that our NCLS achieves remarkable improvement over traditional pipeline methods on both English-to-Chinese and Chinese-to-English CLS human-corrected test sets. In addition, NCLS with multi-task learning can further significantly improve the quality of generated summaries. We make our dataset and code publicly available here: <a href="http://www.nlpr.ia.ac.cn/cip/dataset.htm">http://www.nlpr.ia.ac.cn/cip/dataset.htm</a>.</p>
<p>Keywords:</p>
<h3 id="303. Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning.">303. Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1303">Paper Link</a>    Pages:3063-3073</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/84/586.html">Peng Xu</a> ; <a href="https://dblp.uni-trier.de/pid/180/5537.html">Chien-Sheng Wu</a> ; <a href="https://dblp.uni-trier.de/pid/174/2905.html">Andrea Madotto</a> ; <a href="https://dblp.uni-trier.de/pid/29/4187.html">Pascale Fung</a></p>
<p>Abstract:
Sensational headlines are headlines that capture peoples attention and generate reader interest. Conventional abstractive headline generation methods, unlike human writers, do not optimize for maximal reader attention. In this paper, we propose a model that generates sensational headlines without labeled data. We first train a sensationalism scorer by classifying online headlines with many comments (clickbait) against a baseline of headlines generated from a summarization model. The score from the sensationalism scorer is used as the reward for a reinforcement learner. However, maximizing the noisy sensationalism reward will generate unnatural phrases instead of sensational headlines. To effectively leverage this noisy reward, we propose a novel loss function, Auto-tuned Reinforcement Learning (ARL), to dynamically balance reinforcement learning (RL) with maximum likelihood estimation (MLE). Human evaluation shows that 60.8% of samples generated by our model are sensational, which is significantly better than the Pointer-Gen baseline and other RL models.</p>
<p>Keywords:</p>
<h3 id="304. Concept Pointer Network for Abstractive Summarization.">304. Concept Pointer Network for Abstractive Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1304">Paper Link</a>    Pages:3074-3083</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/132/5158.html">Wenbo Wang</a> ; <a href="https://dblp.uni-trier.de/pid/89/4402-16.html">Yang Gao</a> ; <a href="https://dblp.uni-trier.de/pid/27/8686.html">Heyan Huang</a> ; <a href="https://dblp.uni-trier.de/pid/27/10149.html">Yuxiang Zhou</a></p>
<p>Abstract:
A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details. Inspired by the popular pointer generator sequence-to-sequence model, this paper presents a concept pointer network for improving these aspects of abstractive summarization. The network leverages knowledge-based, context-aware conceptualizations to derive an extended set of candidate concepts. The model then points to the most appropriate choice using both the concept set and original source text. This joint approach generates abstractive summaries with higher-level semantic concepts. The training model is also optimized in a way that adapts to different data, which is based on a novel method of distant-supervised learning guided by reference summaries and testing set. Overall, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the DUC-2004 and Gigaword datasets. A human evaluation of the models abstractive abilities also supports the quality of the summaries produced within this framework.</p>
<p>Keywords:</p>
<h3 id="305. Surface Realisation Using Full Delexicalisation.">305. Surface Realisation Using Full Delexicalisation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1305">Paper Link</a>    Pages:3084-3094</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/19/10057.html">Anastasia Shimorina</a> ; <a href="https://dblp.uni-trier.de/pid/71/6819.html">Claire Gardent</a></p>
<p>Abstract:
Surface realisation (SR) maps a meaning representation to a sentence and can be viewed as consisting of three subtasks: word ordering, morphological inflection and contraction generation (e.g., clitic attachment in Portuguese or elision in French). We propose a modular approach to surface realisation which models each of these components separately, and evaluate our approach on the 10 languages covered by the SR18 Surface Realisation Shared Task shallow track. We provide a detailed evaluation of how word order, morphological realisation and contractions are handled by the model and an analysis of the differences in word ordering performance across languages.</p>
<p>Keywords:</p>
<h3 id="306. IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation.">306. IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1306">Paper Link</a>    Pages:3095-3107</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/229/4267.html">Zhijing Jin</a> ; <a href="https://dblp.uni-trier.de/pid/67/1861.html">Di Jin</a> ; <a href="https://dblp.uni-trier.de/pid/178/3250.html">Jonas Mueller</a> ; <a href="https://dblp.uni-trier.de/pid/234/9140.html">Nicholas Matthews</a> ; <a href="https://dblp.uni-trier.de/pid/148/4556.html">Enrico Santus</a></p>
<p>Abstract:
Text attribute transfer aims to automatically rewrite sentences such that they possess certain linguistic attributes, while simultaneously preserving their semantic content. This task remains challenging due to a lack of supervised parallel data. Existing approaches try to explicitly disentangle content and attribute information, but this is difficult and often results in poor content-preservation and ungrammaticality. In contrast, we propose a simpler approach, Iterative Matching and Translation (IMaT), which: (1) constructs a pseudo-parallel corpus by aligning a subset of semantically similar sentences from the source and the target corpora; (2) applies a standard sequence-to-sequence model to learn the attribute transfer; (3) iteratively improves the learned transfer function by refining imperfections in the alignment. In sentiment modification and formality transfer tasks, our method outperforms complex state-of-the-art systems by a large margin. As an auxiliary contribution, we produce a publicly-available test set with human-generated transfer references.</p>
<p>Keywords:</p>
<h3 id="307. Better Rewards Yield Better Summaries: Learning to Summarise Without References.">307. Better Rewards Yield Better Summaries: Learning to Summarise Without References.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1307">Paper Link</a>    Pages:3108-3118</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7586.html">Florian Bhm</a> ; <a href="https://dblp.uni-trier.de/pid/89/4402-21.html">Yang Gao</a> ; <a href="https://dblp.uni-trier.de/pid/18/7930.html">Christian M. Meyer</a> ; <a href="https://dblp.uni-trier.de/pid/205/9013.html">Ori Shapira</a> ; <a href="https://dblp.uni-trier.de/pid/95/284.html">Ido Dagan</a> ; <a href="https://dblp.uni-trier.de/pid/85/6201.html">Iryna Gurevych</a></p>
<p>Abstract:
Reinforcement Learning (RL)based document summarisation systems yield state-of-the-art performance in terms of ROUGE scores, because they directly use ROUGE as the rewards during training. However, summaries with high ROUGE scores often receive low human judgement. To find a better reward function that can guide RL to generate human-appealing summaries, we learn a reward function from human ratings on 2,500 summaries. Our reward function only takes the document and system summary as input. Hence, once trained, it can be used to train RL based summarisation systems without using any reference summaries. We show that our learned rewards have significantly higher correlation with human ratings than previous approaches. Human evaluation experiments show that, compared to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at <a href="https://github.com/yg211/summary-reward-no-reference">https://github.com/yg211/summary-reward-no-reference</a>.</p>
<p>Keywords:</p>
<h3 id="308. Mixture Content Selection for Diverse Sequence Generation.">308. Mixture Content Selection for Diverse Sequence Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1308">Paper Link</a>    Pages:3119-3129</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/130/8348.html">Jaemin Cho</a> ; <a href="https://dblp.uni-trier.de/pid/149/1367.html">Min Joon Seo</a> ; <a href="https://dblp.uni-trier.de/pid/52/1296.html">Hannaneh Hajishirzi</a></p>
<p>Abstract:
Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at <a href="https://github.com/clovaai/FocusSeq2Seq">https://github.com/clovaai/FocusSeq2Seq</a>.</p>
<p>Keywords:</p>
<h3 id="309. An End-to-End Generative Architecture for Paraphrase Generation.">309. An End-to-End Generative Architecture for Paraphrase Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1309">Paper Link</a>    Pages:3130-3140</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/15/3199.html">Qian Yang</a> ; <a href="https://dblp.uni-trier.de/pid/178/8562.html">Zhouyuan Huo</a> ; <a href="https://dblp.uni-trier.de/pid/202/2287.html">Dinghan Shen</a> ; <a href="https://dblp.uni-trier.de/pid/34/6276.html">Yong Cheng</a> ; <a href="https://dblp.uni-trier.de/pid/42/6170.html">Wenlin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/05/3838-2.html">Guoyin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/23/2168.html">Lawrence Carin</a></p>
<p>Abstract:
Generating high-quality paraphrases is a fundamental yet challenging natural language processing task. Despite the effectiveness of previous work based on generative models, there remain problems with exposure bias in recurrent neural networks, and often a failure to generate realistic sentences. To overcome these challenges, we propose the first end-to-end conditional generative architecture for generating paraphrases via adversarial training, which does not depend on extra linguistic information. Extensive experiments on four public datasets demonstrate the proposed method achieves state-of-the-art results, outperforming previous generative architectures on both automatic metrics (BLEU, METEOR, and TER) and human evaluations.</p>
<p>Keywords:</p>
<h3 id="310. Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time).">310. Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time).</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1310">Paper Link</a>    Pages:3141-3150</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/14/4180.html">Heng Gong</a> ; <a href="https://dblp.uni-trier.de/pid/173/5091.html">Xiaocheng Feng</a> ; <a href="https://dblp.uni-trier.de/pid/86/5934.html">Bing Qin</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a></p>
<p>Abstract:
Although Seq2Seq models for table-to-text generation have achieved remarkable progress, modeling table representation in one dimension is inadequate. This is because (1) the table consists of multiple rows and columns, which means that encoding a table should not depend only on one dimensional sequence or set of records and (2) most of the tables are time series data (e.g. NBA game data, stock market data), which means that the description of the current table may be affected by its historical data. To address aforementioned problems, not only do we model each table cell considering other records in the same row, we also enrich tables representation by modeling each table cell in context of other cells in the same column or with historical (time dimension) data respectively. In addition, we develop a table cell fusion gate to combine representations from row, column and time dimension into one dense vector according to the saliency of each dimensions representation. We evaluated our methods on ROTOWIRE, a benchmark dataset of NBA basketball games. Both automatic and human evaluation results demonstrate the effectiveness of our model with improvement of 2.66 in BLEU over the strong baseline and outperformance of state-of-the-art model.</p>
<p>Keywords:</p>
<h3 id="311. Subtopic-driven Multi-Document Summarization.">311. Subtopic-driven Multi-Document Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1311">Paper Link</a>    Pages:3151-3160</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/13/6922.html">Xin Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/78/5155.html">Aixin Sun</a> ; <a href="https://dblp.uni-trier.de/pid/181/2820.html">Jing Li</a> ; <a href="https://dblp.uni-trier.de/pid/07/7314.html">Karthik Muthuswamy</a></p>
<p>Abstract:
In multi-document summarization, a set of documents to be summarized is assumed to be on the same topic, known as the underlying topic in this paper. That is, the underlying topic can be collectively represented by all the documents in the set. Meanwhile, different documents may cover various different subtopics and the same subtopic can be across several documents. Inspired by topic model, the underlying topic of a document set can also be viewed as a collection of different subtopics of different importance. In this paper, we propose a summarization model called STDS. The model generates the underlying topic representation from both document view and subtopic view in parallel. The learning objective is to minimize the distance between the representations learned from the two views. The contextual information is encoded through a hierarchical RNN architecture. Sentence salience is estimated in a hierarchical way with subtopic salience and relative sentence salience, by considering the contextual information. Top ranked sentences are then extracted as a summary. Note that the notion of subtopic enables us to bring in additional information (e.g. comments to news articles) that is helpful for document summarization. Experimental results show that the proposed solution outperforms state-of-the-art methods on benchmark datasets.</p>
<p>Keywords:</p>
<h3 id="312. Referring Expression Generation Using Entity Profiles.">312. Referring Expression Generation Using Entity Profiles.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1312">Paper Link</a>    Pages:3161-3170</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/67/833.html">Meng Cao</a> ; <a href="https://dblp.uni-trier.de/pid/00/9012.html">Jackie Chi Kit Cheung</a></p>
<p>Abstract:
Referring Expression Generation (REG) is the task of generating contextually appropriate references to entities. A limitation of existing REG systems is that they rely on entity-specific supervised training, which means that they cannot handle entities not seen during training. In this study, we address this in two ways. First, we propose task setups in which we specifically test a REG systems ability to generalize to entities not seen during training. Second, we propose a profile-based deep neural network model, ProfileREG, which encodes both the local context and an external profile of the entity to generate reference realizations. Our model generates tokens by learning to choose between generating pronouns, generating from a fixed vocabulary, or copying a word from the profile. We evaluate our model on three different splits of the WebNLG dataset, and show that it outperforms competitive baselines in all settings according to automatic and human evaluations.</p>
<p>Keywords:</p>
<h3 id="313. Exploring Diverse Expressions for Paraphrase Generation.">313. Exploring Diverse Expressions for Paraphrase Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1313">Paper Link</a>    Pages:3171-3180</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/167/5564.html">Lihua Qian</a> ; <a href="https://dblp.uni-trier.de/pid/82/4490.html">Lin Qiu</a> ; <a href="https://dblp.uni-trier.de/pid/28/10261-1.html">Weinan Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/42/4142.html">Xin Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/43/5685-1.html">Yong Yu</a></p>
<p>Abstract:
Paraphrasing plays an important role in various natural language processing (NLP) tasks, such as question answering, information retrieval and sentence simplification. Recently, neural generative models have shown promising results in paraphrase generation. However, prior work mainly focused on single paraphrase generation, while ignoring the fact that diversity is essential for enhancing generalization capability and robustness of downstream applications. Few works have been done to solve diverse paraphrase generation. In this paper, we propose a novel approach with two discriminators and multiple generators to generate a variety of different paraphrases. A reinforcement learning algorithm is applied to train our model. Our experiments on two real-world datasets demonstrate that our model not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines.</p>
<p>Keywords:</p>
<h3 id="314. Enhancing AMR-to-Text Generation with Dual Graph Representations.">314. Enhancing AMR-to-Text Generation with Dual Graph Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1314">Paper Link</a>    Pages:3181-3192</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/245/8769.html">Leonardo F. R. Ribeiro</a> ; <a href="https://dblp.uni-trier.de/pid/71/6819.html">Claire Gardent</a> ; <a href="https://dblp.uni-trier.de/pid/85/6201.html">Iryna Gurevych</a></p>
<p>Abstract:
Generating text from graph-based data, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets</p>
<p>Keywords:</p>
<h3 id="315. Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning.">315. Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1315">Paper Link</a>    Pages:3193-3203</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/63/5017.html">Toru Nishino</a> ; <a href="https://dblp.uni-trier.de/pid/205/9008.html">Shotaro Misawa</a> ; <a href="https://dblp.uni-trier.de/pid/208/5902.html">Ryuji Kano</a> ; <a href="https://dblp.uni-trier.de/pid/203/9759.html">Tomoki Taniguchi</a> ; <a href="https://dblp.uni-trier.de/pid/161/8787.html">Yasuhide Miura</a> ; <a href="https://dblp.uni-trier.de/pid/161/8818.html">Tomoko Ohkuma</a></p>
<p>Abstract:
The automated generation of information indicating the characteristics of articles such as headlines, key phrases, summaries and categories helps writers to alleviate their workload. Previous research has tackled these tasks using neural abstractive summarization and classification methods. However, the outputs may be inconsistent if they are generated individually. The purpose of our study is to generate multiple outputs consistently. We introduce a multi-task learning model with a shared encoder and multiple decoders for each task. We propose a novel loss function called hierarchical consistency loss to maintain consistency among the attention weights of the decoders. To evaluate the consistency, we employ a human evaluation. The results show that our model generates more consistent headlines, key phrases and categories. In addition, our model outperforms the baseline model on the ROUGE scores, and generates more adequate and fluent headlines.</p>
<p>Keywords:</p>
<h3 id="316. Toward a Task of Feedback Comment Generation for Writing Learning.">316. Toward a Task of Feedback Comment Generation for Writing Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1316">Paper Link</a>    Pages:3204-3213</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/03/2470.html">Ryo Nagata</a></p>
<p>Abstract:
In this paper, we introduce a novel task called feedback comment generation  a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning for non-native learners of English. There has been almost no work on this task nor corpus annotated with feedback comments. We have taken the first step by creating learner corpora consisting of approximately 1,900 essays where all preposition errors are manually annotated with feedback comments. We have tested three baseline methods on the dataset, showing that a simple neural retrieval-based method sets a baseline performance with an F-measure of 0.34 to 0.41. Finally, we have looked into the results to explore what modifications we need to make to achieve better performance. We also have explored problems unaddressed in this work</p>
<p>Keywords:</p>
<h3 id="317. Improving Question Generation With to the Point Context.">317. Improving Question Generation With to the Point Context.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1317">Paper Link</a>    Pages:3214-3224</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/65/4699.html">Jingjing Li</a> ; <a href="https://dblp.uni-trier.de/pid/79/3190-1.html">Yifan Gao</a> ; <a href="https://dblp.uni-trier.de/pid/53/6625.html">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pid/k/IrwinKing.html">Irwin King</a> ; <a href="https://dblp.uni-trier.de/pid/l/MichaelRLyu.html">Michael R. Lyu</a></p>
<p>Abstract:
Question generation (QG) is the task of generating a question from a reference sentence and a specified answer within the sentence. A major challenge in QG is to identify answer-relevant context words to finish the declarative-to-interrogative sentence transformation. Existing sequence-to-sequence neural models achieve this goal by proximity-based answer position encoding under the intuition that neighboring words of answers are of high possibility to be answer-relevant. However, such intuition may not apply to all cases especially for sentences with complex answer-relevant relations. Consequently, the performance of these models drops sharply when the relative distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question increases. To address this issue, we propose a method to jointly model the unstructured sentence and the structured answer-relevant relation (extracted from the sentence in advance) for question generation. Specifically, the structured answer-relevant relation acts as the to the point context and it thus naturally helps keep the generated question to the point, while the unstructured sentence provides the full information. Extensive experiments show that to the point context helps our question generation model achieve significant improvements on several automatic evaluation metrics. Furthermore, our model is capable of generating diverse questions for a sentence which conveys multiple relations of its answer fragment.</p>
<p>Keywords:</p>
<h3 id="318. Deep Copycat Networks for Text-to-Text Generation.">318. Deep Copycat Networks for Text-to-Text Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1318">Paper Link</a>    Pages:3225-3234</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/185/5565.html">Julia Ive</a> ; <a href="https://dblp.uni-trier.de/pid/151/8501.html">Pranava Madhyastha</a> ; <a href="https://dblp.uni-trier.de/pid/23/2900.html">Lucia Specia</a></p>
<p>Abstract:
Most text-to-text generation tasks, for example text summarisation and text simplification, require copying words from the input to the output. We introduce Copycat, a transformer-based pointer network for such tasks which obtains competitive results in abstractive text summarisation and generates more abstractive summaries. We propose a further extension of this architecture for automatic post-editing, where generation is conditioned over two inputs (source language and machine translation), and the model is capable of deciding where to copy information from. This approach achieves competitive performance when compared to state-of-the-art automated post-editing systems. More importantly, we show that it addresses a well-known limitation of automatic post-editing - overcorrecting translations - and that our novel mechanism for copying source language words improves the results.</p>
<p>Keywords:</p>
<h3 id="319. Towards Controllable and Personalized Review Generation.">319. Towards Controllable and Personalized Review Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1319">Paper Link</a>    Pages:3235-3243</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/72/2643-8.html">Pan Li</a> ; <a href="https://dblp.uni-trier.de/pid/t/AlexanderTuzhilin.html">Alexander Tuzhilin</a></p>
<p>Abstract:
In this paper, we propose a novel model RevGAN that automatically generates controllable and personalized user reviews based on the arbitrarily given sentimental and stylistic information. RevGAN utilizes the combination of three novel components, including self-attentive recursive autoencoders, conditional discriminators, and personalized decoders. We test its performance on the several real-world datasets, where our model significantly outperforms state-of-the-art generation models in terms of sentence quality, coherence, personalization, and human evaluations. We also empirically show that the generated reviews could not be easily distinguished from the organically produced reviews and that they follow the same statistical linguistics laws.</p>
<p>Keywords:</p>
<h3 id="320. Answers Unite! Unsupervised Metrics for Reinforced Summarization Models.">320. Answers Unite! Unsupervised Metrics for Reinforced Summarization Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1320">Paper Link</a>    Pages:3244-3254</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/237/8957.html">Thomas Scialom</a> ; <a href="https://dblp.uni-trier.de/pid/28/4095.html">Sylvain Lamprier</a> ; <a href="https://dblp.uni-trier.de/pid/15/2146.html">Benjamin Piwowarski</a> ; <a href="https://dblp.uni-trier.de/pid/94/2681.html">Jacopo Staiano</a></p>
<p>Abstract:
Abstractive summarization approaches based on Reinforcement Learning (RL) have recently been proposed to overcome classical likelihood maximization. RL enables to consider complex, possibly non differentiable, metrics that globally assess the quality and relevance of the generated outputs. ROUGE, the most used summarization metric, is known to suffer from bias towards lexical similarity as well as from sub-optimal accounting for fluency and readability of the generated abstracts. We thus explore and propose alternative evaluation measures: the reported human-evaluation analysis shows that the proposed metrics, based on Question Answering, favorably compare to ROUGE  with the additional property of not requiring reference summaries. Training a RL-based model on these metrics leads to improvements (both in terms of human or automated metrics) over current approaches that use ROUGE as reward.</p>
<p>Keywords:</p>
<h3 id="321. Long and Diverse Text Generation with Planning-based Hierarchical Variational Model.">321. Long and Diverse Text Generation with Planning-based Hierarchical Variational Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1321">Paper Link</a>    Pages:3255-3266</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/247/5748.html">Zhihong Shao</a> ; <a href="https://dblp.uni-trier.de/pid/47/6668.html">Minlie Huang</a> ; <a href="https://dblp.uni-trier.de/pid/67/5635.html">Jiangtao Wen</a> ; <a href="https://dblp.uni-trier.de/pid/213/7425.html">Wenfei Xu</a> ; <a href="https://dblp.uni-trier.de/pid/50/1222-1.html">Xiaoyan Zhu</a></p>
<p>Abstract:
Existing neural methods for data-to-text generation are still struggling to produce long and diverse texts: they are insufficient to model input data dynamically during generation, to capture inter-sentence coherence, or to generate diversified expressions. To address these issues, we propose a Planning-based Hierarchical Variational Model (PHVM). Our model first plans a sequence of groups (each group is a subset of input items to be covered by a sentence) and then realizes each sentence conditioned on the planning result and the previously generated context, thereby decomposing long text generation into dependent sentence generation sub-tasks. To capture expression diversity, we devise a hierarchical latent structure where a global planning latent variable models the diversity of reasonable planning and a sequence of local latent variables controls sentence realization. Experiments show that our model outperforms state-of-the-art baselines in long and diverse text generation.</p>
<p>Keywords:</p>
<h3 id="322. "Transforming" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer.">322. "Transforming" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1322">Paper Link</a>    Pages:3267-3277</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/1362.html">Akhilesh Sudhakar</a> ; <a href="https://dblp.uni-trier.de/pid/247/9490.html">Bhargav Upadhyay</a> ; <a href="https://dblp.uni-trier.de/pid/247/9584.html">Arjun Maheswaran</a></p>
<p>Abstract:
Text style transfer is the task of transferring the style of text having certain stylistic attributes, while preserving non-stylistic or content information. In this work we introduce the Generative Style Transformer (GST) - a new approach to rewriting sentences to a target style in the absence of parallel style corpora. GST leverages the power of both, large unsupervised pre-trained language models as well as the Transformer. GST is a part of a larger Delete Retrieve Generate framework, in which we also propose a novel method of deleting style attributes from the source sentence by exploiting the inner workings of the Transformer. Our models outperform state-of-art systems across 5 datasets on sentiment, gender and political slant transfer. We also propose the use of the GLEU metric as an automatic metric of evaluation of style transfer, which we found to compare better with human ratings than the predominantly used BLEU score.</p>
<p>Keywords:</p>
<h3 id="323. An Entity-Driven Framework for Abstractive Summarization.">323. An Entity-Driven Framework for Abstractive Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1323">Paper Link</a>    Pages:3278-3289</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/217/9648.html">Eva Sharma</a> ; <a href="https://dblp.uni-trier.de/pid/228/2534.html">Luyang Huang</a> ; <a href="https://dblp.uni-trier.de/pid/88/6765.html">Zhe Hu</a> ; <a href="https://dblp.uni-trier.de/pid/49/3800-8.html">Lu Wang</a></p>
<p>Abstract:
Abstractive summarization systems aim to produce more coherent and concise summaries than their extractive counterparts. Popular neural models have achieved impressive results for single-document summarization, yet their outputs are often incoherent and unfaithful to the input. In this paper, we introduce SENECA, a novel System for ENtity-drivEn Coherent Abstractive summarization framework that leverages entity information to generate informative and coherent abstracts. Our framework takes a two-step approach: (1) an entity-aware content selection module first identifies salient sentences from the input, then (2) an abstract generation module conducts cross-sentence information compression and abstraction to generate the final summary, which is trained with rewards to promote coherence, conciseness, and clarity. The two components are further connected using reinforcement learning. Automatic evaluation shows that our model significantly outperforms previous state-of-the-art based on ROUGE and our proposed coherence measures on New York Times and CNN/Daily Mail datasets. Human judges further rate our system summaries as more informative and coherent than those by popular summarization models.</p>
<p>Keywords:</p>
<h3 id="324. Neural Extractive Text Summarization with Syntactic Compression.">324. Neural Extractive Text Summarization with Syntactic Compression.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1324">Paper Link</a>    Pages:3290-3301</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/188/6025.html">Jiacheng Xu</a> ; <a href="https://dblp.uni-trier.de/pid/69/7968.html">Greg Durrett</a></p>
<p>Abstract:
Recent neural network approaches to summarization are largely either selection-based extraction or generation-based abstraction. In this work, we present a neural model for single-document summarization based on joint extraction and syntactic compression. Our model chooses sentences from the document, identifies possible compressions based on constituency parses, and scores those compressions with a neural model to produce the final summary. For learning, we construct oracle extractive-compressive summaries, then learn both of our components jointly with this supervision. Experimental results on the CNN/Daily Mail and New York Times datasets show that our model achieves strong performance (comparable to state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-the-shelf compression module, and human and manual evaluation shows that our models output generally remains grammatical.</p>
<p>Keywords:</p>
<h3 id="325. Domain Adaptive Text Style Transfer.">325. Domain Adaptive Text Style Transfer.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1325">Paper Link</a>    Pages:3302-3311</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/200/8750.html">Dianqi Li</a> ; <a href="https://dblp.uni-trier.de/pid/132/4966.html">Yizhe Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/41/7845.html">Zhe Gan</a> ; <a href="https://dblp.uni-trier.de/pid/96/3060.html">Yu Cheng</a> ; <a href="https://dblp.uni-trier.de/pid/95/6500.html">Chris Brockett</a> ; <a href="https://dblp.uni-trier.de/pid/13/486.html">Bill Dolan</a> ; <a href="https://dblp.uni-trier.de/pid/82/1447.html">Ming-Ting Sun</a></p>
<p>Abstract:
Text style transfer without parallel data has achieved some practical success. However, in the scenario where less data is available, these methods may yield poor performance. In this paper, we examine domain adaptation for text style transfer to leverage massively available data from other domains. These data may demonstrate domain shift, which impedes the benefits of utilizing such data for training. To address this challenge, we propose simple yet effective domain adaptive text style transfer models, enabling domain-adaptive information exchange. The proposed models presumably learn from the source domain to: (i) distinguish stylized information and generic content information; (ii) maximally preserve content information; and (iii) adaptively transfer the styles in a domain-aware manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines.</p>
<p>Keywords:</p>
<h3 id="326. Let's Ask Again: Refine Network for Automatic Question Generation.">326. Let's Ask Again: Refine Network for Automatic Question Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1326">Paper Link</a>    Pages:3312-3321</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/199/1835.html">Preksha Nema</a> ; <a href="https://dblp.uni-trier.de/pid/248/8114.html">Akash Kumar Mohankumar</a> ; <a href="https://dblp.uni-trier.de/pid/90/7967.html">Mitesh M. Khapra</a> ; <a href="https://dblp.uni-trier.de/pid/31/4182.html">Balaji Vasan Srinivasan</a> ; <a href="https://dblp.uni-trier.de/pid/69/2281.html">Balaraman Ravindran</a></p>
<p>Abstract:
In this work, we focus on the task of Automatic Question Generation (AQG) where given a passage and an answer the task is to generate the corresponding question. It is desired that the generated question should be (i) grammatically correct (ii) answerable from the passage and (iii) specific to the given answer. An analysis of existing AQG models shows that they produce questions which do not adhere to one or more of the above-mentioned qualities. In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement. To alleviate this shortcoming, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it. More specifically, we propose Refine Network (RefNet) which contains two decoders. The second decoder uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first decoder. In effect, it refines the question generated by the first decoder, thereby making it more correct and complete. We evaluate RefNet on three datasets, viz., SQuAD, HOTPOT-QA, and DROP, and show that it outperforms existing state-of-the-art methods by 7-16% on all of these datasets. Lastly, we show that we can improve the quality of the second decoder on specific metrics, such as, fluency and answerability by explicitly rewarding revisions that improve on the corresponding metric during training. The code has been made publicly available .</p>
<p>Keywords:</p>
<h3 id="327. Earlier Isn't Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization.">327. Earlier Isn't Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1327">Paper Link</a>    Pages:3322-3333</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/2704.html">Taehee Jung</a> ; <a href="https://dblp.uni-trier.de/pid/69/9056.html">Dongyeop Kang</a> ; <a href="https://dblp.uni-trier.de/pid/180/2037.html">Lucas Mentch</a> ; <a href="https://dblp.uni-trier.de/pid/47/2454.html">Eduard H. Hovy</a></p>
<p>Abstract:
Despite the recent developments on neural summarization systems, the underlying logic behind the improvements from the systems and its corpus-dependency remains largely unexplored. Position of sentences in the original text, for example, is a well known bias for news summarization. Following in the spirit of the claim that summarization is a combination of sub-functions, we define three sub-aspects of summarization: position, importance, and diversity and conduct an extensive analysis of the biases of each sub-aspect with respect to the domain of nine different summarization corpora (e.g., news, academic papers, meeting minutes, movie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system.</p>
<p>Keywords:</p>
<h3 id="328. Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction.">328. Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1328">Paper Link</a>    Pages:3334-3339</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/225/7708.html">Yova Kementchedjhieva</a> ; <a href="https://dblp.uni-trier.de/pid/206/6859.html">Mareike Hartmann</a> ; <a href="https://dblp.uni-trier.de/pid/30/2756.html">Anders Sgaard</a></p>
<p>Abstract:
The task of bilingual dictionary induction (BDI) is commonly used for intrinsic evaluation of cross-lingual word embeddings. The largest dataset for BDI was generated automatically, so its quality is dubious. We study the composition and quality of the test sets for five diverse languages from this dataset, with concerning findings: (1) a quarter of the data consists of proper nouns, which can be hardly indicative of BDI performance, and (2) there are pervasive gaps in the gold-standard targets. These issues appear to affect the ranking between cross-lingual embedding systems on individual languages, and the overall degree to which the systems differ in performance. With proper nouns removed from the data, the margin between the top two systems included in the study grows from 3.4% to 17.2%. Manual verification of the predictions, on the other hand, reveals that gaps in the gold standard targets artificially inflate the margin between the two systems on English to Bulgarian BDI from 0.1% to 6.7%. We thus suggest that future research either avoids drawing conclusions from quantitative results on this BDI dataset, or accompanies such evaluation with rigorous error analysis.</p>
<p>Keywords:</p>
<h3 id="329. Towards Realistic Practices In Low-Resource Natural Language Processing: The Development Set.">329. Towards Realistic Practices In Low-Resource Natural Language Processing: The Development Set.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1329">Paper Link</a>    Pages:3340-3347</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/182/1923.html">Katharina Kann</a> ; <a href="https://dblp.uni-trier.de/pid/41/9736.html">Kyunghyun Cho</a> ; <a href="https://dblp.uni-trier.de/pid/116/0502.html">Samuel R. Bowman</a></p>
<p>Abstract:
Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions: Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does it lead to overestimation or underestimation of performance? We repeat multiple experiments from recent work on neural models for low-resource NLP and compare results for models obtained by training with and without development sets. On average over languages, absolute accuracy differs by up to 1.4%. However, for some languages and tasks, differences are as big as 18.0% accuracy. Our results highlight the importance of realistic experimental setups in the publication of low-resource NLP research results.</p>
<p>Keywords:</p>
<h3 id="330. Synchronously Generating Two Languages with Interactive Decoding.">330. Synchronously Generating Two Languages with Interactive Decoding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1330">Paper Link</a>    Pages:3348-3353</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/04/7235.html">Yining Wang</a> ; <a href="https://dblp.uni-trier.de/pid/71/6950.html">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/79/7764.html">Long Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/69/10440.html">Yuchen Liu</a> ; <a href="https://dblp.uni-trier.de/pid/38/6093.html">Chengqing Zong</a></p>
<p>Abstract:
In this paper, we introduce a novel interactive approach to translate a source language into two different languages simultaneously and interactively. Specifically, the generation of one language relies on not only previously generated outputs by itself, but also the outputs predicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model.</p>
<p>Keywords:</p>
<h3 id="331. On NMT Search Errors and Model Errors: Cat Got Your Tongue?">331. On NMT Search Errors and Model Errors: Cat Got Your Tongue?</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1331">Paper Link</a>    Pages:3354-3360</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/125/7456.html">Felix Stahlberg</a> ; <a href="https://dblp.uni-trier.de/pid/b/WilliamJByrne.html">Bill Byrne</a></p>
<p>Abstract:
We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.</p>
<p>Keywords:</p>
<h3 id="332. "Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding.">332. "Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1332">Paper Link</a>    Pages:3361-3367</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/5276.html">Ben Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/71/10515.html">Daniel Khashabi</a> ; <a href="https://dblp.uni-trier.de/pid/127/1927.html">Qiang Ning</a> ; <a href="https://dblp.uni-trier.de/pid/r/DanRoth.html">Dan Roth</a></p>
<p>Abstract:
Understanding time is crucial for understanding events expressed in natural language. Because people rarely say the obvious, it is often necessary to have commonsense knowledge about various temporal aspects of events, such as duration, frequency, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to develop a new dataset, MCTACO, that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20%, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic.</p>
<p>Keywords:</p>
<h3 id="333. QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization.">333. QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1333">Paper Link</a>    Pages:3368-3373</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/07/2659.html">Yi-Ting Yeh</a> ; <a href="https://dblp.uni-trier.de/pid/04/9878.html">Yun-Nung Chen</a></p>
<p>Abstract:
Standard accuracy metrics indicate that modern reading comprehension systems have achieved strong performance in many question answering datasets. However, the extent these systems truly understand language remains unknown, and existing systems are not good at distinguishing distractor sentences which look related but do not answer the question. To address this problem, we propose QAInfomax as a regularizer in reading comprehension systems by maximizing mutual information among passages, a question, and its answer. QAInfomax helps regularize the model to not simply learn the superficial correlation for answering the questions. The experiments show that our proposed QAInfomax achieves the state-of-the-art performance on the benchmark Adversarial-SQuAD dataset.</p>
<p>Keywords:</p>
<h3 id="334. Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations.">334. Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1334">Paper Link</a>    Pages:3374-3379</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/52/1775.html">Xin Lv</a> ; <a href="https://dblp.uni-trier.de/pid/248/2313.html">Yuxian Gu</a> ; <a href="https://dblp.uni-trier.de/pid/19/3011-7.html">Xu Han</a> ; <a href="https://dblp.uni-trier.de/pid/32/5685-1.html">Lei Hou</a> ; <a href="https://dblp.uni-trier.de/pid/l/JuanZiLi.html">Juanzi Li</a> ; <a href="https://dblp.uni-trier.de/pid/53/3245-1.html">Zhiyuan Liu</a></p>
<p>Abstract:
Multi-hop knowledge graph (KG) reasoning is an effective and explainable method for predicting the target entity via reasoning paths in query answering (QA) task. Most previous methods assume that every relation in KGs has enough triples for training, regardless of those few-shot relations which cannot provide sufficient triples for training robust reasoning models. In fact, the performance of existing multi-hop reasoning methods drops significantly on few-shot relations. In this paper, we propose a meta-based multi-hop reasoning method (Meta-KGR), which adopts meta-learning to learn effective meta parameters from high-frequency relations that could quickly adapt to few-shot relations. We evaluate Meta-KGR on two public datasets sampled from Freebase and NELL, and the experimental results show that Meta-KGR outperforms state-of-the-art methods in few-shot scenarios. In the future, our codes and datasets will also be available to provide more details.</p>
<p>Keywords:</p>
<h3 id="335. How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the Winograd Schema Challenge and SWAG.">335. How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the Winograd Schema Challenge and SWAG.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1335">Paper Link</a>    Pages:3380-3385</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/211/8019.html">Paul Trichelair</a> ; <a href="https://dblp.uni-trier.de/pid/75/10772.html">Ali Emami</a> ; <a href="https://dblp.uni-trier.de/pid/177/9137.html">Adam Trischler</a> ; <a href="https://dblp.uni-trier.de/pid/143/3542.html">Kaheer Suleman</a> ; <a href="https://dblp.uni-trier.de/pid/00/9012.html">Jackie Chi Kit Cheung</a></p>
<p>Abstract:
Recent studies have significantly improved the state-of-the-art on common-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge (WSC) and SWAG. The question we ask in this paper is whether improved performance on these benchmarks represents genuine progress towards common-sense-enabled systems. We make case studies of both benchmarks and design protocols that clarify and qualify the results of previous work by analyzing threats to the validity of previous experimental designs. Our protocols account for several properties prevalent in common-sense benchmarks including size limitations, structural regularities, and variable instance difficulty.</p>
<p>Keywords:</p>
<h3 id="336. Pun-GAN: Generative Adversarial Network for Pun Generation.">336. Pun-GAN: Generative Adversarial Network for Pun Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1336">Paper Link</a>    Pages:3386-3391</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/4216.html">Fuli Luo</a> ; <a href="https://dblp.uni-trier.de/pid/131/4303.html">Shunyao Li</a> ; <a href="https://dblp.uni-trier.de/pid/140/6685.html">Pengcheng Yang</a> ; <a href="https://dblp.uni-trier.de/pid/13/7007.html">Lei Li</a> ; <a href="https://dblp.uni-trier.de/pid/91/6051.html">Baobao Chang</a> ; <a href="https://dblp.uni-trier.de/pid/22/5834.html">Zhifang Sui</a> ; <a href="https://dblp.uni-trier.de/pid/37/1971-1.html">Xu Sun</a></p>
<p>Abstract:
In this paper, we focus on the task of generating a pun sentence given a pair of word senses. A major challenge for pun generation is the lack of large-scale pun corpus to guide supervised learning. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN). It consists of a generator to produce pun sentences, and a discriminator to distinguish between the generated pun sentences and the real sentences with specific word senses. The output of the discriminator is then used as a reward to train the generator via reinforcement learning, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.</p>
<p>Keywords:</p>
<h3 id="337. Multi-Task Learning with Language Modeling for Question Generation.">337. Multi-Task Learning with Language Modeling for Question Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1337">Paper Link</a>    Pages:3392-3397</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/137/9454.html">Wenjie Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/24/6799.html">Minghua Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/83/3463.html">Yunfang Wu</a></p>
<p>Abstract:
This paper explores the task of answer-aware questions generation. Based on the attention-based pointer generator model, we propose to incorporate an auxiliary task of language modeling to help question generation in a hierarchical multi-task learning structure. Our joint-learning model enables the encoder to learn a better representation of the input sequence, which will guide the decoder to generate more coherent and fluent questions. On both SQuAD and MARCO datasets, our multi-task learning model boosts the performance, achieving state-of-the-art results. Moreover, human evaluation further proves the high quality of our generated questions.</p>
<p>Keywords:</p>
<h3 id="338. Autoregressive Text Generation Beyond Feedback Loops.">338. Autoregressive Text Generation Beyond Feedback Loops.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1338">Paper Link</a>    Pages:3398-3404</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/61/4643-5.html">Florian Schmidt</a> ; <a href="https://dblp.uni-trier.de/pid/147/5018.html">Stephan Mandt</a> ; <a href="https://dblp.uni-trier.de/pid/h/ThHofmann.html">Thomas Hofmann</a></p>
<p>Abstract:
Autoregressive state transitions, where predictions are conditioned on past predictions, are the predominant choice for both deterministic and stochastic sequential models. However, autoregressive feedback exposes the evolution of the hidden state trajectory to potential biases from well-known train-test discrepancies. In this paper, we combine a latent state space model with a CRF observation model. We argue that such autoregressive observation models form an interesting middle ground that expresses local correlations on the word level but keeps the state evolution non-autoregressive. On unconditional sentence generation we show performance improvements compared to RNN and GAN baselines while avoiding some prototypical failure modes of autoregressive models.</p>
<p>Keywords:</p>
<h3 id="339. The Woman Worked as a Babysitter: On Biases in Language Generation.">339. The Woman Worked as a Babysitter: On Biases in Language Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1339">Paper Link</a>    Pages:3405-3410</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/2314.html">Emily Sheng</a> ; <a href="https://dblp.uni-trier.de/pid/18/2428.html">Kai-Wei Chang</a> ; <a href="https://dblp.uni-trier.de/pid/40/2362.html">Premkumar Natarajan</a> ; <a href="https://dblp.uni-trier.de/pid/117/4036.html">Nanyun Peng</a></p>
<p>Abstract:
We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.</p>
<p>Keywords:</p>
<h3 id="340. On the Importance of Delexicalization for Fact Verification.">340. On the Importance of Delexicalization for Fact Verification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1340">Paper Link</a>    Pages:3411-3416</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/232/5613.html">Sandeep Suntwal</a> ; <a href="https://dblp.uni-trier.de/pid/37/9569.html">Mithun Paul</a> ; <a href="https://dblp.uni-trier.de/pid/145/3979.html">Rebecca Sharp</a> ; <a href="https://dblp.uni-trier.de/pid/18/3479.html">Mihai Surdeanu</a></p>
<p>Abstract:
While neural networks produce state-of-the-art performance in many NLP tasks, they generally learn from lexical information, which may transfer poorly between domains. Here, we investigate the importance that a model assigns to various aspects of data while learning and making predictions, specifically, in a recognizing textual entailment (RTE) task. By inspecting the attention weights assigned by the model, we confirm that most of the weights are assigned to noun phrases. To mitigate this dependence on lexicalized information, we experiment with two strategies of masking. First, we replace named entities with their corresponding semantic tags along with a unique identifier to indicate lexical overlap between claim and evidence. Second, we similarly replace other word classes in the sentence (nouns, verbs, adjectives, and adverbs) with their super sense tags (Ciaramita and Johnson, 2003). Our results show that, while performance on the in-domain dataset remains on par with that of the model trained on fully lexicalized data, it improves considerably when tested out of domain. For example, the performance of a state-of-the-art RTE model trained on the masked Fake News Challenge (Pomerleau and Rao, 2017) data and evaluated on Fact Extraction and Verification (Thorne et al., 2018) data improved by over 10% in accuracy score compared to the fully lexicalized model.</p>
<p>Keywords:</p>
<h3 id="341. Towards Debiasing Fact Verification Models.">341. Towards Debiasing Fact Verification Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1341">Paper Link</a>    Pages:3417-3423</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/190/7491.html">Tal Schuster</a> ; <a href="https://dblp.uni-trier.de/pid/227/2738.html">Darsh J. Shah</a> ; <a href="https://dblp.uni-trier.de/pid/247/1299.html">Yun Jie Serene Yeo</a> ; <a href="https://dblp.uni-trier.de/pid/247/1310.html">Daniel Filizzola</a> ; <a href="https://dblp.uni-trier.de/pid/148/4556.html">Enrico Santus</a> ; <a href="https://dblp.uni-trier.de/pid/b/ReginaBarzilay.html">Regina Barzilay</a></p>
<p>Abstract:
Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.</p>
<p>Keywords:</p>
<h3 id="342. Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks.">342. Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1342">Paper Link</a>    Pages:3424-3429</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/197/4435.html">Xingwei Tan</a> ; <a href="https://dblp.uni-trier.de/pid/58/3467.html">Yi Cai</a> ; <a href="https://dblp.uni-trier.de/pid/241/7775.html">Changxi Zhu</a></p>
<p>Abstract:
Aspect-level sentiment classification, which is a fine-grained sentiment analysis task, has received lots of attention these years. There is a phenomenon that people express both positive and negative sentiments towards an aspect at the same time. Such opinions with conflicting sentiments, however, are ignored by existing studies, which design models based on the absence of them. We argue that the exclusion of conflict opinions is problematic, for the reason that it represents an important style of human thinking  dialectic thinking. If a real-world sentiment classification system ignores the existence of conflict opinions when it is designed, it will incorrectly mixed conflict opinions into other sentiment polarity categories in action. Existing models have problems when recognizing conflicting opinions, such as data sparsity. In this paper, we propose a multi-label classification model with dual attention mechanism to address these problems.</p>
<p>Keywords:</p>
<h3 id="343. Investigating Dynamic Routing in Tree-Structured LSTM for Sentiment Analysis.">343. Investigating Dynamic Routing in Tree-Structured LSTM for Sentiment Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1343">Paper Link</a>    Pages:3430-3435</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/92/1375-8.html">Jin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/68/5894.html">Liang-Chih Yu</a> ; <a href="https://dblp.uni-trier.de/pid/05/199.html">K. Robert Lai</a> ; <a href="https://dblp.uni-trier.de/pid/68/3522-2.html">Xuejie Zhang</a></p>
<p>Abstract:
Deep neural network models such as long short-term memory (LSTM) and tree-LSTM have been proven to be effective for sentiment analysis. However, sequential LSTM is a bias model wherein the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. Even tree-LSTM, with useful structural information, could not avoid the bias problem because the root node will be dominant and the nodes in the bottom of the parse tree will be less emphasized even though they may contain salient information. To overcome the bias problem, this study proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement.</p>
<p>Keywords:</p>
<h3 id="344. A Label Informative Wide & Deep Classifier for Patents and Papers.">344. A Label Informative Wide &amp; Deep Classifier for Patents and Papers.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1344">Paper Link</a>    Pages:3436-3441</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/7915.html">Muyao Niu</a> ; <a href="https://dblp.uni-trier.de/pid/95/3916.html">Jie Cai</a></p>
<p>Abstract:
In this paper, we provide a simple and effective baseline for classifying both patents and papers to the well-established Cooperative Patent Classification (CPC). We propose a label-informative classifier based on the Wide &amp; Deep structure, where the Wide part encodes string-level similarities between texts and labels, and the Deep part captures semantic-level similarities via non-linear transformations. Our model trains on millions of patents, and transfers to papers by developing distant-supervised training set and domain-specific features. Extensive experiments show that our model achieves comparable performance to the state-of-the-art model used in industry on both patents and papers. The output of this work should facilitate the searching, granting and filing of innovative ideas for patent examiners, attorneys and researchers.</p>
<p>Keywords:</p>
<h3 id="345. Text Level Graph Neural Network for Text Classification.">345. Text Level Graph Neural Network for Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1345">Paper Link</a>    Pages:3442-3448</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/250/2953.html">Lianzhe Huang</a> ; <a href="https://dblp.uni-trier.de/pid/32/2706.html">Dehong Ma</a> ; <a href="https://dblp.uni-trier.de/pid/05/4288.html">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pid/37/4356.html">Xiaodong Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/38/1358.html">Houfeng Wang</a></p>
<p>Abstract:
Recently, researches have explored the graph neural network (GNN) techniques on text classification, since GNN does well in handling complex structures and preserving global information. However, previous methods based on GNN are mainly faced with the practical problems of fixed corpus level graph structure which dont support online testing and high memory consumption. To tackle the problems, we propose a new GNN based model that builds graphs for each input text with global parameters sharing instead of a single graph for the whole corpus. This method removes the burden of dependence between an individual text and entire corpus which support online testing, but still preserve global information. Besides, we build graphs by much smaller windows in the text, which not only extract more local features but also significantly reduce the edge numbers as well as memory consumption. Experiments show that our model outperforms existing models on several text classification datasets even with consuming less memory.</p>
<p>Keywords:</p>
<h3 id="346. Semantic Relatedness Based Re-ranker for Text Spotting.">346. Semantic Relatedness Based Re-ranker for Text Spotting.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1346">Paper Link</a>    Pages:3449-3455</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/6577.html">Ahmed Sabir</a> ; <a href="https://dblp.uni-trier.de/pid/44/991.html">Francesc Moreno</a> ; <a href="https://dblp.uni-trier.de/pid/56/5747.html">Llus Padr</a></p>
<p>Abstract:
Applications such as textual entailment, plagiarism detection or document clustering rely on the notion of semantic similarity, and are usually approached with dimension reduction techniques like LDA or with embedding-based neural approaches. We present a scenario where semantic similarity is not enough, and we devise a neural approach to learn semantic relatedness. The scenario is text spotting in the wild, where a text in an image (e.g. street sign, advertisement or bus destination) must be identified and recognized. Our goal is to improve the performance of vision systems by leveraging semantic information. Our rationale is that the text to be spotted is often related to the image context in which it appears (word pairs such as Delta-airplane, or quarters-parking are not similar, but are clearly related). We show how learning a word-to-word or word-to-sentence relatedness score can improve the performance of text spotting systems up to 2.9 points, outperforming other measures in a benchmark dataset.</p>
<p>Keywords:</p>
<h3 id="347. Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings.">347. Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1347">Paper Link</a>    Pages:3456-3461</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/183/6275.html">Hwiyeol Jo</a> ; <a href="https://dblp.uni-trier.de/pid/213/8369.html">Ceyda Cinarel</a></p>
<p>Abstract:
We propose a novel and simple method for semi-supervised text classification. The method stems from the hypothesis that a classifier with pretrained word embeddings always outperforms the same classifier with randomly initialized word embeddings, as empirically observed in NLP tasks. Our method first builds two sets of classifiers as a form of model ensemble, and then initializes their word embeddings differently: one using random, the other using pretrained word embeddings. We focus on different predictions between the two classifiers on unlabeled data while following the self-training framework. We also use early-stopping in meta-epoch to improve the performance of our method. Our method, Delta-training, outperforms the self-training and the co-training framework in 4 different text classification datasets, showing robustness against error accumulation.</p>
<p>Keywords:</p>
<h3 id="348. Visual Detection with Context for Document Layout Analysis.">348. Visual Detection with Context for Document Layout Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1348">Paper Link</a>    Pages:3462-3468</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/191/6617.html">Carlos Soto</a> ; <a href="https://dblp.uni-trier.de/pid/69/1062.html">Shinjae Yoo</a></p>
<p>Abstract:
We present 1) a work in progress method to visually segment key regions of scientific articles using an object detection technique augmented with contextual features, and 2) a novel dataset of region-labeled articles. A continuing challenge in scientific literature mining is the difficulty of consistently extracting high-quality text from formatted PDFs. To address this, we adapt the object-detection technique Faster R-CNN for document layout detection, incorporating contextual information that leverages the inherently localized nature of article contents to improve the region detection performance. Due to the limited availability of high-quality region-labels for scientific articles, we also contribute a novel dataset of region annotations, the first version of which covers 9 region classes and 822 article pages. Initial experimental results demonstrate a 23.9% absolute improvement in mean average precision over the baseline model by incorporating contextual features, and a processing speed 14x faster than a text-based technique. Ongoing work on further improvements is also discussed.</p>
<p>Keywords:</p>
<h3 id="349. Evaluating Topic Quality with Posterior Variability.">349. Evaluating Topic Quality with Posterior Variability.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1349">Paper Link</a>    Pages:3469-3475</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/9346.html">Linzi Xing</a> ; <a href="https://dblp.uni-trier.de/pid/00/9714.html">Michael J. Paul</a> ; <a href="https://dblp.uni-trier.de/pid/30/3994.html">Giuseppe Carenini</a></p>
<p>Abstract:
Probabilistic topic models such as latent Dirichlet allocation (LDA) are popularly used with Bayesian inference methods such as Gibbs sampling to learn posterior distributions over topic model parameters. We derive a novel measure of LDA topic quality using the variability of the posterior distributions. Compared to several existing baselines for automatic topic evaluation, the proposed metric achieves state-of-the-art correlations with human judgments of topic quality in experiments on three corpora. We additionally demonstrate that topic quality estimation can be further improved using a supervised estimator that combines multiple metrics.</p>
<p>Keywords:</p>
<h3 id="350. Neural Topic Model with Reinforcement Learning.">350. Neural Topic Model with Reinforcement Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1350">Paper Link</a>    Pages:3476-3481</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/34/8605-3.html">Lin Gui</a> ; <a href="https://dblp.uni-trier.de/pid/254/8224.html">Jia Leng</a> ; <a href="https://dblp.uni-trier.de/pid/180/3707.html">Gabriele Pergola</a> ; <a href="https://dblp.uni-trier.de/pid/36/2728-25.html">Yu Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/93/5407.html">Ruifeng Xu</a> ; <a href="https://dblp.uni-trier.de/pid/75/5430.html">Yulan He</a></p>
<p>Abstract:
In recent years, advances in neural variational inference have achieved many successes in text processing. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of reinforcement learning and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models.</p>
<p>Keywords:</p>
<h3 id="351. Modelling Stopping Criteria for Search Results using Poisson Processes.">351. Modelling Stopping Criteria for Search Results using Poisson Processes.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1351">Paper Link</a>    Pages:3482-3487</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/116/7700.html">Alison Sneyd</a> ; <a href="https://dblp.uni-trier.de/pid/68/6.html">Mark Stevenson</a></p>
<p>Abstract:
Text retrieval systems often return large sets of documents, particularly when applied to large collections. Stopping criteria can reduce the number of these documents that need to be manually evaluated for relevance by predicting when a suitable level of recall has been achieved. In this work, a novel method for determining a stopping criterion is proposed that models the rate at which relevant documents occur using a Poisson process. This method allows a user to specify both a minimum desired level of recall to achieve and a desired probability of having achieved it. We evaluate our method on a public dataset and compare it with previous techniques for determining stopping criteria.</p>
<p>Keywords:</p>
<h3 id="352. Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval.">352. Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1352">Paper Link</a>    Pages:3488-3494</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/245/1857.html">Zeynep Akkalyoncu Yilmaz</a> ; <a href="https://dblp.uni-trier.de/pid/03/1094-17.html">Wei Yang</a> ; <a href="https://dblp.uni-trier.de/pid/83/4184.html">Haotian Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/00/7739.html">Jimmy Lin</a></p>
<p>Abstract:
This paper applies BERT to ad hoc document retrieval on news articles, which requires addressing two challenges: relevance judgments in existing test collections are typically provided only at the document level, and documents often exceed the length that BERT was designed to handle. Our solution is to aggregate sentence-level evidence to rank documents. Furthermore, we are able to leverage passage-level relevance judgments fortuitously available in other domains to fine-tune BERT models that are able to capture cross-domain notions of relevance, and can be directly used for ranking news articles. Our simple neural ranking models achieve state-of-the-art effectiveness on three standard test collections.</p>
<p>Keywords:</p>
<h3 id="353. The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval.">353. The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1353">Paper Link</a>    Pages:3495-3500</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/78/4390.html">Constantine Lignos</a> ; <a href="https://dblp.uni-trier.de/pid/85/2738.html">Daniel Cohen</a> ; <a href="https://dblp.uni-trier.de/pid/172/6605.html">Yen-Chieh Lien</a> ; <a href="https://dblp.uni-trier.de/pid/51/9601.html">Pratik Mehta</a> ; <a href="https://dblp.uni-trier.de/pid/c/WBruceCroft.html">W. Bruce Croft</a> ; <a href="https://dblp.uni-trier.de/pid/18/1985.html">Scott Miller</a></p>
<p>Abstract:
When performing cross-language information retrieval (CLIR) for lower-resourced languages, a common approach is to retrieve over the output of machine translation (MT). However, there is no established guidance on how to optimize the resulting MT-IR system. In this paper, we examine the relationship between the performance of MT systems and both neural and term frequency-based IR models to identify how CLIR performance can be best predicted from MT quality. We explore performance at varying amounts of MT training data, byte pair encoding (BPE) merge operations, and across two IR collections and retrieval models. We find that the choice of IR collection can substantially affect the predictive power of MT tuning decisions and evaluation, potentially introducing dissociations between MT-only and overall CLIR performance.</p>
<p>Keywords:</p>
<h3 id="354. Rotate King to get Queen: Word Relationships as Orthogonal Transformations in Embedding Space.">354. Rotate King to get Queen: Word Relationships as Orthogonal Transformations in Embedding Space.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1354">Paper Link</a>    Pages:3501-3506</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/198/6540.html">Kawin Ethayarajh</a></p>
<p>Abstract:
A notable property of word embeddings is that word relationships can exist as linear substructures in the embedding space. For example, gender corresponds to v_woman - v_man and v_queen - v_king. This, in turn, allows word analogies to be solved arithmetically: v_king - v_man + v_woman = v_queen. This property is notable because it suggests that models trained on word embeddings can easily learn such relationships as geometric translations. However, there is no evidence that models exclusively represent relationships in this manner. We document an alternative way in which downstream models might learn these relationships: orthogonal and linear transformations. For example, given a translation vector for gender, we can find an orthogonal matrix R, representing a rotation and reflection, such that R(v_king) = v_queen and R(v_man) = v_woman. Analogical reasoning using orthogonal transformations is almost as accurate as using vector arithmetic; using linear transformations is more accurate than both. Our findings suggest that these transformations can be as good a representation of word relationships as translation vectors.</p>
<p>Keywords:</p>
<h3 id="355. GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge.">355. GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1355">Paper Link</a>    Pages:3507-3512</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/238/0538.html">Luyao Huang</a> ; <a href="https://dblp.uni-trier.de/pid/74/4961.html">Chi Sun</a> ; <a href="https://dblp.uni-trier.de/pid/69/1395.html">Xipeng Qiu</a> ; <a href="https://dblp.uni-trier.de/pid/05/6735.html">Xuanjing Huang</a></p>
<p>Abstract:
Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous word in a particular context. Traditional supervised methods rarely take into consideration the lexical resources like WordNet, which are widely utilized in knowledge-based methods. Recent studies have shown the effectiveness of incorporating gloss (sense definition) into neural networks for WSD. However, compared with traditional word expert supervised methods, they have not achieved much improvement. In this paper, we focus on how to better leverage gloss knowledge in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT based models for WSD. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.</p>
<p>Keywords:</p>
<h3 id="356. Leveraging Adjective-Noun Phrasing Knowledge for Comparison Relation Prediction in Text-to-SQL.">356. Leveraging Adjective-Noun Phrasing Knowledge for Comparison Relation Prediction in Text-to-SQL.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1356">Paper Link</a>    Pages:3513-3518</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/4217.html">Haoyan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/66/5168.html">Lei Fang</a> ; <a href="https://dblp.uni-trier.de/pid/33/85.html">Qian Liu</a> ; <a href="https://dblp.uni-trier.de/pid/11/8555.html">Bei Chen</a> ; <a href="https://dblp.uni-trier.de/pid/37/1917.html">Jian-Guang Lou</a> ; <a href="https://dblp.uni-trier.de/pid/76/2866.html">Zhoujun Li</a></p>
<p>Abstract:
One key component in text-to-SQL is to predict the comparison relations between columns and their values. To the best of our knowledge, no existing models explicitly introduce external common knowledge to address this problem, thus their capabilities of predicting comparison relations are limited beyond training data. In this paper, we propose to leverage adjective-noun phrasing knowledge mined from the web to predict the comparison relations in text-to-SQL. Experimental results on both the original and the re-split Spider dataset show that our approach achieves significant improvement over state-of-the-art methods on comparison relation prediction.</p>
<p>Keywords:</p>
<h3 id="357. Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling.">357. Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1357">Paper Link</a>    Pages:3519-3525</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/5416.html">Koki Washio</a> ; <a href="https://dblp.uni-trier.de/pid/10/163.html">Satoshi Sekine</a> ; <a href="https://dblp.uni-trier.de/pid/03/4243.html">Tsuneaki Kato</a></p>
<p>Abstract:
Definition modeling includes acquiring word embeddings from dictionary definitions and generating definitions of words. While the meanings of defining words are important in dictionary definitions, it is crucial to capture the lexical semantic relations between defined words and defining words. However, thus far, the utilization of such relations has not been explored for definition modeling. In this paper, we propose definition modeling methods that use lexical semantic relations. To utilize implicit semantic relations in definitions, we use unsupervisedly obtained pattern-based word-pair embeddings that represent semantic relations of word pairs. Experimental results indicate that our methods improve the performance in learning embeddings from definitions, as well as definition generation.</p>
<p>Keywords:</p>
<h3 id="358. Don't Just Scratch the Surface: Enhancing Word Representations for Korean with Hanja.">358. Don't Just Scratch the Surface: Enhancing Word Representations for Korean with Hanja.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1358">Paper Link</a>    Pages:3526-3531</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/163/5657.html">Kang Min Yoo</a> ; <a href="https://dblp.uni-trier.de/pid/205/3110.html">Taeuk Kim</a> ; <a href="https://dblp.uni-trier.de/pid/67/5511.html">Sang-goo Lee</a></p>
<p>Abstract:
We propose a simple yet effective approach for improving Korean word representations using additional linguistic annotation (i.e. Hanja). We employ cross-lingual transfer learning in training word representations by leveraging the fact that Hanja is closely related to Chinese. We evaluate the intrinsic quality of representations learned through our approach using the word analogy and similarity tests. In addition, we demonstrate their effectiveness on several downstream tasks, including a novel Korean news headline generation task.</p>
<p>Keywords:</p>
<h3 id="359. SyntagNet: Challenging Supervised Word Sense Disambiguation with Lexical-Semantic Combinations.">359. SyntagNet: Challenging Supervised Word Sense Disambiguation with Lexical-Semantic Combinations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1359">Paper Link</a>    Pages:3532-3538</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8144.html">Marco Maru</a> ; <a href="https://dblp.uni-trier.de/pid/167/6875.html">Federico Scozzafava</a> ; <a href="https://dblp.uni-trier.de/pid/251/9333.html">Federico Martelli</a> ; <a href="https://dblp.uni-trier.de/pid/n/RobertoNavigli.html">Roberto Navigli</a></p>
<p>Abstract:
Current research in knowledge-based Word Sense Disambiguation (WSD) indicates that performances depend heavily on the Lexical Knowledge Base (LKB) employed. This paper introduces SyntagNet, a novel resource consisting of manually disambiguated lexical-semantic combinations. By capturing sense distinctions evoked by syntagmatic relations, SyntagNet enables knowledge-based WSD systems to establish a new state of the art which challenges the hitherto unrivaled performances attained by supervised approaches. To the best of our knowledge, SyntagNet is the first large-scale manually-curated resource of this kind made available to the community (at <a href="http://syntagnet.org">http://syntagnet.org</a>).</p>
<p>Keywords:</p>
<h3 id="360. Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition.">360. Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1360">Paper Link</a>    Pages:3539-3545</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/6318.html">Genta Indra Winata</a> ; <a href="https://dblp.uni-trier.de/pid/228/9217.html">Zhaojiang Lin</a> ; <a href="https://dblp.uni-trier.de/pid/225/5387.html">Jamin Shin</a> ; <a href="https://dblp.uni-trier.de/pid/46/9231.html">Zihan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/29/4187.html">Pascale Fung</a></p>
<p>Abstract:
In countries that speak multiple main languages, mixing up different languages within a conversation is commonly called code-switching. Previous works addressing this challenge mainly focused on word-level aspects such as word embeddings. However, in many cases, languages share common subwords, especially for closely related languages, but also for languages that are seemingly irrelevant. Therefore, we propose Hierarchical Meta-Embeddings (HME) that learn to combine multiple monolingual word-level and subword-level embeddings to create language-agnostic lexical representations. On the task of Named Entity Recognition for English-Spanish code-switching data, our model achieves the state-of-the-art performance in the multilingual settings. We also show that, in cross-lingual settings, our model not only leverages closely related languages, but also learns from languages with different roots. Finally, we show that combining different subunits are crucial for capturing code-switching entities.</p>
<p>Keywords:</p>
<h3 id="361. Fine-tune BERT with Sparse Self-Attention Mechanism.">361. Fine-tune BERT with Sparse Self-Attention Mechanism.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1361">Paper Link</a>    Pages:3546-3551</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/207/7581.html">Baiyun Cui</a> ; <a href="https://dblp.uni-trier.de/pid/119/1901.html">Yingming Li</a> ; <a href="https://dblp.uni-trier.de/pid/99/311.html">Ming Chen</a> ; <a href="https://dblp.uni-trier.de/pid/z/ZhongfeiMarkZhang.html">Zhongfei Zhang</a></p>
<p>Abstract:
In this paper, we develop a novel Sparse Self-Attention Fine-tuning model (referred as SSAF) which integrates sparsity into self-attention mechanism to enhance the fine-tuning performance of BERT. In particular, sparsity is introduced into the self-attention by replacing softmax function with a controllable sparse transformation when fine-tuning with BERT. It enables us to learn a structurally sparse attention distribution, which leads to a more interpretable representation for the whole input. The proposed model is evaluated on sentiment analysis, question answering, and natural language inference tasks. The extensive experimental results across multiple datasets demonstrate its effectiveness and superiority to the baseline methods.</p>
<p>Keywords:</p>
<h3 id="362. Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels.">362. Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1362">Paper Link</a>    Pages:3552-3557</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/5288.html">Lukas Lange</a> ; <a href="https://dblp.uni-trier.de/pid/223/4272.html">Michael A. Hedderich</a> ; <a href="https://dblp.uni-trier.de/pid/00/1846.html">Dietrich Klakow</a></p>
<p>Abstract:
In low-resource settings, the performance of supervised labeling models can be improved with automatically annotated or distantly supervised data, which is cheap to create but often noisy. Previous works have shown that significant improvements can be reached by injecting information about the confusion between clean and noisy labels in this additional training data into the classifier training. However, for noise estimation, these approaches either do not take the input features (in our case word embeddings) into account, or they need to learn the noise modeling from scratch which can be difficult in a low-resource setting. We propose to cluster the training data using the input features and then compute different confusion matrices for each cluster. To the best of our knowledge, our approach is the first to leverage feature-dependent noise modeling with pre-initialized confusion matrices. We evaluate on low-resource named entity recognition settings in several languages, showing that our methods improve upon other confusion-matrix based methods by up to 9%.</p>
<p>Keywords:</p>
<h3 id="363. A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation.">363. A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1363">Paper Link</a>    Pages:3558-3563</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/226/2020.html">Hagai Taitelbaum</a> ; <a href="https://dblp.uni-trier.de/pid/c/GalChechik.html">Gal Chechik</a> ; <a href="https://dblp.uni-trier.de/pid/65/6574.html">Jacob Goldberger</a></p>
<p>Abstract:
In this paper we present a novel approach to simultaneously representing multiple languages in a common space. Procrustes Analysis (PA) is commonly used to find the optimal orthogonal word mapping in the bilingual case. The proposed Multi Pairwise Procrustes Analysis (MPPA) is a natural extension of the PA algorithm to multilingual word mapping. Unlike previous PA extensions that require a k-way dictionary, this approach requires only pairwise bilingual dictionaries that are much easier to construct.</p>
<p>Keywords:</p>
<h3 id="364. Out-of-Domain Detection for Low-Resource Text Classification Tasks.">364. Out-of-Domain Detection for Low-Resource Text Classification Tasks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1364">Paper Link</a>    Pages:3564-3570</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/25/1150.html">Ming Tan</a> ; <a href="https://dblp.uni-trier.de/pid/46/2181-29.html">Yang Yu</a> ; <a href="https://dblp.uni-trier.de/pid/50/8499-2.html">Haoyu Wang</a> ; <a href="https://dblp.uni-trier.de/pid/161/3389.html">Dakuo Wang</a> ; <a href="https://dblp.uni-trier.de/pid/194/3158.html">Saloni Potdar</a> ; <a href="https://dblp.uni-trier.de/pid/28/9988.html">Shiyu Chang</a> ; <a href="https://dblp.uni-trier.de/pid/32/7445.html">Mo Yu</a></p>
<p>Abstract:
Out-of-domain (OOD) detection for low-resource text classification is a realistic but understudied task. The goal is to detect the OOD cases with limited in-domain (ID) training data, since in machine learning applications we observe that training data is often insufficient. In this work, we propose an OOD-resistant Prototypical Network to tackle this zero-shot OOD detection and few-shot ID classification task. Evaluations on real-world datasets show that the proposed solution outperforms state-of-the-art methods in zero-shot OOD detection task, while maintaining a competitive performance on ID classification task.</p>
<p>Keywords:</p>
<h3 id="365. Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer.">365. Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1365">Paper Link</a>    Pages:3571-3576</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/26/3093.html">Yunli Wang</a> ; <a href="https://dblp.uni-trier.de/pid/22/0.html">Yu Wu</a> ; <a href="https://dblp.uni-trier.de/pid/127/0779.html">Lili Mou</a> ; <a href="https://dblp.uni-trier.de/pid/76/2866.html">Zhoujun Li</a> ; <a href="https://dblp.uni-trier.de/pid/83/4112.html">Wenhan Chao</a></p>
<p>Abstract:
Formality text style transfer plays an important role in various NLP applications, such as non-native speaker assistants and child education. Early studies normalize informal sentences with rules, before statistical and neural models become a prevailing method in the field. While a rule-based system is still a common preprocessing step for formality style transfer in the neural era, it could introduce noise if we use the rules in a naive way such as data preprocessing. To mitigate this problem, we study how to harness rules into a state-of-the-art neural network that is typically pretrained on massive corpora. We propose three fine-tuning methods in this paper and achieve a new state-of-the-art on benchmark datasets</p>
<p>Keywords:</p>
<h3 id="366. Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training.">366. Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1366">Paper Link</a>    Pages:3577-3582</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8180.html">Chih-Te Lai</a> ; <a href="https://dblp.uni-trier.de/pid/169/0281.html">Yi-Te Hong</a> ; <a href="https://dblp.uni-trier.de/pid/228/5569.html">Hong-You Chen</a> ; <a href="https://dblp.uni-trier.de/pid/40/243.html">Chi-Jen Lu</a> ; <a href="https://dblp.uni-trier.de/pid/60/7120.html">Shou-De Lin</a></p>
<p>Abstract:
The objective of non-parallel text style transfer, or controllable text generation, is to alter specific attributes (e.g. sentiment, mood, tense, politeness, etc) of a given text while preserving its remaining attributes and content. Generative adversarial network (GAN) is a popular model to ensure the transferred sentences are realistic and have the desired target styles. However, training GAN often suffers from mode collapse problem, which causes that the transferred text is little related to the original text. In this paper, we propose a new GAN model with a word-level conditional architecture and a two-phase training procedure. By using a style-related condition architecture before generating a word, our model is able to maintain style-unrelated words while changing the others. By separating the training procedure into reconstruction and transfer phases, our model is able to learn a proper text generation process, which further improves the content preservation. We test our model on polarity sentiment transfer and multiple-attribute transfer tasks. The empirical results show that our model achieves comparable evaluation scores in both transfer accuracy and fluency but significantly outperforms other state-of-the-art models in content compatibility on three real-world datasets.</p>
<p>Keywords:</p>
<h3 id="367. Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition.">367. Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1367">Paper Link</a>    Pages:3583-3588</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/183/5939.html">Yufan Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/88/9232.html">Chi Hu</a> ; <a href="https://dblp.uni-trier.de/pid/05/5091.html">Tong Xiao</a> ; <a href="https://dblp.uni-trier.de/pid/54/8637.html">Chunliang Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/73/2129.html">Jingbo Zhu</a></p>
<p>Abstract:
In this paper, we study differentiable neural architecture search (NAS) methods for natural language processing. In particular, we improve differentiable architecture search by removing the softmax-local constraint. Also, we apply differentiable NAS to named entity recognition (NER). It is the first time that differentiable NAS methods are adopted in NLP tasks other than language modeling. On both the PTB language modeling and CoNLL-2003 English NER data, our method outperforms strong baselines. It achieves a new state-of-the-art on the NER task.</p>
<p>Keywords:</p>
<h3 id="368. Using Pairwise Occurrence Information to Improve Knowledge Graph Completion on Large-Scale Datasets.">368. Using Pairwise Occurrence Information to Improve Knowledge Graph Completion on Large-Scale Datasets.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1368">Paper Link</a>    Pages:3589-3594</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/164/5636.html">Esma Balkir</a> ; <a href="https://dblp.uni-trier.de/pid/251/9221.html">Masha Naslidnyk</a> ; <a href="https://dblp.uni-trier.de/pid/218/6593.html">Dave Palfrey</a> ; <a href="https://dblp.uni-trier.de/pid/20/6147.html">Arpit Mittal</a></p>
<p>Abstract:
Bilinear models such as DistMult and ComplEx are effective methods for knowledge graph (KG) completion. However, they require large batch sizes, which becomes a performance bottleneck when training on large scale datasets due to memory constraints. In this paper we use occurrences of entity-relation pairs in the dataset to construct a joint learning model and to increase the quality of sampled negatives during training. We show on three standard datasets that when these two techniques are combined, they give a significant improvement in performance, especially when the batch size and the number of generated negative examples are low relative to the size of the dataset. We then apply our techniques to a dataset containing 2 million entities and demonstrate that our model outperforms the baseline by 2.8% absolute on hits@1.</p>
<p>Keywords:</p>
<h3 id="369. Single Training Dimension Selection for Word Embedding with PCA.">369. Single Training Dimension Selection for Word Embedding with PCA.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1369">Paper Link</a>    Pages:3595-3600</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/02/5889.html">Yu Wang</a></p>
<p>Abstract:
In this paper, we present a fast and reliable method based on PCA to select the number of dimensions for word embeddings. First, we train one embedding with a generous upper bound (e.g. 1,000) of dimensions. Then we transform the embeddings using PCA and incrementally remove the lesser dimensions one at a time while recording the embeddings performance on language tasks. Lastly, we select the number of dimensions, balancing model size and accuracy. Experiments using various datasets and language tasks demonstrate that we are able to train about 10 times fewer sets of embeddings while retaining optimal performance. Researchers interested in training the best-performing embeddings for downstream tasks, such as sentiment analysis, question answering and hypernym extraction, as well as those interested in embedding compression should find the method helpful.</p>
<p>Keywords:</p>
<h3 id="370. A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text.">370. A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1370">Paper Link</a>    Pages:3601-3612</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/123/2549.html">Bohan Li</a> ; <a href="https://dblp.uni-trier.de/pid/188/6127.html">Junxian He</a> ; <a href="https://dblp.uni-trier.de/pid/03/8155.html">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pid/22/8160.html">Taylor Berg-Kirkpatrick</a> ; <a href="https://dblp.uni-trier.de/pid/25/1666.html">Yiming Yang</a></p>
<p>Abstract:
When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling.</p>
<p>Keywords:</p>
<h3 id="371. SciBERT: A Pretrained Language Model for Scientific Text.">371. SciBERT: A Pretrained Language Model for Scientific Text.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1371">Paper Link</a>    Pages:3613-3618</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/2026.html">Iz Beltagy</a> ; <a href="https://dblp.uni-trier.de/pid/220/2020.html">Kyle Lo</a> ; <a href="https://dblp.uni-trier.de/pid/160/1727.html">Arman Cohan</a></p>
<p>Abstract:
Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at <a href="https://github.com/allenai/scibert/">https://github.com/allenai/scibert/</a>.</p>
<p>Keywords:</p>
<h3 id="372. Humor Detection: A Transformer Gets the Last Laugh.">372. Humor Detection: A Transformer Gets the Last Laugh.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1372">Paper Link</a>    Pages:3619-3623</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7910.html">Orion Weller</a> ; <a href="https://dblp.uni-trier.de/pid/33/1885.html">Kevin D. Seppi</a></p>
<p>Abstract:
Much previous work has been done in attempting to identify humor in text. In this paper we extend that capability by proposing a new task: assessing whether or not a joke is humorous. We present a novel way of approaching this problem by building a model that learns to identify humorous jokes based on ratings gleaned from Reddit pages, consisting of almost 16,000 labeled instances. Using these ratings to determine the level of humor, we then employ a Transformer architecture for its advantages in learning from sentence context. We demonstrate the effectiveness of this approach and show results that are comparable to human performance. We further demonstrate our models increased capabilities on humor identification problems, such as the previously created datasets for short jokes and puns. These experiments show that this method outperforms all previous work done on these tasks, with an F-measure of 93.1% for the Puns dataset and 98.6% on the Short Jokes dataset.</p>
<p>Keywords:</p>
<h3 id="373. Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training.">373. Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1373">Paper Link</a>    Pages:3624-3629</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/188/8762.html">Alham Fikri Aji</a> ; <a href="https://dblp.uni-trier.de/pid/81/3540.html">Kenneth Heafield</a> ; <a href="https://dblp.uni-trier.de/pid/184/3755.html">Nikolay Bogoychev</a></p>
<p>Abstract:
One way to reduce network traffic in multi-node data-parallel stochastic gradient descent is to only exchange the largest gradients. However, doing so damages the gradient and degrades the models performance. Transformer models degrade dramatically while the impact on RNNs is smaller. We restore gradient quality by combining the compressed global gradient with the nodes locally computed uncompressed gradient. Neural machine translation experiments show that Transformer convergence is restored while RNNs converge faster. With our method, training on 4 nodes converges up to 1.5x as fast as with uncompressed gradients and scales 3.5x relative to single-node training.</p>
<p>Keywords:</p>
<h3 id="374. Small and Practical BERT Models for Sequence Labeling.">374. Small and Practical BERT Models for Sequence Labeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1374">Paper Link</a>    Pages:3630-3634</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/54/1201.html">Henry Tsai</a> ; <a href="https://dblp.uni-trier.de/pid/20/9013.html">Jason Riesa</a> ; <a href="https://dblp.uni-trier.de/pid/186/7972.html">Melvin Johnson</a> ; <a href="https://dblp.uni-trier.de/pid/177/8974.html">Naveen Arivazhagan</a> ; <a href="https://dblp.uni-trier.de/pid/09/1365.html">Xin Li</a> ; <a href="https://dblp.uni-trier.de/pid/248/7804.html">Amelia Archer</a></p>
<p>Abstract:
We propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU. Starting from a public multilingual BERT checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art multilingual baseline. We show that our model especially outperforms on low-resource languages, and works on codemixed input text without being explicitly trained on codemixed examples. We showcase the effectiveness of our method by reporting on part-of-speech tagging and morphological prediction on 70 treebanks and 48 languages.</p>
<p>Keywords:</p>
<h3 id="375. Data Augmentation with Atomic Templates for Spoken Language Understanding.">375. Data Augmentation with Atomic Templates for Spoken Language Understanding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1375">Paper Link</a>    Pages:3635-3641</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/42/2820.html">Zijian Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/160/8144.html">Su Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/197/1322-4.html">Kai Yu</a></p>
<p>Abstract:
Spoken Language Understanding (SLU) converts user utterances into structured semantic representations. Data sparsity is one of the main obstacles of SLU due to the high cost of human annotation, especially when domain changes or a new domain comes. In this work, we propose a data augmentation method with atomic templates for SLU, which involves minimum human efforts. The atomic templates produce exemplars for fine-grained constituents of semantic representations. We propose an encoder-decoder model to generate the whole utterance from atomic exemplars. Moreover, the generator could be transferred from source domains to help a new domain which has little data. Experimental results show that our method achieves significant improvements on DSTC 2&amp;3 dataset which is a domain adaptation setting of SLU.</p>
<p>Keywords:</p>
<h3 id="376. PaLM: A Hybrid Parser and Language Model.">376. PaLM: A Hybrid Parser and Language Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1376">Paper Link</a>    Pages:3642-3649</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/7742.html">Hao Peng</a> ; <a href="https://dblp.uni-trier.de/pid/19/376.html">Roy Schwartz</a> ; <a href="https://dblp.uni-trier.de/pid/90/5204.html">Noah A. Smith</a></p>
<p>Abstract:
We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on language modeling, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving language modeling performance.</p>
<p>Keywords:</p>
<h3 id="377. A Pilot Study for Chinese SQL Semantic Parsing.">377. A Pilot Study for Chinese SQL Semantic Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1377">Paper Link</a>    Pages:3650-3656</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/249/8137.html">Qingkai Min</a> ; <a href="https://dblp.uni-trier.de/pid/249/8110.html">Yuefeng Shi</a> ; <a href="https://dblp.uni-trier.de/pid/47/722-4.html">Yue Zhang</a></p>
<p>Abstract:
The task of semantic parsing is highly useful for dialogue and question answering systems. Many datasets have been proposed to map natural language text into SQL, among which the recent Spider dataset provides cross-domain samples with multiple tables and complex queries. We build a Spider dataset for Chinese, which is currently a low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and word-based encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL.</p>
<p>Keywords:</p>
<h3 id="378. Global Reasoning over Database Structures for Text-to-SQL Parsing.">378. Global Reasoning over Database Structures for Text-to-SQL Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1378">Paper Link</a>    Pages:3657-3662</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/202/2034.html">Ben Bogin</a> ; <a href="https://dblp.uni-trier.de/pid/00/8046.html">Matt Gardner</a> ; <a href="https://dblp.uni-trier.de/pid/31/8178.html">Jonathan Berant</a></p>
<p>Abstract:
State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zero-shot), the parser often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. %since their decisions are based on weak, local information only. In this work, we propose a semantic parser that globally reasons about the structure of the output query to make a more contextually-informed selection of database constants. We use message-passing through a graph neural network to softly select a subset of database constants for the output query, conditioned on the question. Moreover, we train a model to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-of-the-art model for Spider, a zero-shot semantic parsing dataset with complex databases, increasing accuracy from 39.4% to 47.4%.</p>
<p>Keywords:</p>
<h3 id="379. Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis.">379. Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1379">Paper Link</a>    Pages:3663-3669</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/148/4520.html">Hiroki Ouchi</a> ; <a href="https://dblp.uni-trier.de/pid/78/6923.html">Jun Suzuki</a> ; <a href="https://dblp.uni-trier.de/pid/90/3315.html">Kentaro Inui</a></p>
<p>Abstract:
In transductive learning, an unlabeled test set is used for model training. Although this setting deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, wherein the texts to be processed are known in advance. However, despite its practical advantages, transductive learning is underexplored in natural language processing. Here we conduct an empirical study of transductive learning for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-of-the-art neural models in in-domain and out-of-domain settings.</p>
<p>Keywords:</p>
<h3 id="380. Efficient Sentence Embedding using Discrete Cosine Transform.">380. Efficient Sentence Embedding using Discrete Cosine Transform.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1380">Paper Link</a>    Pages:3670-3676</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/184/8722.html">Nada AlMarwani</a> ; <a href="https://dblp.uni-trier.de/pid/186/7198.html">Hanan Aldarmaki</a> ; <a href="https://dblp.uni-trier.de/pid/15/4305.html">Mona T. Diab</a></p>
<p>Abstract:
Vector averaging remains one of the most popular sentence embedding methods in spite of its obvious disregard for syntactic structure. While more complex sequential or convolutional networks potentially yield superior classification performance, the improvements in classification accuracy are typically mediocre compared to the simple vector averaging. As an efficient alternative, we propose the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving manner. The lower order DCT coefficients represent the overall feature patterns in sentences, which results in suitable embeddings for tasks that could benefit from syntactic features. Our results in semantic probing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of DCT to preserve word order information.</p>
<p>Keywords:</p>
<h3 id="381. A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection.">381. A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1381">Paper Link</a>    Pages:3677-3684</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/117/2869.html">Kurt Junshean Espinosa</a> ; <a href="https://dblp.uni-trier.de/pid/29/456.html">Makoto Miwa</a> ; <a href="https://dblp.uni-trier.de/pid/47/4142.html">Sophia Ananiadou</a></p>
<p>Abstract:
We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute events, from the relation graph. We define actions to construct events and use all the beams in a beam search to detect all event structures that may be overlapping and nested. The search process constructs events in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using neural networks. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our model is more computationally efficient while yielding higher F1-score performance.</p>
<p>Keywords:</p>
<h3 id="382. PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification.">382. PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1382">Paper Link</a>    Pages:3685-3690</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/117/4082.html">Yinfei Yang</a> ; <a href="https://dblp.uni-trier.de/pid/48/2168.html">Yuan Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/188/6414.html">Chris Tar</a> ; <a href="https://dblp.uni-trier.de/pid/90/6617.html">Jason Baldridge</a></p>
<p>Abstract:
Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23% over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.</p>
<p>Keywords:</p>
<h3 id="383. Pretrained Language Models for Sequential Sentence Classification.">383. Pretrained Language Models for Sequential Sentence Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1383">Paper Link</a>    Pages:3691-3697</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/160/1727.html">Arman Cohan</a> ; <a href="https://dblp.uni-trier.de/pid/220/2026.html">Iz Beltagy</a> ; <a href="https://dblp.uni-trier.de/pid/98/11100.html">Daniel King</a> ; <a href="https://dblp.uni-trier.de/pid/78/6527.html">Bhavana Dalvi</a> ; <a href="https://dblp.uni-trier.de/pid/w/DanielSWeld.html">Daniel S. Weld</a></p>
<p>Abstract:
As a step toward better document-level understanding, we explore classification of a sequence of sentences into their corresponding categories, a task that requires understanding sentences in context of the document. Recent successful models for this task have used hierarchical models to contextualize sentence representations, and Conditional Random Fields (CRFs) to incorporate dependencies between subsequent labels. In this work, we show that pretrained language models, BERT (Devlin et al., 2018) in particular, can be used for this task to capture contextual dependencies without the need for hierarchical encoding nor a CRF. Specifically, we construct a joint sentence representation that allows BERT Transformer layers to directly utilize contextual information from all words in all sentences. Our approach achieves state-of-the-art results on four datasets, including a new dataset of structured scientific abstracts.</p>
<p>Keywords:</p>
<h3 id="384. Emergent Linguistic Phenomena in Multi-Agent Communication Games.">384. Emergent Linguistic Phenomena in Multi-Agent Communication Games.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1384">Paper Link</a>    Pages:3698-3708</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/211/7071.html">Laura Graesser</a> ; <a href="https://dblp.uni-trier.de/pid/41/9736.html">Kyunghyun Cho</a> ; <a href="https://dblp.uni-trier.de/pid/136/9140.html">Douwe Kiela</a></p>
<p>Abstract:
We describe a multi-agent communication framework for examining high-level linguistic phenomena at the community-level. We demonstrate that complex linguistic behavior observed in natural language can be reproduced in this simple setting: i) the outcome of contact between communities is a function of inter- and intra-group connectivity; ii) linguistic contact either converges to the majority protocol, or in balanced cases leads to novel creole languages of lower complexity; and iii) a linguistic continuum emerges where neighboring languages are more mutually intelligible than farther removed languages. We conclude that at least some of the intricate properties of language evolution need not depend on complex evolved linguistic capabilities, but can emerge from simple social exchanges between perceptually-enabled agents playing communication games.</p>
<p>Keywords:</p>
<h3 id="385. TalkDown: A Corpus for Condescension Detection in Context.">385. TalkDown: A Corpus for Condescension Detection in Context.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1385">Paper Link</a>    Pages:3709-3717</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/03/4540-2.html">Zijian Wang</a> ; <a href="https://dblp.uni-trier.de/pid/13/2617.html">Christopher Potts</a></p>
<p>Abstract:
Condescending language use is caustic; it can bring dialogues to an end and bifurcate communities. Thus, systems for condescension detection could have a large positive impact. A challenge here is that condescension is often impossible to detect from isolated utterances, as it depends on the discourse and social context. To address this, we present TalkDown, a new labeled dataset of condescending linguistic acts in context. We show that extending a language-only model with representations of the discourse improves performance, and we motivate techniques for dealing with the low rates of condescension overall. We also use our model to estimate condescension rates in various online communities and relate these differences to differing community norms.</p>
<p>Keywords:</p>
<h3 id="386. Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization.">386. Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1386">Paper Link</a>    Pages:3718-3727</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/222/9395.html">Daniel Deutsch</a> ; <a href="https://dblp.uni-trier.de/pid/r/DanRoth.html">Dan Roth</a></p>
<p>Abstract:
A key challenge in topic-focused summarization is determining what information should be included in the summary, a problem known as content selection. In this work, we propose a new method for studying content selection in topic-focused summarization called the summary cloze task. The goal of the summary cloze task is to generate the next sentence of a summary conditioned on the beginning of the summary, a topic, and a reference document(s). The main challenge is deciding what information in the references is relevant to the topic and partial summary and should be included in the summary. Although the cloze task does not address all aspects of the traditional summarization problem, the more narrow scope of the task allows us to collect a large-scale datset of nearly 500k summary cloze instances from Wikipedia. We report experimental results on this new dataset using various extractive models and a two-step abstractive model that first extractively selects a small number of sentences and then abstractively summarizes them. Our results show that the topic and partial summary help the models identify relevant content, but the task remains a significant challenge.</p>
<p>Keywords:</p>
<h3 id="387. Text Summarization with Pretrained Encoders.">387. Text Summarization with Pretrained Encoders.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1387">Paper Link</a>    Pages:3728-3738</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/51/3710-124.html">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/59/6701.html">Mirella Lapata</a></p>
<p>Abstract:
Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.</p>
<p>Keywords:</p>
<h3 id="388. How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing.">388. How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1388">Paper Link</a>    Pages:3739-3749</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/85/7967.html">Shen Gao</a> ; <a href="https://dblp.uni-trier.de/pid/33/11343.html">Xiuying Chen</a> ; <a href="https://dblp.uni-trier.de/pid/77/8278.html">Piji Li</a> ; <a href="https://dblp.uni-trier.de/pid/224/5573.html">Zhangming Chan</a> ; <a href="https://dblp.uni-trier.de/pid/63/1870.html">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/19/2405-1.html">Rui Yan</a></p>
<p>Abstract:
Under special circumstances, summaries should conform to a particular style with patterns, such as court judgments and abstracts in academic papers. To this end, the prototype document-summary pairs can be utilized to generate better summaries. There are two main challenges in this task: (1) the model needs to incorporate learned patterns from the prototype, but (2) should avoid copying contents other than the patternized wordssuch as irrelevant factsinto the generated summaries. To tackle these challenges, we design a model named Prototype Editing based Summary Generator (PESG). PESG first learns summary patterns and prototype facts by analyzing the correlation between a prototype document and its summary. Prototype facts are then utilized to help extract facts from the input document. Next, an editing generator generates new summary based on the summary pattern or extracted facts. Finally, to address the second challenge, a fact checker is used to estimate mutual information between the input document and generated summary, providing an additional signal for the generator. Extensive experiments conducted on a large-scale real-world text summarization dataset show that PESG achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.</p>
<p>Keywords:</p>
<h3 id="389. BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle.">389. BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1389">Paper Link</a>    Pages:3750-3759</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/179/4587.html">Peter West</a> ; <a href="https://dblp.uni-trier.de/pid/205/9029.html">Ari Holtzman</a> ; <a href="https://dblp.uni-trier.de/pid/157/2104.html">Jan Buys</a> ; <a href="https://dblp.uni-trier.de/pid/89/579.html">Yejin Choi</a></p>
<p>Abstract:
The principle of the Information Bottleneck (Tishby et al., 1999) produces a summary of information X optimized to predict some other relevant information Y. In this paper, we propose a novel approach to unsupervised sentence summarization by mapping the Information Bottleneck principle to a conditional language modelling objective: given a sentence, our approach seeks a compressed sentence that can best predict the next sentence. Our iterative algorithm under the Information Bottleneck objective searches gradually shorter subsequences of the given sentence while maximizing the probability of the next sentence conditioned on the summary. Using only pretrained language models with no direct supervision, our approach can efficiently perform extractive sentence summarization over a large corpus. Building on our unsupervised extractive summarization, we also present a new approach to self-supervised abstractive summarization, where a transformer-based language model is trained on the output summaries of our unsupervised method. Empirical results demonstrate that our extractive method outperforms other unsupervised models on multiple automatic metrics. In addition, we find that our self-supervised abstractive model outperforms unsupervised baselines (including our own) by human evaluation along multiple attributes.</p>
<p>Keywords:</p>
<h3 id="390. Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator.">390. Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1390">Paper Link</a>    Pages:3760-3771</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/85/7634-1.html">Xiaoyu Shen</a> ; <a href="https://dblp.uni-trier.de/pid/50/2082.html">Yang Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/89/1838.html">Hui Su</a> ; <a href="https://dblp.uni-trier.de/pid/00/1846.html">Dietrich Klakow</a></p>
<p>Abstract:
Pointer Generators have been the de facto standard for modern summarization systems. However, this architecture faces two major drawbacks: Firstly, the pointer is limited to copying the exact words while ignoring possible inflections or abstractions, which restricts its power of capturing richer latent alignment. Secondly, the copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text. In this paper, we address these problems by allowing the model to edit pointed tokens instead of always hard copying them. The editing is performed by transforming the pointed word vector into a target space with a learned relation embedding. On three large-scale summarization dataset, we show the model is able to (1) capture more latent alignment relations than exact word matches, (2) improve word alignment accuracy, allowing for better model interpretation and controlling, (3) generate higher-quality summaries validated by both qualitative and quantitative evaluations and (4) bring more abstraction to the generated summaries.</p>
<p>Keywords:</p>
<h3 id="391. Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs.">391. Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1391">Paper Link</a>    Pages:3772-3783</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/7334.html">Bailin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/08/5391.html">Ivan Titov</a> ; <a href="https://dblp.uni-trier.de/pid/59/6701.html">Mirella Lapata</a></p>
<p>Abstract:
Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a denotation. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating programs as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an inductive bias in the parser to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain structural constraints were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial abstract program and (2) refining it while modeling structured alignments with differential dynamic programming. We obtain state-of-the-art performance on the WikiTableQuestions and WikiSQL datasets. When compared to a standard attention baseline, we observe that the proposed structured-alignment mechanism is highly beneficial.</p>
<p>Keywords:</p>
<h3 id="392. Broad-Coverage Semantic Parsing as Transduction.">392. Broad-Coverage Semantic Parsing as Transduction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1392">Paper Link</a>    Pages:3784-3796</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/6137-12.html">Sheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/163/2004.html">Xutai Ma</a> ; <a href="https://dblp.uni-trier.de/pid/58/3217.html">Kevin Duh</a> ; <a href="https://dblp.uni-trier.de/pid/06/4775.html">Benjamin Van Durme</a></p>
<p>Abstract:
We unify different broad-coverage semantic parsing tasks into a transduction parsing paradigm, and propose an attention-based neural transducer that incrementally builds meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the neural transducer can be effectively trained without relying on a pre-trained aligner. Experiments separately conducted on three broad-coverage semantic parsing tasks  AMR, SDP and UCCA  demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.</p>
<p>Keywords:</p>
<h3 id="393. Core Semantic First: A Top-down Approach for AMR Parsing.">393. Core Semantic First: A Top-down Approach for AMR Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1393">Paper Link</a>    Pages:3797-3807</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/c/DCai.html">Deng Cai</a> ; <a href="https://dblp.uni-trier.de/pid/48/1707.html">Wai Lam</a></p>
<p>Abstract:
We introduce a novel scheme for parsing a piece of text into its Abstract Meaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel characteristic of GSP is that it constructs a parse graph incrementally in a top-down fashion. Starting from the root, at each step, a new node and its connections to existing nodes will be jointly predicted. The output graph spans the nodes by the distance to the root, following the intuition of first grasping the main ideas then digging into more details. The core semantic first principle emphasizes capturing the main ideas of a sentence, which is of great interest. We evaluate our model on the latest AMR sembank and achieve the state-of-the-art performance in the sense that no heuristic graph re-categorization is adopted. More importantly, the experiments show that our parser is especially good at obtaining the core semantics.</p>
<p>Keywords:</p>
<h3 id="394. Don't paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing.">394. Don't paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1394">Paper Link</a>    Pages:3808-3818</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/133/3687.html">Jonathan Herzig</a> ; <a href="https://dblp.uni-trier.de/pid/31/8178.html">Jonathan Berant</a></p>
<p>Abstract:
A major hurdle on the road to conversational interfaces is the difficulty in collecting data that maps language utterances to logical forms. One prominent approach for data collection has been to automatically generate pseudo-language paired with logical forms, and paraphrase the pseudo-language to natural language through crowdsourcing (Wang et al., 2015). However, this data collection procedure often leads to low performance on real data, due to a mismatch between the true distribution of examples and the distribution induced by the data collection procedure. In this paper, we thoroughly analyze two sources of mismatch in this process: the mismatch in logical form distribution and the mismatch in language distribution between the true and induced distributions. We quantify the effects of these mismatches, and propose a new data collection approach that mitigates them. Assuming access to unlabeled utterances from the true distribution, we combine crowdsourcing with a paraphrase model to detect correct logical forms for the unlabeled utterances. On two datasets, our method leads to 70.6 accuracy on average on the true distribution, compared to 51.3 in paraphrasing-based data collection.</p>
<p>Keywords:</p>
<h3 id="395. Improving Distantly-Supervised Relation Extraction with Joint Label Embedding.">395. Improving Distantly-Supervised Relation Extraction with Joint Label Embedding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1395">Paper Link</a>    Pages:3819-3827</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/141/4440.html">Linmei Hu</a> ; <a href="https://dblp.uni-trier.de/pid/254/8164.html">Luhao Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/64/3041.html">Chuan Shi</a> ; <a href="https://dblp.uni-trier.de/pid/92/8277.html">Liqiang Nie</a> ; <a href="https://dblp.uni-trier.de/pid/236/2820.html">Weili Guan</a> ; <a href="https://dblp.uni-trier.de/pid/49/1457.html">Cheng Yang</a></p>
<p>Abstract:
Distantly-supervised relation extraction has proven to be effective to find relational facts from texts. However, the existing approaches treat labels as independent and meaningless one-hot vectors, which cause a loss of potential label information for selecting valid instances. In this paper, we propose a novel multi-layer attention-based model to improve relation extraction with joint label embedding. The model makes full use of both structural information from Knowledge Graphs and textual information from entity descriptions to learn label embeddings through gating integration while avoiding the imposed noise with an attention mechanism. Then the learned label embeddings are used as another atten- tion over the instances (whose embeddings are also enhanced with the entity descriptions) for improving relation extraction. Extensive experiments demonstrate that our model significantly outperforms state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="396. Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network.">396. Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1396">Paper Link</a>    Pages:3828-3838</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8270.html">Dianbo Sui</a> ; <a href="https://dblp.uni-trier.de/pid/90/7879.html">Yubo Chen</a> ; <a href="https://dblp.uni-trier.de/pid/42/4903.html">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/47/2026-1.html">Jun Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/21/5679.html">Shengping Liu</a></p>
<p>Abstract:
The lack of word boundaries information has been seen as one of the main obstacles to develop a high performance Chinese named entity recognition (NER) system. Fortunately, the automatically constructed lexicon contains rich word boundaries information and word semantic information. However, integrating lexical knowledge in Chinese NER tasks still faces challenges when it comes to self-matched lexical words as well as the nearest contextual lexical words. We present a Collaborative Graph Network to solve these challenges. Experiments on various datasets show that our model not only outperforms the state-of-the-art (SOTA) results, but also achieves a speed that is six to fifteen times faster than that of the SOTA model.</p>
<p>Keywords:</p>
<h3 id="397. Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction.">397. Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1397">Paper Link</a>    Pages:3839-3848</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/239/5731.html">Qinyuan Ye</a> ; <a href="https://dblp.uni-trier.de/pid/06/1624.html">Liyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/96/1986.html">Maosen Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/36/360.html">Xiang Ren</a></p>
<p>Abstract:
In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models cannot automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates models learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23% relative F1 improvement, which verifies our assumptions. Our code and data can be found at <a href="https://github.com/INK-USC/shifted-label-distribution">https://github.com/INK-USC/shifted-label-distribution</a>.</p>
<p>Keywords:</p>
<h3 id="398. Easy First Relation Extraction with Information Redundancy.">398. Easy First Relation Extraction with Information Redundancy.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1398">Paper Link</a>    Pages:3849-3859</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/35/6569.html">Shuai Ma</a> ; <a href="https://dblp.uni-trier.de/pid/71/4292.html">Gang Wang</a> ; <a href="https://dblp.uni-trier.de/pid/25/2643.html">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pid/58/1722.html">Jinpeng Huai</a></p>
<p>Abstract:
Many existing relation extraction (RE) models make decisions globally using integer linear programming (ILP). However, it is nontrivial to make use of integer linear programming as a blackbox solver for RE. Its cost of time and memory may become unacceptable with the increase of data scale, and redundant information needs to be encoded cautiously for ILP. In this paper, we propose an easy first approach for relation extraction with information redundancies, embedded in the results produced by local sentence level extractors, during which conflict decisions are resolved with domain and uniqueness constraints. Information redundancies are leveraged to support both easy first collective inference for easy decisions in the first stage and ILP for hard decisions in a subsequent stage. Experimental study shows that our approach improves the efficiency and accuracy of RE, and outperforms both ILP and neural network-based methods.</p>
<p>Keywords:</p>
<h3 id="399. Dependency-Guided LSTM-CRF for Named Entity Recognition.">399. Dependency-Guided LSTM-CRF for Named Entity Recognition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1399">Paper Link</a>    Pages:3860-3870</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/151/8425.html">Zhanming Jie</a> ; <a href="https://dblp.uni-trier.de/pid/98/6613-11.html">Wei Lu</a></p>
<p>Abstract:
Dependency tree structures capture long-distance and syntactic relationships between words in a sentence. The syntactic relations (e.g., nominal subject, object) can potentially infer the existence of certain named entities. In addition, the performance of a named entity recognizer could benefit from the long-distance dependencies between the words in dependency trees. In this work, we propose a simple yet effective dependency-guided LSTM-CRF model to encode the complete dependency trees and capture the above properties for the task of named entity recognition (NER). The data statistics show strong correlations between the entity types and dependency relations. We conduct extensive experiments on several standard datasets and demonstrate the effectiveness of the proposed model in improving NER and achieving state-of-the-art performance. Our analysis reveals that the significant improvements mainly result from the dependency relations and long-distance interactions provided by dependency trees.</p>
<p>Keywords:</p>
<h3 id="400. Cross-Cultural Transfer Learning for Text Classification.">400. Cross-Cultural Transfer Learning for Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1400">Paper Link</a>    Pages:3871-3881</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8141.html">Dor Ringel</a> ; <a href="https://dblp.uni-trier.de/pid/42/4440.html">Gal Lavee</a> ; <a href="https://dblp.uni-trier.de/pid/46/650.html">Ido Guy</a> ; <a href="https://dblp.uni-trier.de/pid/08/6560.html">Kira Radinsky</a></p>
<p>Abstract:
Large training datasets are required to achieve competitive performance in most natural language tasks. The acquisition process for these datasets is labor intensive, expensive, and time consuming. This process is also prone to human errors. In this work, we show that cross-cultural differences can be harnessed for natural language text classification. We present a transfer-learning framework that leverages widely-available unaligned bilingual corpora for classification tasks, using no task-specific data. Our empirical evaluation on two tasks  formality classification and sarcasm detection  shows that the cross-cultural difference between German and American English, as manifested in product review text, can be applied to achieve good performance for formality classification, while the difference between Japanese and American English can be applied to achieve good performance for sarcasm detection  both without any task-specific labeled data.</p>
<p>Keywords:</p>
<h3 id="401. Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification.">401. Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1401">Paper Link</a>    Pages:3882-3891</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/15/2488.html">Oren Melamud</a> ; <a href="https://dblp.uni-trier.de/pid/45/6755.html">Mihaela A. Bornea</a> ; <a href="https://dblp.uni-trier.de/pid/b/KenBarker2.html">Ken Barker</a></p>
<p>Abstract:
Supervised learning models often perform poorly at low-shot tasks, i.e. tasks for which little labeled data is available for training. One prominent approach for improving low-shot learning is to use unsupervised pre-trained neural models. Another approach is to obtain richer supervision by collecting annotator rationales (explanations supporting label annotations). In this work, we combine these two approaches to improve low-shot text classification with two novel methods: a simple bag-of-words embedding approach; and a more complex context-aware method, based on the BERT model. In experiments with two English text classification datasets, we demonstrate substantial performance gains from combining pre-training with rationales. Furthermore, our investigation of a range of train-set sizes reveals that the simple bag-of-words approach is the clear top performer when there are only a few dozen training instances or less, while more complex models, such as BERT or CNN, require more training data to shine.</p>
<p>Keywords:</p>
<h3 id="402. ProSeqo: Projection Sequence Networks for On-Device Text Classification.">402. ProSeqo: Projection Sequence Networks for On-Device Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1402">Paper Link</a>    Pages:3892-3901</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/63/5321.html">Zornitsa Kozareva</a> ; <a href="https://dblp.uni-trier.de/pid/39/5111.html">Sujith Ravi</a></p>
<p>Abstract:
We propose a novel on-device sequence model for text classification using recurrent projections. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks. We conducted exhaustive evaluation on multiple text classification tasks. Results show that ProSeqo outperformed state-of-the-art neural and on-device approaches for short text classification tasks such as dialog act and intent prediction. To the best of our knowledge, ProSeqo is the first on-device long text classification neural model. It achieved comparable results to previous neural approaches for news article, answers and product categorization, while preserving small memory footprint and maintaining high accuracy.</p>
<p>Keywords:</p>
<h3 id="403. Induction Networks for Few-Shot Text Classification.">403. Induction Networks for Few-Shot Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1403">Paper Link</a>    Pages:3902-3911</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/197/1625.html">Ruiying Geng</a> ; <a href="https://dblp.uni-trier.de/pid/236/5662.html">Binhua Li</a> ; <a href="https://dblp.uni-trier.de/pid/16/4349.html">Yongbin Li</a> ; <a href="https://dblp.uni-trier.de/pid/93/310.html">Xiaodan Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/64/8451.html">Ping Jian</a> ; <a href="https://dblp.uni-trier.de/pid/68/4942.html">Jian Sun</a></p>
<p>Abstract:
Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes. In such challenging scenarios, recent studies have used meta-learning to simulate the few-shot task, in which new queries are compared to a small support set at the sample-wise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in meta-learning. In this way, we find the model is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification.</p>
<p>Keywords:</p>
<h3 id="404. Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach.">404. Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1404">Paper Link</a>    Pages:3912-3921</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/117/7310-1.html">Wenpeng Yin</a> ; <a href="https://dblp.uni-trier.de/pid/248/7545.html">Jamaal Hay</a> ; <a href="https://dblp.uni-trier.de/pid/r/DanRoth.html">Dan Roth</a></p>
<p>Abstract:
Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects: the topic aspect includes sports and politics as labels; the emotion aspect includes joy and anger; the situation aspect includes medical assistance and water shortage. ii) We extend the existing evaluation setup (label-partially-unseen)  given a dataset, train on some labels, test on all labels  to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way.</p>
<p>Keywords:</p>
<h3 id="405. A Logic-Driven Framework for Consistency of Neural Models.">405. A Logic-Driven Framework for Consistency of Neural Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1405">Paper Link</a>    Pages:3922-3933</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/75/4601.html">Tao Li</a> ; <a href="https://dblp.uni-trier.de/pid/71/5332-1.html">Vivek Gupta</a> ; <a href="https://dblp.uni-trier.de/pid/218/5511.html">Maitrey Mehta</a> ; <a href="https://dblp.uni-trier.de/pid/37/44.html">Vivek Srikumar</a></p>
<p>Abstract:
While neural models show remarkable accuracy on individual predictions, their internal beliefs can be inconsistent across examples. In this paper, we formalize such inconsistency as a generalization of prediction error. We propose a learning framework for constraining models using logic rules to regularize them away from inconsistency. Our framework can leverage both labeled and unlabeled examples and is directly compatible with off-the-shelf learning schemes without model redesign. We instantiate our framework on natural language inference, where experiments show that enforcing invariants stated in logic can help make the predictions of neural models both accurate and consistent.</p>
<p>Keywords:</p>
<h3 id="406. Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites.">406. Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1406">Paper Link</a>    Pages:3934-3943</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/82/8978.html">Alexey Tikhonov</a> ; <a href="https://dblp.uni-trier.de/pid/247/6363.html">Viacheslav Shibaev</a> ; <a href="https://dblp.uni-trier.de/pid/247/6442.html">Aleksander Nagaev</a> ; <a href="https://dblp.uni-trier.de/pid/230/8047.html">Aigul Nugmanova</a> ; <a href="https://dblp.uni-trier.de/pid/178/9094.html">Ivan P. Yamshchikov</a></p>
<p>Abstract:
This paper shows that standard assessment methodology for style transfer has several significant problems. First, the standard metrics for style accuracy and semantics preservation vary significantly on different re-runs. Therefore one has to report error margins for the obtained results. Second, starting with certain values of bilingual evaluation understudy (BLEU) between input and output and accuracy of the sentiment transfer the optimization of these two standard metrics diverge from the intuitive goal of the style transfer task. Finally, due to the nature of the task itself, there is a specific dependence between these two metrics that could be easily manipulated. Under these circumstances, we suggest taking BLEU between input and human-written reformulations into consideration for benchmarks. We also propose three new architectures that outperform state of the art in terms of this metric.</p>
<p>Keywords:</p>
<h3 id="407. Implicit Deep Latent Variable Models for Text Generation.">407. Implicit Deep Latent Variable Models for Text Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1407">Paper Link</a>    Pages:3944-3954</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/08/7426.html">Le Fang</a> ; <a href="https://dblp.uni-trier.de/pid/64/9590.html">Chunyuan Li</a> ; <a href="https://dblp.uni-trier.de/pid/92/5339.html">Jianfeng Gao</a> ; <a href="https://dblp.uni-trier.de/pid/84/3520-1.html">Wen Dong</a> ; <a href="https://dblp.uni-trier.de/pid/65/2802.html">Changyou Chen</a></p>
<p>Abstract:
Deep latent variable models (LVM) such as variational auto-encoder (VAE) have recently played an important role in text generation. One key factor is the exploitation of smooth latent structures to guide the generation. However, the representation power of VAEs is limited due to two reasons: (1) the Gaussian assumption is often made on the variational posteriors; and meanwhile (2) a notorious posterior collapse issue occurs. In this paper, we advocate sample-based representations of variational distributions for natural language, leading to implicit latent features, which can provide flexible representation power compared with Gaussian-based posteriors. We further develop an LVM to directly match the aggregated posterior to the prior. It can be viewed as a natural extension of VAEs with a regularization of maximizing mutual information, mitigating the posterior collapse issue. We demonstrate the effectiveness and versatility of our models in various text generation scenarios, including language modeling, unaligned style transfer, and dialog response generation. The source code to reproduce our experimental results is available on GitHub.</p>
<p>Keywords:</p>
<h3 id="408. Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach.">408. Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1408">Paper Link</a>    Pages:3955-3965</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/145/6090.html">Zhenjie Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/06/4530.html">Xiaojuan Ma</a></p>
<p>Abstract:
Text emotion distribution learning (EDL) aims to develop models that can predict the intensity values of a sentence across a set of emotion categories. Existing methods based on supervised learning require a large amount of well-labelled training data, which is difficult to obtain due to inconsistent perception of fine-grained emotion intensity. In this paper, we propose a meta-learning approach to learn text emotion distributions from a small sample. Specifically, we propose to learn low-rank sentence embeddings by tensor decomposition to capture their contextual semantic similarity, and use K-nearest neighbors (KNNs) of each sentence in the embedding space to generate sample clusters. We then train a meta-learner that can adapt to new data with only a few training samples on the clusters, and further fit the meta-learner on KNNs of a testing sample for EDL. In this way, we effectively augment the learning ability of a model on the small sample. To demonstrate the performance, we compare the proposed approach with state-of-the-art EDL methods on a widely used EDL dataset: SemEval 2007 Task 14 (Strapparava and Mihalcea, 2007). Results show the superiority of our method on small-sample emotion distribution learning.</p>
<p>Keywords:</p>
<h3 id="409. Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.">409. Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1409">Paper Link</a>    Pages:3966-3979</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/150/5069.html">Cristina Garbacea</a> ; <a href="https://dblp.uni-trier.de/pid/04/11301.html">Samuel Carton</a> ; <a href="https://dblp.uni-trier.de/pid/162/9067.html">Shiyan Yan</a> ; <a href="https://dblp.uni-trier.de/pid/30/5059.html">Qiaozhu Mei</a></p>
<p>Abstract:
We conduct a large-scale, systematic study to evaluate the existing evaluation methods for natural language generation in the context of generating online product reviews. We compare human-based evaluators with a variety of automated evaluation procedures, including discriminative evaluators that measure how well machine-generated text can be distinguished from human-written text, as well as word overlap metrics that assess how similar the generated text compares to human-written references. We determine to what extent these different evaluators agree on the ranking of a dozen of state-of-the-art generators for online product reviews. We find that human evaluators do not correlate well with discriminative evaluators, leaving a bigger question of whether adversarial accuracy is the correct objective for natural language generation. In general, distinguishing machine-generated text is challenging even for human evaluators, and human decisions correlate better with lexical overlaps. We find lexical diversity an intriguing metric that is indicative of the assessments of different evaluators. A post-experiment survey of participants provides insights into how to evaluate and improve the quality of natural language generation systems.</p>
<p>Keywords:</p>
<h3 id="410. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.">410. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1410">Paper Link</a>    Pages:3980-3990</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/04/7936.html">Nils Reimers</a> ; <a href="https://dblp.uni-trier.de/pid/85/6201.html">Iryna Gurevych</a></p>
<p>Abstract:
BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.</p>
<p>Keywords:</p>
<h3 id="411. Learning Only from Relevant Keywords and Unlabeled Documents.">411. Learning Only from Relevant Keywords and Unlabeled Documents.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1411">Paper Link</a>    Pages:3991-4000</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/3074.html">Nontawat Charoenphakdee</a> ; <a href="https://dblp.uni-trier.de/pid/234/8949.html">Jongyeong Lee</a> ; <a href="https://dblp.uni-trier.de/pid/188/9223.html">Yiping Jin</a> ; <a href="https://dblp.uni-trier.de/pid/01/8372.html">Dittaya Wanvarie</a> ; <a href="https://dblp.uni-trier.de/pid/35/1228.html">Masashi Sugiyama</a></p>
<p>Abstract:
We consider a document classification problem where document labels are absent but only relevant keywords of a target class and unlabeled documents are given. Although heuristic methods based on pseudo-labeling have been considered, theoretical understanding of this problem has still been limited. Moreover, previous methods cannot easily incorporate well-developed techniques in supervised text classification. In this paper, we propose a theoretically guaranteed learning framework that is simple to implement and has flexible choices of models, e.g., linear models or neural networks. We demonstrate how to optimize the area under the receiver operating characteristic curve (AUC) effectively and also discuss how to adjust it to optimize other well-known evaluation metrics such as the accuracy and F1-measure. Finally, we show the effectiveness of our framework using benchmark datasets.</p>
<p>Keywords:</p>
<h3 id="412. Denoising based Sequence-to-Sequence Pre-training for Text Generation.">412. Denoising based Sequence-to-Sequence Pre-training for Text Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1412">Paper Link</a>    Pages:4001-4013</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/56/4499.html">Liang Wang</a> ; <a href="https://dblp.uni-trier.de/pid/z/WeiZhao.html">Wei Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/132/4543.html">Ruoyu Jia</a> ; <a href="https://dblp.uni-trier.de/pid/05/4288.html">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pid/218/9980.html">Jingming Liu</a></p>
<p>Abstract:
This paper presents a new sequence-to-sequence (seq2seq) pre-training method PoDA (Pre-training of Denoising Autoencoders), which learns representations suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pre-trains both the encoder and decoder by denoising the noise-corrupted text, and it also has the advantage of keeping the network architecture unchanged in the subsequent fine-tuning stage. Meanwhile, we design a hybrid model of Transformer and pointer-generator networks as the backbone architecture for PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence.</p>
<p>Keywords:</p>
<h3 id="413. Dialog Intent Induction with Deep Multi-View Clustering.">413. Dialog Intent Induction with Deep Multi-View Clustering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1413">Paper Link</a>    Pages:4014-4023</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/136/5700.html">Hugh Perkins</a> ; <a href="https://dblp.uni-trier.de/pid/33/4854.html">Yi Yang</a></p>
<p>Abstract:
We introduce the dialog intent induction task and present a novel deep multi-view clustering approach to tackle the problem. Dialog intent induction aims at discovering user intents from user query utterances in human-human conversations such as dialogs between customer support agents and customers. Motivated by the intuition that a dialog intent is not only expressed in the user query utterance but also captured in the rest of the dialog, we split a conversation into two independent views and exploit multi-view clustering techniques for inducing the dialog intent. In par- ticular, we propose alternating-view k-means (AV-KMEANS) for joint multi-view represen- tation learning and clustering analysis. The key innovation is that the instance-view representations are updated iteratively by predicting the cluster assignment obtained from the alternative view, so that the multi-view representations of the instances lead to similar cluster assignments. Experiments on two public datasets show that AV-KMEANS can induce better dialog intent clusters than state-of-the-art unsupervised representation learning methods and standard multi-view clustering approaches.</p>
<p>Keywords:</p>
<h3 id="414. Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction.">414. Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1414">Paper Link</a>    Pages:4024-4034</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/117/4904.html">Sahil Garg</a> ; <a href="https://dblp.uni-trier.de/pid/16/3411.html">Aram Galstyan</a> ; <a href="https://dblp.uni-trier.de/pid/82/9058.html">Greg Ver Steeg</a> ; <a href="https://dblp.uni-trier.de/pid/79/6762.html">Guillermo A. Cecchi</a></p>
<p>Abstract:
Recently, kernelized locality sensitive hashcodes have been successfully employed as representations of natural language text, especially showing high relevance to biomedical relation extraction tasks. In this paper, we propose to optimize the hashcode representations in a nearly unsupervised manner, in which we only use data points, but not their class labels, for learning. The optimized hashcode representations are then fed to a supervised classifi er following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function, which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks, obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.</p>
<p>Keywords:</p>
<h3 id="415. Auditing Deep Learning processes through Kernel-based Explanatory Models.">415. Auditing Deep Learning processes through Kernel-based Explanatory Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1415">Paper Link</a>    Pages:4035-4044</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/38/2048.html">Danilo Croce</a> ; <a href="https://dblp.uni-trier.de/pid/231/8108.html">Daniele Rossini</a> ; <a href="https://dblp.uni-trier.de/pid/79/2000.html">Roberto Basili</a></p>
<p>Abstract:
While NLP systems become more pervasive, their accountability gains value as a focal point of effort. Epistemological opaqueness of nonlinear learning methods, such as deep learning models, can be a major drawback for their adoptions. In this paper, we discuss the application of Layerwise Relevance Propagation over a linguistically motivated neural architecture, the Kernel-based Deep Architecture, in order to trace back connections between linguistic properties of input instances and system decisions. Such connections then guide the construction of argumentations on networks inferences, i.e., explanations based on real examples, semantically related to the input. We propose here a methodology to evaluate the transparency and coherence of analogy-based explanations modeling an audit stage for the system. Quantitative analysis on two semantic tasks, i.e., question classification and semantic role labeling, show that the explanatory capabilities (native in KDAs) are effective and they pave the way to more complex argumentation methods.</p>
<p>Keywords:</p>
<h3 id="416. Enhancing Variational Autoencoders with Mutual Information Neural Estimation for Text Generation.">416. Enhancing Variational Autoencoders with Mutual Information Neural Estimation for Text Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1416">Paper Link</a>    Pages:4045-4055</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/82/7696.html">Dong Qian</a> ; <a href="https://dblp.uni-trier.de/pid/w/WKWCheung.html">William K. Cheung</a></p>
<p>Abstract:
While broadly applicable to many natural language processing (NLP) tasks, variational autoencoders (VAEs) are hard to train due to the posterior collapse issue where the latent variable fails to encode the input data effectively. Various approaches have been proposed to alleviate this problem to improve the capability of the VAE. In this paper, we propose to introduce a mutual information (MI) term between the input and its latent variable to regularize the objective of the VAE. Since estimating the MI in the high-dimensional space is intractable, we employ neural networks for the estimation of the MI and provide a training algorithm based on the convex duality approach. Our experimental results on three benchmark datasets demonstrate that the proposed model, compared to the state-of-the-art baselines, exhibits less posterior collapse and has comparable or better performance in language modeling and text generation. We also qualitatively evaluate the inferred latent space and show that the proposed model can generate more reasonable and diverse sentences via linear interpolation in the latent space.</p>
<p>Keywords:</p>
<h3 id="417. Sampling Bias in Deep Active Classification: An Empirical Study.">417. Sampling Bias in Deep Active Classification: An Empirical Study.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1417">Paper Link</a>    Pages:4056-4066</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/181/4512.html">Ameya Prabhu</a> ; <a href="https://dblp.uni-trier.de/pid/246/5115.html">Charles Dognin</a> ; <a href="https://dblp.uni-trier.de/pid/263/9205-1.html">Maneesh Singh</a></p>
<p>Abstract:
The exploding cost and time needed for data labeling and model training are bottlenecks for training DNN models on large datasets. Identifying smaller representative data samples with strategies like active learning can help mitigate such bottlenecks. Previous works on active learning in NLP identify the problem of sampling bias in the samples acquired by uncertainty-based querying and develop costly approaches to address it. Using a large empirical study, we demonstrate that active set selection using the posterior entropy of deep models like FastText.zip (FTZ) is robust to sampling biases and to various algorithmic choices (query size and strategies) unlike that suggested by traditional literature. We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches (e.g ensemble networks). Finally, we show the effectiveness of the selected samples by creating tiny high-quality datasets, and utilizing them for fast and cheap training of large models. Based on the above, we propose a simple baseline for deep active text classification that outperforms the state of the art. We expect the presented work to be useful and informative for dataset compression and for problems involving active, semi-supervised or online learning scenarios. Code and models are available at: <a href="https://github.com/drimpossible/Sampling-Bias-Active-Learning">https://github.com/drimpossible/Sampling-Bias-Active-Learning</a>.</p>
<p>Keywords:</p>
<h3 id="418. Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases.">418. Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1418">Paper Link</a>    Pages:4067-4080</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/20/2237.html">Christopher Clark</a> ; <a href="https://dblp.uni-trier.de/pid/64/8396.html">Mark Yatskar</a> ; <a href="https://dblp.uni-trier.de/pid/21/6793.html">Luke Zettlemoyer</a></p>
<p>Abstract:
State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply entailment, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a model to be more robust to domain shift. Our method has two stages: we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.</p>
<p>Keywords:</p>
<h3 id="419. Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation.">419. Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1419">Paper Link</a>    Pages:4081-4091</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/25/9874.html">Po-Sen Huang</a> ; <a href="https://dblp.uni-trier.de/pid/217/2131.html">Robert Stanforth</a> ; <a href="https://dblp.uni-trier.de/pid/150/3294.html">Johannes Welbl</a> ; <a href="https://dblp.uni-trier.de/pid/41/6895.html">Chris Dyer</a> ; <a href="https://dblp.uni-trier.de/pid/08/8178.html">Dani Yogatama</a> ; <a href="https://dblp.uni-trier.de/pid/75/8368.html">Sven Gowal</a> ; <a href="https://dblp.uni-trier.de/pid/16/8758.html">Krishnamurthy Dvijotham</a> ; <a href="https://dblp.uni-trier.de/pid/94/248.html">Pushmeet Kohli</a></p>
<p>Abstract:
Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a systems robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation  a formal model verification method. We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries.</p>
<p>Keywords:</p>
<h3 id="420. Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control.">420. Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1420">Paper Link</a>    Pages:4092-4101</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/32/7445.html">Mo Yu</a> ; <a href="https://dblp.uni-trier.de/pid/28/9988.html">Shiyu Chang</a> ; <a href="https://dblp.uni-trier.de/pid/06/6785.html">Yang Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/j/TommiJaakkola.html">Tommi S. Jaakkola</a></p>
<p>Abstract:
Selective rationalization has become a common mechanism to ensure that predictive models reveal how they use any available features. The selection may be soft or hard, and identifies a subset of input features relevant for prediction. The setup can be viewed as a co-operate game between the selector (aka rationale generator) and the predictor making use of only the selected features. The co-operative setting may, however, be compromised for two reasons. First, the generator typically has no direct access to the outcome it aims to justify, resulting in poor performance. Second, theres typically no control exerted on the information left outside the selection. We revise the overall co-operative framework to address these challenges. We introduce an introspective model which explicitly predicts and incorporates the outcome into the selection process. Moreover, we explicitly control the rationale complement via an adversary so as not to leave any useful information out of the selection. We show that the two complementary mechanisms maintain both high predictive accuracy and lead to comprehensive rationales.</p>
<p>Keywords:</p>
<h3 id="421. Experimenting with Power Divergences for Language Modeling.">421. Experimenting with Power Divergences for Language Modeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1421">Paper Link</a>    Pages:4102-4112</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/152/4095.html">Matthieu Labeau</a> ; <a href="https://dblp.uni-trier.de/pid/04/5629.html">Shay B. Cohen</a></p>
<p>Abstract:
Neural language models are usually trained using Maximum-Likelihood Estimation (MLE). The corresponding objective function for MLE is derived from the Kullback-Leibler (KL) divergence between the empirical probability distribution representing the data and the parametric probability distribution output by the model. However, the word frequency discrepancies in natural language make performance extremely uneven: while the perplexity is usually very low for frequent words, it is especially difficult to predict rare words. In this paper, we experiment with several families (alpha, beta and gamma) of power divergences, generalized from the KL divergence, for learning language models with an objective different than standard MLE. Intuitively, these divergences should affect the way the probability mass is spread during learning, notably by prioritizing performances on high or low-frequency words. In addition, we implement and experiment with various sampling-based objectives, where the computation of the output layer is only done on a small subset of the vocabulary. They are derived as power generalizations of a softmax approximated via Importance Sampling, and Noise Contrastive Estimation, for accelerated learning. Our experiments on the Penn Treebank and Wikitext-2 show that these power divergences can indeed be used to prioritize learning on the frequent or rare words, and lead to general performance improvements in the case of sampling-based learning.</p>
<p>Keywords:</p>
<h3 id="422. Hierarchically-Refined Label Attention Network for Sequence Labeling.">422. Hierarchically-Refined Label Attention Network for Sequence Labeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1422">Paper Link</a>    Pages:4113-4126</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/247/6181.html">Leyang Cui</a> ; <a href="https://dblp.uni-trier.de/pid/47/722-4.html">Yue Zhang</a></p>
<p>Abstract:
CRF has been used as a powerful model for statistical sequence labeling. For neural sequence labeling, however, BiLSTM-CRF does not always lead to better results compared with BiLSTM-softmax local classification. This can be because the simple Markov label transition model of CRF does not give much information gain over strong neural encoding. For better representing label sequences, we investigate a hierarchically-refined label attention network, which explicitly leverages label embeddings and captures potential long-term label dependency by giving each word incrementally refined label distributions with hierarchical attention. Results on POS tagging, NER and CCG supertagging show that the proposed model not only improves the overall tagging accuracy with similar number of parameters, but also significantly speeds up the training and testing compared to BiLSTM-CRF.</p>
<p>Keywords:</p>
<h3 id="423. Certified Robustness to Adversarial Word Substitutions.">423. Certified Robustness to Adversarial Word Substitutions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1423">Paper Link</a>    Pages:4127-4140</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/182/2556.html">Robin Jia</a> ; <a href="https://dblp.uni-trier.de/pid/166/1409.html">Aditi Raghunathan</a> ; <a href="https://dblp.uni-trier.de/pid/248/7494.html">Kerem Gksel</a> ; <a href="https://dblp.uni-trier.de/pid/04/1701.html">Percy Liang</a></p>
<p>Abstract:
State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75% adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI; in comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 12% and 41%, respectively.</p>
<p>Keywords:</p>
<h3 id="424. Visualizing and Understanding the Effectiveness of BERT.">424. Visualizing and Understanding the Effectiveness of BERT.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1424">Paper Link</a>    Pages:4141-4150</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/173/4242.html">Yaru Hao</a> ; <a href="https://dblp.uni-trier.de/pid/85/5090-4.html">Li Dong</a> ; <a href="https://dblp.uni-trier.de/pid/72/5870.html">Furu Wei</a> ; <a href="https://dblp.uni-trier.de/pid/x/KeXu.html">Ke Xu</a></p>
<p>Abstract:
Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.</p>
<p>Keywords:</p>
<h3 id="425. Topics to Avoid: Demoting Latent Confounds in Text Classification.">425. Topics to Avoid: Demoting Latent Confounds in Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1425">Paper Link</a>    Pages:4151-4161</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/31/4484.html">Sachin Kumar</a> ; <a href="https://dblp.uni-trier.de/pid/25/3224.html">Shuly Wintner</a> ; <a href="https://dblp.uni-trier.de/pid/90/5204.html">Noah A. Smith</a> ; <a href="https://dblp.uni-trier.de/pid/75/8157.html">Yulia Tsvetkov</a></p>
<p>Abstract:
Despite impressive performance on many text classification tasks, deep neural networks tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of native language identification. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the authors native language is Swedish). We propose a method that represents the latent topical confounds and a model which unlearns confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.</p>
<p>Keywords:</p>
<h3 id="426. Learning to Ask for Conversational Machine Learning.">426. Learning to Ask for Conversational Machine Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1426">Paper Link</a>    Pages:4162-4172</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/126/1619.html">Shashank Srivastava</a> ; <a href="https://dblp.uni-trier.de/pid/53/6801.html">Igor Labutov</a> ; <a href="https://dblp.uni-trier.de/pid/81/1460.html">Tom M. Mitchell</a></p>
<p>Abstract:
Natural language has recently been explored as a new medium of supervision for training machine learning models. Here, we explore learning classification tasks using language in a conversational setting  where the automated learner does not simply receive language input from a teacher, but can proactively engage the teacher by asking questions. We present a reinforcement learning framework, where the learners actions correspond to question types and the reward for asking a question is based on how the teachers response changes performance of the resulting machine learning model on the learning task. In this framework, learning good question-asking strategies corresponds to asking sequences of questions that maximize the cumulative (discounted) reward, and hence quickly lead to effective classifiers. Empirical analysis across three domains shows that learned question-asking strategies expedite classifier training by asking appropriate questions at different points in the learning process. The approach allows learning classifiers from a blend of strategies, including learning from observations, explanations and clarifications.</p>
<p>Keywords:</p>
<h3 id="427. Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training.">427. Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1427">Paper Link</a>    Pages:4173-4183</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/167/5312.html">Hila Gonen</a> ; <a href="https://dblp.uni-trier.de/pid/68/5296.html">Yoav Goldberg</a></p>
<p>Abstract:
We focus on the problem of language modeling for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons: (1) lack of available large-scale code-switched data for training; (2) lack of a replicable evaluation setup that is ASR directed yet isolates language modeling performance from the other intricacies of the ASR system; and (3) the reliance on generative modeling. We tackle these three issues: we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we explore a variety of training protocols and verify the effectiveness of training with large amounts of monolingual data followed by fine-tuning with small amounts of code-switched data, for both the generative and discriminative cases.</p>
<p>Keywords:</p>
<h3 id="428. Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs.">428. Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1428">Paper Link</a>    Pages:4184-4194</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/192/1872.html">Angela Fan</a> ; <a href="https://dblp.uni-trier.de/pid/71/6819.html">Claire Gardent</a> ; <a href="https://dblp.uni-trier.de/pid/151/8532.html">Chlo Braud</a> ; <a href="https://dblp.uni-trier.de/pid/49/4572.html">Antoine Bordes</a></p>
<p>Abstract:
Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.</p>
<p>Keywords:</p>
<h3 id="429. Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation.">429. Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1429">Paper Link</a>    Pages:4195-4204</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/50/4759.html">Huiyun Yang</a> ; <a href="https://dblp.uni-trier.de/pid/57/8451.html">Shujian Huang</a> ; <a href="https://dblp.uni-trier.de/pid/39/5815.html">Xin-Yu Dai</a> ; <a href="https://dblp.uni-trier.de/pid/42/4315.html">Jiajun Chen</a></p>
<p>Abstract:
In sequence labeling, previous domain adaptation methods focus on the adaptation from the source domain to the entire target domain without considering the diversity of individual target domain samples, which may lead to negative transfer results for certain samples. Besides, an important characteristic of sequence labeling tasks is that different elements within a given sample may also have diverse domain relevance, which requires further consideration. To take the multi-level domain relevance discrepancy into account, in this paper, we propose a fine-grained knowledge fusion model with the domain relevance modeling scheme to control the balance between learning from the target domain data and learning from the source domain model. Experiments on three sequence labeling tasks show that our fine-grained knowledge fusion model outperforms strong baselines and other state-of-the-art sequence labeling domain adaptation methods.</p>
<p>Keywords:</p>
<h3 id="430. Exploiting Monolingual Data at Scale for Neural Machine Translation.">430. Exploiting Monolingual Data at Scale for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1430">Paper Link</a>    Pages:4205-4215</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/68/1284.html">Lijun Wu</a> ; <a href="https://dblp.uni-trier.de/pid/188/9032.html">Yiren Wang</a> ; <a href="https://dblp.uni-trier.de/pid/144/7737.html">Yingce Xia</a> ; <a href="https://dblp.uni-trier.de/pid/14/6841.html">Tao Qin</a> ; <a href="https://dblp.uni-trier.de/pid/78/1117.html">Jianhuang Lai</a> ; <a href="https://dblp.uni-trier.de/pid/l/TieYanLiu.html">Tie-Yan Liu</a></p>
<p>Abstract:
While target-side monolingual data has been proven to be very useful to improve neural machine translation (briefly, NMT) through back translation, source-side monolingual data is not well investigated. In this work, we study how to use both the source-side and target-side monolingual data for NMT, and propose an effective strategy leveraging both of them. First, we generate synthetic bitext by translating monolingual data from the two domains into the other domain using the models pretrained on genuine bitext. Next, a model is trained on a noised version of the concatenated synthetic bitext where each source sequence is randomly corrupted. Finally, the model is fine-tuned on the genuine bitext and a clean version of a subset of the synthetic bitext without adding any noise. Our approach achieves state-of-the-art results on WMT16, WMT17, WMT18 EnglishGerman translations and WMT19 GermanFrench translations, which demonstrate the effectiveness of our method. We also conduct a comprehensive study on how each part in the pipeline works.</p>
<p>Keywords:</p>
<h3 id="431. Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs.">431. Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1431">Paper Link</a>    Pages:4216-4225</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/125/2319.html">Mingyang Chen</a> ; <a href="https://dblp.uni-trier.de/pid/43/2368-15.html">Wen Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/10/4661-127.html">Wei Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/62/2719.html">Qiang Chen</a> ; <a href="https://dblp.uni-trier.de/pid/94/5089.html">Huajun Chen</a></p>
<p>Abstract:
Link prediction is an important way to complete knowledge graphs (KGs), while embedding-based methods, effective for link prediction in KGs, perform poorly on relations that only have a few associative triples. In this work, we propose a Meta Relational Learning (MetaR) framework to do the common but challenging few-shot link prediction in KGs, namely predicting new triples about a relation by only observing a few associative triples. We solve few-shot link prediction by focusing on transferring relation-specific meta information to make model learn the most important knowledge and learn faster, corresponding to relation meta and gradient meta respectively in MetaR. Empirically, our model achieves state-of-the-art results on few-shot link prediction KG benchmarks.</p>
<p>Keywords:</p>
<h3 id="432. Distributionally Robust Language Modeling.">432. Distributionally Robust Language Modeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1432">Paper Link</a>    Pages:4226-4236</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/206/6316.html">Yonatan Oren</a> ; <a href="https://dblp.uni-trier.de/pid/248/7578.html">Shiori Sagawa</a> ; <a href="https://dblp.uni-trier.de/pid/66/7232.html">Tatsunori B. Hashimoto</a> ; <a href="https://dblp.uni-trier.de/pid/04/1701.html">Percy Liang</a></p>
<p>Abstract:
Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.</p>
<p>Keywords:</p>
<h3 id="433. Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling.">433. Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1433">Paper Link</a>    Pages:4237-4247</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/216/6755.html">Xiaochuang Han</a> ; <a href="https://dblp.uni-trier.de/pid/82/2305.html">Jacob Eisenstein</a></p>
<p>Abstract:
Contextualized word embeddings such as ELMo and BERT provide a foundation for strong performance across a wide range of natural language processing tasks by pretraining on large corpora of unlabeled text. However, the applicability of this approach is unknown when the target domain varies substantially from the pretraining corpus. We are specifically interested in the scenario in which labeled data is available in only a canonical source domain such as newstext, and the target domain is distinct from both the labeled and pretraining texts. To address this scenario, we propose domain-adaptive fine-tuning, in which the contextualized embeddings are adapted by masked language modeling on text from the target domain. We test this approach on sequence labeling in two challenging domains: Early Modern English and Twitter. Both domains differ substantially from existing pretraining corpora, and domain-adaptive fine-tuning yields substantial improvements over strong BERT baselines, with particularly impressive results on out-of-vocabulary words. We conclude that domain-adaptive fine-tuning offers a simple and effective approach for the unsupervised adaptation of sequence labeling to difficult new domains.</p>
<p>Keywords:</p>
<h3 id="434. Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial Crowds.">434. Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial Crowds.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1434">Paper Link</a>    Pages:4248-4258</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/159/0111.html">John P. Lalor</a> ; <a href="https://dblp.uni-trier.de/pid/72/4250.html">Hao Wu</a> ; <a href="https://dblp.uni-trier.de/pid/55/6749-1.html">Hong Yu</a></p>
<p>Abstract:
Incorporating Item Response Theory (IRT) into NLP tasks can provide valuable information about model performance and behavior. Traditionally, IRT models are learned using human response pattern (RP) data, presenting a significant bottleneck for large data sets like those required for training deep neural networks (DNNs). In this work we propose learning IRT models using RPs generated from artificial crowds of DNN models. We demonstrate the effectiveness of learning IRT models using DNN-generated data through quantitative and qualitative analyses for two NLP tasks. Parameters learned from human and machine RPs for natural language inference and sentiment analysis exhibit medium to large positive correlations. We demonstrate a use-case for latent difficulty item parameters, namely training set filtering, and show that using difficulty to sample training data outperforms baseline methods. Finally, we highlight cases where human expectation about item difficulty does not match difficulty as estimated from the machine RPs.</p>
<p>Keywords:</p>
<h3 id="435. Parallel Iterative Edit Models for Local Sequence Transduction.">435. Parallel Iterative Edit Models for Local Sequence Transduction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1435">Paper Link</a>    Pages:4259-4269</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/233/8164.html">Abhijeet Awasthi</a> ; <a href="https://dblp.uni-trier.de/pid/s/SunitaSarawagi.html">Sunita Sarawagi</a> ; <a href="https://dblp.uni-trier.de/pid/250/2355.html">Rasna Goyal</a> ; <a href="https://dblp.uni-trier.de/pid/66/1489.html">Sabyasachi Ghosh</a> ; <a href="https://dblp.uni-trier.de/pid/161/3626.html">Vihari Piratla</a></p>
<p>Abstract:
We present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding, giving up the advantage of modeling full dependency in the output, yet it achieves accuracy competitive with the ED model for four reasons: 1. predicting edits instead of tokens, 2. labeling sequences instead of generating sequences, 3. iteratively refining predictions to capture dependencies, and 4. factorizing logits over edits and their token argument to harness pre-trained language models like BERT. Experiments on tasks spanning GEC, OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction.</p>
<p>Keywords:</p>
<h3 id="436. ARAML: A Stable Adversarial Training Framework for Text Generation.">436. ARAML: A Stable Adversarial Training Framework for Text Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1436">Paper Link</a>    Pages:4270-4280</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/10/2179.html">Pei Ke</a> ; <a href="https://dblp.uni-trier.de/pid/h/FeiHuang.html">Fei Huang</a> ; <a href="https://dblp.uni-trier.de/pid/47/6668.html">Minlie Huang</a> ; <a href="https://dblp.uni-trier.de/pid/50/1222-1.html">Xiaoyan Zhu</a></p>
<p>Abstract:
Most of the existing generative adversarial networks (GAN) for text generation suffer from the instability of reinforcement learning training algorithms such as policy gradient, leading to unstable performance. To tackle this problem, we propose a novel framework called Adversarial Reward Augmented Maximum Likelihood (ARAML). During adversarial training, the discriminator assigns rewards to samples which are acquired from a stationary distribution near the data rather than the generators distribution. The generator is optimized with maximum likelihood estimation augmented by the discriminators rewards instead of policy gradient. Experiments show that our model can outperform state-of-the-art text GANs with a more stable training process.</p>
<p>Keywords:</p>
<h3 id="437. FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow.">437. FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1437">Paper Link</a>    Pages:4281-4291</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/127/0230.html">Xuezhe Ma</a> ; <a href="https://dblp.uni-trier.de/pid/161/2679.html">Chunting Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/82/1763.html">Xian Li</a> ; <a href="https://dblp.uni-trier.de/pid/03/8155.html">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pid/47/2454.html">Eduard H. Hovy</a></p>
<p>Abstract:
Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.</p>
<p>Keywords:</p>
<h3 id="438. Compositional Generalization for Primitive Substitutions.">438. Compositional Generalization for Primitive Substitutions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1438">Paper Link</a>    Pages:4292-4301</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/08/8174.html">Yuanpeng Li</a> ; <a href="https://dblp.uni-trier.de/pid/63/5422.html">Liang Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/98/6805.html">Jianyu Wang</a> ; <a href="https://dblp.uni-trier.de/pid/60/3063.html">Joel Hestness</a></p>
<p>Abstract:
Compositional generalization is a basic mechanism in human language learning, but current neural networks lack such ability. In this paper, we conduct fundamental research for encoding compositionality in neural networks. Conventional methods use a single representation for the input sentence, making it hard to apply prior knowledge of compositionality. In contrast, our approach leverages such knowledge with two representations, one generating attention maps, and the other mapping attended input words to output symbols. We reduce the entropy in each representation to improve generalization. Our experiments demonstrate significant improvements over the conventional methods in five NLP tasks including instruction learning and machine translation. In the SCAN domain, it boosts accuracies from 14.0% to 98.8% in Jump task, and from 92.0% to 99.7% in TurnLeft task. It also beats human performance on a few-shot learning task. We hope the proposed approach can help ease future research towards human-level compositional language learning.</p>
<p>Keywords:</p>
<h3 id="439. WikiCREM: A Large Unsupervised Corpus for Coreference Resolution.">439. WikiCREM: A Large Unsupervised Corpus for Coreference Resolution.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1439">Paper Link</a>    Pages:4302-4311</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/241/6009.html">Vid Kocijan</a> ; <a href="https://dblp.uni-trier.de/pid/231/7673.html">Oana-Maria Camburu</a> ; <a href="https://dblp.uni-trier.de/pid/31/2956.html">Ana-Maria Cretu</a> ; <a href="https://dblp.uni-trier.de/pid/241/5920.html">Yordan Yordanov</a> ; <a href="https://dblp.uni-trier.de/pid/96/4705.html">Phil Blunsom</a> ; <a href="https://dblp.uni-trier.de/pid/l/ThomasLukasiewicz.html">Thomas Lukasiewicz</a></p>
<p>Abstract:
Pronoun resolution is a major area of natural language understanding. However, large-scale training sets are still scarce, since manually labelling data is costly. In this work, we introduce WikiCREM (Wikipedia CoREferences Masked) a large-scale, yet accurate dataset of pronoun disambiguation instances. We use a language-model-based approach for pronoun resolution in combination with our WikiCREM dataset. We compare a series of models on a collection of diverse and challenging coreference resolution problems, where we match or outperform previous state-of-the-art approaches on 6 out of 7 datasets, such as GAP, DPR, WNLI, PDP, WinoBias, and WinoGender. We release our model to be used off-the-shelf for solving pronoun disambiguation.</p>
<p>Keywords:</p>
<h3 id="440. Identifying and Explaining Discriminative Attributes.">440. Identifying and Explaining Discriminative Attributes.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1440">Paper Link</a>    Pages:4312-4321</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8115.html">Armins Stepanjans</a> ; <a href="https://dblp.uni-trier.de/pid/47/9409.html">Andr Freitas</a></p>
<p>Abstract:
Identifying what is at the center of the meaning of a word and what discriminates it from other words is a fundamental natural language inference task. This paper describes an explicit word vector representation model (WVM) to support the identification of discriminative attributes. A core contribution of the paper is a quantitative and qualitative comparative analysis of different types of data sources and Knowledge Bases in the construction of explainable and explicit WVMs: (i) knowledge graphs built from dictionary definitions, (ii) entity-attribute-relationships graphs derived from images and (iii) commonsense knowledge graphs. Using a detailed quantitative and qualitative analysis, we demonstrate that these data sources have complementary semantic aspects, supporting the creation of explicit semantic vector spaces. The explicit vector spaces are evaluated using the task of discriminative attribute identification, showing comparable performance to the state-of-the-art systems in the task (F1-score = 0.69), while delivering full model transparency and explainability.</p>
<p>Keywords:</p>
<h3 id="441. Patient Knowledge Distillation for BERT Model Compression.">441. Patient Knowledge Distillation for BERT Model Compression.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1441">Paper Link</a>    Pages:4322-4331</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/120/1735.html">Siqi Sun</a> ; <a href="https://dblp.uni-trier.de/pid/96/3060-1.html">Yu Cheng</a> ; <a href="https://dblp.uni-trier.de/pid/41/7845.html">Zhe Gan</a> ; <a href="https://dblp.uni-trier.de/pid/30/3008-1.html">Jingjing Liu</a></p>
<p>Abstract:
Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (i) PKD-Last: learning from the last k layers; and (ii) PKD-Skip: learning from every k layers. These two patient distillation schemes enable the exploitation of rich information in the teachers hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with a significant gain in training efficiency, without sacrificing model accuracy.</p>
<p>Keywords:</p>
<h3 id="442. Neural Gaussian Copula for Variational Autoencoder.">442. Neural Gaussian Copula for Variational Autoencoder.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1442">Paper Link</a>    Pages:4332-4342</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/239/4286.html">Prince Zizhuang Wang</a> ; <a href="https://dblp.uni-trier.de/pid/08/9282.html">William Yang Wang</a></p>
<p>Abstract:
Variational language models seek to estimate the posterior of latent variables with an approximated variational posterior. The model often assumes the variational posterior to be factorized even when the true posterior is not. The learned variational posterior under this assumption does not capture the dependency relationships over latent variables. We argue that this would cause a typical training problem called posterior collapse observed in all other variational language models. We propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula is widely used to model correlation and dependencies of high-dimensional random variables, and therefore it is helpful to maintain the dependency relationships that are lost in VAE. The empirical results show that by modeling the correlation of latent variables explicitly using a neural parametric copula, we can avert this training difficulty while getting competitive results among all other VAE approaches.</p>
<p>Keywords:</p>
<h3 id="443. Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel.">443. Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1443">Paper Link</a>    Pages:4343-4352</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/154/3702.html">Yao-Hung Hubert Tsai</a> ; <a href="https://dblp.uni-trier.de/pid/217/1742.html">Shaojie Bai</a> ; <a href="https://dblp.uni-trier.de/pid/56/4937.html">Makoto Yamada</a> ; <a href="https://dblp.uni-trier.de/pid/31/739.html">Louis-Philippe Morency</a> ; <a href="https://dblp.uni-trier.de/pid/62/5884.html">Ruslan Salakhutdinov</a></p>
<p>Abstract:
Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformers attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformers attention. As an example, we propose a new variant of Transformers attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.</p>
<p>Keywords:</p>
<h3 id="444. Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification.">444. Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1444">Paper Link</a>    Pages:4353-4363</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/37/10763.html">Jiawei Wu</a> ; <a href="https://dblp.uni-trier.de/pid/203/8542.html">Wenhan Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/08/9282.html">William Yang Wang</a></p>
<p>Abstract:
Many tasks in natural language processing can be viewed as multi-label classification problems. However, most of the existing models are trained with the standard cross-entropy loss function and use a fixed prediction policy (e.g., a threshold of 0.5) for all the labels, which completely ignores the complexity and dependencies among different labels. In this paper, we propose a meta-learning method to capture these complex label dependencies. More specifically, our method utilizes a meta-learner to jointly learn the training policies and prediction policies for different labels. The training policies are then used to train the classifier with the cross-entropy loss function, and the prediction policies are further implemented for prediction. Experimental results on fine-grained entity typing and text classification demonstrate that our proposed method can obtain more accurate multi-label classification results.</p>
<p>Keywords:</p>
<h3 id="445. Revealing the Dark Secrets of BERT.">445. Revealing the Dark Secrets of BERT.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1445">Paper Link</a>    Pages:4364-4373</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/228/5668.html">Olga Kovaleva</a> ; <a href="https://dblp.uni-trier.de/pid/121/7944.html">Alexey Romanov</a> ; <a href="https://dblp.uni-trier.de/pid/203/9462.html">Anna Rogers</a> ; <a href="https://dblp.uni-trier.de/pid/63/873.html">Anna Rumshisky</a></p>
<p>Abstract:
BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERTs heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.</p>
<p>Keywords:</p>
<h3 id="446. Machine Translation With Weakly Paired Documents.">446. Machine Translation With Weakly Paired Documents.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1446">Paper Link</a>    Pages:4374-4383</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/68/1284.html">Lijun Wu</a> ; <a href="https://dblp.uni-trier.de/pid/18/1965.html">Jinhua Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/74/184.html">Di He</a> ; <a href="https://dblp.uni-trier.de/pid/16/722.html">Fei Gao</a> ; <a href="https://dblp.uni-trier.de/pid/14/6841.html">Tao Qin</a> ; <a href="https://dblp.uni-trier.de/pid/78/1117.html">Jianhuang Lai</a> ; <a href="https://dblp.uni-trier.de/pid/l/TieYanLiu.html">Tie-Yan Liu</a></p>
<p>Abstract:
Neural machine translation, which achieves near human-level performance in some languages, strongly relies on the large amounts of parallel sentences, which hinders its applicability to low-resource language pairs. Recent works explore the possibility of unsupervised machine translation with monolingual data only, leading to much lower accuracy compared with the supervised one. Observing that weakly paired bilingual documents are much easier to collect than bilingual sentences, e.g., from Wikipedia, news websites or books, in this paper, we investigate training translation models with weakly paired bilingual documents. Our approach contains two components. 1) We provide a simple approach to mine implicitly bilingual sentence pairs from document pairs which can then be used as supervised training signals. 2) We leverage the topic consistency of two weakly paired documents and learn the sentence translation model by constraining the word distribution-level alignments. We evaluate our method on weakly paired documents from Wikipedia on six tasks, the widely used WMT16 GermanEnglish, WMT13 SpanishEnglish and WMT16 RomanianEnglish translation tasks. We obtain 24.1/30.3, 28.1/27.6 and 30.1/27.6 BLEU points separately, outperforming previous results by more than 5 BLEU points in each direction and reducing the gap between unsupervised translation and supervised translation up to 50%.</p>
<p>Keywords:</p>
<h3 id="447. Countering Language Drift via Visual Grounding.">447. Countering Language Drift via Visual Grounding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1447">Paper Link</a>    Pages:4384-4394</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/91/3466.html">Jason Lee</a> ; <a href="https://dblp.uni-trier.de/pid/41/9736.html">Kyunghyun Cho</a> ; <a href="https://dblp.uni-trier.de/pid/136/9140.html">Douwe Kiela</a></p>
<p>Abstract:
Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language. We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift. We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning.</p>
<p>Keywords:</p>
<h3 id="448. The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.">448. The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1448">Paper Link</a>    Pages:4395-4405</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/4162.html">Elena Voita</a> ; <a href="https://dblp.uni-trier.de/pid/00/8341.html">Rico Sennrich</a> ; <a href="https://dblp.uni-trier.de/pid/08/5391.html">Ivan Titov</a></p>
<p>Abstract:
We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We chose the Transformers for our analysis as they have been shown effective with various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and observe that the choice of the objective determines this process. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.</p>
<p>Keywords:</p>
<h3 id="449. Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?">449. Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1449">Paper Link</a>    Pages:4406-4417</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/77/9768.html">Ivan Vulic</a> ; <a href="https://dblp.uni-trier.de/pid/50/11059.html">Goran Glavas</a> ; <a href="https://dblp.uni-trier.de/pid/96/5429.html">Roi Reichart</a> ; <a href="https://dblp.uni-trier.de/pid/14/6532.html">Anna Korhonen</a></p>
<p>Abstract:
Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any supervision makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.</p>
<p>Keywords:</p>
<h3 id="450. Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings.">450. Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1450">Paper Link</a>    Pages:4418-4429</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/131/4030.html">Haozhou Wang</a> ; <a href="https://dblp.uni-trier.de/pid/h/JamesHenderson.html">James Henderson</a> ; <a href="https://dblp.uni-trier.de/pid/94/2907.html">Paola Merlo</a></p>
<p>Abstract:
Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information not only in a single language but also across different languages. Current unsupervised adversarial approaches show that it is possible to build a mapping matrix that aligns two sets of monolingual word embeddings without high quality parallel data, such as a dictionary or a sentence-aligned corpus. However, without an additional step of refinement, the preliminary mapping learnt by these methods is unsatisfactory, leading to poor performance for typologically distant languages. In this paper, we propose a weakly-supervised adversarial training method to overcome this limitation, based on the intuition that mapping across languages is better done at the concept level than at the word level. We propose a concept-based adversarial training method which improves the performance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs.</p>
<p>Keywords:</p>
<h3 id="451. Aligning Cross-Lingual Entities with Multi-Aspect Information.">451. Aligning Cross-Lingual Entities with Multi-Aspect Information.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1451">Paper Link</a>    Pages:4430-4440</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/0080.html">Hsiu-Wei Yang</a> ; <a href="https://dblp.uni-trier.de/pid/48/6274.html">Yanyan Zou</a> ; <a href="https://dblp.uni-trier.de/pid/172/7638.html">Peng Shi</a> ; <a href="https://dblp.uni-trier.de/pid/98/6613-11.html">Wei Lu</a> ; <a href="https://dblp.uni-trier.de/pid/00/7739.html">Jimmy Lin</a> ; <a href="https://dblp.uni-trier.de/pid/37/1971-1.html">Xu Sun</a></p>
<p>Abstract:
Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode entities from multilingual KGs into the same vector space, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multi-aspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERT-based modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our method significantly outperforms existing systems.</p>
<p>Keywords:</p>
<h3 id="452. Contrastive Language Adaptation for Cross-Lingual Stance Detection.">452. Contrastive Language Adaptation for Cross-Lingual Stance Detection.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1452">Paper Link</a>    Pages:4441-4451</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/77/7405.html">Mitra Mohtarami</a> ; <a href="https://dblp.uni-trier.de/pid/37/6580.html">James R. Glass</a> ; <a href="https://dblp.uni-trier.de/pid/19/1947.html">Preslav Nakov</a></p>
<p>Abstract:
We study cross-lingual stance detection, which aims to leverage labeled data in one language to identify the relative perspective (or stance) of a given document with respect to a claim in a different target language. In particular, we introduce a novel contrastive language adaptation approach applied to memory networks, which ensures accurate alignment of stances in the source and target languages, and can effectively deal with the challenge of limited labeled data in the target language. The evaluation results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach.</p>
<p>Keywords:</p>
<h3 id="453. Jointly Learning to Align and Translate with Transformer Models.">453. Jointly Learning to Align and Translate with Transformer Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1453">Paper Link</a>    Pages:4452-4461</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/239/5896.html">Sarthak Garg</a> ; <a href="https://dblp.uni-trier.de/pid/68/11425.html">Stephan Peitz</a> ; <a href="https://dblp.uni-trier.de/pid/76/7551.html">Udhyakumar Nallasamy</a> ; <a href="https://dblp.uni-trier.de/pid/25/7825.html">Matthias Paulik</a></p>
<p>Abstract:
The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets.</p>
<p>Keywords:</p>
<h3 id="454. Social IQa: Commonsense Reasoning about Social Interactions.">454. Social IQa: Commonsense Reasoning about Social Interactions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1454">Paper Link</a>    Pages:4462-4472</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/153/9519.html">Maarten Sap</a> ; <a href="https://dblp.uni-trier.de/pid/164/6090.html">Hannah Rashkin</a> ; <a href="https://dblp.uni-trier.de/pid/225/7737.html">Derek Chen</a> ; <a href="https://dblp.uni-trier.de/pid/63/7554.html">Ronan Le Bras</a> ; <a href="https://dblp.uni-trier.de/pid/89/579.html">Yejin Choi</a></p>
<p>Abstract:
We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this? A: Make sure no one else could hear). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (&gt;20% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).</p>
<p>Keywords:</p>
<h3 id="455. Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.">455. Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1455">Paper Link</a>    Pages:4473-4483</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/118/7206.html">Yichen Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/32/5243.html">Mohit Bansal</a></p>
<p>Abstract:
Multi-hop QA requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. The recently proposed HotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four different multi-hop reasoning paradigms (two bridge entity setups, checking multiple properties, and comparing two entities), making it challenging for a single neural network to handle all four. In this work, we present an interpretable, controller-based Self-Assembling Neural Modular Network (Hu et al., 2017, 2018) for multi-hop reasoning, where we design four novel modules (Find, Relocate, Compare, NoOp) to perform unique types of language reasoning. Based on a question, our layout controller RNN dynamically infers a series of reasoning modules to construct the entire network. Empirically, we show that our dynamic, multi-hop modular network achieves significant improvements over the static, single-hop baseline (on both regular and adversarial evaluation). We further demonstrate the interpretability of our model via three analyses. First, the controller can softly decompose the multi-hop question into multiple single-hop sub-questions to promote compositional reasoning behavior of the main network. Second, the controller can predict layouts that conform to the layouts designed by human experts. Finally, the intermediate module can infer the entity that connects two distantly-located supporting facts by addressing the sub-question from the controller.</p>
<p>Keywords:</p>
<h3 id="456. Posing Fair Generalization Tasks for Natural Language Inference.">456. Posing Fair Generalization Tasks for Natural Language Inference.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1456">Paper Link</a>    Pages:4484-4494</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/229/4086.html">Atticus Geiger</a> ; <a href="https://dblp.uni-trier.de/pid/126/6263.html">Ignacio Cases</a> ; <a href="https://dblp.uni-trier.de/pid/71/5675.html">Lauri Karttunen</a> ; <a href="https://dblp.uni-trier.de/pid/13/2617.html">Christopher Potts</a></p>
<p>Abstract:
Deep learning models for semantics are generally evaluated using naturalistic corpora. Adversarial testing methods, in which models are evaluated on new examples with known semantic properties, have begun to reveal that good performance at these naturalistic tasks can hide serious shortcomings. However, we should insist that these evaluations be fair  that the models are given data sufficient to support the requisite kinds of generalization. In this paper, we define and motivate a formal notion of fairness in this sense. We then apply these ideas to natural language inference by constructing very challenging but provably fair artificial datasets and showing that standard neural models fail to generalize in the required ways; only task-specific models that jointly compose the premise and hypothesis are able to achieve high performance, and even these models do not solve the task perfectly.</p>
<p>Keywords:</p>
<h3 id="457. Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text.">457. Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1457">Paper Link</a>    Pages:4495-4504</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/78/6527.html">Bhavana Dalvi</a> ; <a href="https://dblp.uni-trier.de/pid/29/9923.html">Niket Tandon</a> ; <a href="https://dblp.uni-trier.de/pid/184/3742.html">Antoine Bosselut</a> ; <a href="https://dblp.uni-trier.de/pid/07/7129.html">Wen-tau Yih</a> ; <a href="https://dblp.uni-trier.de/pid/34/1184.html">Peter Clark</a></p>
<p>Abstract:
Our goal is to better comprehend procedural text, e.g., a paragraph about photosynthesis, by not only predicting what happens, but <em>why</em> some actions need to happen before others. Our approach builds on a prior process comprehension framework for predicting actions effects, to also identify subsequent steps that those effects enable. We present our new model (XPAD) that biases effect predictions towards those that (1) explain more of the actions in the paragraph and (2) are more plausible with respect to background knowledge. We also extend an existing benchmark dataset for procedural text comprehension, ProPara, by adding the new task of explaining actions by predicting their dependencies. We find that XPAD significantly outperforms prior systems on this task, while maintaining the performance on the original task in ProPara. The dataset is available at <a href="http://data.allenai.org/propara">http://data.allenai.org/propara</a></p>
<p>Keywords:</p>
<h3 id="458. CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text.">458. CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1458">Paper Link</a>    Pages:4505-4514</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/210/0890.html">Koustuv Sinha</a> ; <a href="https://dblp.uni-trier.de/pid/173/5111.html">Shagun Sodhani</a> ; <a href="https://dblp.uni-trier.de/pid/31/507.html">Jin Dong</a> ; <a href="https://dblp.uni-trier.de/pid/p/JoellePineau.html">Joelle Pineau</a> ; <a href="https://dblp.uni-trier.de/pid/137/3314.html">William L. Hamilton</a></p>
<p>Abstract:
The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by the classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a models ability for systematic generalization by evaluating on held-out combinations of logical rules, and allows us to evaluate a models robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputswith the graph-based model exhibiting both stronger generalization and greater robustness.</p>
<p>Keywords:</p>
<h3 id="459. Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset.">459. Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1459">Paper Link</a>    Pages:4515-4524</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/b/WilliamJByrne.html">Bill Byrne</a> ; <a href="https://dblp.uni-trier.de/pid/248/8343.html">Karthik Krishnamoorthi</a> ; <a href="https://dblp.uni-trier.de/pid/155/0592.html">Chinnadhurai Sankar</a> ; <a href="https://dblp.uni-trier.de/pid/142/8636.html">Arvind Neelakantan</a> ; <a href="https://dblp.uni-trier.de/pid/00/7566.html">Ben Goodrich</a> ; <a href="https://dblp.uni-trier.de/pid/10/8371.html">Daniel Duckworth</a> ; <a href="https://dblp.uni-trier.de/pid/129/1217.html">Semih Yavuz</a> ; <a href="https://dblp.uni-trier.de/pid/27/1374.html">Amit Dubey</a> ; <a href="https://dblp.uni-trier.de/pid/05/436.html">Kyu-Young Kim</a> ; <a href="https://dblp.uni-trier.de/pid/81/9380.html">Andy Cedilnik</a></p>
<p>Abstract:
A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken Wizard of Oz (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is self-dialog in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.</p>
<p>Keywords:</p>
<h3 id="460. Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data.">460. Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1460">Paper Link</a>    Pages:4525-4535</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/203/9242.html">Denis Peskov</a> ; <a href="https://dblp.uni-trier.de/pid/86/2909.html">Nancy E. Clarke</a> ; <a href="https://dblp.uni-trier.de/pid/215/3826.html">Jason Krone</a> ; <a href="https://dblp.uni-trier.de/pid/254/8197.html">Brigi Fodor</a> ; <a href="https://dblp.uni-trier.de/pid/64/6544.html">Yi Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/122/9734.html">Adel Youssef</a> ; <a href="https://dblp.uni-trier.de/pid/15/4305.html">Mona T. Diab</a></p>
<p>Abstract:
The need for high-quality, large-scale, goal-oriented dialogue datasets continues to grow as virtual assistants become increasingly wide-spread. However, publicly available datasets useful for this area are limited either in their size, linguistic diversity, domain coverage, or annotation granularity. In this paper, we present strategies toward curating and annotating large scale goal oriented dialogue data. We introduce the MultiDoGO dataset to overcome these limitations. With a total of over 81K dialogues harvested across six domains, MultiDoGO is over 8 times the size of MultiWOZ, the other largest comparable dialogue dataset currently available to the public. Over 54K of these harvested conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz approach wherein a crowd-sourced worker (the customer) is paired with a trained annotator (the agent). The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies. We provide distinct class label tags for agents vs. customer utterances, along with applicable slot labels. We also compare and contrast our strategies on annotation granularity, i.e. turn vs. sentence level. Furthermore, we compare and contrast annotations curated by leveraging professional annotators vs the crowd. We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future. To demonstrate the efficacy of our devised strategies we establish neural baselines for classification on the agent and customer utterances as well as slot labeling for each domain.</p>
<p>Keywords:</p>
<h3 id="461. Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack.">461. Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1461">Paper Link</a>    Pages:4536-4545</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/213/7927.html">Emily Dinan</a> ; <a href="https://dblp.uni-trier.de/pid/139/2584.html">Samuel Humeau</a> ; <a href="https://dblp.uni-trier.de/pid/247/5927.html">Bharath Chintagunta</a> ; <a href="https://dblp.uni-trier.de/pid/29/6977.html">Jason Weston</a></p>
<p>Abstract:
The detection of offensive language in the context of a dialogue has become an increasingly important application of natural language processing. The detection of trolls in public forums (Galn-Garca et al., 2016), and the deployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, fix it scheme with humans and models in the loop. In detailed experiments we show this approach is considerably more robust than previous systems. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly collected tasks and methods are all made open source and publicly available.</p>
<p>Keywords:</p>
<h3 id="462. GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue.">462. GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1462">Paper Link</a>    Pages:4546-4556</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/249/5657.html">Jun Quan</a> ; <a href="https://dblp.uni-trier.de/pid/55/6548.html">Deyi Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/95/4733.html">Bonnie Webber</a> ; <a href="https://dblp.uni-trier.de/pid/86/1649.html">Changjian Hu</a></p>
<p>Abstract:
Ellipsis and co-reference are common and ubiquitous especially in multi-turn dialogues. In this paper, we treat the resolution of ellipsis and co-reference in dialogue as a problem of generating omitted or referred expressions from the dialogue context. We therefore propose a unified end-to-end Generative Ellipsis and CO-reference Resolution model (GECOR) in the context of dialogue. The model can generate a new pragmatically complete user utterance by alternating the generation and copy mode for each user utterance. A multi-task learning framework is further proposed to integrate the GECOR into an end-to-end task-oriented dialogue. In order to train both the GECOR and the multi-task learning framework, we manually construct a new dataset on the basis of the public dataset CamRest676 with both ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on the resolution of ellipsis and co-reference show that the GECOR model significantly outperforms the sequence-to-sequence (seq2seq) baseline model in terms of EM, BLEU and F1 while extrinsic evaluations on the downstream dialogue task demonstrate that our multi-task learning framework with GECOR achieves a higher success rate of task completion than TSCP, a state-of-the-art end-to-end task-oriented dialogue model.</p>
<p>Keywords:</p>
<h3 id="463. Task-Oriented Conversation Generation Using Heterogeneous Memory Networks.">463. Task-Oriented Conversation Generation Using Heterogeneous Memory Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1463">Paper Link</a>    Pages:4557-4566</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/247/6061.html">Zehao Lin</a> ; <a href="https://dblp.uni-trier.de/pid/214/6359.html">Xinjing Huang</a> ; <a href="https://dblp.uni-trier.de/pid/50/6349.html">Feng Ji</a> ; <a href="https://dblp.uni-trier.de/pid/34/6243.html">Haiqing Chen</a> ; <a href="https://dblp.uni-trier.de/pid/91/3045.html">Yin Zhang</a></p>
<p>Abstract:
How to incorporate external knowledge into a neural dialogue model is critically important for dialogue systems to behave like real humans. To handle this problem, memory networks are usually a great choice and a promising way. However, existing memory networks do not perform well when leveraging heterogeneous information from different sources. In this paper, we propose a novel and versatile external memory networks called Heterogeneous Memory Networks (HMNs), to simultaneously utilize user utterances, dialogue history and background knowledge tuples. In our method, historical sequential dialogues are encoded and stored into the context-aware memory enhanced by gating mechanism while grounding knowledge tuples are encoded and stored into the context-free memory. During decoding, the decoder augmented with HMNs recurrently selects each word in one response utterance from these two memories and a general vocabulary. Experimental results on multiple real-world datasets show that HMNs significantly outperform the state-of-the-art data-driven task-oriented dialogue models in most domains.</p>
<p>Keywords:</p>
<h3 id="464. Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks.">464. Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1464">Paper Link</a>    Pages:4567-4577</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/94/4084.html">Chen Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/166/3079.html">Qiuchi Li</a> ; <a href="https://dblp.uni-trier.de/pid/47/6784-1.html">Dawei Song</a></p>
<p>Abstract:
Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a mechanism to account for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we propose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models, and further demonstrate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.</p>
<p>Keywords:</p>
<h3 id="465. Coupling Global and Local Context for Unsupervised Aspect Extraction.">465. Coupling Global and Local Context for Unsupervised Aspect Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1465">Paper Link</a>    Pages:4578-4588</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/88/6511.html">Ming Liao</a> ; <a href="https://dblp.uni-trier.de/pid/181/2820-49.html">Jing Li</a> ; <a href="https://dblp.uni-trier.de/pid/220/2004.html">Haisong Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/66/8430.html">Lingzhi Wang</a> ; <a href="https://dblp.uni-trier.de/pid/125/2836.html">Xixin Wu</a> ; <a href="https://dblp.uni-trier.de/pid/w/KamFaiWong.html">Kam-Fai Wong</a></p>
<p>Abstract:
Aspect words, indicating opinion targets, are essential in expressing and understanding human opinions. To identify aspects, most previous efforts focus on using sequence tagging models trained on human-annotated data. This work studies unsupervised aspect extraction and explores how words appear in global context (on sentence level) and local context (conveyed by neighboring words). We propose a novel neural model, capable of coupling global and local representation to discover aspect words. Experimental results on two benchmarks, laptop and restaurant reviews, show that our model significantly outperforms the state-of-the-art models from previous studies evaluated with varying metrics. Analysis on model output show our ability to learn meaningful and coherent aspect representations. We further investigate how words distribute in global and local context, and find that aspect and non-aspect words do exhibit different context, interpreting our superiority in unsupervised aspect extraction.</p>
<p>Keywords:</p>
<h3 id="466. Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning.">466. Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1466">Paper Link</a>    Pages:4589-4599</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/10/1143-18.html">Zheng Li</a> ; <a href="https://dblp.uni-trier.de/pid/09/1365.html">Xin Li</a> ; <a href="https://dblp.uni-trier.de/pid/14/4899-1.html">Ying Wei</a> ; <a href="https://dblp.uni-trier.de/pid/53/6625.html">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pid/50/671-6.html">Yu Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/82/6362-1.html">Qiang Yang</a></p>
<p>Abstract:
Joint extraction of aspects and sentiments can be effectively formulated as a sequence labeling problem. However, such formulation hinders the effectiveness of supervised methods due to the lack of annotated sequence data in many domains. To address this issue, we firstly explore an unsupervised domain adaptation setting for this task. Prior work can only use common syntactic relations between aspect and opinion words to bridge the domain gaps, which highly relies on external linguistic resources. To resolve it, we propose a novel Selective Adversarial Learning (SAL) method to align the inferred correlation vectors that automatically capture their latent relations. The SAL method can dynamically learn an alignment weight for each word such that more important words can possess higher alignment weights to achieve fine-grained (word-level) adaptation. Empirically, extensive experiments demonstrate the effectiveness of the proposed SAL method.</p>
<p>Keywords:</p>
<h3 id="467. CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis.">467. CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1467">Paper Link</a>    Pages:4600-4609</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/199/5022.html">Mengting Hu</a> ; <a href="https://dblp.uni-trier.de/pid/73/1947.html">Shiwan Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/89/5992.html">Li Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/37/5626.html">Keke Cai</a> ; <a href="https://dblp.uni-trier.de/pid/87/1363.html">Zhong Su</a> ; <a href="https://dblp.uni-trier.de/pid/137/0571.html">Renhong Cheng</a> ; <a href="https://dblp.uni-trier.de/pid/14/6811.html">Xiaowei Shen</a></p>
<p>Abstract:
Aspect level sentiment classification is a fine-grained sentiment analysis task. To detect the sentiment towards a particular aspect in a sentence, previous studies have developed various attention-based methods for generating aspect-specific sentence representations. However, the attention may inherently introduce noise and downgrade the performance. In this paper, we propose constrained attention networks (CAN), a simple yet effective solution, to regularize the attention for multi-aspect sentiment analysis, which alleviates the drawback of the attention mechanism. Specifically, we introduce orthogonal regularization on multiple aspects and sparse regularization on each single aspect. Experimental results on two public datasets demonstrate the effectiveness of our approach. We further extend our approach to multi-task settings and outperform the state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="468. Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training.">468. Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1468">Paper Link</a>    Pages:4610-4620</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/192/1911.html">Giannis Karamanolakis</a> ; <a href="https://dblp.uni-trier.de/pid/h/DanielHsu.html">Daniel Hsu</a> ; <a href="https://dblp.uni-trier.de/pid/g/LuisGravano.html">Luis Gravano</a></p>
<p>Abstract:
User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., price, quality, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, unsupervised topic models often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches fail to leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher); in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous weakly supervised approaches (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.</p>
<p>Keywords:</p>
<h3 id="469. Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts.">469. Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1469">Paper Link</a>    Pages:4621-4631</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/239/5785.html">Julia Kruk</a> ; <a href="https://dblp.uni-trier.de/pid/239/6008.html">Jonah Lubin</a> ; <a href="https://dblp.uni-trier.de/pid/119/1499.html">Karan Sikka</a> ; <a href="https://dblp.uni-trier.de/pid/09/1280.html">Xiao Lin</a> ; <a href="https://dblp.uni-trier.de/pid/31/985.html">Dan Jurafsky</a> ; <a href="https://dblp.uni-trier.de/pid/73/467.html">Ajay Divakaran</a></p>
<p>Abstract:
Computing author intent from multimodal data like Instagram posts requires modeling a complex relationship between text and image. For example, a caption might evoke an ironic contrast with the image, so neither caption nor image is a mere transcript of the other. Instead they combinevia what has been called meaning multiplication (Bateman et al.)- to create a new meaning that has a more complex relation to the literal meanings of text and image. Here we introduce a multimodal dataset of 1299 Instagram posts labeled for three orthogonal taxonomies: the authorial intent behind the image-caption pair, the contextual relationship between the literal meanings of the image and caption, and the semiotic relationship between the signified meanings of the image and caption. We build a baseline deep multimodal classifier to validate the taxonomy, showing that employing both text and image improves intent detection by 9.6 compared to using only the image modality, demonstrating the commonality of non-intersective meaning multiplication. The gain with multimodality is greatest when the image and caption diverge semiotically. Our dataset offers a new resource for the study of the rich meanings that result from pairing text and image.</p>
<p>Keywords:</p>
<h3 id="470. Neural Conversation Recommendation with Online Interaction Modeling.">470. Neural Conversation Recommendation with Online Interaction Modeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1470">Paper Link</a>    Pages:4632-4642</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/2024.html">Xingshan Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/181/2820-49.html">Jing Li</a> ; <a href="https://dblp.uni-trier.de/pid/49/3800-8.html">Lu Wang</a> ; <a href="https://dblp.uni-trier.de/pid/w/KamFaiWong.html">Kam-Fai Wong</a></p>
<p>Abstract:
The prevalent use of social media leads to a vast amount of online conversations being produced on a daily basis. It presents a concrete challenge for individuals to better discover and engage in social media discussions. In this paper, we present a novel framework to automatically recommend conversations to users based on their prior conversation behaviors. Built on neural collaborative filtering, our model explores deep semantic features that measure how a users preferences match an ongoing conversations context. Furthermore, to identify salient characteristics from interleaving user interactions, our model incorporates graph-structured networks, where both replying relations and temporal features are encoded as conversation context. Experimental results on two large-scale datasets collected from Twitter and Reddit show that our model yields better performance than previous state-of-the-art models, which only utilize lexical features and ignore past user interactions in the conversations.</p>
<p>Keywords:</p>
<h3 id="471. Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection.">471. Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1471">Paper Link</a>    Pages:4643-4652</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/214/2499.html">Lianwei Wu</a> ; <a href="https://dblp.uni-trier.de/pid/73/4103.html">Yuan Rao</a> ; <a href="https://dblp.uni-trier.de/pid/207/8891.html">Haolin Jin</a> ; <a href="https://dblp.uni-trier.de/pid/226/6984.html">Ambreen Nazir</a> ; <a href="https://dblp.uni-trier.de/pid/08/6547-4.html">Ling Sun</a></p>
<p>Abstract:
Recently, neural networks based on multi-task learning have achieved promising performance on fake news detection, which focuses on learning shared features among tasks as complementarity features to serve different tasks. However, in most of the existing approaches, the shared features are completely assigned to different tasks without selection, which may lead to some useless and even adverse features integrated into specific tasks. In this paper, we design a sifted multi-task learning method with a selected sharing layer for fake news detection. The selected sharing layer adopts gate mechanism and attention mechanism to filter and select shared feature flows between tasks. Experiments on two public and widely used competition datasets, i.e. RumourEval and PHEME, demonstrate that our proposed method achieves the state-of-the-art performance and boosts the F1-score by more than 0.87%, 1.31%, respectively.</p>
<p>Keywords:</p>
<h3 id="472. Text-based inference of moral sentiment change.">472. Text-based inference of moral sentiment change.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1472">Paper Link</a>    Pages:4653-4662</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/229/8336.html">Jing Yi Xie</a> ; <a href="https://dblp.uni-trier.de/pid/243/3620.html">Renato Ferreira Pinto Junior</a> ; <a href="https://dblp.uni-trier.de/pid/h/GraemeHirst.html">Graeme Hirst</a> ; <a href="https://dblp.uni-trier.de/pid/61/3906.html">Yang Xu</a></p>
<p>Abstract:
We present a text-based framework for investigating moral sentiment change of the public via longitudinal corpora. Our framework is based on the premise that language use can inform peoples moral perception toward right or wrong, and we build our methodology by exploring moral biases learned from diachronic word embeddings. We demonstrate how a parameter-free model supports inference of historical shifts in moral sentiment toward concepts such as slavery and democracy over centuries at three incremental levels: moral relevance, moral polarity, and fine-grained moral dimensions. We apply this methodology to visualizing moral time courses of individual concepts and analyzing the relations between psycholinguistic variables and rates of moral sentiment change at scale. Our work offers opportunities for applying natural language processing toward characterizing moral sentiment change in society.</p>
<p>Keywords:</p>
<h3 id="473. Detecting Causal Language Use in Science Findings.">473. Detecting Causal Language Use in Science Findings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1473">Paper Link</a>    Pages:4663-4673</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/28/4556.html">Bei Yu</a> ; <a href="https://dblp.uni-trier.de/pid/205/9311.html">Yingya Li</a> ; <a href="https://dblp.uni-trier.de/pid/125/8189.html">Jun Wang</a></p>
<p>Abstract:
Causal interpretation of correlational findings from observational studies has been a major type of misinformation in science communication. Prior studies on identifying inappropriate use of causal language relied on manual content analysis, which is not scalable for examining a large volume of science publications. In this study, we first annotated a corpus of over 3,000 PubMed research conclusion sentences, then developed a BERT-based prediction model that classifies conclusion sentences into no relationship, correlational, conditional causal, and direct causal categories, achieving an accuracy of 0.90 and a macro-F1 of 0.88. We then applied the prediction model to measure the causal language use in the research conclusions of about 38,000 observational studies in PubMed. The prediction result shows that 21.7% studies used direct causal language exclusively in their conclusions, and 32.4% used some direct causal language. We also found that the ratio of causal language use differs among authors from different countries, challenging the notion of a shared consensus on causal language use in the global science community. Our prediction model could also be used to help identify the inappropriate use of causal language in science publications.</p>
<p>Keywords:</p>
<h3 id="474. Multilingual and Multi-Aspect Hate Speech Analysis.">474. Multilingual and Multi-Aspect Hate Speech Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1474">Paper Link</a>    Pages:4674-4683</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/2832.html">Nedjma Ousidhoum</a> ; <a href="https://dblp.uni-trier.de/pid/222/1857.html">Zizheng Lin</a> ; <a href="https://dblp.uni-trier.de/pid/48/859.html">Hongming Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/86/2159.html">Yangqiu Song</a> ; <a href="https://dblp.uni-trier.de/pid/41/5668.html">Dit-Yan Yeung</a></p>
<p>Abstract:
Current research on hate speech analysis is typically oriented towards monolingual and single classification tasks. In this paper, we present a new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches. We evaluate our dataset in various classification settings, then we discuss how to leverage our annotations in order to improve hate speech detection and classification in general.</p>
<p>Keywords:</p>
<h3 id="475. MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims.">475. MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1475">Paper Link</a>    Pages:4684-4696</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/93/11424.html">Isabelle Augenstein</a> ; <a href="https://dblp.uni-trier.de/pid/16/1917.html">Christina Lioma</a> ; <a href="https://dblp.uni-trier.de/pid/21/841.html">Dongsheng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/238/0439.html">Lucas Chaves Lima</a> ; <a href="https://dblp.uni-trier.de/pid/205/2958.html">Casper Hansen</a> ; <a href="https://dblp.uni-trier.de/pid/57/2217-4.html">Christian Hansen</a> ; <a href="https://dblp.uni-trier.de/pid/s/JakobGrueSimonsen.html">Jakob Grue Simonsen</a></p>
<p>Abstract:
We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.</p>
<p>Keywords:</p>
<h3 id="476. A Deep Neural Information Fusion Architecture for Textual Network Embeddings.">476. A Deep Neural Information Fusion Architecture for Textual Network Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1476">Paper Link</a>    Pages:4697-4705</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/2481.html">Zenan Xu</a> ; <a href="https://dblp.uni-trier.de/pid/87/7936.html">Qinliang Su</a> ; <a href="https://dblp.uni-trier.de/pid/90/5936.html">Xiaojun Quan</a> ; <a href="https://dblp.uni-trier.de/pid/158/5387.html">Weijia Zhang</a></p>
<p>Abstract:
Textual network embeddings aim to learn a low-dimensional representation for every node in the network so that both the structural and textual information from the networks can be well preserved in the representations. Traditionally, the structural and textual embeddings were learned by models that rarely take the mutual influences between them into account. In this paper, a deep neural architecture is proposed to effectively fuse the two kinds of informations into one representation. The novelties of the proposed architecture are manifested in the aspects of a newly defined objective function, the complementary information fusion method for structural and textual features, and the mutual gate mechanism for textual feature extraction. Experimental results show that the proposed model outperforms the comparing methods on all three datasets.</p>
<p>Keywords:</p>
<h3 id="477. You Shall Know a User by the Company It Keeps: Dynamic Representations for Social Media Users in NLP.">477. You Shall Know a User by the Company It Keeps: Dynamic Representations for Social Media Users in NLP.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1477">Paper Link</a>    Pages:4706-4716</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/184/8927.html">Marco Del Tredici</a> ; <a href="https://dblp.uni-trier.de/pid/64/8468.html">Diego Marcheggiani</a> ; <a href="https://dblp.uni-trier.de/pid/11/580.html">Sabine Schulte im Walde</a> ; <a href="https://dblp.uni-trier.de/pid/02/5384.html">Raquel Fernndez</a></p>
<p>Abstract:
Information about individuals can help to better understand what they say, particularly in social media where texts are short. Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. This ignores the fact, well known in sociolinguistics, that an individual may be part of several communities which are not equally relevant in all communicative situations. We present a model based on Graph Attention Networks that captures this observation. It dynamically explores the social graph of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods.</p>
<p>Keywords:</p>
<h3 id="478. Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis.">478. Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1478">Paper Link</a>    Pages:4717-4729</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/251/8901.html">Shrey Desai</a> ; <a href="https://dblp.uni-trier.de/pid/251/9082.html">Barea Sinno</a> ; <a href="https://dblp.uni-trier.de/pid/185/7651.html">Alex Rosenfeld</a> ; <a href="https://dblp.uni-trier.de/pid/148/9553.html">Junyi Jessy Li</a></p>
<p>Abstract:
Insightful findings in political science often require researchers to analyze documents of a certain subject or type, yet these documents are usually contained in large corpora that do not distinguish between pertinent and non-pertinent documents. In contrast, we can find corpora that label relevant documents but have limitations (e.g., from a single source or era), preventing their use for political science research. To bridge this gap, we present adaptive ensembling, an unsupervised domain adaptation framework, equipped with a novel text classification model and time-aware training to ensure our methods work well with diachronic corpora. Experiments on an expert-annotated dataset show that our framework outperforms strong benchmarks. Further analysis indicates that our methods are more stable, learn better representations, and extract cleaner corpora for fine-grained analysis.</p>
<p>Keywords:</p>
<h3 id="479. A Hierarchical Location Prediction Neural Network for Twitter User Geolocation.">479. A Hierarchical Location Prediction Neural Network for Twitter User Geolocation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1480">Paper Link</a>    Pages:4731-4741</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/195/5963.html">Binxuan Huang</a> ; <a href="https://dblp.uni-trier.de/pid/72/6492.html">Kathleen M. Carley</a></p>
<p>Abstract:
Accurate estimation of user location is important for many online services. Previous neural network based methods largely ignore the hierarchical structure among locations. In this paper, we propose a hierarchical location prediction neural network for Twitter user geolocation. Our model first predicts the home country for a user, then uses the country result to guide the city-level prediction. In addition, we employ a character-aware word embedding layer to overcome the noisy information in tweets. With the feature fusion layer, our model can accommodate various feature combinations and achieves state-of-the-art results over three commonly used benchmarks under different feature settings. It not only improves the prediction accuracy but also greatly reduces the mean error distance.</p>
<p>Keywords:</p>
<h3 id="480. Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop.">480. Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1481">Paper Link</a>    Pages:4742-4753</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/4231.html">Jonathan P. Chang</a> ; <a href="https://dblp.uni-trier.de/pid/49/3976.html">Cristian Danescu-Niculescu-Mizil</a></p>
<p>Abstract:
Online discussions often derail into toxic exchanges between participants. Recent efforts mostly focused on detecting antisocial behavior after the fact, by analyzing single comments in isolation. To provide more timely notice to human moderators, a system needs to preemptively detect that a conversation is heading towards derailment before it actually turns toxic. This means modeling derailment as an emerging property of a conversation rather than as an isolated utterance-level event. Forecasting emerging conversational properties, however, poses several inherent modeling challenges. First, since conversations are dynamic, a forecasting model needs to capture the flow of the discussion, rather than properties of individual comments. Second, real conversations have an unknown horizon: they can end or derail at any time; thus a practical forecasting model needs to assess the risk in an online fashion, as the conversation develops. In this work we introduce a conversational forecasting model that learns an unsupervised representation of conversational dynamics and exploits it to predict future derailment as the conversation develops. By applying this model to two new diverse datasets of online conversations with labels for antisocial events, we show that it outperforms state-of-the-art systems at forecasting derailment.</p>
<p>Keywords:</p>
<h3 id="481. A Benchmark Dataset for Learning to Intervene in Online Hate Speech.">481. A Benchmark Dataset for Learning to Intervene in Online Hate Speech.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1482">Paper Link</a>    Pages:4754-4763</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/71/944.html">Jing Qian</a> ; <a href="https://dblp.uni-trier.de/pid/248/8050.html">Anna Bethke</a> ; <a href="https://dblp.uni-trier.de/pid/37/2521.html">Yinyin Liu</a> ; <a href="https://dblp.uni-trier.de/pid/b/EMBeldingRoyer.html">Elizabeth M. Belding</a> ; <a href="https://dblp.uni-trier.de/pid/08/9282.html">William Yang Wang</a></p>
<p>Abstract:
Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and Reddit. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.</p>
<p>Keywords:</p>
<h3 id="482. Detecting and Reducing Bias in a High Stakes Domain.">482. Detecting and Reducing Bias in a High Stakes Domain.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1483">Paper Link</a>    Pages:4764-4774</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/222/3024.html">Ruiqi Zhong</a> ; <a href="https://dblp.uni-trier.de/pid/212/0154.html">Yanda Chen</a> ; <a href="https://dblp.uni-trier.de/pid/155/9661.html">Desmond Patton</a> ; <a href="https://dblp.uni-trier.de/pid/248/2357.html">Charlotte Selous</a> ; <a href="https://dblp.uni-trier.de/pid/m/KathleenMcKeown.html">Kathy McKeown</a></p>
<p>Abstract:
Gang-involved youth in cities such as Chicago sometimes post on social media to express their aggression towards rival gangs and previous research has demonstrated that a deep learning approach can predict aggression and loss in posts. To address the possibility of bias in this sensitive application, we developed an approach to systematically interpret the state of the art model. We found, surprisingly, that it frequently bases its predictions on stop words such as a or on, an approach that could harm social media users who have no aggressive intentions. To tackle this bias, domain experts annotated the rationales, highlighting words that explain why a tweet is labeled as aggression. These new annotations enable us to quantitatively measure how justified the model predictions are, and build models that drastically reduce bias. Our study shows that in high stake scenarios, accuracy alone cannot guarantee a good system and we need new evaluation methods.</p>
<p>Keywords:</p>
<h3 id="483. CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums.">483. CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1484">Paper Link</a>    Pages:4775-4785</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/27/4864.html">Ella Rabinovich</a> ; <a href="https://dblp.uni-trier.de/pid/248/2989.html">Masih Sultani</a> ; <a href="https://dblp.uni-trier.de/pid/99/4001.html">Suzanne Stevenson</a></p>
<p>Abstract:
In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse dataset of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of spoken language thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into written code-switching in discussion forums. The released dataset can further facilitate a range of research and practical activities.</p>
<p>Keywords:</p>
<h3 id="484. Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity.">484. Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1485">Paper Link</a>    Pages:4786-4797</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/132/1369.html">Penghui Wei</a> ; <a href="https://dblp.uni-trier.de/pid/26/4304.html">Nan Xu</a> ; <a href="https://dblp.uni-trier.de/pid/16/2159.html">Wenji Mao</a></p>
<p>Abstract:
Automatically verifying rumorous information has become an important and challenging task in natural language processing and social media analytics. Previous studies reveal that peoples stances towards rumorous messages can provide indicative clues for identifying the veracity of rumors, and thus determining the stances of public reactions is a crucial preceding step for rumor veracity prediction. In this paper, we propose a hierarchical multi-task learning framework for jointly predicting rumor stance and veracity on Twitter, which consists of two components. The bottom component of our framework classifies the stances of tweets in a conversation discussing a rumor via modeling the structural property based on a novel graph convolutional network. The top component predicts the rumor veracity by exploiting the temporal dynamics of stance evolution. Experimental results on two benchmark datasets show that our method outperforms previous methods in both rumor stance classification and veracity prediction.</p>
<p>Keywords:</p>
<h3 id="485. Reconstructing Capsule Networks for Zero-shot Intent Classification.">485. Reconstructing Capsule Networks for Zero-shot Intent Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1486">Paper Link</a>    Pages:4798-4808</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/35/2899-8.html">Han Liu</a> ; <a href="https://dblp.uni-trier.de/pid/31/2303-3.html">Xiaotong Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/47/137.html">Lu Fan</a> ; <a href="https://dblp.uni-trier.de/pid/249/5850.html">Xuandi Fu</a> ; <a href="https://dblp.uni-trier.de/pid/213/8201.html">Qimai Li</a> ; <a href="https://dblp.uni-trier.de/pid/98/2898.html">Xiao-Ming Wu</a> ; <a href="https://dblp.uni-trier.de/pid/99/8205.html">Albert Y. S. Lam</a></p>
<p>Abstract:
Intent classification is an important building block of dialogue systems. With the burgeoning of conversational AI, existing systems are not capable of handling numerous fast-emerging intents, which motivates zero-shot intent classification. Nevertheless, research on this problem is still in the incipient stage and few methods are available. A recently proposed zero-shot intent classification method, IntentCapsNet, has been shown to achieve state-of-the-art performance. However, it has two unaddressed limitations: (1) it cannot deal with polysemy when extracting semantic capsules; (2) it hardly recognizes the utterances of unseen intents in the generalized zero-shot intent classification setting. To overcome these limitations, we propose to reconstruct capsule networks for zero-shot intent classification. First, we introduce a dimensional attention mechanism to fight against polysemy. Second, we reconstruct the transformation matrices for unseen intents by utilizing abundant latent information of the labeled utterances, which significantly improves the model generalization ability. Experimental results on two task-oriented dialogue datasets in different languages show that our proposed method outperforms IntentCapsNet and other strong baselines.</p>
<p>Keywords:</p>
<h3 id="486. Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network.">486. Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1487">Paper Link</a>    Pages:4809-4819</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/217/1794.html">Shuqing Bian</a> ; <a href="https://dblp.uni-trier.de/pid/52/8700.html">Wayne Xin Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/24/4470.html">Yang Song</a> ; <a href="https://dblp.uni-trier.de/pid/15/4777.html">Tao Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/w/JRWen.html">Ji-Rong Wen</a></p>
<p>Abstract:
Person-job fit has been an important task which aims to automatically match job positions with suitable candidates. Previous methods mainly focus on solving the match task in single-domain setting, which may not work well when labeled data is limited. We study the domain adaptation problem for person-job fit. We first propose a deep global match network for capturing the global semantic interactions between two sentences from a job posting and a candidate resume respectively. Furthermore, we extend the match network and implement domain adaptation in three levels, sentence-level representation, sentence-level match, and global match. Extensive experiment results on a large real-world dataset consisting of six domains have demonstrated the effectiveness of the proposed model, especially when there is not sufficient labeled data.</p>
<p>Keywords:</p>
<h3 id="487. Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification.">487. Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1488">Paper Link</a>    Pages:4820-4829</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/141/4440.html">Linmei Hu</a> ; <a href="https://dblp.uni-trier.de/pid/20/2167.html">Tianchi Yang</a> ; <a href="https://dblp.uni-trier.de/pid/64/3041.html">Chuan Shi</a> ; <a href="https://dblp.uni-trier.de/pid/223/8227.html">Houye Ji</a> ; <a href="https://dblp.uni-trier.de/pid/l/XiaoliLi.html">Xiaoli Li</a></p>
<p>Abstract:
Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on long texts and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the graph. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly.</p>
<p>Keywords:</p>
<h3 id="488. Comparing and Developing Tools to Measure the Readability of Domain-Specific Texts.">488. Comparing and Developing Tools to Measure the Readability of Domain-Specific Texts.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1489">Paper Link</a>    Pages:4830-4841</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/141/9244.html">Elissa M. Redmiles</a> ; <a href="https://dblp.uni-trier.de/pid/254/8119.html">Lisa Maszkiewicz</a> ; <a href="https://dblp.uni-trier.de/pid/254/8051.html">Emily Hwang</a> ; <a href="https://dblp.uni-trier.de/pid/218/5259.html">Dhruv Kuchhal</a> ; <a href="https://dblp.uni-trier.de/pid/203/1577.html">Everest Liu</a> ; <a href="https://dblp.uni-trier.de/pid/186/4928.html">Miraida Morales</a> ; <a href="https://dblp.uni-trier.de/pid/203/9242.html">Denis Peskov</a> ; <a href="https://dblp.uni-trier.de/pid/165/0769.html">Sudha Rao</a> ; <a href="https://dblp.uni-trier.de/pid/79/512.html">Rock Stevens</a> ; <a href="https://dblp.uni-trier.de/pid/218/5344.html">Kristina Gligoric</a> ; <a href="https://dblp.uni-trier.de/pid/159/0234.html">Sean Kross</a> ; <a href="https://dblp.uni-trier.de/pid/20/7983.html">Michelle L. Mazurek</a> ; <a href="https://dblp.uni-trier.de/pid/77/2856.html">Hal Daum III</a></p>
<p>Abstract:
The readability of a digital text can influence peoples ability to learn new things about a range topics from digital resources (e.g., Wikipedia, WebMD). Readability also impacts search rankings, and is used to evaluate the performance of NLP systems. Despite this, we lack a thorough understanding of how to validly measure readability at scale, especially for domain-specific texts. In this work, we present a comparison of the validity of well-known readability measures and introduce a novel approach, Smart Cloze, which is designed to address shortcomings of existing measures. We compare these approaches across four different corpora: crowdworker-generated stories, Wikipedia articles, security and privacy advice, and health information. On these corpora, we evaluate the convergent and content validity of each measure, and detail tradeoffs in score precision, domain-specificity, and participant burden. These results provide a foundation for more accurate readability measurements and better evaluation of new natural-language-processing systems and tools.</p>
<p>Keywords:</p>
<h3 id="489. News2vec: News Network Embedding with Subnode Information.">489. News2vec: News Network Embedding with Subnode Information.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1490">Paper Link</a>    Pages:4842-4851</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/51/6507.html">Ye Ma</a> ; <a href="https://dblp.uni-trier.de/pid/254/8210.html">Lu Zong</a> ; <a href="https://dblp.uni-trier.de/pid/188/6973.html">Yikang Yang</a> ; <a href="https://dblp.uni-trier.de/pid/175/0585.html">Jionglong Su</a></p>
<p>Abstract:
With the development of NLP technologies, news can be automatically categorized and labeled according to a variety of characteristics, at the same time be represented as low dimensional embeddings. However, it lacks a systematic approach that effectively integrates the inherited features and inter-textual knowledge of news to represent the collective information with a dense vector. With the aim of filling this gap, the News2vec model is proposed to allow the distributed representation of news taking into account its associated features. To describe the cross-document linkages between news, a network consisting of news and its attributes is constructed. Moreover, the News2vec model treats the news node as a bag of features by developing the Subnode model. Based on the biased random walk and the skip-gram model, each news feature is mapped to a vector, and the news is thus represented as the sum of its features. This approach offers an easy solution to create embeddings for unseen news nodes based on its attributes. To evaluate our model, dimension reduction plots and correlation heat-maps are created to visualize the news vectors, together with the application of two downstream tasks, the stock movement prediction and news recommendation. By comparing with other established text/sentence embedding models, we show that News2vec achieves state-of-the-art performance on these news-related tasks.</p>
<p>Keywords:</p>
<h3 id="490. Recursive Context-Aware Lexical Simplification.">490. Recursive Context-Aware Lexical Simplification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1491">Paper Link</a>    Pages:4852-4862</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/3643.html">Sian Gooding</a> ; <a href="https://dblp.uni-trier.de/pid/140/3465.html">Ekaterina Kochmar</a></p>
<p>Abstract:
This paper presents a novel architecture for recursive context-aware lexical simplification, REC-LS, that is capable of (1) making use of the wider context when detecting the words in need of simplification and suggesting alternatives, and (2) taking previous simplification steps into account. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and outperforms the current state-of-the-art systems in lexical simplification.</p>
<p>Keywords:</p>
<h3 id="491. Leveraging Medical Literature for Section Prediction in Electronic Health Records.">491. Leveraging Medical Literature for Section Prediction in Electronic Health Records.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1492">Paper Link</a>    Pages:4863-4872</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/15/8158.html">Sara Rosenthal</a> ; <a href="https://dblp.uni-trier.de/pid/b/KenBarker2.html">Ken Barker</a> ; <a href="https://dblp.uni-trier.de/pid/63/737.html">Zhicheng Liang</a></p>
<p>Abstract:
Electronic Health Records (EHRs) contain both structured content and unstructured (text) content about a patients medical history. In the unstructured text parts, there are common sections such as Assessment and Plan, Social History, and Medications. These sections help physicians find information easily and can be used by an information retrieval system to return specific information sought by a user. However, it is common that the exact format of sections in a particular EHR does not adhere to known patterns. Therefore, being able to predict sections and headers in EHRs automatically is beneficial to physicians. Prior approaches in EHR section prediction have only used text data from EHRs and have required significant manual annotation. We propose using sections from medical literature (e.g., textbooks, journals, web content) that contain content similar to that found in EHR sections. Our approach uses data from a different kind of source where labels are provided without the need of a time-consuming annotation effort. We use this data to train two models: an RNN and a BERT-based model. We apply the learned models along with source data via transfer learning to predict sections in EHRs. Our results show that medical literature can provide helpful supervision signal for this classification task.</p>
<p>Keywords:</p>
<h3 id="492. Neural News Recommendation with Heterogeneous User Behavior.">492. Neural News Recommendation with Heterogeneous User Behavior.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1493">Paper Link</a>    Pages:4873-4882</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/1864.html">Chuhan Wu</a> ; <a href="https://dblp.uni-trier.de/pid/136/7955.html">Fangzhao Wu</a> ; <a href="https://dblp.uni-trier.de/pid/230/3834.html">Mingxiao An</a> ; <a href="https://dblp.uni-trier.de/pid/130/7814.html">Tao Qi</a> ; <a href="https://dblp.uni-trier.de/pid/207/1901.html">Jianqiang Huang</a> ; <a href="https://dblp.uni-trier.de/pid/76/6824.html">Yongfeng Huang</a> ; <a href="https://dblp.uni-trier.de/pid/08/6809-1.html">Xing Xie</a></p>
<p>Abstract:
News recommendation is important for online news platforms to help users find interested news and alleviate information overload. Existing news recommendation methods usually rely on the news click history to model user interest. However, these methods may suffer from the data sparsity problem, since the news click behaviors of many users in online news platforms are usually very limited. Fortunately, some other kinds of user behaviors such as webpage browsing and search queries can also provide useful clues of users news reading interest. In this paper, we propose a neural news recommendation approach which can exploit heterogeneous user behaviors. Our approach contains two major modules, i.e., news representation and user representation. In the news representation module, we learn representations of news from their titles via CNN networks, and apply attention networks to select important words. In the user representation module, we propose an attentive multi-view learning framework to learn unified representations of users from their heterogeneous behaviors such as search queries, clicked news and browsed webpages. In addition, we use word- and record-level attentions to select informative words and behavior records. Experiments on a real-world dataset validate the effectiveness of our approach.</p>
<p>Keywords:</p>
<h3 id="493. Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation with Hierarchical Attentive Graph Neural Network.">493. Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation with Hierarchical Attentive Graph Neural Network.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1494">Paper Link</a>    Pages:4883-4892</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/1864.html">Chuhan Wu</a> ; <a href="https://dblp.uni-trier.de/pid/136/7955.html">Fangzhao Wu</a> ; <a href="https://dblp.uni-trier.de/pid/130/7814.html">Tao Qi</a> ; <a href="https://dblp.uni-trier.de/pid/242/4765.html">Suyu Ge</a> ; <a href="https://dblp.uni-trier.de/pid/76/6824.html">Yongfeng Huang</a> ; <a href="https://dblp.uni-trier.de/pid/08/6809-1.html">Xing Xie</a></p>
<p>Abstract:
User and item representation learning is critical for recommendation. Many of existing recommendation methods learn representations of users and items based on their ratings and reviews. However, the user-user and item-item relatedness are usually not considered in these methods, which may be insufficient. In this paper, we propose a neural recommendation approach which can utilize useful information from both review content and user-item graphs. Since reviews and graphs have different characteristics, we propose to use a multi-view learning framework to incorporate them as different views. In the review content-view, we propose to use a hierarchical model to first learn sentence representations from words, then learn review representations from sentences, and finally learn user/item representations from reviews. In addition, we propose to incorporate a three-level attention network into this view to select important words, sentences and reviews for learning informative user and item representations. In the graph-view, we propose a hierarchical graph neural network to jointly model the user-item, user-user and item-item relatedness by capturing the first- and second-order interactions between users and items in the user-item graph. In addition, we apply attention mechanism to model the importance of these interactions to learn informative user and item representations. Extensive experiments on four benchmark datasets validate the effectiveness of our approach.</p>
<p>Keywords:</p>
<h3 id="494. Event Representation Learning Enhanced with External Commonsense Knowledge.">494. Event Representation Learning Enhanced with External Commonsense Knowledge.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1495">Paper Link</a>    Pages:4893-4902</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/44/10861.html">Xiao Ding</a> ; <a href="https://dblp.uni-trier.de/pid/221/3714.html">Kuo Liao</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pid/128/6689.html">Zhongyang Li</a> ; <a href="https://dblp.uni-trier.de/pid/153/9564.html">Junwen Duan</a></p>
<p>Abstract:
Prior work has proposed effective methods to learn event representations that can capture syntactic and semantic information over text corpus, demonstrating their effectiveness for downstream tasks such as script event prediction. On the other hand, events extracted from raw texts lacks of commonsense knowledge, such as the intents and emotions of the event participants, which are useful for distinguishing event pairs when there are only subtle differences in their surface realizations. To address this issue, this paper proposes to leverage external commonsense knowledge about the intent and sentiment of the event. Experiments on three event-related tasks, i.e., event similarity, script event prediction and stock market prediction, show that our model obtains much better event embeddings for the tasks, achieving 78% improvements on hard similarity task, yielding more precise inferences on subsequent events under given contexts, and better accuracies in predicting the volatilities of the stock market.</p>
<p>Keywords:</p>
<h3 id="495. Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification.">495. Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1496">Paper Link</a>    Pages:4903-4912</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/146/9862.html">Yichao Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/130/8167.html">Jyun-Yu Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/18/2428.html">Kai-Wei Chang</a> ; <a href="https://dblp.uni-trier.de/pid/w/WeiWang.html">Wei Wang</a></p>
<p>Abstract:
Adversarial attacks against machine learning models have threatened various real-world applications such as spam filtering and sentiment analysis. In this paper, we propose a novel framework, learning to discriminate perturbations (DISP), to identify and adjust malicious perturbations, thereby blocking adversarial attacks for text classification models. To identify adversarial attacks, a perturbation discriminator validates how likely a token in the text is perturbed and provides a set of potential perturbations. For each potential perturbation, an embedding estimator learns to restore the embedding of the original word based on the context and a replacement token is chosen based on approximate kNN search. DISP can block adversarial attacks for any NLP model without modifying the model structure or training procedure. Extensive experiments on two benchmark datasets demonstrate that DISP significantly outperforms baseline methods in blocking adversarial attacks for text classification. In addition, in-depth analysis shows the robustness of DISP across different situations.</p>
<p>Keywords:</p>
<h3 id="496. A Neural Citation Count Prediction Model based on Peer Review Text.">496. A Neural Citation Count Prediction Model based on Peer Review Text.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1497">Paper Link</a>    Pages:4913-4923</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/240/0322.html">Siqing Li</a> ; <a href="https://dblp.uni-trier.de/pid/52/8700.html">Wayne Xin Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/254/7949.html">Eddy Jing Yin</a> ; <a href="https://dblp.uni-trier.de/pid/w/JRWen.html">Ji-Rong Wen</a></p>
<p>Abstract:
Citation count prediction (CCP) has been an important research task for automatically estimating the future impact of a scholarly paper. Previous studies mainly focus on extracting or mining useful features from the paper itself or the associated authors. An important kind of data signals, peer review text, has not been utilized for the CCP task. In this paper, we take the initiative to utilize peer review data for the CCP task with a neural prediction model. Our focus is to learn a comprehensive semantic representation for peer review text for improving the prediction performance. To achieve this goal, we incorporate the abstract-review match mechanism and the cross-review match mechanism to learn deep features from peer review text. We also consider integrating hand-crafted features via a wide component. The deep and wide components jointly make the prediction. Extensive experiments have demonstrated the usefulness of the peer review data and the effectiveness of the proposed model. Our dataset has been released online.</p>
<p>Keywords:</p>
<h3 id="497. Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs.">497. Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1498">Paper Link</a>    Pages:4924-4935</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/186/7107.html">Fenia Christopoulou</a> ; <a href="https://dblp.uni-trier.de/pid/29/456.html">Makoto Miwa</a> ; <a href="https://dblp.uni-trier.de/pid/47/4142.html">Sophia Ananiadou</a></p>
<p>Abstract:
Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.</p>
<p>Keywords:</p>
<h3 id="498. Semi-supervised Text Style Transfer: Cross Projection in Latent Space.">498. Semi-supervised Text Style Transfer: Cross Projection in Latent Space.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1499">Paper Link</a>    Pages:4936-4945</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/220/3102.html">Mingyue Shang</a> ; <a href="https://dblp.uni-trier.de/pid/77/8278.html">Piji Li</a> ; <a href="https://dblp.uni-trier.de/pid/209/8408.html">Zhenxin Fu</a> ; <a href="https://dblp.uni-trier.de/pid/53/6625.html">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pid/63/1870.html">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/s/ShumingShi-1.html">Shuming Shi</a> ; <a href="https://dblp.uni-trier.de/pid/19/2405-1.html">Rui Yan</a></p>
<p>Abstract:
Text style transfer task requires the model to transfer a sentence of one style to another style while retaining its original content meaning, which is a challenging problem that has long suffered from the shortage of parallel data. In this paper, we first propose a semi-supervised text style transfer model that combines the small-scale parallel data with the large-scale nonparallel data. With these two types of training data, we introduce a projection function between the latent space of different styles and design two constraints to train it. We also introduce two other simple but effective semi-supervised methods to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese.</p>
<p>Keywords:</p>
<h3 id="499. Question Answering for Privacy Policies: Combining Computational and Legal Perspectives.">499. Question Answering for Privacy Policies: Combining Computational and Legal Perspectives.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1500">Paper Link</a>    Pages:4946-4957</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/170/4795.html">Abhilasha Ravichander</a> ; <a href="https://dblp.uni-trier.de/pid/b/AlanWBlack.html">Alan W. Black</a> ; <a href="https://dblp.uni-trier.de/pid/98/8884.html">Shomir Wilson</a> ; <a href="https://dblp.uni-trier.de/pid/183/8184.html">Thomas B. Norton</a> ; <a href="https://dblp.uni-trier.de/pid/18/5502.html">Norman M. Sadeh</a></p>
<p>Abstract:
Privacy policies are long and complex documents that are difficult for users to read and understand. Yet, they have legal effects on how user data can be collected, managed and used. Ideally, we would like to empower users to inform themselves about the issues that matter to them, and enable them to selectively explore these issues. We present PrivacyQA, a corpus consisting of 1750 questions about the privacy policies of mobile applications, and over 3500 expert annotations of relevant answers. We observe that a strong neural baseline underperforms human performance by almost 0.3 F1 on PrivacyQA, suggesting considerable room for improvement for future systems. Further, we use this dataset to categorically identify challenges to question answerability, with domain-general implications for any question answering system. The PrivacyQA corpus offers a challenging corpus for question answering, with genuine real world utility.</p>
<p>Keywords:</p>
<h3 id="500. Stick to the Facts: Learning towards a Fidelity-oriented E-Commerce Product Description Generation.">500. Stick to the Facts: Learning towards a Fidelity-oriented E-Commerce Product Description Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1501">Paper Link</a>    Pages:4958-4967</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/224/5573.html">Zhangming Chan</a> ; <a href="https://dblp.uni-trier.de/pid/33/11343.html">Xiuying Chen</a> ; <a href="https://dblp.uni-trier.de/pid/89/5684.html">Yongliang Wang</a> ; <a href="https://dblp.uni-trier.de/pid/32/971.html">Juntao Li</a> ; <a href="https://dblp.uni-trier.de/pid/67/2010-11.html">Zhiqiang Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/59/2902.html">Kun Gai</a> ; <a href="https://dblp.uni-trier.de/pid/63/1870.html">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/19/2405-1.html">Rui Yan</a></p>
<p>Abstract:
Different from other text generation tasks, in product description generation, it is of vital importance to generate faithful descriptions that stick to the product attribute information. However, little attention has been paid to this problem. To bridge this gap we propose a model named Fidelity-oriented Product Description Generator (FPDG). FPDG takes the entity label of each word into account, since the product attribute information is always conveyed by entity words. Specifically, we first propose a Recurrent Neural Network (RNN) decoder based on the Entity-label-guided Long Short-Term Memory (ELSTM) cell, taking both the embedding and the entity label of each word as input. Second, we establish a keyword memory that stores the entity labels as keys and keywords as values, and FPDG will attend to keywords through attending to their entity labels. Experiments conducted a large-scale real-world product description dataset show that our model achieves the state-of-the-art performance in terms of both traditional generation metrics as well as human evaluations. Specifically, FPDG increases the fidelity of the generated descriptions by 25%.</p>
<p>Keywords:</p>
<h3 id="501. Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks.">501. Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1502">Paper Link</a>    Pages:4968-4977</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/190/7770.html">Hailong Jin</a> ; <a href="https://dblp.uni-trier.de/pid/32/5685-1.html">Lei Hou</a> ; <a href="https://dblp.uni-trier.de/pid/l/JuanZiLi.html">Juanzi Li</a> ; <a href="https://dblp.uni-trier.de/pid/24/6781.html">Tiansi Dong</a></p>
<p>Abstract:
This paper addresses the problem of inferring the fine-grained type of an entity from a knowledge base. We convert this problem into the task of graph-based semi-supervised classification, and propose Hierarchical Multi Graph Convolutional Network (HMGCN), a novel Deep Learning architecture to tackle this problem. We construct three kinds of connectivity matrices to capture different kinds of semantic correlations between entities. A recursive regularization is proposed to model the subClassOf relations between types in given type hierarchy. Extensive experiments with two large-scale public datasets show that our proposed method significantly outperforms four state-of-the-art methods.</p>
<p>Keywords:</p>
<h3 id="502. Learning to Infer Entities, Properties and their Relations from Clinical Conversations.">502. Learning to Infer Entities, Properties and their Relations from Clinical Conversations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1503">Paper Link</a>    Pages:4978-4989</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/86/4539.html">Nan Du</a> ; <a href="https://dblp.uni-trier.de/pid/150/0985.html">Mingqiu Wang</a> ; <a href="https://dblp.uni-trier.de/pid/130/8465.html">Linh Tran</a> ; <a href="https://dblp.uni-trier.de/pid/254/8113.html">Gang Lee</a> ; <a href="https://dblp.uni-trier.de/pid/66/3591.html">Izhak Shafran</a></p>
<p>Abstract:
Recently we proposed the Span Attribute Tagging (SAT) Model to infer clinical entities (e.g., symptoms) and their properties (e.g., duration). It tackles the challenge of large label space and limited training data using a hierarchical two-stage approach that identifies the span of interest in a tagging step and assigns labels to the span in a classification step. We extend the SAT model to jointly infer not only entities and their properties but also relations between them. Most relation extraction models restrict inferring relations between tokens within a few neighboring sentences, mainly to avoid high computational complexity. In contrast, our proposed Relation-SAT (R-SAT) model is computationally efficient and can infer relations over the entire conversation, spanning an average duration of 10 minutes. We evaluate our model on a corpus of clinical conversations. When the entities are given, the R-SAT outperforms baselines in identifying relations between symptoms and their properties by about 32% (0.82 vs 0.62 F-score) and by about 50% (0.60 vs 0.41 F-score) on medications and their properties. On the more difficult task of jointly inferring entities and relations, the R-SAT model achieves a performance of 0.34 and 0.45 for symptoms and medications respectively, which is significantly better than 0.18 and 0.35 for the baseline model. The contributions of different components of the model are quantified using ablation analysis.</p>
<p>Keywords:</p>
<h3 id="503. Practical Correlated Topic Modeling and Analysis via the Rectified Anchor Word Algorithm.">503. Practical Correlated Topic Modeling and Analysis via the Rectified Anchor Word Algorithm.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1504">Paper Link</a>    Pages:4990-5000</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/132/1761.html">Moontae Lee</a> ; <a href="https://dblp.uni-trier.de/pid/254/8021.html">Sungjun Cho</a> ; <a href="https://dblp.uni-trier.de/pid/96/6719.html">David Bindel</a> ; <a href="https://dblp.uni-trier.de/pid/39/5487.html">David Mimno</a></p>
<p>Abstract:
Despite great scalability on large data and their ability to understand correlations between topics, spectral topic models have not been widely used due to the absence of reliability in real data and lack of practical implementations. This paper aims to solidify the foundations of spectral topic inference and provide a practical implementation for anchor-based topic modeling. Beginning with vocabulary curation, we scrutinize every single inference step with other viable options. We also evaluate our matrix-based approach against popular alternatives including a tensor-based spectral method as well as probabilistic algorithms. Our quantitative and qualitative experiments demonstrate the power of Rectified Anchor Word algorithm in various real datasets, providing a complete guide to practical correlated topic modeling.</p>
<p>Keywords:</p>
<h3 id="504. Modeling the Relationship between User Comments and Edits in Document Revision.">504. Modeling the Relationship between User Comments and Edits in Document Revision.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1505">Paper Link</a>    Pages:5001-5010</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/188/3475.html">Xuchao Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/127/0193.html">Dheeraj Rajagopal</a> ; <a href="https://dblp.uni-trier.de/pid/g/MichaelGamon.html">Michael Gamon</a> ; <a href="https://dblp.uni-trier.de/pid/136/8739.html">Sujay Kumar Jauhar</a> ; <a href="https://dblp.uni-trier.de/pid/08/4367.html">Chang-Tien Lu</a></p>
<p>Abstract:
Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a documents evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we achieve 74.4% accuracy on Edit Anchoring.</p>
<p>Keywords:</p>
<h3 id="505. PRADO: Projection Attention Networks for Document Classification On-Device.">505. PRADO: Projection Attention Networks for Document Classification On-Device.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1506">Paper Link</a>    Pages:5011-5020</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8343.html">Karthik Krishnamoorthi</a> ; <a href="https://dblp.uni-trier.de/pid/39/5111.html">Sujith Ravi</a> ; <a href="https://dblp.uni-trier.de/pid/63/5321.html">Zornitsa Kozareva</a></p>
<p>Abstract:
Recently, there has been a great interest in the development of small and accurate neural networks that run entirely on devices such as mobile phones, smart watches and IoT. This enables user privacy, consistent user experience and low latency. Although a wide range of applications have been targeted from wake word detection to short text classification, yet there are no on-device networks for long text classification. We propose a novel projection attention neural network PRADO that combines trainable projections with attention and convolutions. We evaluate our approach on multiple large document text classification tasks. Our results show the effectiveness of the trainable projection model in finding semantically similar phrases and reaching high performance while maintaining compact size. Using this approach, we train tiny neural networks just 200 Kilobytes in size that improve over prior CNN and LSTM models and achieve near state of the art performance on multiple long document classification tasks. We also apply our model for transfer learning, show its robustness and ability to further improve the performance in limited data scenarios.</p>
<p>Keywords:</p>
<h3 id="506. Subword Language Model for Query Auto-Completion.">506. Subword Language Model for Query Auto-Completion.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1507">Paper Link</a>    Pages:5021-5031</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/172/0889.html">Gyuwan Kim</a></p>
<p>Abstract:
Current neural query auto-completion (QAC) systems rely on character-level language models, but they slow down when queries are long. We present how to utilize subword language models for the fast and accurate generation of query completion candidates. Representing queries with subwords shorten a decoding length significantly. To deal with issues coming from introducing subword language model, we develop a retrace algorithm and a reranking method by approximate marginalization. As a result, our model achieves up to 2.5 times faster while maintaining a similar quality of generated results compared to the character-level baseline. Also, we propose a new evaluation metric, mean recoverable length (MRL), measuring how many upcoming characters the model could complete correctly. It provides more explicit meaning and eliminates the need for prefix length sampling for existing rank-based metrics. Moreover, we performed a comprehensive analysis with ablation study to figure out the importance of each component.</p>
<p>Keywords:</p>
<h3 id="507. Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph.">507. Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1508">Paper Link</a>    Pages:5032-5041</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/7914.html">Xinzhu Lin</a> ; <a href="https://dblp.uni-trier.de/pid/21/8619.html">Xiahui He</a> ; <a href="https://dblp.uni-trier.de/pid/43/2709.html">Qin Chen</a> ; <a href="https://dblp.uni-trier.de/pid/211/4152.html">Huaixiao Tou</a> ; <a href="https://dblp.uni-trier.de/pid/31/10489.html">Zhongyu Wei</a> ; <a href="https://dblp.uni-trier.de/pid/19/1766.html">Ting Chen</a></p>
<p>Abstract:
Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset.</p>
<p>Keywords:</p>
<h3 id="508. Counterfactual Story Reasoning and Generation.">508. Counterfactual Story Reasoning and Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1509">Paper Link</a>    Pages:5042-5052</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/184/3753.html">Lianhui Qin</a> ; <a href="https://dblp.uni-trier.de/pid/184/3742.html">Antoine Bosselut</a> ; <a href="https://dblp.uni-trier.de/pid/205/9029.html">Ari Holtzman</a> ; <a href="https://dblp.uni-trier.de/pid/151/3093.html">Chandra Bhagavatula</a> ; <a href="https://dblp.uni-trier.de/pid/148/6935.html">Elizabeth Clark</a> ; <a href="https://dblp.uni-trier.de/pid/89/579.html">Yejin Choi</a></p>
<p>Abstract:
Counterfactual reasoning requires predicting how alternative events, contrary to what actually happened, might have resulted in different outcomes. Despite being considered a necessary component of AI-complete systems, few resources have been developed for evaluating counterfactual reasoning in narratives. In this paper, we propose Counterfactual Story Rewriting: given an original story and an intervening counterfactual event, the task is to minimally revise the story to make it compatible with the given counterfactual event. Solving this task will require deep understanding of causal narrative chains and counterfactual invariance, and integration of such story reasoning capabilities into conditional language generation models. We present TIMETRAVEL, a new dataset of 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. Additionally, we include 81,407 counterfactual branches without a rewritten storyline to support future work on semi- or un-supervised approaches to counterfactual story rewriting. Finally, we evaluate the counterfactual rewriting capacities of several competitive baselines based on pretrained language models, and assess whether common overlap and model-based automatic metrics for text generation correlate well with human scores for counterfactual rewriting.</p>
<p>Keywords:</p>
<h3 id="509. Encode, Tag, Realize: High-Precision Text Editing.">509. Encode, Tag, Realize: High-Precision Text Editing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1510">Paper Link</a>    Pages:5053-5064</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/46/10259.html">Eric Malmi</a> ; <a href="https://dblp.uni-trier.de/pid/95/9281.html">Sebastian Krause</a> ; <a href="https://dblp.uni-trier.de/pid/148/9544.html">Sascha Rothe</a> ; <a href="https://dblp.uni-trier.de/pid/54/10257.html">Daniil Mirylenka</a> ; <a href="https://dblp.uni-trier.de/pid/86/8354.html">Aliaksei Severyn</a></p>
<p>Abstract:
We propose LaserTagger - a sequence tagging approach that casts text generation as a text editing task. Target texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. To predict the edit operations, we propose a novel model, which combines a BERT encoder with an autoregressive Transformer decoder. This approach is evaluated on English text on four tasks: sentence fusion, sentence splitting, abstractive summarization, and grammar correction. LaserTagger achieves new state-of-the-art results on three of these tasks, performs comparably to a set of strong seq2seq baselines with a large number of training examples, and outperforms them when the number of examples is limited. Furthermore, we show that at inference time tagging can be more than two orders of magnitude faster than comparable seq2seq models, making it more attractive for running in a live environment.</p>
<p>Keywords:</p>
<h3 id="510. Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation.">510. Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1511">Paper Link</a>    Pages:5065-5075</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/79/1394.html">Weichao Wang</a> ; <a href="https://dblp.uni-trier.de/pid/97/1374.html">Shi Feng</a> ; <a href="https://dblp.uni-trier.de/pid/37/2233.html">Daling Wang</a> ; <a href="https://dblp.uni-trier.de/pid/55/5266-3.html">Yifei Zhang</a></p>
<p>Abstract:
Generating intriguing question is a key step towards building human-like open-domain chatbots. Although some recent works have focused on this task, compared with questions raised by humans, significant gaps remain in maintaining semantic coherence with post, which may result in generating dull or deviated questions. We observe that the answer has strong semantic coherence to its question and post, which can be used to guide question generation. Thus, we devise two methods to further enhance semantic coherence between post and question under the guidance of answer. First, the coherence score between generated question and answer is used as the reward function in a reinforcement learning framework, to encourage the cases that are consistent with the answer in semantic. Second, we incorporate adversarial training to explicitly control question generation in the direction of question-answer coherence. Extensive experiments show that our two methods outperform state-of-the-art baseline algorithms with large margins in raising semantic coherent questions.</p>
<p>Keywords:</p>
<h3 id="511. Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation.">511. Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1512">Paper Link</a>    Pages:5076-5088</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/158/1178.html">Ze Yang</a> ; <a href="https://dblp.uni-trier.de/pid/33/965.html">Can Xu</a> ; <a href="https://dblp.uni-trier.de/pid/95/6985-14.html">Wei Wu</a> ; <a href="https://dblp.uni-trier.de/pid/76/2866.html">Zhoujun Li</a></p>
<p>Abstract:
Automatic news comment generation is beneficial for real applications but has not attracted enough attention from the research community. In this paper, we propose a read-attend-comment procedure for news comment generation and formalize the procedure with a reading network and a generation network. The reading network comprehends a news article and distills some important points from it, then the generation network creates a comment by attending to the extracted discrete points and the news title. We optimize the model in an end-to-end manner by maximizing a variational lower bound of the true objective using the back-propagation algorithm. Experimental results on two public datasets indicate that our model can significantly outperform existing methods in terms of both automatic evaluation and human judgment.</p>
<p>Keywords:</p>
<h3 id="512. A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features.">512. A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1513">Paper Link</a>    Pages:5089-5098</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/184/8085.html">Hongyin Tang</a> ; <a href="https://dblp.uni-trier.de/pid/39/9.html">Miao Li</a> ; <a href="https://dblp.uni-trier.de/pid/75/1099.html">Beihong Jin</a></p>
<p>Abstract:
Text generation is among the most fundamental tasks in natural language processing. In this paper, we propose a text generation model that learns semantics and structural features simultaneously. This model captures structural features by a sequential variational autoencoder component and leverages a topic modeling component based on Gaussian distribution to enhance the recognition of text semantics. To make the reconstructed text more coherent to the topics, the model further adapts the encoder of the topic modeling component for a discriminator. The results of experiments over several datasets demonstrate that our model outperforms several states of the art models in terms of text perplexity and topic coherence. Moreover, the latent representations learned by our model is superior to others in a text classification task. Finally, given the input texts, our model can generate meaningful texts which hold similar structures but under different topics.</p>
<p>Keywords:</p>
<h3 id="513. LXMERT: Learning Cross-Modality Encoder Representations from Transformers.">513. LXMERT: Learning Cross-Modality Encoder Representations from Transformers.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1514">Paper Link</a>    Pages:5099-5110</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/94/877.html">Hao Tan</a> ; <a href="https://dblp.uni-trier.de/pid/32/5243.html">Mohit Bansal</a></p>
<p>Abstract:
Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: <a href="https://github.com/airsplay/lxmert">https://github.com/airsplay/lxmert</a></p>
<p>Keywords:</p>
<h3 id="514. Phrase Grounding by Soft-Label Chain Conditional Random Field.">514. Phrase Grounding by Soft-Label Chain Conditional Random Field.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1515">Paper Link</a>    Pages:5111-5121</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/202/3902-4.html">Jiacheng Liu</a> ; <a href="https://dblp.uni-trier.de/pid/64/2448.html">Julia Hockenmaier</a></p>
<p>Abstract:
The phrase grounding task aims to ground each entity mention in a given caption of an image to a corresponding region in that image. Although there are clear dependencies between how different mentions of the same caption should be grounded, previous structured prediction methods that aim to capture such dependencies need to resort to approximate inference or non-differentiable losses. In this paper, we formulate phrase grounding as a sequence labeling task where we treat candidate regions as potential labels, and use neural chain Conditional Random Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast to standard sequence labeling tasks, the phrase grounding task is defined such that there may be multiple correct candidate regions. To address this multiplicity of gold labels, we define so-called Soft-Label Chain CRFs, and present an algorithm that enables convenient end-to-end training. Our method establishes a new state-of-the-art on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our model benefits both from the entity dependencies captured by the CRF and from the soft-label training regime. Our code is available at github.com/liujch1998/SoftLabelCCRF</p>
<p>Keywords:</p>
<h3 id="515. What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues.">515. What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1516">Paper Link</a>    Pages:5122-5131</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/142/2892.html">Xintong Yu</a> ; <a href="https://dblp.uni-trier.de/pid/48/859.html">Hongming Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/86/2159.html">Yangqiu Song</a> ; <a href="https://dblp.uni-trier.de/pid/09/1398.html">Yan Song</a> ; <a href="https://dblp.uni-trier.de/pid/z/ChangshuiZhang.html">Changshui Zhang</a></p>
<p>Abstract:
Grounding a pronoun to a visual object it refers to requires complex reasoning from various information sources, especially in conversational scenarios. For example, when people in a conversation talk about something all speakers can see, they often directly use pronouns (e.g., it) to refer to it without previous introduction. This fact brings a huge challenge for modern natural language understanding systems, particularly conventional context-based pronoun coreference models. To tackle this challenge, in this paper, we formally define the task of visual-aware pronoun coreference resolution (PCR) and introduce VisPro, a large-scale dialogue PCR dataset, to investigate whether and how the visual information can help resolve pronouns in dialogues. We then propose a novel visual-aware PCR model, VisCoref, for this task and conduct comprehensive experiments and case studies on our dataset. Results demonstrate the importance of the visual information in this PCR case and show the effectiveness of the proposed model.</p>
<p>Keywords:</p>
<h3 id="516. YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension.">516. YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1517">Paper Link</a>    Pages:5132-5142</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/7810.html">Weiying Wang</a> ; <a href="https://dblp.uni-trier.de/pid/06/4175.html">Yongcheng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/153/0734.html">Shizhe Chen</a> ; <a href="https://dblp.uni-trier.de/pid/47/2670.html">Qin Jin</a></p>
<p>Abstract:
Multimodal semantic comprehension has attracted increasing research interests recently such as visual question answering and caption generation. However, due to the data limitation, fine-grained semantic comprehension has not been well investigated, which requires to capture semantic details of multimodal contents. In this work, we introduce YouMakeup, a large-scale multimodal instructional video dataset to support fine-grained semantic comprehension research in specific domain. YouMakeup contains 2,800 videos from YouTube, spanning more than 420 hours in total. Each video is annotated with a sequence of natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. The annotated steps in a video involve subtle difference in actions, products and regions, which requires fine-grained understanding and reasoning both temporally and spatially. In order to evaluate models ability for fined-grained comprehension, we further propose two groups of tasks including generation tasks and visual question answering from different aspects. We also establish a baseline of step caption generation for future comparison. The dataset will be publicly available at <a href="https://github">https://github</a>. com/AIM3-RUC/YouMakeup to support research investigation in fine-grained semantic comprehension.</p>
<p>Keywords:</p>
<h3 id="517. DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization.">517. DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1518">Paper Link</a>    Pages:5143-5152</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8243.html">Chujie Lu</a> ; <a href="https://dblp.uni-trier.de/pid/64/5725-16.html">Long Chen</a> ; <a href="https://dblp.uni-trier.de/pid/236/9183.html">Chilie Tan</a> ; <a href="https://dblp.uni-trier.de/pid/56/10261.html">Xiaolin Li</a> ; <a href="https://dblp.uni-trier.de/pid/71/2308-1.html">Jun Xiao</a></p>
<p>Abstract:
In this paper, we focus on natural language video localization: localizing (ie, grounding) a natural language description in a long and untrimmed video sequence. All currently published models for addressing this problem can be categorized into two types: (i) top-down approach: it does classification and regression for a set of pre-cut video segment candidates; (ii) bottom-up approach: it directly predicts probabilities for each video frame as the temporal boundaries (ie, start and end time point). However, both two approaches suffer several limitations: the former is computation-intensive for densely placed candidates, while the latter has trailed the performance of the top-down counterpart thus far. To this end, we propose a novel dense bottom-up framework: DEnse Bottom-Up Grounding (DEBUG). DEBUG regards all frames falling in the ground truth segment as foreground, and each foreground frame regresses the unique distances from its location to bi-directional ground truth boundaries. Extensive experiments on three challenging benchmarks (TACoS, Charades-STA, and ActivityNet Captions) show that DEBUG is able to match the speed of bottom-up models while surpassing the performance of the state-of-the-art top-down models.</p>
<p>Keywords:</p>
<h3 id="518. CrossWeigh: Training Named Entity Tagger from Imperfect Annotations.">518. CrossWeigh: Training Named Entity Tagger from Imperfect Annotations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1519">Paper Link</a>    Pages:5153-5162</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/152/5077.html">Zihan Wang</a> ; <a href="https://dblp.uni-trier.de/pid/151/3145.html">Jingbo Shang</a> ; <a href="https://dblp.uni-trier.de/pid/06/1624.html">Liyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/174/4349.html">Lihao Lu</a> ; <a href="https://dblp.uni-trier.de/pid/202/3902-4.html">Jiacheng Liu</a> ; <a href="https://dblp.uni-trier.de/pid/h/JiaweiHan.html">Jiawei Han</a></p>
<p>Abstract:
Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo <a href="https://github.com/ZihanWangKi/CrossWeigh">https://github.com/ZihanWangKi/CrossWeigh</a>.</p>
<p>Keywords:</p>
<h3 id="519. A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers.">519. A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1520">Paper Link</a>    Pages:5163-5173</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/225/7684.html">Aditi Chaudhary</a> ; <a href="https://dblp.uni-trier.de/pid/203/0839.html">Jiateng Xie</a> ; <a href="https://dblp.uni-trier.de/pid/136/4968.html">Zaid Sheikh</a> ; <a href="https://dblp.uni-trier.de/pid/03/8155.html">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pid/56/3395.html">Jaime G. Carbonell</a></p>
<p>Abstract:
Most state-of-the-art models for named entity recognition (NER) rely on the availability of large amounts of labeled data, making them challenging to extend to new, lower-resourced languages. However, there are now many proposed solutions to this problem involving either cross-lingual transfer learning, which learns from other highly resourced languages, or active learning, which efficiently selects effective training data based on model predictions. In this paper, we ask the question: given this recent progress, and some amount of human annotation, what is the most effective method for efficiently creating high-quality entity recognizers in under-resourced languages? Based on extensive experimentation using both simulated and real human annotation, we settle on a recipe of starting with a cross-lingual transferred model, then performing targeted annotation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data.</p>
<p>Keywords:</p>
<h3 id="520. Open Domain Web Keyphrase Extraction Beyond Language Modeling.">520. Open Domain Web Keyphrase Extraction Beyond Language Modeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1521">Paper Link</a>    Pages:5174-5183</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/252/5432.html">Lee Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/47/11058.html">Chuan Hu</a> ; <a href="https://dblp.uni-trier.de/pid/18/10886.html">Chenyan Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/64/1314.html">Daniel Campos</a> ; <a href="https://dblp.uni-trier.de/pid/16/7404.html">Arnold Overwijk</a></p>
<p>Abstract:
This paper studies keyphrase extraction in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond language understanding using visual presentations of documents and weak supervision from search queries. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of learning from the open domain data compared to a specific domain.</p>
<p>Keywords:</p>
<h3 id="521. TuckER: Tensor Factorization for Knowledge Graph Completion.">521. TuckER: Tensor Factorization for Knowledge Graph Completion.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1522">Paper Link</a>    Pages:5184-5193</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/185/0837.html">Ivana Balazevic</a> ; <a href="https://dblp.uni-trier.de/pid/220/5654.html">Carl Allen</a> ; <a href="https://dblp.uni-trier.de/pid/32/3545.html">Timothy M. Hospedales</a></p>
<p>Abstract:
Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively straightforward but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms previous state-of-the-art models across standard link prediction datasets, acting as a strong baseline for more elaborate models. We show that TuckER is a fully expressive model, derive sufficient bounds on its embedding dimensionalities and demonstrate that several previously introduced linear models can be viewed as special cases of TuckER.</p>
<p>Keywords:</p>
<h3 id="522. Human-grounded Evaluations of Explanation Methods for Text Classification.">522. Human-grounded Evaluations of Explanation Methods for Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1523">Paper Link</a>    Pages:5194-5204</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/3272.html">Piyawat Lertvittayakumjorn</a> ; <a href="https://dblp.uni-trier.de/pid/t/FrancescaToni.html">Francesca Toni</a></p>
<p>Abstract:
Due to the black-box nature of deep learning models, methods for explaining the models results are crucial to gain trust from humans and support collaboration between AIs and humans. In this paper, we consider several model-agnostic and model-specific explanation methods for CNNs for text classification and conduct three human-grounded evaluations, focusing on different purposes of explanations: (1) revealing model behavior, (2) justifying model predictions, and (3) helping humans investigate uncertain predictions. The results highlight dissimilar qualities of the various explanation methods we consider and show the degree to which these methods could serve for each purpose.</p>
<p>Keywords:</p>
<h3 id="523. A Context-based Framework for Modeling the Role and Function of On-line Resource Citations in Scientific Literature.">523. A Context-based Framework for Modeling the Role and Function of On-line Resource Citations in Scientific Literature.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1524">Paper Link</a>    Pages:5205-5214</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/98/3487.html">He Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/82/11518.html">Zhunchen Luo</a> ; <a href="https://dblp.uni-trier.de/pid/11/4926.html">Chong Feng</a> ; <a href="https://dblp.uni-trier.de/pid/254/8308.html">Anqing Zheng</a> ; <a href="https://dblp.uni-trier.de/pid/15/1247.html">Xiaopeng Liu</a></p>
<p>Abstract:
We introduce a new task of modeling the role and function for on-line resource citations in scientific literature. By categorizing the on-line resources and analyzing the purpose of resource citations in scientific texts, it can greatly help resource search and recommendation systems to better understand and manage the scientific resources. For this novel task, we are the first to create an annotation scheme, which models the different granularity of information from a hierarchical perspective. And we construct a dataset SciRes, which includes 3,088 manually annotated resource contexts. In this paper, we propose a possible solution by using a multi-task framework to build the scientific resource classifier (SciResCLF) for jointly recognizing the role and function types. Then we use the classification results to help a scientific resource recommendation (SciResREC) task. Experiments show that our model achieves the best results on both the classification task and the recommendation task. The SciRes dataset is released for future research.</p>
<p>Keywords:</p>
<h3 id="524. Adversarial Reprogramming of Text Classification Neural Networks.">524. Adversarial Reprogramming of Text Classification Neural Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1525">Paper Link</a>    Pages:5215-5224</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/194/3168.html">Paarth Neekhara</a> ; <a href="https://dblp.uni-trier.de/pid/227/2207.html">Shehzeen Hussain</a> ; <a href="https://dblp.uni-trier.de/pid/89/4032.html">Shlomo Dubnov</a> ; <a href="https://dblp.uni-trier.de/pid/k/FarinazKoushanfar.html">Farinaz Koushanfar</a></p>
<p>Abstract:
In this work, we develop methods to repurpose text classification neural networks for alternate tasks without modifying the network architecture or parameters. We propose a context based vocabulary remapping method that performs a computationally inexpensive input transformation to reprogram a victim classification model for a new set of sequences. We propose algorithms for training such an input transformation in both white box and black box settings where the adversary may or may not have access to the victim models architecture and parameters. We demonstrate the application of our model and the vulnerability of neural networks by adversarially repurposing various text-classification models including LSTM, bi-directional LSTM and CNN for alternate classification tasks.</p>
<p>Keywords:</p>
<h3 id="525. Document Hashing with Mixture-Prior Generative Models.">525. Document Hashing with Mixture-Prior Generative Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1526">Paper Link</a>    Pages:5225-5234</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/92/748.html">Wei Dong</a> ; <a href="https://dblp.uni-trier.de/pid/87/7936.html">Qinliang Su</a> ; <a href="https://dblp.uni-trier.de/pid/202/2287.html">Dinghan Shen</a> ; <a href="https://dblp.uni-trier.de/pid/65/2802.html">Changyou Chen</a></p>
<p>Abstract:
Hashing is promising for large-scale information retrieval tasks thanks to the efficiency of distance evaluation between binary codes. Generative hashing is often used to generate hashing codes in an unsupervised way. However, existing generative hashing methods only considered the use of simple priors, like Gaussian and Bernoulli priors, which limits these methods to further improve their performance. In this paper, two mixture-prior generative models are proposed, under the objective to produce high-quality hashing codes for documents. Specifically, a Gaussian mixture prior is first imposed onto the variational auto-encoder (VAE), followed by a separate step to cast the continuous latent representation of VAE into binary code. To avoid the performance loss caused by the separate casting, a model using a Bernoulli mixture prior is further developed, in which an end-to-end training is admitted by resorting to the straight-through (ST) discrete gradient estimator. Experimental results on several benchmark datasets demonstrate that the proposed methods, especially the one using Bernoulli mixture priors, consistently outperform existing ones by a substantial margin.</p>
<p>Keywords:</p>
<h3 id="526. On Efficient Retrieval of Top Similarity Vectors.">526. On Efficient Retrieval of Top Similarity Vectors.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1527">Paper Link</a>    Pages:5235-5245</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/97/8692.html">Shulong Tan</a> ; <a href="https://dblp.uni-trier.de/pid/84/5743.html">Zhixin Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/195/4352.html">Zhaozhuo Xu</a> ; <a href="https://dblp.uni-trier.de/pid/62/5860-1.html">Ping Li</a></p>
<p>Abstract:
Retrieval of relevant vectors produced by representation learning critically influences the efficiency in natural language processing (NLP) tasks. In this paper, we demonstrate an efficient method for searching vectors via a typical non-metric matching function: inner product. Our method, which constructs an approximate Inner Product Delaunay Graph (IPDG) for top-1 Maximum Inner Product Search (MIPS), transforms retrieving the most suitable latent vectors into a graph search problem with great benefits of efficiency. Experiments on data representations learned for different machine learning tasks verify the outperforming effectiveness and efficiency of the proposed IPDG.</p>
<p>Keywords:</p>
<h3 id="527. Multiplex Word Embeddings for Selectional Preference Acquisition.">527. Multiplex Word Embeddings for Selectional Preference Acquisition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1528">Paper Link</a>    Pages:5246-5255</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/48/859.html">Hongming Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/250/9281.html">Jiaxin Bai</a> ; <a href="https://dblp.uni-trier.de/pid/09/1398.html">Yan Song</a> ; <a href="https://dblp.uni-trier.de/pid/29/6948.html">Kun Xu</a> ; <a href="https://dblp.uni-trier.de/pid/76/238.html">Changlong Yu</a> ; <a href="https://dblp.uni-trier.de/pid/86/2159.html">Yangqiu Song</a> ; <a href="https://dblp.uni-trier.de/pid/n/WilfredNg.html">Wilfred Ng</a> ; <a href="https://dblp.uni-trier.de/pid/71/4598-1.html">Dong Yu</a></p>
<p>Abstract:
Conventional word embeddings represent words with fixed vectors, which are usually trained based on co-occurrence patterns among words. In doing so, however, the power of such representations is limited, where the same word might be functionalized separately under different syntactic relations. To address this limitation, one solution is to incorporate relational dependencies of different words into their embeddings. Therefore, in this paper, we propose a multiplex word embedding model, which can be easily extended according to various relations among words. As a result, each word has a center embedding to represent its overall semantics, and several relational embeddings to represent its relational dependencies. Compared to existing models, our model can effectively distinguish words with respect to different relations without introducing unnecessary sparseness. Moreover, to accommodate various relations, we use a small dimension for relational embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance.</p>
<p>Keywords:</p>
<h3 id="528. MulCode: A Multiplicative Multi-way Model for Compressing Neural Language Model.">528. MulCode: A Multiplicative Multi-way Model for Compressing Neural Language Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1529">Paper Link</a>    Pages:5256-5265</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/169/7255.html">Yukun Ma</a> ; <a href="https://dblp.uni-trier.de/pid/222/2938.html">Patrick H. Chen</a> ; <a href="https://dblp.uni-trier.de/pid/14/2770.html">Cho-Jui Hsieh</a></p>
<p>Abstract:
It is challenging to deploy deep neural nets on memory-constrained devices due to the explosion of numbers of parameters. Especially, the input embedding layer and Softmax layer usually dominate the memory usage in an RNN-based language model. For example, input embedding and Softmax matrices in IWSLT-2014 German-to-English data set account for more than 80% of the total model parameters. To compress these embedding layers, we propose MulCode, a novel multi-way multiplicative neural compressor. MulCode learns an adaptively created matrix and its multiplicative compositions. Together with a prior weighted loss, Multicode is more effective than the state-of-the-art compression methods. On the IWSLT-2014 machine translation data set, MulCode achieved 17 times compression rate for the embedding and Softmax matrices, and when combined with quantization technique, our method can achieve 41.38 times compression rate with very little loss in performance.</p>
<p>Keywords:</p>
<h3 id="529. It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution.">529. It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1530">Paper Link</a>    Pages:5266-5274</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7613.html">Rowan Hall Maudslay</a> ; <a href="https://dblp.uni-trier.de/pid/167/5312.html">Hila Gonen</a> ; <a href="https://dblp.uni-trier.de/pid/146/4361.html">Ryan Cotterell</a> ; <a href="https://dblp.uni-trier.de/pid/61/6424.html">Simone Teufel</a></p>
<p>Abstract:
This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.</p>
<p>Keywords:</p>
<h3 id="530. Examining Gender Bias in Languages with Grammatical Gender.">530. Examining Gender Bias in Languages with Grammatical Gender.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1531">Paper Link</a>    Pages:5275-5283</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/23/8064.html">Pei Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/132/8060.html">Weijia Shi</a> ; <a href="https://dblp.uni-trier.de/pid/59/2379.html">Jieyu Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/24/255.html">Kuan-Hao Huang</a> ; <a href="https://dblp.uni-trier.de/pid/173/2608.html">Muhao Chen</a> ; <a href="https://dblp.uni-trier.de/pid/146/4361.html">Ryan Cotterell</a> ; <a href="https://dblp.uni-trier.de/pid/18/2428.html">Kai-Wei Chang</a></p>
<p>Abstract:
Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding of these languages under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.</p>
<p>Keywords:</p>
<h3 id="531. Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge Distillation.">531. Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge Distillation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1532">Paper Link</a>    Pages:5284-5295</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/147/9150.html">Yogarshi Vyas</a> ; <a href="https://dblp.uni-trier.de/pid/71/1827.html">Marine Carpuat</a></p>
<p>Abstract:
Words in different languages rarely cover the exact same semantic space. This work characterizes differences in meaning between words across languages using semantic relations that have been used to relate the meaning of English words. However, because of translation ambiguity, semantic relations are not always preserved by translation. We introduce a cross-lingual relation classifier trained only with English examples and a bilingual dictionary. Our classifier relies on a novel attention-based distillation approach to account for translation ambiguity when transferring knowledge from English to cross-lingual settings. On new English-Chinese and English-Hindi test sets, the resulting models largely outperform baselines that more naively rely on bilingual embeddings or dictionaries for cross-lingual transfer, and approach the performance of fully supervised systems on English tasks.</p>
<p>Keywords:</p>
<h3 id="532. Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations.">532. Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1533">Paper Link</a>    Pages:5296-5305</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/157/2220.html">Christian Hadiwinoto</a> ; <a href="https://dblp.uni-trier.de/pid/97/3037.html">Hwee Tou Ng</a> ; <a href="https://dblp.uni-trier.de/pid/245/8646.html">Wee Chung Gan</a></p>
<p>Abstract:
Contextualized word representations are able to give different representations for the same word in different contexts, and they have been shown to be effective in downstream natural language processing tasks, such as question answering, named entity recognition, and sentiment analysis. However, evaluation on word sense disambiguation (WSD) in prior work shows that using contextualized word representations does not outperform the state-of-the-art approach that makes use of non-contextualized word embeddings. In this paper, we explore different strategies of integrating pre-trained contextualized word representations and our best strategy achieves accuracies exceeding the best prior published accuracies by significant margins on multiple benchmark WSD datasets.</p>
<p>Keywords:</p>
<h3 id="533. Do NLP Models Know Numbers? Probing Numeracy in Embeddings.">533. Do NLP Models Know Numbers? Probing Numeracy in Embeddings.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1534">Paper Link</a>    Pages:5306-5314</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/6165.html">Eric Wallace</a> ; <a href="https://dblp.uni-trier.de/pid/79/3601.html">Yizhong Wang</a> ; <a href="https://dblp.uni-trier.de/pid/05/4288.html">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pid/13/3568-1.html">Sameer Singh</a> ; <a href="https://dblp.uni-trier.de/pid/00/8046.html">Matt Gardner</a></p>
<p>Abstract:
The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokensthey embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more preciseELMo captures numeracy the best for all pre-trained methodsbut BERT, which uses sub-word units, is less exact.</p>
<p>Keywords:</p>
<h3 id="534. A Split-and-Recombine Approach for Follow-up Query Analysis.">534. A Split-and-Recombine Approach for Follow-up Query Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1535">Paper Link</a>    Pages:5315-5325</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/33/85.html">Qian Liu</a> ; <a href="https://dblp.uni-trier.de/pid/11/8555.html">Bei Chen</a> ; <a href="https://dblp.uni-trier.de/pid/220/4217.html">Haoyan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/37/1917.html">Jian-Guang Lou</a> ; <a href="https://dblp.uni-trier.de/pid/66/5168.html">Lei Fang</a> ; <a href="https://dblp.uni-trier.de/pid/66/3973.html">Bin Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/87/461.html">Dongmei Zhang</a></p>
<p>Abstract:
Context-dependent semantic parsing has proven to be an important yet challenging task. To leverage the advances in context-independent semantic parsing, we propose to perform follow-up query analysis, aiming to restate context-dependent natural language queries with contextual information. To accomplish the task, we propose STAR, a novel approach with a well-designed two-phase process. It is parser-independent and able to handle multifarious follow-up scenarios in different domains. Experiments on the FollowUp dataset show that STAR outperforms the state-of-the-art baseline by a large margin of nearly 8%. The superiority on parsing results verifies the feasibility of follow-up query analysis. We also explore the extensibility of STAR on the SQA dataset, which is very promising.</p>
<p>Keywords:</p>
<h3 id="535. Text2Math: End-to-end Parsing Text into Math Expressions.">535. Text2Math: End-to-end Parsing Text into Math Expressions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1536">Paper Link</a>    Pages:5326-5336</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/48/6274.html">Yanyan Zou</a> ; <a href="https://dblp.uni-trier.de/pid/98/6613-11.html">Wei Lu</a></p>
<p>Abstract:
We propose Text2Math, a model for semantically parsing text into math expressions. The model can be used to solve different math related problems including arithmetic word problems and equation parsing problems. Unlike previous approaches, we tackle the problem from an end-to-end structured prediction perspective where our algorithm aims to predict the complete math expression at once as a tree structure, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach.</p>
<p>Keywords:</p>
<h3 id="536. Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions.">536. Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1537">Paper Link</a>    Pages:5337-5348</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/60/2536-37.html">Rui Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/67/1014.html">Tao Yu</a> ; <a href="https://dblp.uni-trier.de/pid/242/8249.html">Heyang Er</a> ; <a href="https://dblp.uni-trier.de/pid/242/7898.html">Sungrok Shim</a> ; <a href="https://dblp.uni-trier.de/pid/248/7781.html">Eric Xue</a> ; <a href="https://dblp.uni-trier.de/pid/215/5264.html">Xi Victoria Lin</a> ; <a href="https://dblp.uni-trier.de/pid/154/6550.html">Tianze Shi</a> ; <a href="https://dblp.uni-trier.de/pid/80/7282.html">Caiming Xiong</a> ; <a href="https://dblp.uni-trier.de/pid/79/128.html">Richard Socher</a> ; <a href="https://dblp.uni-trier.de/pid/r/DragomirRRadev.html">Dragomir R. Radev</a></p>
<p>Abstract:
We focus on the cross-domain context-dependent text-to-SQL generation task. Based on the observation that adjacent natural language questions are often linguistically dependent and their corresponding SQL queries tend to overlap, we utilize the interaction history by editing the previous predicted query to improve the generation quality. Our editing mechanism views SQL as sequences and reuses generation results at the token level in a simple manner. It is flexible to change individual tokens and robust to error propagation. Furthermore, to deal with complex table structures in different domains, we employ an utterance-table encoder and a table-aware decoder to incorporate the context of the user utterance and the table schema. We evaluate our approach on the SParC dataset and demonstrate the benefit of editing compared with the state-of-the-art baselines which generate SQL from scratch. Our code is available at <a href="https://github.com/ryanzhumich/sparc_atis_pytorch">https://github.com/ryanzhumich/sparc_atis_pytorch</a>.</p>
<p>Keywords:</p>
<h3 id="537. Syntax-aware Multilingual Semantic Role Labeling.">537. Syntax-aware Multilingual Semantic Role Labeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1538">Paper Link</a>    Pages:5349-5358</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/222/9538.html">Shexia He</a> ; <a href="https://dblp.uni-trier.de/pid/198/9339.html">Zuchao Li</a> ; <a href="https://dblp.uni-trier.de/pid/25/1145.html">Hai Zhao</a></p>
<p>Abstract:
Recently, semantic role labeling (SRL) has earned a series of success with even higher performance improvements, which can be mainly attributed to syntactic integration and enhanced word representation. However, most of these efforts focus on English, while SRL on multiple languages more than English has received relatively little attention so that is kept underdevelopment. Thus this paper intends to fill the gap on multilingual SRL with special focus on the impact of syntax and contextualized word representation. Unlike existing work, we propose a novel method guided by syntactic rule to prune arguments, which enables us to integrate syntax into multilingual SRL model simply and effectively. We present a unified SRL model designed for multiple languages together with the proposed uniform syntax enhancement. Our model achieves new state-of-the-art results on the CoNLL-2009 benchmarks of all seven languages. Besides, we pose a discussion on the syntactic role among different languages and verify the effectiveness of deep enhanced representation for multilingual SRL.</p>
<p>Keywords:</p>
<h3 id="538. Cloze-driven Pretraining of Self-attention Networks.">538. Cloze-driven Pretraining of Self-attention Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1539">Paper Link</a>    Pages:5359-5368</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/3374.html">Alexei Baevski</a> ; <a href="https://dblp.uni-trier.de/pid/166/8381.html">Sergey Edunov</a> ; <a href="https://dblp.uni-trier.de/pid/238/0128.html">Yinhan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/21/6793.html">Luke Zettlemoyer</a> ; <a href="https://dblp.uni-trier.de/pid/11/9768.html">Michael Auli</a></p>
<p>Abstract:
We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.</p>
<p>Keywords:</p>
<h3 id="539. Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling.">539. Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1540">Paper Link</a>    Pages:5369-5380</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/134/5708.html">Jinfeng Rao</a> ; <a href="https://dblp.uni-trier.de/pid/36/7028.html">Linqing Liu</a> ; <a href="https://dblp.uni-trier.de/pid/188/6350.html">Yi Tay</a> ; <a href="https://dblp.uni-trier.de/pid/204/0080.html">Hsiu-Wei Yang</a> ; <a href="https://dblp.uni-trier.de/pid/172/7638.html">Peng Shi</a> ; <a href="https://dblp.uni-trier.de/pid/00/7739.html">Jimmy Lin</a></p>
<p>Abstract:
A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a users query. On the other hand, many NLP problems, such as question answering and paraphrase identification, can be considered variants of semantic matching, which is to measure the semantic distance between two pieces of short texts. While at a high level both relevance and semantic matching require modeling textual similarity, many existing techniques for one cannot be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders.</p>
<p>Keywords:</p>
<h3 id="540. A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling.">540. A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1541">Paper Link</a>    Pages:5381-5391</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/185/0855.html">Qingrong Xia</a> ; <a href="https://dblp.uni-trier.de/pid/72/8937.html">Zhenghua Li</a> ; <a href="https://dblp.uni-trier.de/pid/83/5342-5.html">Min Zhang</a></p>
<p>Abstract:
Semantic role labeling (SRL) aims to identify the predicate-argument structure of a sentence. Inspired by the strong correlation between syntax and semantics, previous works pay much attention to improve SRL performance on exploiting syntactic knowledge, achieving significant results. Pipeline methods based on automatic syntactic trees and multi-task learning (MTL) approaches using standard syntactic trees are two common research orientations. In this paper, we adopt a simple unified span-based model for both span-based and word-based Chinese SRL as a strong baseline. Besides, we present a MTL framework that includes the basic SRL module and a dependency parser module. Different from the commonly used hard parameter sharing strategy in MTL, the main idea is to extract implicit syntactic representations from the dependency parser as external inputs for the basic SRL model. Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL-2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax.</p>
<p>Keywords:</p>
<h3 id="541. Transfer Fine-Tuning: A BERT Case Study.">541. Transfer Fine-Tuning: A BERT Case Study.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1542">Paper Link</a>    Pages:5392-5403</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/25/1605.html">Yuki Arase</a> ; <a href="https://dblp.uni-trier.de/pid/t/JunichiTsujii.html">Jun&apos;ichi Tsujii</a></p>
<p>Abstract:
A semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of tasks crucial for research on natural language understanding. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While BERTs performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated model exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.</p>
<p>Keywords:</p>
<h3 id="542. Data-Anonymous Encoding for Text-to-SQL Generation.">542. Data-Anonymous Encoding for Text-to-SQL Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1543">Paper Link</a>    Pages:5404-5413</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/60/1749.html">Zhen Dong</a> ; <a href="https://dblp.uni-trier.de/pid/136/7917.html">Shizhao Sun</a> ; <a href="https://dblp.uni-trier.de/pid/85/3862-1.html">Hongzhi Liu</a> ; <a href="https://dblp.uni-trier.de/pid/37/1917.html">Jian-Guang Lou</a> ; <a href="https://dblp.uni-trier.de/pid/87/461.html">Dongmei Zhang</a></p>
<p>Abstract:
On text-to-SQL generation, the input utterance usually contains lots of tokens that are related to column names or cells in the table, called table-related tokens. These table-related tokens are troublesome for the downstream neural semantic parser because it brings complex semantics and hinders the sharing across the training examples. However, existing approaches either ignore handling these tokens before the semantic parser or simply use deterministic approaches based on string-match or word embedding similarity. In this work, we propose a more efficient approach to handle table-related tokens before the semantic parser. First, we formulate it as a sequential tagging problem and propose a two-stage anonymization model to learn the semantic relationship between tables and input utterances. Then, we leverage the implicit supervision from SQL queries by policy gradient to guide the training. Experiments demonstrate that our approach consistently improves performances of different neural semantic parsers and significantly outperforms deterministic approaches.</p>
<p>Keywords:</p>
<h3 id="543. Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks.">543. Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1544">Paper Link</a>    Pages:5414-5424</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/163/1920.html">Xinchi Chen</a> ; <a href="https://dblp.uni-trier.de/pid/172/1054.html">Chunchuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pid/08/5391.html">Ivan Titov</a></p>
<p>Abstract:
Semantic role labeling (SRL) involves extracting propositions (i.e. predicates and their typed arguments) from natural language sentences. State-of-the-art SRL models rely on powerful encoders (e.g., LSTMs) and do not model non-local interaction between arguments. We propose a new approach to modeling these interactions while maintaining efficient inference. Specifically, we use Capsule Networks (Sabour et al., 2017): each proposition is encoded as a tuple of capsules, one capsule per argument type (i.e. role). These tuples serve as embeddings of entire propositions. In every network layer, the capsules interact with each other and with representations of words in the sentence. Each iteration results in updated proposition embeddings and updated predictions about the SRL structure. Our model substantially outperforms the non-refinement baseline model on all 7 CoNLL-2019 languages and achieves state-of-the-art results on 5 languages (including English) for dependency SRL. We analyze the types of mistakes corrected by the refinement procedure. For example, each role is typically (but not always) filled with at most one argument. Whereas enforcing this approximate constraint is not useful with the modern SRL system, iterative procedure corrects the mistakes by capturing this intuition in a flexible and context-sensitive way.</p>
<p>Keywords:</p>
<h3 id="544. Learning Programmatic Idioms for Scalable Semantic Parsing.">544. Learning Programmatic Idioms for Scalable Semantic Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1545">Paper Link</a>    Pages:5425-5434</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/78/4928.html">Srinivasan Iyer</a> ; <a href="https://dblp.uni-trier.de/pid/16/4915.html">Alvin Cheung</a> ; <a href="https://dblp.uni-trier.de/pid/21/6793.html">Luke Zettlemoyer</a></p>
<p>Abstract:
Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2% BLEU score while reducing training time by more than 50%. This improved speed enables us to scale up the model by training on an extended training set that is 5 larger, to further move up the SOTA by an additional 2.3% BLEU and 0.9% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.</p>
<p>Keywords:</p>
<h3 id="545. JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation.">545. JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1546">Paper Link</a>    Pages:5435-5445</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/250/2611.html">Rajas Agashe</a> ; <a href="https://dblp.uni-trier.de/pid/78/4928.html">Srinivasan Iyer</a> ; <a href="https://dblp.uni-trier.de/pid/21/6793.html">Luke Zettlemoyer</a></p>
<p>Abstract:
Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.</p>
<p>Keywords:</p>
<h3 id="546. Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study.">546. Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1547">Paper Link</a>    Pages:5446-5457</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/178/8600.html">Ziyu Yao</a> ; <a href="https://dblp.uni-trier.de/pid/38/1070-1.html">Yu Su</a> ; <a href="https://dblp.uni-trier.de/pid/33/2952.html">Huan Sun</a> ; <a href="https://dblp.uni-trier.de/pid/07/7129.html">Wen-tau Yih</a></p>
<p>Abstract:
As a promising paradigm, interactive semantic parsing has shown to improve both semantic parsing accuracy and user confidence in the results. In this paper, we propose a new, unified formulation of the interactive semantic parsing problem, where the goal is to design a model-based intelligent agent. The agent maintains its own state as the current predicted semantic parse, decides whether and where human intervention is needed, and generates a clarification question in natural language. A key part of the agent is a world model: it takes a percept (either an initial question or subsequent feedback from the user) and transitions to a new state. We then propose a simple yet remarkably effective instantiation of our framework, demonstrated on two text-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base semantic parsers. Compared to an existing interactive semantic parsing approach that treats the base parser as a black box, our approach solicits less user feedback but yields higher run-time accuracy.</p>
<p>Keywords:</p>
<h3 id="547. Modeling Graph Structure in Transformer for Better AMR-to-Text Generation.">547. Modeling Graph Structure in Transformer for Better AMR-to-Text Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1548">Paper Link</a>    Pages:5458-5467</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/55/5471.html">Jie Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/08/500.html">Junhui Li</a> ; <a href="https://dblp.uni-trier.de/pid/24/2477.html">Muhua Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/38/7161.html">Longhua Qian</a> ; <a href="https://dblp.uni-trier.de/pid/83/5342-5.html">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/42/6620.html">Guodong Zhou</a></p>
<p>Abstract:
Recent studies on AMR-to-text generation often formalize the task as a sequence-to-sequence (seq2seq) learning problem by converting an Abstract Meaning Representation (AMR) graph into a word sequences. Graph structures are further modeled into the seq2seq framework in order to utilize the structural information in the AMR graphs. However, previous approaches only consider the relations between directly connected concepts while ignoring the rich structure in AMR graphs. In this paper we eliminate such a strong limitation and propose a novel structure-aware self-attention approach to better model the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e. the Transformer. In particular, a few different methods are explored to learn structural representations between two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the state-of-the-art with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by supervised models on the benchmarks.</p>
<p>Keywords:</p>
<h3 id="548. Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks.">548. Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1549">Paper Link</a>    Pages:5468-5476</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/195/5963.html">Binxuan Huang</a> ; <a href="https://dblp.uni-trier.de/pid/72/6492.html">Kathleen M. Carley</a></p>
<p>Abstract:
Aspect level sentiment classification aims to identify the sentiment expressed towards an aspect given a context sentence. Previous neural network based methods largely ignore the syntax structure in one sentence. In this paper, we propose a novel target-dependent graph attention network (TD-GAT) for aspect level sentiment classification, which explicitly utilizes the dependency relationship among words. Using the dependency graph, it propagates sentiment features directly from the syntactic context of an aspect target. In our experiments, we show our method outperforms multiple baselines with GloVe embeddings. We also demonstrate that using BERT representations further substantially boosts the performance.</p>
<p>Keywords:</p>
<h3 id="549. Learning Explicit and Implicit Structures for Targeted Sentiment Analysis.">549. Learning Explicit and Implicit Structures for Targeted Sentiment Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1550">Paper Link</a>    Pages:5477-5487</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/17/5705.html">Hao Li</a> ; <a href="https://dblp.uni-trier.de/pid/98/6613-11.html">Wei Lu</a></p>
<p>Abstract:
Targeted sentiment analysis is the task of jointly predicting target entities and their associated sentiment information. Existing research efforts mostly regard this joint task as a sequence labeling problem, building models that can capture explicit structures in the output space. However, the importance of capturing implicit global structural information that resides in the input space is largely unexplored. In this work, we argue that both types of information (implicit and explicit structural information) are crucial for building a successful targeted sentiment analysis model. Our experimental results show that properly capturing both information is able to lead to better performance than competitive existing approaches. We also conduct extensive experiments to investigate our models effectiveness and robustness.</p>
<p>Keywords:</p>
<h3 id="550. Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification.">550. Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1551">Paper Link</a>    Pages:5488-5497</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8020.html">Chunning Du</a> ; <a href="https://dblp.uni-trier.de/pid/00/11044.html">Haifeng Sun</a> ; <a href="https://dblp.uni-trier.de/pid/37/2749-1.html">Jingyu Wang</a> ; <a href="https://dblp.uni-trier.de/pid/80/6406-1.html">Qi Qi</a> ; <a href="https://dblp.uni-trier.de/pid/60/4951.html">Jianxin Liao</a> ; <a href="https://dblp.uni-trier.de/pid/70/6770.html">Tong Xu</a> ; <a href="https://dblp.uni-trier.de/pid/20/2039.html">Ming Liu</a></p>
<p>Abstract:
Aspect-level sentiment classification is a crucial task for sentiment analysis, which aims to identify the sentiment polarities of specific targets in their context. The main challenge comes from multi-aspect sentences, which express multiple sentiment polarities towards different targets, resulting in overlapped feature representation. However, most existing neural models tend to utilize static pooling operation or attention mechanism to identify sentimental words, which therefore insufficient for dealing with overlapped features. To solve this problem, we propose to utilize capsule network to construct vector-based feature representation and cluster features by an EM routing algorithm. Furthermore, interactive attention mechanism is introduced in the capsule routing procedure to model the semantic relationship between aspect terms and context. The iterative routing also enables encoding sentence from a global perspective. Experimental results on three datasets show that our proposed model achieves state-of-the-art performance.</p>
<p>Keywords:</p>
<h3 id="551. Emotion Detection with Neural Personal Discrimination.">551. Emotion Detection with Neural Personal Discrimination.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1552">Paper Link</a>    Pages:5498-5506</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/161/0414.html">Xiabing Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/20/9924.html">Zhongqing Wang</a> ; <a href="https://dblp.uni-trier.de/pid/83/4790.html">Shoushan Li</a> ; <a href="https://dblp.uni-trier.de/pid/42/6620.html">Guodong Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/83/5342-5.html">Min Zhang</a></p>
<p>Abstract:
There have been a recent line of works to automatically predict the emotions of posts in social media. Existing approaches consider the posts individually and predict their emotions independently. Different from previous researches, we explore the dependence among relevant posts via the authors backgrounds, since the authors with similar backgrounds, e.g., gender, location, tend to express similar emotions. However, such personal attributes are not easy to obtain in most social media websites, and it is hard to capture attributes-aware words to connect similar people. Accordingly, we propose a Neural Personal Discrimination (NPD) approach to address above challenges by determining personal attributes from posts, and connecting relevant posts with similar attributes to jointly learn their emotions. In particular, we employ adversarial discriminators to determine the personal attributes, with attention mechanisms to aggregate attributes-aware words. In this way, social correlationship among different posts can be better addressed. Experimental results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models.</p>
<p>Keywords:</p>
<h3 id="552. Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification.">552. Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1553">Paper Link</a>    Pages:5507-5516</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/140/6685.html">Pengcheng Yang</a> ; <a href="https://dblp.uni-trier.de/pid/215/3823.html">Junyang Lin</a> ; <a href="https://dblp.uni-trier.de/pid/25/624.html">Jingjing Xu</a> ; <a href="https://dblp.uni-trier.de/pid/33/3881.html">Jun Xie</a> ; <a href="https://dblp.uni-trier.de/pid/25/221.html">Qi Su</a> ; <a href="https://dblp.uni-trier.de/pid/37/1971-1.html">Xu Sun</a></p>
<p>Abstract:
The task of unsupervised sentiment modification aims to reverse the sentiment polarity of the input text while preserving its semantic content without any parallel data. Most previous work follows a two-step process. They first separate the content from the original sentiment, and then directly generate text with the target sentiment only based on the content produced by the first step. However, the second step bears both the target sentiment addition and content reconstruction, thus resulting in a lack of specific information like proper nouns in the generated text. To remedy this, we propose a specificity-driven cascading approach in this work, which can effectively increase the specificity of the generated text and further improve content preservation. In addition, we propose a more reasonable metric to evaluate sentiment modification. The experiments show that our approach outperforms competitive baselines by a large margin, which achieves 11% and 38% relative improvements of the overall metric on the Yelp and Amazon datasets, respectively.</p>
<p>Keywords:</p>
<h3 id="553. LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification.">553. LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1554">Paper Link</a>    Pages:5517-5526</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/25/624.html">Jingjing Xu</a> ; <a href="https://dblp.uni-trier.de/pid/63/5422.html">Liang Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/254/8174.html">Hanqi Yan</a> ; <a href="https://dblp.uni-trier.de/pid/39/7992.html">Qi Zeng</a> ; <a href="https://dblp.uni-trier.de/pid/83/2265.html">Yun Liang</a> ; <a href="https://dblp.uni-trier.de/pid/37/1971-1.html">Xu Sun</a></p>
<p>Abstract:
Recent work has shown that current text classification models are fragile and sensitive to simple perturbations. In this work, we propose a novel adversarial training approach, LexicalAT, to improve the robustness of current classification models. The proposed approach consists of a generator and a classifier. The generator learns to generate examples to attack the classifier while the classifier learns to defend these attacks. Considering the diversity of attacks, the generator uses a large-scale lexical knowledge base, WordNet, to generate attacking examples by replacing some words in training examples with their synonyms (e.g., sad and unhappy), neighbor words (e.g., fox and wolf), or super-superior words (e.g., chair and armchair). Due to the discrete generation step in the generator, we use policy gradient, a reinforcement learning approach, to train the two modules. Experiments show LexicalAT outperforms strong baselines and reduces test errors on various neural networks, including CNN, RNN, and BERT.</p>
<p>Keywords:</p>
<h3 id="554. Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery.">554. Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1555">Paper Link</a>    Pages:5527-5537</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/87/5809-4.html">Zhe Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/s/MunindarPSingh.html">Munindar P. Singh</a></p>
<p>Abstract:
Opinionated text often involves attributes such as authorship and location that influence the sentiments expressed for different aspects. We posit that structural and semantic correspondence is both prevalent in opinionated text, especially when associated with attributes, and crucial in accurately revealing its latent aspect and sentiment structure. However, it is not recognized by existing approaches. We propose Trait, an unsupervised probabilistic model that discovers aspects and sentiments from text and associates them with different attributes. To this end, Trait infers and leverages structural and semantic correspondence using a Markov Random Field. We show empirically that by incorporating attributes explicitly Trait significantly outperforms state-of-the-art baselines both by generating attribute profiles that accord with our intuitions, as shown via visualization, and yielding topics of greater semantic cohesion.</p>
<p>Keywords:</p>
<h3 id="555. From the Token to the Review: A Hierarchical Multimodal approach to Opinion Mining.">555. From the Token to the Review: A Hierarchical Multimodal approach to Opinion Mining.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1556">Paper Link</a>    Pages:5538-5547</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/217/2092.html">Alexandre Garcia</a> ; <a href="https://dblp.uni-trier.de/pid/229/3167.html">Pierre Colombo</a> ; <a href="https://dblp.uni-trier.de/pid/96/4104.html">Florence d&apos;Alch-Buc</a> ; <a href="https://dblp.uni-trier.de/pid/53/6904.html">Slim Essid</a> ; <a href="https://dblp.uni-trier.de/pid/50/2768.html">Chlo Clavel</a></p>
<p>Abstract:
The task of predicting fine grained user opinion based on spontaneous spoken language is a key problem arising in the development of Computational Agents as well as in the development of social network based opinion miners. Unfortunately, gathering reliable data on which a model can be trained is notoriously difficult and existing works rely only on coarsely labeled opinions. In this work we aim at bridging the gap separating fine grained opinion models already developed for written language and coarse grained models developed for spontaneous multimodal opinion mining. We take advantage of the implicit hierarchical structure of opinions to build a joint fine and coarse grained opinion model that exploits different views of the opinion expression. The resulting model shares some properties with attention-based models and is shown to provide competitive results on a recently released multimodal fine grained annotated corpus.</p>
<p>Keywords:</p>
<h3 id="556. Shallow Domain Adaptive Embeddings for Sentiment Analysis.">556. Shallow Domain Adaptive Embeddings for Sentiment Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1557">Paper Link</a>    Pages:5548-5557</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/2471.html">Prathusha Kameswara Sarma</a> ; <a href="https://dblp.uni-trier.de/pid/88/7458.html">Yingyu Liang</a> ; <a href="https://dblp.uni-trier.de/pid/72/3104.html">William A. Sethares</a></p>
<p>Abstract:
This paper proposes a way to improve the performance of existing algorithms for text classification in domains with strong language semantics. A proposed domain adaptation layer learns weights to combine a generic and a domain specific (DS) word embedding into a domain adapted (DA) embedding. The DA word embeddings are then used as inputs to a generic encoder + classifier framework to perform a downstream task such as classification. This adaptation layer is particularly suited to data sets that are modest in size, and which are, therefore, not ideal candidates for (re)training a deep neural network architecture. Results on binary and multi-class classification tasks using popular encoder architectures, including current state-of-the-art methods (with and without the shallow adaptation layer) show the effectiveness of the proposed approach.</p>
<p>Keywords:</p>
<h3 id="557. Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification.">557. Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1558">Paper Link</a>    Pages:5558-5567</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/199/5022.html">Mengting Hu</a> ; <a href="https://dblp.uni-trier.de/pid/246/5764.html">Yike Wu</a> ; <a href="https://dblp.uni-trier.de/pid/73/1947.html">Shiwan Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/48/4458.html">Honglei Guo</a> ; <a href="https://dblp.uni-trier.de/pid/137/0571.html">Renhong Cheng</a> ; <a href="https://dblp.uni-trier.de/pid/87/1363.html">Zhong Su</a></p>
<p>Abstract:
Cross-domain sentiment classification has drawn much attention in recent years. Most existing approaches focus on learning domain-invariant representations in both the source and target domains, while few of them pay attention to the domain-specific information. Despite the non-transferability of the domain-specific information, simultaneously learning domain-dependent representations can facilitate the learning of domain-invariant representations. In this paper, we focus on aspect-level cross-domain sentiment classification, and propose to distill the domain-invariant sentiment features with the help of an orthogonal domain-dependent task, i.e. aspect detection, which is built on the aspects varying widely in different domains. We conduct extensive experiments on three public datasets and the experimental results demonstrate the effectiveness of our method.</p>
<p>Keywords:</p>
<h3 id="558. A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis.">558. A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1559">Paper Link</a>    Pages:5568-5579</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/177/5130.html">Yunlong Liang</a> ; <a href="https://dblp.uni-trier.de/pid/117/4056.html">Fandong Meng</a> ; <a href="https://dblp.uni-trier.de/pid/127/3143.html">Jinchao Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/67/3124.html">Jinan Xu</a> ; <a href="https://dblp.uni-trier.de/pid/64/5715.html">Yufeng Chen</a> ; <a href="https://dblp.uni-trier.de/pid/00/5012-16.html">Jie Zhou</a></p>
<p>Abstract:
Aspect based sentiment analysis (ABSA) aims to identify the sentiment polarity towards the given aspect in a sentence, while previous models typically exploit an aspect-independent (weakly associative) encoder for sentence representation generation. In this paper, we propose a novel Aspect-Guided Deep Transition model, named AGDT, which utilizes the given aspect to guide the sentence encoding from scratch with the specially-designed deep transition architecture. Furthermore, an aspect-oriented objective is designed to enforce AGDT to reconstruct the given aspect with the generated sentence representation. In doing so, our AGDT can accurately generate aspect-specific sentence representation, and thus conduct more accurate sentiment predictions. Experimental results on multiple SemEval datasets demonstrate the effectiveness of our proposed approach, which significantly outperforms the best reported results with the same setting.</p>
<p>Keywords:</p>
<h3 id="559. Human-Like Decision Making: Document-level Aspect Sentiment Classification via Hierarchical Reinforcement Learning.">559. Human-Like Decision Making: Document-level Aspect Sentiment Classification via Hierarchical Reinforcement Learning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1560">Paper Link</a>    Pages:5580-5589</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/62/2631.html">Jingjing Wang</a> ; <a href="https://dblp.uni-trier.de/pid/212/2092.html">Changlong Sun</a> ; <a href="https://dblp.uni-trier.de/pid/83/4790.html">Shoushan Li</a> ; <a href="https://dblp.uni-trier.de/pid/34/10219.html">Jiancheng Wang</a> ; <a href="https://dblp.uni-trier.de/pid/14/1217.html">Luo Si</a> ; <a href="https://dblp.uni-trier.de/pid/83/5342-5.html">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/11/6389.html">Xiaozhong Liu</a> ; <a href="https://dblp.uni-trier.de/pid/42/6620.html">Guodong Zhou</a></p>
<p>Abstract:
Recently, neural networks have shown promising results on Document-level Aspect Sentiment Classification (DASC). However, these approaches often offer little transparency w.r.t. their inner working mechanisms and lack interpretability. In this paper, to simulating the steps of analyzing aspect sentiment in a document by human beings, we propose a new Hierarchical Reinforcement Learning (HRL) approach to DASC. This approach incorporates clause selection and word selection strategies to tackle the data noise problem in the task of DASC. First, a high-level policy is proposed to select aspect-relevant clauses and discard noisy clauses. Then, a low-level policy is proposed to select sentiment-relevant words and discard noisy words inside the selected clauses. Finally, a sentiment rating predictor is designed to provide reward signals to guide both clause and word selection. Experimental results demonstrate the impressive effectiveness of the proposed approach to DASC over the state-of-the-art baselines.</p>
<p>Keywords:</p>
<h3 id="560. A Dataset of General-Purpose Rebuttal.">560. A Dataset of General-Purpose Rebuttal.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1561">Paper Link</a>    Pages:5590-5600</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/118/5290.html">Matan Orbach</a> ; <a href="https://dblp.uni-trier.de/pid/62/2678.html">Yonatan Bilu</a> ; <a href="https://dblp.uni-trier.de/pid/245/8586.html">Ariel Gera</a> ; <a href="https://dblp.uni-trier.de/pid/149/8379.html">Yoav Kantor</a> ; <a href="https://dblp.uni-trier.de/pid/151/8447.html">Lena Dankin</a> ; <a href="https://dblp.uni-trier.de/pid/136/3952.html">Tamar Lavee</a> ; <a href="https://dblp.uni-trier.de/pid/40/8776.html">Lili Kotlerman</a> ; <a href="https://dblp.uni-trier.de/pid/74/6962.html">Shachar Mirkin</a> ; <a href="https://dblp.uni-trier.de/pid/33/4591.html">Michal Jacovi</a> ; <a href="https://dblp.uni-trier.de/pid/205/9223.html">Ranit Aharonov</a> ; <a href="https://dblp.uni-trier.de/pid/62/7001.html">Noam Slonim</a></p>
<p>Abstract:
In Natural Language Understanding, the task of response generation is usually focused on responses to short texts, such as tweets or a turn in a dialog. Here we present a novel task of producing a critical response to a long argumentative text, and suggest a method based on general rebuttal arguments to address it. We do this in the context of the recently-suggested task of listening comprehension over argumentative content: given a speech on some specified topic, and a list of relevant arguments, the goal is to determine which of the arguments appear in the speech. The general rebuttals we describe here (in English) overcome the need for topic-specific arguments to be provided, by proving to be applicable for a large set of topics. This allows creating responses beyond the scope of topics for which specific arguments are available. All data collected during this work is freely available for research.</p>
<p>Keywords:</p>
<h3 id="561. Rethinking Attribute Representation and Injection for Sentiment Classification.">561. Rethinking Attribute Representation and Injection for Sentiment Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1562">Paper Link</a>    Pages:5601-5612</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/6971.html">Reinald Kim Amplayo</a></p>
<p>Abstract:
Text attributes, such as user and product information in product reviews, have been used to improve the performance of sentiment classification models. The de facto standard method is to incorporate them as additional biases in the attention mechanism, and more performance gains are achieved by extending the model architecture. In this paper, we show that the above method is the least effective way to represent and inject attributes. To demonstrate this hypothesis, unlike previous models with complicated architectures, we limit our base model to a simple BiLSTM with attention classifier, and instead focus on how and where the attributes should be incorporated in the model. We propose to represent attributes as chunk-wise importance weight matrices and consider four locations in the model (i.e., embedding, encoding, attention, classifier) to inject attributes. Experiments show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also outperform the state-of-the-art despite our use of a simple base model. Finally, we show that these representations transfer well to other tasks. Model implementation and datasets are released here: <a href="https://github.com/rktamplayo/CHIM">https://github.com/rktamplayo/CHIM</a>.</p>
<p>Keywords:</p>
<h3 id="562. A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis.">562. A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1563">Paper Link</a>    Pages:5613-5623</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/221/3565.html">Chuang Fan</a> ; <a href="https://dblp.uni-trier.de/pid/254/8227.html">Hongyu Yan</a> ; <a href="https://dblp.uni-trier.de/pid/172/9266.html">Jiachen Du</a> ; <a href="https://dblp.uni-trier.de/pid/34/8605-3.html">Lin Gui</a> ; <a href="https://dblp.uni-trier.de/pid/53/6625.html">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pid/02/1640-7.html">Min Yang</a> ; <a href="https://dblp.uni-trier.de/pid/93/5407.html">Ruifeng Xu</a> ; <a href="https://dblp.uni-trier.de/pid/253/9848.html">Ruibin Mao</a></p>
<p>Abstract:
Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in sentiment analysis. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and common knowledge. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08% in F-measure.</p>
<p>Keywords:</p>
<h3 id="563. Automatic Argument Quality Assessment - New Datasets and Methods.">563. Automatic Argument Quality Assessment - New Datasets and Methods.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1564">Paper Link</a>    Pages:5624-5634</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/128/2033.html">Assaf Toledo</a> ; <a href="https://dblp.uni-trier.de/pid/158/8488.html">Shai Gretz</a> ; <a href="https://dblp.uni-trier.de/pid/242/8962.html">Edo Cohen-Karlik</a> ; <a href="https://dblp.uni-trier.de/pid/248/7906.html">Roni Friedman</a> ; <a href="https://dblp.uni-trier.de/pid/206/6812.html">Elad Venezian</a> ; <a href="https://dblp.uni-trier.de/pid/245/8695.html">Dan Lahav</a> ; <a href="https://dblp.uni-trier.de/pid/33/4591.html">Michal Jacovi</a> ; <a href="https://dblp.uni-trier.de/pid/205/9223.html">Ranit Aharonov</a> ; <a href="https://dblp.uni-trier.de/pid/62/7001.html">Noam Slonim</a></p>
<p>Abstract:
We explore the task of automatic assessment of argument quality. To that end, we actively collected 6.3k arguments, more than a factor of five compared to previously examined data. Each argument was explicitly and carefully annotated for its quality. In addition, 14k pairs of arguments were annotated independently, identifying the higher quality argument in each pair. In spite of the inherent subjective nature of the task, both annotation schemes led to surprisingly consistent results. We release the labeled datasets to the community. Furthermore, we suggest neural methods based on a recently released language model, for argument ranking as well as for argument-pair classification. In the former task, our results are comparable to state-of-the-art; in the latter task our results significantly outperform earlier methods.</p>
<p>Keywords:</p>
<h3 id="564. Fine-Grained Analysis of Propaganda in News Article.">564. Fine-Grained Analysis of Propaganda in News Article.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1565">Paper Link</a>    Pages:5635-5645</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/25/3236.html">Giovanni Da San Martino</a> ; <a href="https://dblp.uni-trier.de/pid/66/7064.html">Seunghak Yu</a> ; <a href="https://dblp.uni-trier.de/pid/40/3383.html">Alberto Barrn-Cedeo</a> ; <a href="https://dblp.uni-trier.de/pid/250/3156.html">Rostislav Petrov</a> ; <a href="https://dblp.uni-trier.de/pid/19/1947.html">Preslav Nakov</a></p>
<p>Abstract:
Propaganda aims at influencing peoples mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.</p>
<p>Keywords:</p>
<h3 id="565. Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis.">565. Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1566">Paper Link</a>    Pages:5646-5656</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/235/1106.html">Dushyant Singh Chauhan</a> ; <a href="https://dblp.uni-trier.de/pid/184/8579.html">Md. Shad Akhtar</a> ; <a href="https://dblp.uni-trier.de/pid/11/3590.html">Asif Ekbal</a> ; <a href="https://dblp.uni-trier.de/pid/p/PushpakBhattacharyya.html">Pushpak Bhattacharyya</a></p>
<p>Abstract:
In recent times, multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing, computer vision, and speech processing. The prime objective of such studies is to leverage the diversified information, (e.g., textual, acoustic and visual), for learning a model. The effective interaction among these modalities often leads to a better system in terms of performance. In this paper, we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. The proposed model learns the inter-modal interaction among the participating modalities through an auto-encoder mechanism. We employ a context-aware attention module to exploit the correspondence among the neighboring utterances. We evaluate our proposed approach for five standard multi-modal affect analysis datasets. Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state-of-the-art systems.</p>
<p>Keywords:</p>
<h3 id="566. Sequential Learning of Convolutional Features for Effective Text Classification.">566. Sequential Learning of Convolutional Features for Effective Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1567">Paper Link</a>    Pages:5657-5666</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/241/5153.html">Avinash Madasu</a> ; <a href="https://dblp.uni-trier.de/pid/222/9403.html">Vijjini Anvesh Rao</a></p>
<p>Abstract:
Text classification has been one of the major problems in natural language processing. With the advent of deep learning, convolutional neural network (CNN) has been a popular solution to this task. However, CNNs which were first proposed for images, face many crucial challenges in the context of text processing, namely in their elementary blocks: convolution filters and max pooling. These challenges have largely been overlooked by the most existing CNN models proposed for text classification. In this paper, we present an experimental study on the fundamental blocks of CNNs in text categorization. Based on this critique, we propose Sequential Convolutional Attentive Recurrent Network (SCARN). The proposed SCARN model utilizes both the advantages of recurrent and convolutional structures efficiently in comparison to previously proposed recurrent convolutional models. We test our model on different text classification datasets across tasks like sentiment analysis and question classification. Extensive experiments establish that SCARN outperforms other recurrent convolutional architectures with significantly less parameters. Furthermore, SCARN achieves better performance compared to equally large various deep CNN and LSTM architectures.</p>
<p>Keywords:</p>
<h3 id="567. The Role of Pragmatic and Discourse Context in Determining Argument Impact.">567. The Role of Pragmatic and Discourse Context in Determining Argument Impact.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1568">Paper Link</a>    Pages:5667-5677</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/6227.html">Esin Durmus</a> ; <a href="https://dblp.uni-trier.de/pid/194/1214.html">Faisal Ladhak</a> ; <a href="https://dblp.uni-trier.de/pid/c/ClaireCardie.html">Claire Cardie</a></p>
<p>Abstract:
Research in the social sciences and psychology has shown that the persuasiveness of an argument depends not only the language employed, but also on attributes of the source/communicator, the audience, and the appropriateness and strength of the arguments claims given the pragmatic and discourse context of the argument. Among these characteristics of persuasive arguments, prior work in NLP does not explicitly investigate the effect of the pragmatic and discourse context when determining argument quality. This paper presents a new dataset to initiate the study of this aspect of argumentation: it consists of a diverse collection of arguments covering 741 controversial topics and comprising over 47,000 claims. We further propose predictive models that incorporate the pragmatic and discourse context of argumentative claims and show that they outperform models that rely only on claim-specific linguistic features for predicting the perceived impact of individual claims within a particular line of argument.</p>
<p>Keywords:</p>
<h3 id="568. Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree.">568. Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1569">Paper Link</a>    Pages:5678-5687</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/09/1171.html">Kai Sun</a> ; <a href="https://dblp.uni-trier.de/pid/61/1229.html">Richong Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/185/1515.html">Samuel Mensah</a> ; <a href="https://dblp.uni-trier.de/pid/86/2933.html">Yongyi Mao</a> ; <a href="https://dblp.uni-trier.de/pid/90/2144-1.html">Xudong Liu</a></p>
<p>Abstract:
We propose a method based on neural networks to identify the sentiment polarity of opinion words expressed on a specific aspect of a sentence. Although a large majority of works typically focus on leveraging the expressive power of neural networks in handling this task, we explore the possibility of integrating dependency trees with neural networks for representation learning. To this end, we present a convolution over a dependency tree (CDT) model which exploits a Bi-directional Long Short Term Memory (Bi-LSTM) to learn representations for features of a sentence, and further enhance the embeddings with a graph convolutional network (GCN) which operates directly on the dependency tree of the sentence. Our approach propagates both contextual and dependency information from opinion words to aspect words, offering discriminative properties for supervision. Experimental results ranks our approach as the new state-of-the-art in aspect-based sentiment classification.</p>
<p>Keywords:</p>
<h3 id="569. Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization.">569. Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1570">Paper Link</a>    Pages:5688-5694</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/31/10776.html">Guanlin Li</a> ; <a href="https://dblp.uni-trier.de/pid/41/10887.html">Lemao Liu</a> ; <a href="https://dblp.uni-trier.de/pid/165/3047.html">Guoping Huang</a> ; <a href="https://dblp.uni-trier.de/pid/28/2613.html">Conghui Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/35/1787.html">Tiejun Zhao</a></p>
<p>Abstract:
Many Data Augmentation (DA) methods have been proposed for neural machine translation. Existing works measure the superiority of DA methods in terms of their performance on a specific test set, but we find that some DA methods do not exhibit consistent improvements across translation tasks. Based on the observation, this paper makes an initial attempt to answer a fundamental question: what benefits, which are consistent across different methods and tasks, does DA in general obtain? Inspired by recent theoretic advances in deep learning, the paper understands DA from two perspectives towards the generalization ability of a model: input sensitivity and prediction margin, which are defined independent of specific test set thereby may lead to findings with relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives.</p>
<p>Keywords:</p>
<h3 id="570. Simple and Effective Noisy Channel Modeling for Neural Machine Translation.">570. Simple and Effective Noisy Channel Modeling for Neural Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1571">Paper Link</a>    Pages:5695-5700</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/6024.html">Kyra Yee</a> ; <a href="https://dblp.uni-trier.de/pid/22/9988.html">Yann N. Dauphin</a> ; <a href="https://dblp.uni-trier.de/pid/11/9768.html">Michael Auli</a></p>
<p>Abstract:
Previous work on neural noisy channel modeling relied on latent variable models that incrementally process the source and target sentence. This makes decoding decisions based on partial source prefixes even though the full source is available. We pursue an alternative approach based on standard sequence to sequence models which utilize the entire source. These models perform remarkably well as channel models, even though they have neither been trained on, nor designed to factor over incomplete target sentences. Experiments with neural language models trained on billions of words show that noisy channel models can outperform a direct model by up to 3.2 BLEU on WMT17 German-English translation. We evaluate on four language-pairs and our channel models consistently outperform strong alternatives such right-to-left reranking models and ensembles of direct models.</p>
<p>Keywords:</p>
<h3 id="571. MultiFiT: Efficient Multi-lingual Language Model Fine-tuning.">571. MultiFiT: Efficient Multi-lingual Language Model Fine-tuning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1572">Paper Link</a>    Pages:5701-5706</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8056.html">Julian Eisenschlos</a> ; <a href="https://dblp.uni-trier.de/pid/186/7066.html">Sebastian Ruder</a> ; <a href="https://dblp.uni-trier.de/pid/228/8483.html">Piotr Czapla</a> ; <a href="https://dblp.uni-trier.de/pid/124/6867.html">Marcin Kardas</a> ; <a href="https://dblp.uni-trier.de/pid/248/8258.html">Sylvain Gugger</a> ; <a href="https://dblp.uni-trier.de/pid/134/4033.html">Jeremy Howard</a></p>
<p>Abstract:
Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing models requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code.</p>
<p>Keywords:</p>
<h3 id="572. Hint-Based Training for Non-Autoregressive Machine Translation.">572. Hint-Based Training for Non-Autoregressive Machine Translation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1573">Paper Link</a>    Pages:5707-5712</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/96/9986.html">Zhuohan Li</a> ; <a href="https://dblp.uni-trier.de/pid/81/2999.html">Zi Lin</a> ; <a href="https://dblp.uni-trier.de/pid/74/184.html">Di He</a> ; <a href="https://dblp.uni-trier.de/pid/03/8407.html">Fei Tian</a> ; <a href="https://dblp.uni-trier.de/pid/14/6841.html">Tao Qin</a> ; <a href="https://dblp.uni-trier.de/pid/47/1798-1.html">Liwei Wang</a> ; <a href="https://dblp.uni-trier.de/pid/l/TieYanLiu.html">Tie-Yan Liu</a></p>
<p>Abstract:
Due to the unparallelizable nature of the autoregressive factorization, AutoRegressive Translation (ART) models have to generate tokens sequentially during decoding and thus suffer from high inference latency. Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time, but could only achieve inferior translation accuracy. In this paper, we proposed a novel approach to leveraging the hints from hidden states and word alignments to help the training of NART models. The results achieve significant improvement over previous NART models for the WMT14 En-De and De-En datasets and are even comparable to a strong LSTM-based ART baseline but one order of magnitude faster in inference.</p>
<p>Keywords:</p>
<h3 id="573. Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers.">573. Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1574">Paper Link</a>    Pages:5713-5719</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/182/2211.html">Adam Fisch</a> ; <a href="https://dblp.uni-trier.de/pid/27/806.html">Jiang Guo</a> ; <a href="https://dblp.uni-trier.de/pid/b/ReginaBarzilay.html">Regina Barzilay</a></p>
<p>Abstract:
This paper explores the task of leveraging typology in the context of cross-lingual dependency parsing. While this linguistic information has shown great promise in pre-neural parsing, results for neural architectures have been mixed. The aim of our investigation is to better understand this state-of-the-art. Our main findings are as follows: 1) The benefit of typological information is derived from coarsely grouping languages into syntactically-homogeneous clusters rather than from learning to leverage variations along individual typological dimensions in a compositional manner; 2) Typology consistent with the actual corpus statistics yields better transfer performance; 3) Typological similarity is only a rough proxy of cross-lingual transferability with respect to parsing.</p>
<p>Keywords:</p>
<h3 id="574. Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing.">574. Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1575">Paper Link</a>    Pages:5720-5726</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/94/3940-1.html">Yuxuan Wang</a> ; <a href="https://dblp.uni-trier.de/pid/98/4640.html">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pid/27/806.html">Jiang Guo</a> ; <a href="https://dblp.uni-trier.de/pid/89/7780.html">Yijia Liu</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a></p>
<p>Abstract:
This paper investigates the problem of learning cross-lingual representations in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a simple and efficient approach to generate cross-lingual contextualized word embeddings based on publicly available pre-trained BERT models (Devlin et al., 2018). In this approach, a linear transformation is learned from contextual word alignments to align the contextualized embeddings independently trained in different languages. We demonstrate the effectiveness of this approach on zero-shot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results.</p>
<p>Keywords:</p>
<h3 id="575. Multilingual Grammar Induction with Continuous Language Identification.">575. Multilingual Grammar Induction with Continuous Language Identification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1576">Paper Link</a>    Pages:5727-5732</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/188/9071.html">Wenjuan Han</a> ; <a href="https://dblp.uni-trier.de/pid/34/5591-5.html">Ge Wang</a> ; <a href="https://dblp.uni-trier.de/pid/74/1552.html">Yong Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/22/918.html">Kewei Tu</a></p>
<p>Abstract:
The key to multilingual grammar induction is to couple grammar parameters of different languages together by exploiting the similarity between languages. Previous work relies on linguistic phylogenetic knowledge to specify similarity between languages. In this work, we propose a novel universal grammar induction approach that represents language identities with continuous vectors and employs a neural network to predict grammar parameters based on the representation. Without any prior linguistic phylogenetic knowledge, we automatically capture similarity between languages with the vector representations and softly tie the grammar parameters of different languages. In our experiments, we apply our approach to 15 languages across 8 language families and subfamilies in the Universal Dependency Treebank dataset, and we observe substantial performance gain on average over monolingual and multilingual baselines.</p>
<p>Keywords:</p>
<h3 id="576. Quantifying the Semantic Core of Gender Systems.">576. Quantifying the Semantic Core of Gender Systems.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1577">Paper Link</a>    Pages:5733-5738</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/199/2104.html">Adina Williams</a> ; <a href="https://dblp.uni-trier.de/pid/243/2945.html">Damin E. Blasi</a> ; <a href="https://dblp.uni-trier.de/pid/222/1837.html">Lawrence Wolf-Sonkin</a> ; <a href="https://dblp.uni-trier.de/pid/84/3994.html">Hanna M. Wallach</a> ; <a href="https://dblp.uni-trier.de/pid/146/4361.html">Ryan Cotterell</a></p>
<p>Abstract:
Many of the worlds languages employ grammatical gender on the lexeme. For instance, in Spanish, house casa is feminine, whereas the word for paper papel is masculine. To a speaker of a genderless language, this categorization seems to exist with neither rhyme nor reason. But, is the association of nouns to gender classes truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.</p>
<p>Keywords:</p>
<h3 id="577. Perturbation Sensitivity Analysis to Detect Unintended Model Biases.">577. Perturbation Sensitivity Analysis to Detect Unintended Model Biases.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1578">Paper Link</a>    Pages:5739-5744</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/64/9281.html">Vinodkumar Prabhakaran</a> ; <a href="https://dblp.uni-trier.de/pid/56/1256.html">Ben Hutchinson</a> ; <a href="https://dblp.uni-trier.de/pid/56/2856.html">Margaret Mitchell</a></p>
<p>Abstract:
Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models  a sentiment model and a toxicity model  applied on online comments in English language from four different genres.</p>
<p>Keywords:</p>
<h3 id="578. Automatically Inferring Gender Associations from Language.">578. Automatically Inferring Gender Associations from Language.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1579">Paper Link</a>    Pages:5745-5751</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/204/1095.html">Serina Chang</a> ; <a href="https://dblp.uni-trier.de/pid/m/KathleenMcKeown.html">Kathy McKeown</a></p>
<p>Abstract:
In this paper, we pose the question: do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. The datasets allow us to compare how people write about women and men in two different settings  one set draws from celebrity news and the other from student reviews of computer science professors. We demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. Human evaluations show that our methods significantly outperform strong baselines.</p>
<p>Keywords:</p>
<h3 id="579. Reporting the Unreported: Event Extraction for Analyzing the Local Representation of Hate Crimes.">579. Reporting the Unreported: Event Extraction for Analyzing the Local Representation of Hate Crimes.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1580">Paper Link</a>    Pages:5752-5756</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7592.html">Aida Mostafazadeh Davani</a> ; <a href="https://dblp.uni-trier.de/pid/248/7741.html">Leigh Yeh</a> ; <a href="https://dblp.uni-trier.de/pid/248/7783.html">Mohammad Atari</a> ; <a href="https://dblp.uni-trier.de/pid/175/5416.html">Brendan Kennedy</a> ; <a href="https://dblp.uni-trier.de/pid/228/7245.html">Gwenyth Portillo-Wightman</a> ; <a href="https://dblp.uni-trier.de/pid/248/7756.html">Elaine Gonzalez</a> ; <a href="https://dblp.uni-trier.de/pid/248/7631.html">Natalie Delong</a> ; <a href="https://dblp.uni-trier.de/pid/248/7924.html">Rhea Bhatia</a> ; <a href="https://dblp.uni-trier.de/pid/248/7618.html">Arineh Mirinjian</a> ; <a href="https://dblp.uni-trier.de/pid/36/360.html">Xiang Ren</a> ; <a href="https://dblp.uni-trier.de/pid/64/1349.html">Morteza Dehghani</a></p>
<p>Abstract:
Official reports of hate crimes in the US are under-reported relative to the actual number of such incidents. Further, despite statistical approximations, there are no official reports from a large number of US cities regarding incidents of hate. Here, we first demonstrate that event extraction and multi-instance learning, applied to a corpus of local news articles, can be used to predict instances of hate crime. We then use the trained model to detect incidents of hate in cities for which the FBI lacks statistics. Lastly, we train models on predicting homicide and kidnapping, compare the predictions to FBI reports, and establish that incidents of hate are indeed under-reported, compared to other types of crimes, in local press.</p>
<p>Keywords:</p>
<h3 id="580. Minimally Supervised Learning of Affective Events Using Discourse Relations.">580. Minimally Supervised Learning of Affective Events Using Discourse Relations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1581">Paper Link</a>    Pages:5757-5764</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/62/5972.html">Jun Saito</a> ; <a href="https://dblp.uni-trier.de/pid/94/3673.html">Yugo Murawaki</a> ; <a href="https://dblp.uni-trier.de/pid/42/2149.html">Sadao Kurohashi</a></p>
<p>Abstract:
Recognizing affective events that trigger positive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly because the polarity of an event is not necessarily predictable from its constituent words. In this paper, we propose to propagate affective polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small.</p>
<p>Keywords:</p>
<h3 id="581. Event Detection with Multi-Order Graph Convolution and Aggregated Attention.">581. Event Detection with Multi-Order Graph Convolution and Aggregated Attention.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1582">Paper Link</a>    Pages:5765-5769</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/7263.html">Haoran Yan</a> ; <a href="https://dblp.uni-trier.de/pid/00/1728.html">Xiaolong Jin</a> ; <a href="https://dblp.uni-trier.de/pid/117/2876.html">Xiangbin Meng</a> ; <a href="https://dblp.uni-trier.de/pid/02/146.html">Jiafeng Guo</a> ; <a href="https://dblp.uni-trier.de/pid/44/912.html">Xueqi Cheng</a></p>
<p>Abstract:
Syntactic relations are broadly used in many NLP tasks. For event detection, syntactic relation representations based on dependency tree can better capture the interrelations between candidate trigger words and related entities than sentence representations. But, existing studies only use first-order syntactic relations (i.e., the arcs) in dependency trees to identify trigger words. For this reason, this paper proposes a new method for event detection, which uses a dependency tree based graph convolution network with aggregative attention to explicitly model and aggregate multi-order syntactic representations in sentences. Experimental comparison with state-of-the-art baselines shows the superiority of the proposed method.</p>
<p>Keywords:</p>
<h3 id="582. Coverage of Information Extraction from Sentences and Paragraphs.">582. Coverage of Information Extraction from Sentences and Paragraphs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1583">Paper Link</a>    Pages:5770-5775</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/48/10142.html">Simon Razniewski</a> ; <a href="https://dblp.uni-trier.de/pid/155/7193.html">Nitisha Jain</a> ; <a href="https://dblp.uni-trier.de/pid/140/3058.html">Paramita Mirza</a> ; <a href="https://dblp.uni-trier.de/pid/w/GerhardWeikum.html">Gerhard Weikum</a></p>
<p>Abstract:
Scalar implicatures are language features that imply the negation of stronger statements, e.g., She was married twice typically implicates that she was not married thrice. In this paper we discuss the importance of scalar implicatures in the context of textual information extraction. We investigate how textual features can be used to predict whether a given text segment mentions all objects standing in a certain relationship with a certain subject. Preliminary results on Wikipedia indicate that this prediction is feasible, and yields informative assessments.</p>
<p>Keywords:</p>
<h3 id="583. HMEAE: Hierarchical Modular Event Argument Extraction.">583. HMEAE: Hierarchical Modular Event Argument Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1584">Paper Link</a>    Pages:5776-5782</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/03/2015.html">Xiaozhi Wang</a> ; <a href="https://dblp.uni-trier.de/pid/38/8097.html">Ziqi Wang</a> ; <a href="https://dblp.uni-trier.de/pid/19/3011-7.html">Xu Han</a> ; <a href="https://dblp.uni-trier.de/pid/53/3245-1.html">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/l/JuanZiLi.html">Juanzi Li</a> ; <a href="https://dblp.uni-trier.de/pid/83/6353-30.html">Peng Li</a> ; <a href="https://dblp.uni-trier.de/pid/95/3291.html">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pid/00/5012-16.html">Jie Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/36/360.html">Xiang Ren</a></p>
<p>Abstract:
Existing event extraction methods classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective inductive bias from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from <a href="https://github.com/thunlp/HMEAE">https://github.com/thunlp/HMEAE</a>.</p>
<p>Keywords:</p>
<h3 id="584. Entity, Relation, and Event Extraction with Contextualized Span Representations.">584. Entity, Relation, and Event Extraction with Contextualized Span Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1585">Paper Link</a>    Pages:5783-5788</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/239/4346.html">David Wadden</a> ; <a href="https://dblp.uni-trier.de/pid/246/6256.html">Ulme Wennberg</a> ; <a href="https://dblp.uni-trier.de/pid/125/7491.html">Yi Luan</a> ; <a href="https://dblp.uni-trier.de/pid/52/1296.html">Hannaneh Hajishirzi</a></p>
<p>Abstract:
We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at <a href="https://github.com/dwadden/dygiepp">https://github.com/dwadden/dygiepp</a> and can be easily adapted for new tasks or datasets.</p>
<p>Keywords:</p>
<h3 id="585. Next Sentence Prediction helps Implicit Discourse Relation Classification within and across Domains.">585. Next Sentence Prediction helps Implicit Discourse Relation Classification within and across Domains.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1586">Paper Link</a>    Pages:5789-5795</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/44/4066.html">Wei Shi</a> ; <a href="https://dblp.uni-trier.de/pid/80/3304.html">Vera Demberg</a></p>
<p>Abstract:
Implicit discourse relation classification is one of the most difficult tasks in discourse parsing. Previous studies have generally focused on extracting better representations of the relational arguments. In order to solve the task, it is however additionally necessary to capture what events are expected to cause or follow each other. Current discourse relation classifiers fall short in this respect. We here show that this shortcoming can be effectively addressed by using the bidirectional encoder representation from transformers (BERT) proposed by Devlin et al. (2019), which were trained on a next-sentence prediction task, and thus encode a representation of likely next sentences. The BERT-based model outperforms the current state of the art in 11-way classification by 8% points on the standard PDTB dataset. Our experiments also demonstrate that the model can be successfully ported to other domains: on the BioDRB dataset, the model outperforms the state of the art system around 15% points.</p>
<p>Keywords:</p>
<h3 id="586. Split or Merge: Which is Better for Unsupervised RST Parsing?">586. Split or Merge: Which is Better for Unsupervised RST Parsing?</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1587">Paper Link</a>    Pages:5796-5801</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/133/3609.html">Naoki Kobayashi</a> ; <a href="https://dblp.uni-trier.de/pid/68/6820.html">Tsutomu Hirao</a> ; <a href="https://dblp.uni-trier.de/pid/158/3521.html">Kengo Nakamura</a> ; <a href="https://dblp.uni-trier.de/pid/124/2384.html">Hidetaka Kamigaito</a> ; <a href="https://dblp.uni-trier.de/pid/79/125.html">Manabu Okumura</a> ; <a href="https://dblp.uni-trier.de/pid/16/3520.html">Masaaki Nagata</a></p>
<p>Abstract:
Rhetorical Structure Theory (RST) parsing is crucial for many downstream NLP tasks that require a discourse structure for a text. Most of the previous RST parsers have been based on supervised learning approaches. That is, they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus. In this paper, we present two language-independent unsupervised RST parsing methods based on dynamic programming. The first one builds the optimal tree in terms of a dissimilarity score function that is defined for splitting a text span into smaller ones. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers.</p>
<p>Keywords:</p>
<h3 id="587. BERT for Coreference Resolution: Baselines and Analysis.">587. BERT for Coreference Resolution: Baselines and Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1588">Paper Link</a>    Pages:5802-5807</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/85/1261.html">Mandar Joshi</a> ; <a href="https://dblp.uni-trier.de/pid/117/4866.html">Omer Levy</a> ; <a href="https://dblp.uni-trier.de/pid/21/6793.html">Luke Zettlemoyer</a> ; <a href="https://dblp.uni-trier.de/pid/w/DanielSWeld.html">Daniel S. Weld</a></p>
<p>Abstract:
We apply BERT to coreference resolution, achieving a new state of the art on the GAP (+11.5 F1) and OntoNotes (+3.9 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO), but that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. We will release all code and trained models upon publication.</p>
<p>Keywords:</p>
<h3 id="588. Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs.">588. Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1589">Paper Link</a>    Pages:5808-5814</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/9056.html">Dongyeop Kang</a> ; <a href="https://dblp.uni-trier.de/pid/47/2454.html">Eduard H. Hovy</a></p>
<p>Abstract:
Generating a long, coherent text such as a paragraph requires a high-level control of different levels of relations between sentences (e.g., tense, coreference). We call such a logical connection between sentences as a (paragraph) flow. In order to produce a coherent flow of text, we explore two forms of intersentential relations in a paragraph: one is a human-created linguistical relation that forms a structure (e.g., discourse tree) and the other is a relation from latent representation learned from the sentences themselves. Our two proposed models incorporate each form of relations into document-level language models: the former is a supervised model that jointly learns a language model as well as discourse relation prediction, and the latter is an unsupervised model that is hierarchically conditioned by a recurrent neural network (RNN) over the latent information. Our proposed models with both forms of relations outperform the baselines in partially conditioned paragraph generation task. Our codes and data are publicly available.</p>
<p>Keywords:</p>
<h3 id="589. Event Causality Recognition Exploiting Multiple Annotators' Judgments and Background Knowledge.">589. Event Causality Recognition Exploiting Multiple Annotators' Judgments and Background Knowledge.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1590">Paper Link</a>    Pages:5815-5821</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/41/7652.html">Kazuma Kadowaki</a> ; <a href="https://dblp.uni-trier.de/pid/26/5652.html">Ryu Iida</a> ; <a href="https://dblp.uni-trier.de/pid/42/1797.html">Kentaro Torisawa</a> ; <a href="https://dblp.uni-trier.de/pid/52/5475.html">Jong-Hoon Oh</a> ; <a href="https://dblp.uni-trier.de/pid/12/8826.html">Julien Kloetzer</a></p>
<p>Abstract:
We propose new BERT-based methods for recognizing event causality such as smoke cigarettes &gt; die of lung cancer written in web texts. In our methods, we grasp each annotators policy by training multiple classifiers, each of which predicts the labels given by a single annotator, and combine the resulting classifiers outputs to predict the final labels determined by majority vote. Furthermore, we investigate the effect of supplying background knowledge to our classifiers. Since BERT models are pre-trained with a large corpus, some sort of background knowledge for event causality may be learned during pre-training. Our experiments with a Japanese dataset suggest that this is actually the case: Performance improved when we pre-trained the BERT models with web texts containing a large number of event causalities instead of Wikipedia articles or randomly sampled web texts. However, this effect was limited. Therefore, we further improved performance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction.</p>
<p>Keywords:</p>
<h3 id="590. What Part of the Neural Network Does This? Understanding LSTMs by Measuring and Dissecting Neurons.">590. What Part of the Neural Network Does This? Understanding LSTMs by Measuring and Dissecting Neurons.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1591">Paper Link</a>    Pages:5822-5829</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/7227.html">Ji Xin</a> ; <a href="https://dblp.uni-trier.de/pid/00/7739.html">Jimmy Lin</a> ; <a href="https://dblp.uni-trier.de/pid/90/4989.html">Yaoliang Yu</a></p>
<p>Abstract:
Memory neurons of long short-term memory (LSTM) networks encode and process information in powerful yet mysterious ways. While there has been work to analyze their behavior in carrying low-level information such as linguistic properties, how they directly contribute to label prediction remains unclear. We find inspiration from biologists and study the affinity between individual neurons and labels, propose a novel metric to quantify the sensitivity of neurons to each label, and conduct experiments to show the validity of our proposed metric. We discover that some neurons are trained to specialize on a subset of labels, and while dropping an arbitrary neuron has little effect on the overall accuracy of the model, dropping label-specialized neurons predictably and significantly degrades prediction accuracy on the associated label. We further examine the consistency of neuron-label affinity across different models. These observations provide insight into the inner mechanisms of LSTMs.</p>
<p>Keywords:</p>
<h3 id="591. Quantity doesn't buy quality syntax with neural language models.">591. Quantity doesn't buy quality syntax with neural language models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1592">Paper Link</a>    Pages:5830-5836</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/127/0199.html">Marten Van Schijndel</a> ; <a href="https://dblp.uni-trier.de/pid/248/7949.html">Aaron Mueller</a> ; <a href="https://dblp.uni-trier.de/pid/169/3438.html">Tal Linzen</a></p>
<p>Abstract:
Recurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures.</p>
<p>Keywords:</p>
<h3 id="592. Higher-order Comparisons of Sentence Encoder Representations.">592. Higher-order Comparisons of Sentence Encoder Representations.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1593">Paper Link</a>    Pages:5837-5844</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/205/9035.html">Mostafa Abdou</a> ; <a href="https://dblp.uni-trier.de/pid/205/9236.html">Artur Kulmizev</a> ; <a href="https://dblp.uni-trier.de/pid/116/0509.html">Felix Hill</a> ; <a href="https://dblp.uni-trier.de/pid/248/7477.html">Daniel M. Low</a> ; <a href="https://dblp.uni-trier.de/pid/30/2756.html">Anders Sgaard</a></p>
<p>Abstract:
Representational Similarity Analysis (RSA) is a technique developed by neuroscientists for comparing activity patterns of different measurement modalities (e.g., fMRI, electrophysiology, behavior). As a framework, RSA has several advantages over existing approaches to interpretation of language encoders based on probing or diagnostic classification: namely, it does not require large training samples, is not prone to overfitting, and it enables a more transparent comparison between the representational geometries of different models and modalities. We demonstrate the utility of RSA by establishing a previously unknown correspondence between widely-employed pretrained language encoders and human processing difficulty via eye-tracking data, showcasing its potential in the interpretability toolbox for neural models.</p>
<p>Keywords:</p>
<h3 id="593. Text Genre and Training Data Size in Human-like Parsing.">593. Text Genre and Training Data Size in Human-like Parsing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1594">Paper Link</a>    Pages:5845-5851</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/76/2017.html">John Hale</a> ; <a href="https://dblp.uni-trier.de/pid/176/5185.html">Adhiguna Kuncoro</a> ; <a href="https://dblp.uni-trier.de/pid/71/1342.html">Keith Hall</a> ; <a href="https://dblp.uni-trier.de/pid/41/6895.html">Chris Dyer</a> ; <a href="https://dblp.uni-trier.de/pid/60/11220.html">Jonathan Brennan</a></p>
<p>Abstract:
Domain-specific training typically makes NLP systems work better. We show that this extends to cognitive modeling as well by relating the states of a neural phrase-structure parser to electrophysiological measures from human participants. These measures were recorded as participants listened to a spoken recitation of the same literary text that was supplied as input to the neural parser. Given more training data, the system derives a better cognitive model  but only when the training examples come from the same textual genre. This finding is consistent with the idea that humans adapt syntactic expectations to particular genres during language comprehension (Kaan and Chun, 2018; Branigan and Pickering, 2017).</p>
<p>Keywords:</p>
<h3 id="594. Feature2Vec: Distributional semantic modelling of human property knowledge.">594. Feature2Vec: Distributional semantic modelling of human property knowledge.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1595">Paper Link</a>    Pages:5852-5858</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/227/2356.html">Steven Derby</a> ; <a href="https://dblp.uni-trier.de/pid/28/1901-3.html">Paul Miller</a> ; <a href="https://dblp.uni-trier.de/pid/97/4572.html">Barry Devereux</a></p>
<p>Abstract:
Feature norm datasets of human conceptual knowledge, collected in surveys of human volunteers, yield highly interpretable models of word meaning and play an important role in neurolinguistic research on semantic cognition. However, these datasets are limited in size due to practical obstacles associated with exhaustively listing properties for a large number of words. In contrast, the development of distributional modelling techniques and the availability of vast text corpora have allowed researchers to construct effective vector space models of word meaning over large lexicons. However, this comes at the cost of interpretable, human-like information about word meaning. We propose a method for mapping human property knowledge onto a distributional semantic space, which adapts the word2vec architecture to the task of modelling concept features. Our approach gives a measure of concept and feature affinity in a single semantic space, which makes for easy and efficient ranking of candidate human-derived semantic properties for arbitrary words. We compare our model with a previous approach, and show that it performs better on several evaluation tasks. Finally, we discuss how our method could be used to develop efficient sampling techniques to extend existing feature norm datasets in a reliable way.</p>
<p>Keywords:</p>
<h3 id="595. Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation.">595. Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1596">Paper Link</a>    Pages:5859-5864</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/164/9384.html">Arijit Ray</a> ; <a href="https://dblp.uni-trier.de/pid/119/1499.html">Karan Sikka</a> ; <a href="https://dblp.uni-trier.de/pid/73/467.html">Ajay Divakaran</a> ; <a href="https://dblp.uni-trier.de/pid/158/5809.html">Stefan Lee</a> ; <a href="https://dblp.uni-trier.de/pid/236/5230.html">Giedrius Burachas</a></p>
<p>Abstract:
While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers red to What color is the balloon?, it might answer no if asked, Is the balloon red?. These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloons color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQAs answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the Con-VQA datasets and is a strong baseline for further research.</p>
<p>Keywords:</p>
<h3 id="596. GeoSQA: A Benchmark for Scenario-based Question Answering in the Geography Domain at High School Level.">596. GeoSQA: A Benchmark for Scenario-based Question Answering in the Geography Domain at High School Level.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1597">Paper Link</a>    Pages:5865-5870</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/247/5762.html">Zixian Huang</a> ; <a href="https://dblp.uni-trier.de/pid/58/9414.html">Yulin Shen</a> ; <a href="https://dblp.uni-trier.de/pid/66/2069.html">Xiao Li</a> ; <a href="https://dblp.uni-trier.de/pid/247/6068.html">Yuang Wei</a> ; <a href="https://dblp.uni-trier.de/pid/69/1215-1.html">Gong Cheng</a> ; <a href="https://dblp.uni-trier.de/pid/69/6147.html">Lin Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/39/5815.html">Xinyu Dai</a> ; <a href="https://dblp.uni-trier.de/pid/05/1694.html">Yuzhong Qu</a></p>
<p>Abstract:
Scenario-based question answering (SQA) has attracted increasing research attention. It typically requires retrieving and integrating knowledge from multiple sources, and applying general knowledge to a specific case described by a scenario. SQA widely exists in the medical, geography, and legal domainsboth in practice and in the exams. In this paper, we introduce the GeoSQA dataset. It consists of 1,981 scenarios and 4,110 multiple-choice questions in the geography domain at high school level, where diagrams (e.g., maps, charts) have been manually annotated with natural language descriptions to benefit NLP research. Benchmark results on a variety of state-of-the-art methods for question answering, textual entailment, and reading comprehension demonstrate the unique challenges presented by SQA for future research.</p>
<p>Keywords:</p>
<h3 id="597. Revisiting the Evaluation of Theory of Mind through Question Answering.">597. Revisiting the Evaluation of Theory of Mind through Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1598">Paper Link</a>    Pages:5871-5876</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/133/1777.html">Matthew Le</a> ; <a href="https://dblp.uni-trier.de/pid/60/5680.html">Y-Lan Boureau</a> ; <a href="https://dblp.uni-trier.de/pid/83/10622.html">Maximilian Nickel</a></p>
<p>Abstract:
Theory of mind, i.e., the ability to reason about intents and beliefs of agents is an important task in artificial intelligence and central to resolving ambiguous references in natural language dialogue. In this work, we revisit the evaluation of theory of mind through question answering. We show that current evaluation methods are flawed and that existing benchmark tasks can be solved without theory of mind due to dataset biases. Based on prior work, we propose an improved evaluation protocol and dataset in which we explicitly control for data regularities via a careful examination of the answer space. We show that state-of-the-art methods which are successful on existing benchmarks fail to solve theory-of-mind tasks in our proposed approach.</p>
<p>Keywords:</p>
<h3 id="598. Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering.">598. Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1599">Paper Link</a>    Pages:5877-5881</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/80/709.html">Zhiguo Wang</a> ; <a href="https://dblp.uni-trier.de/pid/92/3908.html">Patrick Ng</a> ; <a href="https://dblp.uni-trier.de/pid/120/1069.html">Xiaofei Ma</a> ; <a href="https://dblp.uni-trier.de/pid/59/4797.html">Ramesh Nallapati</a> ; <a href="https://dblp.uni-trier.de/pid/82/5456.html">Bing Xiang</a></p>
<p>Abstract:
BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models.</p>
<p>Keywords:</p>
<h3 id="599. A Span-Extraction Dataset for Chinese Machine Reading Comprehension.">599. A Span-Extraction Dataset for Chinese Machine Reading Comprehension.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1600">Paper Link</a>    Pages:5882-5888</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/130/6308.html">Yiming Cui</a> ; <a href="https://dblp.uni-trier.de/pid/52/5150-1.html">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pid/98/4640.html">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pid/14/5505.html">Li Xiao</a> ; <a href="https://dblp.uni-trier.de/pid/22/8437.html">Zhipeng Chen</a> ; <a href="https://dblp.uni-trier.de/pid/39/8088.html">Wentao Ma</a> ; <a href="https://dblp.uni-trier.de/pid/74/5750-1.html">Shijin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/59/5304.html">Guoping Hu</a></p>
<p>Abstract:
Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in English. In this paper, we introduce a Span-Extraction dataset for Chinese machine reading comprehension to add language diversities in this area. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. We present several baseline systems as well as anonymous submissions for demonstrating the difficulties in this dataset. With the release of the dataset, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research. Resources are available: <a href="https://github.com/ymcui/cmrc2018">https://github.com/ymcui/cmrc2018</a></p>
<p>Keywords:</p>
<h3 id="600. MICRON: Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering.">600. MICRON: Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1601">Paper Link</a>    Pages:5889-5894</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8084.html">Hojae Han</a> ; <a href="https://dblp.uni-trier.de/pid/218/7548.html">Seungtaek Choi</a> ; <a href="https://dblp.uni-trier.de/pid/242/5142.html">Haeju Park</a> ; <a href="https://dblp.uni-trier.de/pid/h/SeungwonHwang.html">Seung-won Hwang</a></p>
<p>Abstract:
This paper studies the problem of non-factoid question answering, where the answer may span over multiple sentences. Existing solutions can be categorized into representation- and interaction-focused approaches. We combine their complementary strength, by a hybrid approach allowing multi-granular interactions, but represented at word level, enabling an easy integration with strong word-level signals. Specifically, we propose MICRON: Multigranular Interaction for Contextualizing RepresentatiON, a novel approach which derives contextualized uni-gram representation from n-grams. Our contributions are as follows: First, we enable multi-granular matches between question and answer n-grams. Second, by contextualizing word representation with surrounding n-grams, MICRON can naturally utilize word-based signals for query term weighting, known to be effective in information retrieval. We validate MICRON in two public non-factoid question answering datasets: WikiPassageQA and InsuranceQA, showing our model achieves the state of the art among baselines with reported performances on both datasets.</p>
<p>Keywords:</p>
<h3 id="601. Machine Reading Comprehension Using Structural Knowledge Graph-aware Network.">601. Machine Reading Comprehension Using Structural Knowledge Graph-aware Network.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1602">Paper Link</a>    Pages:5895-5900</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/250/8855.html">Delai Qiu</a> ; <a href="https://dblp.uni-trier.de/pid/141/4448.html">Yuanzhe Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/60/7758.html">Xinwei Feng</a> ; <a href="https://dblp.uni-trier.de/pid/50/6801.html">Xiangwen Liao</a> ; <a href="https://dblp.uni-trier.de/pid/96/5583.html">Wenbin Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/190/7920.html">Yajuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pid/42/4903.html">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/47/2026-1.html">Jun Zhao</a></p>
<p>Abstract:
Leveraging external knowledge is an emerging trend in machine comprehension task. Previous work usually utilizes knowledge graphs such as ConceptNet as external knowledge, and extracts triples from them to enhance the initial representation of the machine comprehension context. However, such method cannot capture the structural information in the knowledge graph. To this end, we propose a Structural Knowledge Graph-aware Network(SKG) model, constructing sub-graphs for entities in the machine comprehension context. Our method dynamically updates the representation of the knowledge according to the structural information of the constructed sub-graph. Experiments show that SKG achieves state-of-the-art performance on the ReCoRD dataset.</p>
<p>Keywords:</p>
<h3 id="602. Answering Conversational Questions on Structured Data without Logical Forms.">602. Answering Conversational Questions on Structured Data without Logical Forms.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1603">Paper Link</a>    Pages:5901-5909</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/38/6310.html">Thomas Mller</a> ; <a href="https://dblp.uni-trier.de/pid/151/3088.html">Francesco Piccinno</a> ; <a href="https://dblp.uni-trier.de/pid/217/1471.html">Peter Shaw</a> ; <a href="https://dblp.uni-trier.de/pid/136/8001.html">Massimo Nicosia</a> ; <a href="https://dblp.uni-trier.de/pid/85/3389.html">Yasemin Altun</a></p>
<p>Abstract:
We present a novel approach to answering sequential questions based on structured objects such as knowledge bases or tables without using a logical form as an intermediate representation. We encode tables as graphs using a graph neural network model based on the Transformer architecture. The answers are then selected from the encoded graph using a pointer network. This model is appropriate for processing conversations around structured data, where the attention mechanism that selects the answers to a question can also be used to resolve conversational references. We demonstrate the validity of this approach with competitive results on the Sequential Question Answering (SQA) task.</p>
<p>Keywords:</p>
<h3 id="603. Improving Answer Selection and Answer Triggering using Hard Negatives.">603. Improving Answer Selection and Answer Triggering using Hard Negatives.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1604">Paper Link</a>    Pages:5910-5916</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/141/9611.html">Sawan Kumar</a> ; <a href="https://dblp.uni-trier.de/pid/212/0267-1.html">Shweta Garg</a> ; <a href="https://dblp.uni-trier.de/pid/118/3837.html">Kartik Mehta</a> ; <a href="https://dblp.uni-trier.de/pid/82/335.html">Nikhil Rasiwasia</a></p>
<p>Abstract:
In this paper, we establish the effectiveness of using hard negatives, coupled with a siamese network and a suitable loss function, for the tasks of answer selection and answer triggering. We show that the choice of sampling strategy is key for achieving improved performance on these tasks. Evaluating on recent answer selection datasets - InsuranceQA, SelQA, and an internal QA dataset, we show that using hard negatives with relatively simple model architectures (bag of words and LSTM-CNN) drives significant performance gains. On InsuranceQA, this strategy alone improves over previously reported results by a minimum of 1.6 points in P@1. Using hard negatives with a Transformer encoder provides a further improvement of 2.3 points. Further, we propose to use quadruplet loss for answer triggering, with the aim of producing globally meaningful similarity scores. We show that quadruplet loss function coupled with the selection of hard negatives enables bag-of-words models to improve F1 score by 2.3 points over previous baselines, on SelQA answer triggering dataset. Our results provide key insights into answer selection and answer triggering tasks.</p>
<p>Keywords:</p>
<h3 id="604. Can You Unpack That? Learning to Rewrite Questions-in-Context.">604. Can You Unpack That? Learning to Rewrite Questions-in-Context.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1605">Paper Link</a>    Pages:5917-5923</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/03/8978.html">Ahmed Elgohary</a> ; <a href="https://dblp.uni-trier.de/pid/203/9242.html">Denis Peskov</a> ; <a href="https://dblp.uni-trier.de/pid/57/5950.html">Jordan L. Boyd-Graber</a></p>
<p>Abstract:
Question answering is an AI-complete problem, but existing datasets lack key elements of language understanding such as coreference and ellipsis resolution. We consider sequential question answering: multiple questions are asked one-by-one in a conversation between a questioner and an answerer. Answering these questions is only possible through understanding the conversation history. We introduce the task of question-in-context rewriting: given the context of a conversations history, rewrite a context-dependent into a self-contained question with the same answer. We construct, CANARD, a dataset of 40,527 questions based on QuAC (Choi et al., 2018) and train Seq2Seq models for incorporating context into standalone questions.</p>
<p>Keywords:</p>
<h3 id="605. Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning.">605. Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1606">Paper Link</a>    Pages:5924-5931</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/27/7184.html">Pradeep Dasigi</a> ; <a href="https://dblp.uni-trier.de/pid/203/9152.html">Nelson F. Liu</a> ; <a href="https://dblp.uni-trier.de/pid/185/0950.html">Ana Marasovic</a> ; <a href="https://dblp.uni-trier.de/pid/90/5204.html">Noah A. Smith</a> ; <a href="https://dblp.uni-trier.de/pid/00/8046.html">Matt Gardner</a></p>
<p>Abstract:
Machine comprehension of texts longer than a single sentence often requires coreference resolution. However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference. We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia. Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning. We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues. We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmarkthe best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.</p>
<p>Keywords:</p>
<h3 id="606. Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model.">606. Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1607">Paper Link</a>    Pages:5932-5939</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/5265.html">Tsung-Yuan Hsu</a> ; <a href="https://dblp.uni-trier.de/pid/218/5186.html">Chi-Liang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/81/8056.html">Hung-yi Lee</a></p>
<p>Abstract:
Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.</p>
<p>Keywords:</p>
<h3 id="607. QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions.">607. QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1608">Paper Link</a>    Pages:5940-5945</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/178/8640.html">Oyvind Tafjord</a> ; <a href="https://dblp.uni-trier.de/pid/00/8046.html">Matt Gardner</a> ; <a href="https://dblp.uni-trier.de/pid/69/10883.html">Kevin Lin</a> ; <a href="https://dblp.uni-trier.de/pid/34/1184.html">Peter Clark</a></p>
<p>Abstract:
We introduce the first open-domain dataset, called QuaRTz, for reasoning about textual qualitative relationships. QuaRTz contains general qualitative statements, e.g., A sunscreen with a higher SPF protects the skin longer., twinned with 3864 crowdsourced situated questions, e.g., Billy is wearing sunscreen with a lower SPF than Lucy. Who will be best protected from the sun?, plus annotations of the properties being compared. Unlike previous datasets, the general knowledge is textual and not tied to a fixed set of relationships, and tests a systems ability to comprehend and apply textual qualitative knowledge in a novel setting. We find state-of-the-art results are substantially (20%) below human performance, presenting an open challenge to the NLP community.</p>
<p>Keywords:</p>
<h3 id="608. Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension.">608. Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1609">Paper Link</a>    Pages:5946-5951</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/177/8962.html">Daniel Andor</a> ; <a href="https://dblp.uni-trier.de/pid/44/9922.html">Luheng He</a> ; <a href="https://dblp.uni-trier.de/pid/121/7560.html">Kenton Lee</a> ; <a href="https://dblp.uni-trier.de/pid/16/1483.html">Emily Pitler</a></p>
<p>Abstract:
Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable programs which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33% absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.</p>
<p>Keywords:</p>
<h3 id="609. A Gated Self-attention Memory Network for Answer Selection.">609. A Gated Self-attention Memory Network for Answer Selection.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1610">Paper Link</a>    Pages:5952-5958</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/187/2929.html">Tuan Manh Lai</a> ; <a href="https://dblp.uni-trier.de/pid/151/8700.html">Quan Hung Tran</a> ; <a href="https://dblp.uni-trier.de/pid/180/0632.html">Trung Bui</a> ; <a href="https://dblp.uni-trier.de/pid/13/915.html">Daisuke Kihara</a></p>
<p>Abstract:
Answer selection is an important research problem, with applications in many areas. Previous deep learning based approaches for the task mainly adopt the Compare-Aggregate architecture that performs word-level comparison followed by aggregation. In this work, we take a departure from the popular Compare-Aggregate architecture, and instead, propose a new gated self-attention memory network for the task. Combined with a simple transfer learning technique from a large-scale online corpus, our model outperforms previous methods by a large margin, achieving new state-of-the-art results on two standard answer selection datasets: TrecQA and WikiQA.</p>
<p>Keywords:</p>
<h3 id="610. Polly Want a Cracker: Analyzing Performance of Parroting on Paraphrase Generation Datasets.">610. Polly Want a Cracker: Analyzing Performance of Parroting on Paraphrase Generation Datasets.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1611">Paper Link</a>    Pages:5959-5967</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/247/6173.html">Hongren Mao</a> ; <a href="https://dblp.uni-trier.de/pid/81/8056.html">Hung-yi Lee</a></p>
<p>Abstract:
Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze datasets commonly used for paraphrase generation research, and show that simply parroting input sentences surpasses state-of-the-art models in the literature when evaluated on standard metrics. Our findings illustrate that a model could be seemingly adept at generating paraphrases, despite only making trivial changes to the input sentence or even none at all.</p>
<p>Keywords:</p>
<h3 id="611. Query-focused Sentence Compression in Linear Time.">611. Query-focused Sentence Compression in Linear Time.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1612">Paper Link</a>    Pages:5968-5974</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/182/2581.html">Abram Handler</a> ; <a href="https://dblp.uni-trier.de/pid/10/1481.html">Brendan O&apos;Connor</a></p>
<p>Abstract:
Search applications often display shortened sentences which must contain certain query terms and must fit within the space constraints of a user interface. This work introduces a new transition-based sentence compression technique developed for such settings. Our query-focused method constructs length and lexically constrained compressions in linear time, by growing a subgraph in the dependency parse of a sentence. This theoretically efficient approach achieves an 11x empirical speedup over baseline ILP methods, while better reconstructing gold constrained shortenings. Such speedups help query-focused applications, because users are measurably hindered by interface lags. Additionally, our technique does not require an ILP solver or a GPU.</p>
<p>Keywords:</p>
<h3 id="612. Generating Personalized Recipes from Historical User Preferences.">612. Generating Personalized Recipes from Historical User Preferences.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1613">Paper Link</a>    Pages:5975-5981</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/138/6177.html">Bodhisattwa Prasad Majumder</a> ; <a href="https://dblp.uni-trier.de/pid/29/9471.html">Shuyang Li</a> ; <a href="https://dblp.uni-trier.de/pid/161/2449.html">Jianmo Ni</a> ; <a href="https://dblp.uni-trier.de/pid/29/3483.html">Julian J. McAuley</a></p>
<p>Abstract:
Existing approaches to recipe generation are unable to create recipes for users with culinary preferences but incomplete knowledge of ingredients in specific dishes. We propose a new task of personalized recipe generation to help these users: expanding a name and incomplete ingredient details into complete natural-text instructions aligned with the users historical preferences. We attend on technique- and recipe-level representations of a users previously consumed recipes, fusing these user-aware representations in an attention fusion layer to control recipe text generation. Experiments on a new dataset of 180K recipes and 700K interactions show our models ability to generate plausible and personalized recipes compared to non-personalized baselines.</p>
<p>Keywords:</p>
<h3 id="613. Generating Highly Relevant Questions.">613. Generating Highly Relevant Questions.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1614">Paper Link</a>    Pages:5982-5986</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/250/2904.html">Jiazuo Qiu</a> ; <a href="https://dblp.uni-trier.de/pid/55/6548.html">Deyi Xiong</a></p>
<p>Abstract:
The neural seq2seq based question generation (QG) is prone to generating generic and undiversified questions that are poorly relevant to the given passage and target answer. In this paper, we propose two methods to address the issue. (1) By a partial copy mechanism, we prioritize words that are morphologically close to words in the input passage when generating questions; (2) By a QA-based reranker, from the n-best list of question candidates, we select questions that are preferred by both the QA and QG model. Experiments and analyses demonstrate that the proposed two methods substantially improve the relevance of generated questions to passages and answers.</p>
<p>Keywords:</p>
<h3 id="614. Improving Neural Story Generation by Targeted Common Sense Grounding.">614. Improving Neural Story Generation by Targeted Common Sense Grounding.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1615">Paper Link</a>    Pages:5987-5992</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/213/7630.html">Huanru Henry Mao</a> ; <a href="https://dblp.uni-trier.de/pid/138/6177.html">Bodhisattwa Prasad Majumder</a> ; <a href="https://dblp.uni-trier.de/pid/29/3483.html">Julian J. McAuley</a> ; <a href="https://dblp.uni-trier.de/pid/c/GWCottrell.html">Garrison W. Cottrell</a></p>
<p>Abstract:
Stories generated with neural language models have shown promise in grammatical and stylistic consistency. However, the generated stories are still lacking in common sense reasoning, e.g., they often contain sentences deprived of world knowledge. We propose a simple multi-task learning scheme to achieve quantitatively better common sense reasoning in language models by leveraging auxiliary training signals from datasets designed to provide common sense grounding. When combined with our two-stage fine-tuning pipeline, our method achieves improved common sense reasoning and state-of-the-art perplexity on the WritingPrompts (Fan et al., 2018) story generation dataset.</p>
<p>Keywords:</p>
<h3 id="615. Abstract Text Summarization: A Low Resource Challenge.">615. Abstract Text Summarization: A Low Resource Challenge.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1616">Paper Link</a>    Pages:5993-5997</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/142/1577.html">Shantipriya Parida</a> ; <a href="https://dblp.uni-trier.de/pid/15/3204.html">Petr Motlcek</a></p>
<p>Abstract:
Text summarization is considered as a challenging task in the NLP community. The availability of datasets for the task of multilingual text summarization is rare, and such datasets are difficult to construct. In this work, we build an abstract text summarizer for the German language text using the state-of-the-art Transformer model. We propose an iterative data augmentation approach which uses synthetic data along with the real summarization data for the German language. To generate synthetic data, the Common Crawl (German) dataset is exploited, which covers different domains. The synthetic data is effective for the low resource condition and is particularly helpful for our multilingual scenario where availability of summarizing data is still a challenging issue. The data are also useful in deep learning scenarios where the neural models require a large amount of training data for utilization of its capacity. The obtained summarization performance is measured in terms of ROUGE and BLEU score. We achieve an absolute improvement of +1.5 and +16.0 in ROUGE1 F1 (R1_F1) on the development and test sets, respectively, compared to the system which does not rely on data augmentation.</p>
<p>Keywords:</p>
<h3 id="616. Generating Modern Poetry Automatically in Finnish.">616. Generating Modern Poetry Automatically in Finnish.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1617">Paper Link</a>    Pages:5998-6003</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/219/5607.html">Mika Hmlinen</a> ; <a href="https://dblp.uni-trier.de/pid/178/1911.html">Khalid Al-Najjar</a></p>
<p>Abstract:
We present a novel approach for generating poetry automatically for the morphologically rich Finnish language by using a genetic algorithm. The approach improves the state of the art of the previous Finnish poem generators by introducing a higher degree of freedom in terms of structural creativity. Our approach is evaluated and described within the paradigm of computational creativity, where the fitness functions of the genetic algorithm are assimilated with the notion of aesthetics. The output is considered to be a poem 81.5% of the time by human evaluators.</p>
<p>Keywords:</p>
<h3 id="617. SUM-QE: a BERT-based Summary Quality Estimation Model.">617. SUM-QE: a BERT-based Summary Quality Estimation Model.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1618">Paper Link</a>    Pages:6004-6010</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7585.html">Stratos Xenouleas</a> ; <a href="https://dblp.uni-trier.de/pid/16/5137.html">Prodromos Malakasiotis</a> ; <a href="https://dblp.uni-trier.de/pid/53/1712.html">Marianna Apidianaki</a> ; <a href="https://dblp.uni-trier.de/pid/87/6723.html">Ion Androutsopoulos</a></p>
<p>Abstract:
We propose SUM-QE, a novel Quality Estimation model for summarization based on BERT. The model addresses linguistic quality aspects that are only indirectly captured by content-based approaches to summary evaluation, without involving comparison with human references. SUM-QE achieves very high correlations with human ratings, outperforming simpler models addressing these linguistic aspects. Predictions of the SUM-QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text.</p>
<p>Keywords:</p>
<h3 id="618. An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation.">618. An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1619">Paper Link</a>    Pages:6011-6017</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/247/9422.html">Wanyu Du</a> ; <a href="https://dblp.uni-trier.de/pid/94/8323.html">Yangfeng Ji</a></p>
<p>Abstract:
Generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. To learn a decoder, supervised learning which maximizes the likelihood of tokens always suffers from the exposure bias. Although both reinforcement learning (RL) and imitation learning (IL) have been widely used to alleviate the bias, the lack of direct comparison leads to only a partial image on their benefits. In this work, we present an empirical study on how RL and IL can help boost the performance of generating paraphrases, with the pointer-generator as a base model. Experiments on the benchmark datasets show that (1) imitation learning is constantly better than reinforcement learning; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.</p>
<p>Keywords:</p>
<h3 id="619. Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses.">619. Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1620">Paper Link</a>    Pages:6018-6023</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8357.html">Matt Grenander</a> ; <a href="https://dblp.uni-trier.de/pid/84/486.html">Yue Dong</a> ; <a href="https://dblp.uni-trier.de/pid/00/9012.html">Jackie Chi Kit Cheung</a> ; <a href="https://dblp.uni-trier.de/pid/98/4709.html">Annie Louis</a></p>
<p>Abstract:
Sentence position is a strong feature for news summarization, since the lead often (but not always) summarizes the key points of the article. In this paper, we show that recent neural systems excessively exploit this trend, which although powerful for many inputs, is also detrimental when summarizing documents where important content should be extracted from later parts of the article. We propose two techniques to make systems sensitive to the importance of content in different parts of the article. The first technique employs unbiased data; i.e., randomly shuffled sentences of the source document, to pretrain the model. The second technique uses an auxiliary ROUGE-based loss that encourages the model to distribute importance scores throughout a document by mimicking sentence-level ROUGE scores on the training data. We show that these techniques significantly improve the performance of a competitive reinforcement learning based extractive system, with the auxiliary loss being more powerful than pretraining.</p>
<p>Keywords:</p>
<h3 id="620. Learning Rhyming Constraints using Structured Adversaries.">620. Learning Rhyming Constraints using Structured Adversaries.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1621">Paper Link</a>    Pages:6024-6030</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/146/6263.html">Harsh Jhamtani</a> ; <a href="https://dblp.uni-trier.de/pid/225/7804.html">Sanket Vaibhav Mehta</a> ; <a href="https://dblp.uni-trier.de/pid/56/3395.html">Jaime G. Carbonell</a> ; <a href="https://dblp.uni-trier.de/pid/22/8160.html">Taylor Berg-Kirkpatrick</a></p>
<p>Abstract:
Existing recurrent neural language models often fail to capture higher-level structure present in text: for example, rhyming patterns present in poetry. Much prior work on poetry generation uses manually defined constraints which are satisfied during decoding using either specialized decoding procedures or rejection sampling. The rhyming constraints themselves are typically not learned by the generator. We propose an alternate approach that uses a structured discriminator to learn a poetry generator that directly captures rhyming constraints in a generative adversarial setup. By causing the discriminator to compare poems based only on a learned similarity matrix of pairs of line ending words, the proposed approach is able to successfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information</p>
<p>Keywords:</p>
<h3 id="621. Question-type Driven Question Generation.">621. Question-type Driven Question Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1622">Paper Link</a>    Pages:6031-6036</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/137/9454.html">Wenjie Zhou</a> ; <a href="https://dblp.uni-trier.de/pid/24/6799.html">Minghua Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/83/3463.html">Yunfang Wu</a></p>
<p>Abstract:
Question generation is a challenging task which aims to ask a question based on an answer and relevant context. The existing works suffer from the mismatching between question type and answer, i.e. generating a question with type how while the answer is a personal name. We propose to automatically predict the question type based on the input answer and context. Then, the question type is fused into a seq2seq model to guide the question generation, so as to deal with the mismatching problem. We achieve significant improvement on the accuracy of question type prediction and finally obtain state-of-the-art results for question generation on both SQuAD and MARCO datasets.</p>
<p>Keywords:</p>
<h3 id="622. Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization.">622. Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1623">Paper Link</a>    Pages:6037-6043</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/163/8053.html">Siyao Li</a> ; <a href="https://dblp.uni-trier.de/pid/230/3448.html">Deren Lei</a> ; <a href="https://dblp.uni-trier.de/pid/175/8687.html">Pengda Qin</a> ; <a href="https://dblp.uni-trier.de/pid/08/9282.html">William Yang Wang</a></p>
<p>Abstract:
Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward Rouge-L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of Rouge-L, we explore the practicability of utilizing the distributional semantics to measure the matching degrees. With distributional semantics, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language.</p>
<p>Keywords:</p>
<h3 id="623. Clause-Wise and Recursive Decoding for Complex and Cross-Domain Text-to-SQL Generation.">623. Clause-Wise and Recursive Decoding for Complex and Cross-Domain Text-to-SQL Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1624">Paper Link</a>    Pages:6044-6050</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/51/3745.html">Dongjun Lee</a></p>
<p>Abstract:
Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries over a single table. We focus on the Spider dataset, a complex and cross-domain text-to-SQL task, which includes complex queries over multiple tables. In this paper, we propose a SQL clause-wise decoding neural architecture with a self-attention based database schema encoder to address the Spider task. Each of the clause-specific decoders consists of a set of sub-modules, which is defined by the syntax of each clause. Additionally, our model works recursively to support nested queries. When evaluated on the Spider dataset, our approach achieves 4.6% and 9.8% accuracy gain in the test and dev sets, respectively. In addition, we show that our model is significantly more effective at predicting complex and nested queries than previous work.</p>
<p>Keywords:</p>
<h3 id="624. Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects.">624. Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1625">Paper Link</a>    Pages:6051-6057</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/215/4374.html">James Mullenbach</a> ; <a href="https://dblp.uni-trier.de/pid/43/7550.html">Jonathan Gordon</a> ; <a href="https://dblp.uni-trier.de/pid/117/4036.html">Nanyun Peng</a> ; <a href="https://dblp.uni-trier.de/pid/00/4758.html">Jonathan May</a></p>
<p>Abstract:
How do adjectives project from a noun to its parts? If a motorcycle is red, are its wheels red? Is a nuclear submarines captain nuclear? These questions are easy for humans to judge using our commonsense understanding of the world, but are difficult for computers. To attack this challenge, we crowdsource a set of human judgments that answer the English-language question Given a whole described by an adjective, does the adjective also describe a given part? We build strong baselines for this task with a classification approach. Our findings indicate that, despite the recent successes of large language models on tasks aimed to assess commonsense knowledge, these models do not greatly outperform simple word-level models based on pre-trained word embeddings. This provides evidence that the amount of commonsense knowledge encoded in these language models does not extend far beyond that already baked into the word embeddings. Our dataset will serve as a useful testbed for future research in commonsense reasoning, especially as it relates to adjectives and objects</p>
<p>Keywords:</p>
<h3 id="625. Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching.">625. Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1626">Paper Link</a>    Pages:6058-6062</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/66/1074.html">Bo Shao</a> ; <a href="https://dblp.uni-trier.de/pid/06/10400.html">Yeyun Gong</a> ; <a href="https://dblp.uni-trier.de/pid/242/3895.html">Weizhen Qi</a> ; <a href="https://dblp.uni-trier.de/pid/30/8160.html">Nan Duan</a> ; <a href="https://dblp.uni-trier.de/pid/00/6250.html">Xiaola Lin</a></p>
<p>Abstract:
In this work, we propose an aggregation method to combine the Bidirectional Encoder Representations from Transformer (BERT) with a MatchLSTM layer for Sequence Matching. Given a sentence pair, we extract the output representations of it from BERT. Then we extend BERT with a MatchLSTM layer to get further interaction of the sentence pair for sequence matching tasks. Taking natural language inference as an example, we split BERT output into two parts, which is from premise sentence and hypothesis sentence. At each position of the hypothesis sentence, both the weighted representation of the premise sentence and the representation of the current token are fed into LSTM. We jointly train the aggregation layer and pre-trained layer for sequence matching. We conduct an experiment on two publicly available datasets, WikiQA and SNLI. Experiments show that our model achieves significantly improvement compared with state-of-the-art methods on both datasets.</p>
<p>Keywords:</p>
<h3 id="626. What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition.">626. What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1627">Paper Link</a>    Pages:6063-6069</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/176/1912.html">Ting-Yun Chang</a> ; <a href="https://dblp.uni-trier.de/pid/04/9878.html">Yun-Nung Chen</a></p>
<p>Abstract:
Contextualized word embeddings have boosted many NLP tasks compared with traditional static word embeddings. However, the word with a specific sense may have different contextualized embeddings due to its various contexts. To further investigate what contextualized word embeddings capture, this paper analyzes whether they can indicate the corresponding sense definitions and proposes a general framework that is capable of explaining word meanings given contextualized word embeddings for better interpretation. The experiments show that both ELMo and BERT embeddings can be well interpreted via a readable textual form, and the findings may benefit the research community for a better understanding of what the embeddings capture.</p>
<p>Keywords:</p>
<h3 id="627. Pre-Training BERT on Domain Resources for Short Answer Grading.">627. Pre-Training BERT on Domain Resources for Short Answer Grading.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1628">Paper Link</a>    Pages:6070-6074</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/63/4049.html">Chul Sung</a> ; <a href="https://dblp.uni-trier.de/pid/120/8497.html">Tejas I. Dhamecha</a> ; <a href="https://dblp.uni-trier.de/pid/203/9296.html">Swarnadeep Saha</a> ; <a href="https://dblp.uni-trier.de/pid/94/9023.html">Tengfei Ma</a> ; <a href="https://dblp.uni-trier.de/pid/254/7994.html">Vinay Reddy</a> ; <a href="https://dblp.uni-trier.de/pid/172/3310.html">Rishi Arora</a></p>
<p>Abstract:
Pre-trained BERT contextualized representations have achieved state-of-the-art results on multiple downstream NLP tasks by fine-tuning with task-specific data. While there has been a lot of focus on task-specific fine-tuning, there has been limited work on improving the pre-trained representations. In this paper, we explore ways of improving the pre-trained contextual representations for the task of automatic short answer grading, a critical component of intelligent tutoring systems. We show that the pre-trained BERT model can be improved by augmenting data from the domain-specific resources like textbooks. We also present a new approach to use labeled short answering grading data for further enhancement of the language model. Empirical evaluation on multi-domain datasets shows that task-specific fine-tuning on the enhanced pre-trained language model achieves superior performance for short answer grading.</p>
<p>Keywords:</p>
<h3 id="628. WIQA: A dataset for "What if..." reasoning over procedural text.">628. WIQA: A dataset for "What if..." reasoning over procedural text.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1629">Paper Link</a>    Pages:6075-6084</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/29/9923.html">Niket Tandon</a> ; <a href="https://dblp.uni-trier.de/pid/78/6527.html">Bhavana Dalvi</a> ; <a href="https://dblp.uni-trier.de/pid/127/0185.html">Keisuke Sakaguchi</a> ; <a href="https://dblp.uni-trier.de/pid/34/1184.html">Peter Clark</a> ; <a href="https://dblp.uni-trier.de/pid/184/3742.html">Antoine Bosselut</a></p>
<p>Abstract:
We introduce WIQA, the first large-scale dataset of What if... questions over procedural text. WIQA contains a collection of paragraphs, each annotated with multiple influence graphs describing how one change affects another, and a large (40k) collection of What if...? multiple-choice questions derived from these. For example, given a paragraph about beach erosion, would stormy weather hasten or decelerate erosion? WIQA contains three kinds of questions: perturbations to steps mentioned in the paragraph; external (out-of-paragraph) perturbations requiring commonsense knowledge; and irrelevant (no effect) perturbations. We find that state-of-the-art models achieve 73.8% accuracy, well below the human performance of 96.3%. We analyze the challenges, in particular tracking chains of influences, and present the dataset as an open challenge to the community.</p>
<p>Keywords:</p>
<h3 id="629. Evaluating BERT for natural language inference: A case study on the CommitmentBank.">629. Evaluating BERT for natural language inference: A case study on the CommitmentBank.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1630">Paper Link</a>    Pages:6085-6090</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/245/8631.html">Nanjiang Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/36/6578.html">Marie-Catherine de Marneffe</a></p>
<p>Abstract:
Natural language inference (NLI) datasets (e.g., MultiNLI) were collected by soliciting hypotheses for a given premise from annotators. Such data collection led to annotation artifacts: systems can identify the premise-hypothesis relationship without observing the premise (e.g., negation in hypothesis being indicative of contradiction). We address this problem by recasting the CommitmentBank for NLI, which contains items involving reasoning over the extent to which a speaker is committed to complements of clause-embedding verbs under entailment-canceling environments (conditional, negation, modal and question). Instead of being constructed to stand in certain relationships with the premise, hypotheses in the recast CommitmentBank are the complements of the clause-embedding verb in each premise, leading to no annotation artifacts in the hypothesis. A state-of-the-art BERT-based model performs well on the CommitmentBank with 85% F1. However analysis of model behavior shows that the BERT models still do not capture the full complexity of pragmatic reasoning, nor encode some of the linguistic generalizations, highlighting room for improvement.</p>
<p>Keywords:</p>
<h3 id="630. Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs.">630. Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1631">Paper Link</a>    Pages:6091-6096</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7510.html">Soumya Sharma</a> ; <a href="https://dblp.uni-trier.de/pid/191/6050.html">Bishal Santra</a> ; <a href="https://dblp.uni-trier.de/pid/200/8353.html">Abhik Jana</a> ; <a href="https://dblp.uni-trier.de/pid/242/4243.html">Santosh Tokala</a> ; <a href="https://dblp.uni-trier.de/pid/52/6987.html">Niloy Ganguly</a> ; <a href="https://dblp.uni-trier.de/pid/77/2307.html">Pawan Goyal</a></p>
<p>Abstract:
Recently, biomedical version of embeddings obtained from language models such as BioELMo have shown state-of-the-art results for the textual inference task in the medical domain. In this paper, we explore how to incorporate structured domain knowledge, available in the form of a knowledge graph (UMLS), for the Medical NLI task. Specifically, we experiment with fusing embeddings obtained from knowledge graph with the state-of-the-art approaches for NLI task (ESIM model). We also experiment with fusing the domain-specific sentiment information for the task. Experiments conducted on MedNLI dataset clearly show that this strategy improves the baseline BioELMo architecture for the Medical NLI task.</p>
<p>Keywords:</p>
<h3 id="631. The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.">631. The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1632">Paper Link</a>    Pages:6097-6110</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/92/1459.html">Francisco Guzmn</a> ; <a href="https://dblp.uni-trier.de/pid/236/4531.html">Peng-Jen Chen</a> ; <a href="https://dblp.uni-trier.de/pid/92/9767.html">Myle Ott</a> ; <a href="https://dblp.uni-trier.de/pid/78/7560.html">Juan Pino</a> ; <a href="https://dblp.uni-trier.de/pid/169/3307.html">Guillaume Lample</a> ; <a href="https://dblp.uni-trier.de/pid/84/4538.html">Philipp Koehn</a> ; <a href="https://dblp.uni-trier.de/pid/236/4253.html">Vishrav Chaudhary</a> ; <a href="https://dblp.uni-trier.de/pid/28/1732.html">Marc&apos;Aurelio Ranzato</a></p>
<p>Abstract:
For machine translation, a vast majority of language pairs in the world are considered low-resource because they have little parallel data available. Besides the technical challenges of learning with limited supervision, it is difficult to evaluate methods trained on low-resource language pairs because of the lack of freely and publicly available benchmarks. In this work, we introduce the FLORES evaluation datasets for NepaliEnglish and Sinhala English, based on sentences translated from Wikipedia. Compared to English, these are languages with very different morphology and syntax, for which little out-of-domain parallel data is available and for which relatively large amounts of monolingual data are freely available. We describe our process to collect and cross-check the quality of translations, and we report baseline performance using several learning settings: fully supervised, weakly supervised, semi-supervised, and fully unsupervised. Our experiments demonstrate that current state-of-the-art methods perform rather poorly on this benchmark, posing a challenge to the research community working on low-resource MT. Data and code to reproduce our experiments are available at <a href="https://github.com/facebookresearch/flores">https://github.com/facebookresearch/flores</a>.</p>
<p>Keywords:</p>
<h3 id="632. Mask-Predict: Parallel Decoding of Conditional Masked Language Models.">632. Mask-Predict: Parallel Decoding of Conditional Masked Language Models.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1633">Paper Link</a>    Pages:6111-6120</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/50/10813.html">Marjan Ghazvininejad</a> ; <a href="https://dblp.uni-trier.de/pid/117/4866.html">Omer Levy</a> ; <a href="https://dblp.uni-trier.de/pid/238/0128.html">Yinhan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/21/6793.html">Luke Zettlemoyer</a></p>
<p>Abstract:
Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.</p>
<p>Keywords:</p>
<h3 id="633. Learning to Copy for Automatic Post-Editing.">633. Learning to Copy for Automatic Post-Editing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1634">Paper Link</a>    Pages:6121-6131</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/242/5077.html">Xuancheng Huang</a> ; <a href="https://dblp.uni-trier.de/pid/51/3710-5.html">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pid/55/5812.html">Huanbo Luan</a> ; <a href="https://dblp.uni-trier.de/pid/95/8.html">Jingfang Xu</a> ; <a href="https://dblp.uni-trier.de/pid/95/3291.html">Maosong Sun</a></p>
<p>Abstract:
Automatic post-editing (APE), which aims to correct errors in the output of machine translation systems in a post-processing step, is an important task in natural language processing. While recent work has achieved considerable performance gains by using neural networks, how to model the copying mechanism for APE remains a challenge. In this work, we propose a new method for modeling copying for APE. To better identify translation errors, our method learns the representations of source sentences and system outputs in an interactive way. These representations are used to explicitly indicate which words in the system outputs should be copied. Finally, CopyNet (Gu et.al., 2016) can be combined with our method to place the copied words in correct positions in post-edited translations. Experiments on the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms all best published results.</p>
<p>Keywords:</p>
<h3 id="634. Exploring Human Gender Stereotypes with Word Association Test.">634. Exploring Human Gender Stereotypes with Word Association Test.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1635">Paper Link</a>    Pages:6132-6142</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8107.html">Yupei Du</a> ; <a href="https://dblp.uni-trier.de/pid/17/7186.html">Yuanbin Wu</a> ; <a href="https://dblp.uni-trier.de/pid/01/800.html">Man Lan</a></p>
<p>Abstract:
Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really reflect true bias levels? For a small set of words (e.g. occupations), we can rely on human annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which contains rich types of word connections annotated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, comparing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words.</p>
<p>Keywords:</p>
<h3 id="635. A Modular Architecture for Unsupervised Sarcasm Generation.">635. A Modular Architecture for Unsupervised Sarcasm Generation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1636">Paper Link</a>    Pages:6143-6153</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/136/8669.html">Abhijit Mishra</a> ; <a href="https://dblp.uni-trier.de/pid/229/3571.html">Tarun Tater</a> ; <a href="https://dblp.uni-trier.de/pid/74/4923.html">Karthik Sankaranarayanan</a></p>
<p>Abstract:
In this paper, we propose a novel framework for sarcasm generation; the system takes a literal negative opinion as input and translates it into a sarcastic version. Our framework does not require any paired data for training. Sarcasm emanates from context-incongruity which becomes apparent as the sentence unfolds. Our framework introduces incongruity into the literal input version through modules that: (a) filter factual content from the input opinion, (b) retrieve incongruous phrases related to the filtered facts and (c) synthesize sarcastic text from the incongruous filtered and incongruous phrases. The framework employs reinforced neural sequence to sequence learning and information retrieval and is trained only using unlabeled non-sarcastic and sarcastic opinions. Since no labeled dataset exists for such a task, for evaluation, we manually prepare a benchmark dataset containing literal opinions and their sarcastic paraphrases. Qualitative and quantitative performance analyses on the data reveal our systems superiority over baselines built using known unsupervised statistical and neural machine translation and style transfer techniques.</p>
<p>Keywords:</p>
<h3 id="636. Generating Classical Chinese Poems from Vernacular Chinese.">636. Generating Classical Chinese Poems from Vernacular Chinese.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1637">Paper Link</a>    Pages:6154-6163</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/148/4419.html">Zhichao Yang</a> ; <a href="https://dblp.uni-trier.de/pid/198/5476.html">Pengshan Cai</a> ; <a href="https://dblp.uni-trier.de/pid/25/2643.html">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pid/87/3534.html">Fei Li</a> ; <a href="https://dblp.uni-trier.de/pid/154/3675.html">Weijiang Feng</a> ; <a href="https://dblp.uni-trier.de/pid/248/7732.html">Elena Suet-Ying Chiu</a> ; <a href="https://dblp.uni-trier.de/pid/55/6749-1.html">Hong Yu</a></p>
<p>Abstract:
Classical Chinese poetry is a jewel in the treasure house of Chinese culture. Previous poem generation models only allow users to employ keywords to interfere the meaning of generated poems, leaving the dominion of generation to the model. In this paper, we propose a novel task of generating classical Chinese poems from vernacular, which allows users to have more control over the semantic of generated poems. We adapt the approach of unsupervised machine translation (UMT) to our task. We use segmentation-based padding and reinforcement learning to address under-translation and over-translation respectively. According to experiments, our approach significantly improve the perplexity and BLEU compared with typical UMT models. Furthermore, we explored guidelines on how to write the input vernacular to generate better poems. Human evaluation showed our approach can generate high-quality poems which are comparable to amateur poems.</p>
<p>Keywords:</p>
<h3 id="637. Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes.">637. Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1638">Paper Link</a>    Pages:6164-6174</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/185/5544.html">Litton J. Kurisinkel</a> ; <a href="https://dblp.uni-trier.de/pid/97/9234.html">Nancy Chen</a></p>
<p>Abstract:
We present set to ordered text, a natural language generation task applied to automatically generating discharge instructions from admission ICD (International Classification of Diseases) codes. This task differs from other natural language generation tasks in the following ways: (1) The input is a set of identifiable entities (ICD codes) where the relations between individual entity are not explicitly specified. (2) The output text is not a narrative description (e.g. news articles) composed from the input. Rather, inferences are made from the input (symptoms specified in ICD codes) to generate the output (instructions). (3) There is an optimal order in which each sentence (instruction) should appear in the output. Unlike most other tasks, neither the input (ICD codes) nor their corresponding symptoms appear in the output, so the ordering of the output instructions needs to be learned in an unsupervised fashion. Based on clinical intuition, we hypothesize that each instruction in the output is mapped to a subset of ICD codes specified in the input. We propose a neural architecture that jointly models (a) subset selection: choosing relevant subsets from a set of input entities; (b) content ordering: learning the order of instructions; and (c) text generation: representing the instructions corresponding to the selected subsets in natural language. In addition, we penalize redundancy during beam search to improve tractability for long text generation. Our model outperforms baseline models in BLEU scores and human evaluation. We plan to extend this work to other tasks such as recipe generation from ingredients.</p>
<p>Keywords:</p>
<h3 id="638. Constraint-based Learning of Phonological Processes.">638. Constraint-based Learning of Phonological Processes.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1639">Paper Link</a>    Pages:6175-6185</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/228/9106.html">Shraddha Barke</a> ; <a href="https://dblp.uni-trier.de/pid/231/5134.html">Rose Kunkel</a> ; <a href="https://dblp.uni-trier.de/pid/13/7489.html">Nadia Polikarpova</a> ; <a href="https://dblp.uni-trier.de/pid/254/8040.html">Eric Meinhardt</a> ; <a href="https://dblp.uni-trier.de/pid/254/8212.html">Eric Bakovic</a> ; <a href="https://dblp.uni-trier.de/pid/136/8736.html">Leon Bergen</a></p>
<p>Abstract:
Phonological processes are context-dependent sound changes in natural languages. We present an unsupervised approach to learning human-readable descriptions of phonological processes from collections of related utterances. Our approach builds upon a technique from the programming languages community called <em>constraint-based program synthesis</em>. We contribute a novel encoding of the learning problem into Boolean Satisfiability constraints, which enables both data efficiency and fast inference. We evaluate our system on textbook phonology problems and datasets from the literature, and show that it achieves high accuracy at interactive speeds.</p>
<p>Keywords:</p>
<h3 id="639. Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint Embedding for Chinese Character Variation Representation.">639. Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint Embedding for Chinese Character Variation Representation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1640">Paper Link</a>    Pages:6186-6195</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/132/5378.html">Zhuoren Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/20/10069.html">Zhe Gao</a> ; <a href="https://dblp.uni-trier.de/pid/220/2546.html">Guoxiu He</a> ; <a href="https://dblp.uni-trier.de/pid/162/0109.html">Yangyang Kang</a> ; <a href="https://dblp.uni-trier.de/pid/212/2092.html">Changlong Sun</a> ; <a href="https://dblp.uni-trier.de/pid/06/3351.html">Qiong Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/14/1217.html">Luo Si</a> ; <a href="https://dblp.uni-trier.de/pid/11/6389.html">Xiaozhong Liu</a></p>
<p>Abstract:
The task of Chinese text spam detection is very challenging due to both glyph and phonetic variations of Chinese characters. This paper proposes a novel framework to jointly model Chinese variational, semantic, and contextualized representations for Chinese text spam detection task. In particular, a Variation Family-enhanced Graph Embedding (VFGE) algorithm is designed based on a Chinese character variation graph. The VFGE can learn both the graph embeddings of the Chinese characters (local) and the latent variation families (global). Furthermore, an enhanced bidirectional language model, with a combination gate function and an aggregation learning function, is proposed to integrate the graph and text information while capturing the sequential information. Extensive experiments have been conducted on both SMS and review datasets, to show the proposed method outperforms a series of state-of-the-art models for Chinese spam detection.</p>
<p>Keywords:</p>
<h3 id="640. An Attentive Fine-Grained Entity Typing Model with Latent Type Representation.">640. An Attentive Fine-Grained Entity Typing Model with Latent Type Representation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1641">Paper Link</a>    Pages:6196-6201</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/24/3392.html">Ying Lin</a> ; <a href="https://dblp.uni-trier.de/pid/61/2408.html">Heng Ji</a></p>
<p>Abstract:
We propose a fine-grained entity typing model with a novel attention mechanism and a hybrid type classifier. We advance existing methods in two aspects: feature extraction and type prediction. To capture richer contextual information, we adopt contextualized word representations instead of fixed word embeddings used in previous work. In addition, we propose a two-step mention-aware attention mechanism to enable the model to focus on important words in mentions and contexts. We also present a hybrid classification method beyond binary relevance to exploit type inter-dependency with latent type representation. Instead of independently predicting each type, we predict a low-dimensional vector that encodes latent type features and reconstruct the type vector from this latent representation. Experiment results on multiple data sets show that our model significantly advances the state-of-the-art on fine-grained entity typing, obtaining up to 6.1% and 5.5% absolute gains in macro averaged F-score and micro averaged F-score respectively.</p>
<p>Keywords:</p>
<h3 id="641. An Improved Neural Baseline for Temporal Relation Extraction.">641. An Improved Neural Baseline for Temporal Relation Extraction.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1642">Paper Link</a>    Pages:6202-6208</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/127/1927.html">Qiang Ning</a> ; <a href="https://dblp.uni-trier.de/pid/228/8258.html">Sanjay Subramanian</a> ; <a href="https://dblp.uni-trier.de/pid/r/DanRoth.html">Dan Roth</a></p>
<p>Abstract:
Determining temporal relations (e.g., before or after) between events has been a challenging natural language understanding task, partly due to the difficulty to generate large amounts of high-quality training data. Consequently, neural approaches have not been widely used on it, or showed only moderate improvements. This paper proposes a new neural system that achieves about 10% absolute improvement in accuracy over the previous best system (25% error reduction) on two benchmark datasets. The proposed system is trained on the state-of-the-art MATRES dataset and applies contextualized word embeddings, a Siamese encoder of a temporal common sense knowledge base, and global inference via integer linear programming (ILP). We suggest that the new approach could serve as a strong baseline for future research in this area.</p>
<p>Keywords:</p>
<h3 id="642. Improving Fine-grained Entity Typing with Entity Linking.">642. Improving Fine-grained Entity Typing with Entity Linking.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1643">Paper Link</a>    Pages:6209-6214</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/6946.html">Hongliang Dai</a> ; <a href="https://dblp.uni-trier.de/pid/249/5363.html">Donghong Du</a> ; <a href="https://dblp.uni-trier.de/pid/09/1365.html">Xin Li</a> ; <a href="https://dblp.uni-trier.de/pid/86/2159.html">Yangqiu Song</a></p>
<p>Abstract:
Fine-grained entity typing is a challenging problem since it usually involves a relatively large tag set and may require to understand the context of the entity mention. In this paper, we use entity linking to help with the fine-grained entity type classification process. We propose a deep neural model that makes predictions based on both the context and the information obtained from entity linking results. Experimental results on two commonly used datasets demonstrates the effectiveness of our approach. On both datasets, it achieves more than 5% absolute strict accuracy improvement over the state of the art.</p>
<p>Keywords:</p>
<h3 id="643. Combining Spans into Entities: A Neural Two-Stage Approach for Recognizing Discontiguous Entities.">643. Combining Spans into Entities: A Neural Two-Stage Approach for Recognizing Discontiguous Entities.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1644">Paper Link</a>    Pages:6215-6223</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/7334.html">Bailin Wang</a> ; <a href="https://dblp.uni-trier.de/pid/98/6613-11.html">Wei Lu</a></p>
<p>Abstract:
In medical documents, it is possible that an entity of interest not only contains a discontiguous sequence of words but also overlaps with another entity. Entities of such structures are intrinsically hard to recognize due to the large space of possible entity combinations. In this work, we propose a neural two-stage approach to recognizing discontiguous and overlapping entities by decomposing this problem into two subtasks: 1) it first detects all the overlapping spans that either form entities on their own or present as segments of discontiguous entities, based on the representation of segmental hypergraph, 2) next it learns to combine these segments into discontiguous entities with a classifier, which filters out other incorrect combinations of segments. Two neural components are designed for these subtasks respectively and they are learned jointly using a shared encoder for text. Our model achieves the state-of-the-art performance in a standard dataset, even in the absence of external features that previous methods used.</p>
<p>Keywords:</p>
<h3 id="644. Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal Schemas.">644. Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal Schemas.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1645">Paper Link</a>    Pages:6224-6230</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/224/2069.html">Kosuke Akimoto</a> ; <a href="https://dblp.uni-trier.de/pid/31/977.html">Takuya Hiraoka</a> ; <a href="https://dblp.uni-trier.de/pid/14/7536.html">Kunihiko Sadamasa</a> ; <a href="https://dblp.uni-trier.de/pid/n/MathiasNiepert.html">Mathias Niepert</a></p>
<p>Abstract:
Most existing relation extraction approaches exclusively target binary relations, and n-ary relation extraction is relatively unexplored. Current state-of-the-art n-ary relation extraction method is based on a supervised learning approach and, therefore, may suffer from the lack of sufficient relation labels. In this paper, we propose a novel approach to cross-sentence n-ary relation extraction based on universal schemas. To alleviate the sparsity problem and to leverage inherent decomposability of n-ary relations, we propose to learn relation representations of lower-arity facts that result from decomposing higher-arity facts. The proposed method computes a score of a new n-ary fact by aggregating scores of its decomposed lower-arity facts. We conduct experiments with datasets for ternary relation extraction and empirically show that our method improves the n-ary relation extraction performance compared to previous methods.</p>
<p>Keywords:</p>
<h3 id="645. Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition.">645. Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1646">Paper Link</a>    Pages:6231-6236</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/166/1761.html">Hongyu Lin</a> ; <a href="https://dblp.uni-trier.de/pid/15/3214-1.html">Yaojie Lu</a> ; <a href="https://dblp.uni-trier.de/pid/57/2368.html">Xianpei Han</a> ; <a href="https://dblp.uni-trier.de/pid/78/5897-1.html">Le Sun</a> ; <a href="https://dblp.uni-trier.de/pid/11/6024.html">Bin Dong</a> ; <a href="https://dblp.uni-trier.de/pid/65/4185.html">Shanshan Jiang</a></p>
<p>Abstract:
Current region-based NER models only rely on fully-annotated training data to learn effective region encoder, which often face the training data bottleneck. To alleviate this problem, this paper proposes Gazetteer-Enhanced Attentive Neural Networks, which can enhance region-based NER by learning name knowledge of entity mentions from easily-obtainable gazetteers, rather than only from fully-annotated data. Specially, we first propose an attentive neural network (ANN), which explicitly models the mention-context association and therefore is convenient for integrating externally-learned knowledge. Then we design an auxiliary gazetteer network, which can effectively encode name regularity of mentions only using gazetteers. Finally, the learned gazetteer network is incorporated into ANN for better NER. Experiments show that our ANN can achieve the state-of-the-art performance on ACE2005 named entity recognition benchmark. Besides, incorporating gazetteer network can further improve the performance and significantly reduce the requirement of training data.</p>
<p>Keywords:</p>
<h3 id="646. "A Buster Keaton of Linguistics": First Automated Approaches for the Extraction of Vossian Antonomasia.">646. "A Buster Keaton of Linguistics": First Automated Approaches for the Extraction of Vossian Antonomasia.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1647">Paper Link</a>    Pages:6237-6242</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8072.html">Michel Schwab</a> ; <a href="https://dblp.uni-trier.de/pid/26/274.html">Robert Jschke</a> ; <a href="https://dblp.uni-trier.de/pid/13/3984-5.html">Frank Fischer</a> ; <a href="https://dblp.uni-trier.de/pid/28/8510.html">Jannik Strtgen</a></p>
<p>Abstract:
Attributing a particular property to a person by naming another person, who is typically wellknown for the respective property, is called a Vossian Antonomasia (VA). This subtpye of metonymy, which overlaps with metaphor, has a specific syntax and is especially frequent in journalistic texts. While identifying Vossian Antonomasia is of particular interest in the study of stylistics, it is also a source of errors in relation and fact extraction as an explicitly mentioned entity occurs only metaphorically and should not be associated with respective contexts. Despite rather simple syntactic variations, the automatic extraction of VA was never addressed as yet since it requires a deeper semantic understanding of mentioned entities and underlying relations. In this paper, we propose a first method for the extraction of VAs that works completely automatically. Our approaches use named entity recognition, distant supervision based on Wikidata, and a bi-directional LSTM for postprocessing. The evaluation on 1.8 million articles of the New York Times corpus shows that our approach significantly outperforms the only existing semi-automatic approach for VA identification by more than 30 percentage points in precision.</p>
<p>Keywords:</p>
<h3 id="647. Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound Paraphrasing.">647. Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound Paraphrasing.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1648">Paper Link</a>    Pages:6243-6248</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/0124.html">Taiki Watanabe</a> ; <a href="https://dblp.uni-trier.de/pid/99/3914.html">Akihiro Tamura</a> ; <a href="https://dblp.uni-trier.de/pid/67/2953.html">Takashi Ninomiya</a> ; <a href="https://dblp.uni-trier.de/pid/121/4340.html">Takuya Makino</a> ; <a href="https://dblp.uni-trier.de/pid/01/35.html">Tomoya Iwakura</a></p>
<p>Abstract:
We propose a method to improve named entity recognition (NER) for chemical compounds using multi-task learning by jointly training a chemical NER model and a chemical com- pound paraphrase model. Our method en- ables the long short-term memory (LSTM) of the NER model to capture chemical com- pound paraphrases by sharing the parameters of the LSTM and character embeddings be- tween the two models. The experimental re- sults on the BioCreative IVs CHEMDNER task show that our method improves chemi- cal NER and achieves state-of-the-art perfor- mance.</p>
<p>Keywords:</p>
<h3 id="648. FewRel 2.0: Towards More Challenging Few-Shot Relation Classification.">648. FewRel 2.0: Towards More Challenging Few-Shot Relation Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1649">Paper Link</a>    Pages:6249-6254</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/207/8893.html">Tianyu Gao</a> ; <a href="https://dblp.uni-trier.de/pid/19/3011-7.html">Xu Han</a> ; <a href="https://dblp.uni-trier.de/pid/10/3520-6.html">Hao Zhu</a> ; <a href="https://dblp.uni-trier.de/pid/53/3245-1.html">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pid/83/6353-30.html">Peng Li</a> ; <a href="https://dblp.uni-trier.de/pid/95/3291.html">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pid/00/5012-16.html">Jie Zhou</a></p>
<p>Abstract:
We present FewRel 2.0, a more challenging task to investigate two aspects of few-shot relation classification models: (1) Can they adapt to a new domain with only a handful of instances? (2) Can they detect none-of-the-above (NOTA) relations? To construct FewRel 2.0, we build upon the FewRel dataset by adding a new test set in a quite different domain, and a NOTA relation choice. With the new dataset and extensive experimental analysis, we found (1) that the state-of-the-art few-shot relation classification models struggle on these two aspects, and (2) that the commonly-used techniques for domain adaptation and NOTA detection still cannot handle the two challenges well. Our research calls for more attention and further efforts to these two real-world issues. All details and resources about the dataset and baselines are released at <a href="https://github.com/thunlp/fewrel">https://github.com/thunlp/fewrel</a>.</p>
<p>Keywords:</p>
<h3 id="649. ner and pos when nothing is capitalized.">649. ner and pos when nothing is capitalized.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1650">Paper Link</a>    Pages:6255-6260</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/222/9585-1.html">Stephen Mayhew</a> ; <a href="https://dblp.uni-trier.de/pid/238/1604.html">Tatiana Tsygankova</a> ; <a href="https://dblp.uni-trier.de/pid/r/DanRoth.html">Dan Roth</a></p>
<p>Abstract:
For those languages which use it, capitalization is an important signal for the fundamental NLP tasks of Named Entity Recognition (NER) and Part of Speech (POS) tagging. In fact, it is such a strong signal that model performance on these tasks drops sharply in common lowercased scenarios, such as noisy web text or machine translation outputs. In this work, we perform a systematic analysis of solutions to this problem, modifying only the casing of the train or test data using lowercasing and truecasing methods. While prior work and first impressions might suggest training a caseless model, or using a truecaser at test time, we show that the most effective strategy is a concatenation of cased and lowercased training data, producing a single model with high performance on both cased and uncased text. As shown in our experiments, this result holds across tasks and input representations. Finally, we show that our proposed solution gives an 8% F1 improvement in mention detection on noisy out-of-domain Twitter data.</p>
<p>Keywords:</p>
<h3 id="650. CaRB: A Crowdsourced Benchmark for Open IE.">650. CaRB: A Crowdsourced Benchmark for Open IE.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1651">Paper Link</a>    Pages:6261-6266</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/254/8206.html">Sangnie Bhardwaj</a> ; <a href="https://dblp.uni-trier.de/pid/254/7905.html">Samarth Aggarwal</a> ; <a href="https://dblp.uni-trier.de/pid/30/6391.html">Mausam</a></p>
<p>Abstract:
Open Information Extraction (Open IE) systems have been traditionally evaluated via manual annotation. Recently, an automated evaluator with a benchmark dataset (OIE2016) was released  it scores Open IE systems automatically by matching system predictions with predictions in the benchmark dataset. Unfortunately, our analysis reveals that its data is rather noisy, and the tuple matching in the evaluator has issues, making the results of automated comparisons less trustworthy. We contribute CaRB, an improved dataset and framework for testing Open IE systems. To the best of our knowledge, CaRB is the first crowdsourced Open IE dataset and it also makes substantive changes in the matching code and metrics. NLP experts annotate CaRBs dataset to be more accurate than OIE2016. Moreover, we find that on one pair of Open IE systems, CaRB framework provides contradictory results to OIE2016. Human assessment verifies that CaRBs ranking of the two systems is the accurate ranking. We release the CaRB framework along with its crowdsourced dataset.</p>
<p>Keywords:</p>
<h3 id="651. Weakly Supervised Attention Networks for Entity Recognition.">651. Weakly Supervised Attention Networks for Entity Recognition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1652">Paper Link</a>    Pages:6267-6272</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/200/7924.html">Barun Patra</a> ; <a href="https://dblp.uni-trier.de/pid/203/8724.html">Joel Ruben Antony Moniz</a></p>
<p>Abstract:
The task of entity recognition has traditionally been modelled as a sequence labelling task. However, this usually requires a large amount of fine-grained data annotated at the token level, which in turn can be expensive and cumbersome to obtain. In this work, we aim to circumvent this requirement of word-level annotated data. To achieve this, we propose a novel architecture for entity recognition from a corpus containing weak binary presence/absence labels, which are relatively easier to obtain. We show that our proposed weakly supervised model, trained solely on a multi-label classification task, performs reasonably well on the task of entity recognition, despite not having access to any token-level ground truth data.</p>
<p>Keywords:</p>
<h3 id="652. Revealing and Predicting Online Persuasion Strategy with Elementary Units.">652. Revealing and Predicting Online Persuasion Strategy with Elementary Units.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1653">Paper Link</a>    Pages:6273-6278</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/218/0681.html">Gaku Morio</a> ; <a href="https://dblp.uni-trier.de/pid/245/8602.html">Ryo Egawa</a> ; <a href="https://dblp.uni-trier.de/pid/75/6391.html">Katsuhide Fujita</a></p>
<p>Abstract:
In online arguments, identifying how users construct their arguments to persuade others is important in order to understand a persuasive strategy directly. However, existing research lacks empirical investigations on highly semantic aspects of elementary units (EUs), such as propositions for a persuasive online argument. Therefore, this paper focuses on a pilot study, revealing a persuasion strategy using EUs. Our contributions are as follows: (1) annotating five types of EUs in a persuasive forum, the so-called ChangeMyView, (2) revealing both intuitive and non-intuitive strategic insights for the persuasion by analyzing 4612 annotated EUs, and (3) proposing baseline neural models that identify the EU boundary and type. Our observations imply that EUs definitively characterize online persuasion strategies.</p>
<p>Keywords:</p>
<h3 id="653. A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis.">653. A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1654">Paper Link</a>    Pages:6279-6284</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/238/8392.html">Qingnan Jiang</a> ; <a href="https://dblp.uni-trier.de/pid/09/3666-29.html">Lei Chen</a> ; <a href="https://dblp.uni-trier.de/pid/93/5407.html">Ruifeng Xu</a> ; <a href="https://dblp.uni-trier.de/pid/71/1982.html">Xiang Ao</a> ; <a href="https://dblp.uni-trier.de/pid/02/1640-7.html">Min Yang</a></p>
<p>Abstract:
Aspect-based sentiment analysis (ABSA) has attracted increasing attention recently due to its broad applications. In existing ABSA datasets, most sentences contain only one aspect or multiple aspects with the same sentiment polarity, which makes ABSA task degenerate to sentence-level sentiment analysis. In this paper, we present a new large-scale Multi-Aspect Multi-Sentiment (MAMS) dataset, in which each sentence contains at least two different aspects with different sentiment polarities. The release of this dataset would push forward the research in this field. In addition, we propose simple yet effective CapsNet and CapsNet-BERT models which combine the strengths of recent NLP advances. Experiments on our new dataset show that the proposed model significantly outperforms the state-of-the-art baseline methods</p>
<p>Keywords:</p>
<h3 id="654. Learning with Noisy Labels for Sentence-level Sentiment Classification.">654. Learning with Noisy Labels for Sentence-level Sentiment Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1655">Paper Link</a>    Pages:6285-6291</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/181/2812-68.html">Hao Wang</a> ; <a href="https://dblp.uni-trier.de/pid/l/BingLiu1.html">Bing Liu</a> ; <a href="https://dblp.uni-trier.de/pid/169/7349.html">Chaozhuo Li</a> ; <a href="https://dblp.uni-trier.de/pid/37/1091-1.html">Yan Yang</a> ; <a href="https://dblp.uni-trier.de/pid/47/3003.html">Tianrui Li</a></p>
<p>Abstract:
Deep neural networks (DNNs) can fit (or even over-fit) the training data very well. If a DNN model is trained using data with noisy labels and tested on data with clean labels, the model may perform poorly. This paper studies the problem of learning with noisy labels for sentence-level sentiment classification. We propose a novel DNN model called NetAb (as shorthand for convolutional neural Networks with Ab-networks) to handle noisy labels during training. NetAb consists of two convolutional neural networks, one with a noise transition layer for dealing with the input noisy labels and the other for predicting clean labels. We train the two networks using their respective loss functions in a mutual reinforcement manner. Experimental results demonstrate the effectiveness of the proposed model.</p>
<p>Keywords:</p>
<h3 id="655. DENS: A Dataset for Multi-class Emotion Analysis.">655. DENS: A Dataset for Multi-class Emotion Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1656">Paper Link</a>    Pages:6292-6297</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/10/2639.html">Chen Liu</a> ; <a href="https://dblp.uni-trier.de/pid/161/8106.html">Muhammad Osama</a> ; <a href="https://dblp.uni-trier.de/pid/251/8818.html">Anderson de Andrade</a></p>
<p>Abstract:
We introduce a new dataset for multi-class emotion analysis from long-form narratives in English. The Dataset for Emotions of Narrative Sequences (DENS) was collected from both classic literature available on Project Gutenberg and modern online narratives avail- able on Wattpad, annotated using Amazon Mechanical Turk. A number of statistics and baseline benchmarks are provided for the dataset. Of the tested techniques, we find that the fine-tuning of a pre-trained BERT model achieves the best results, with an average micro-F1 score of 60.4%. Our results show that the dataset provides a novel opportunity in emotion analysis that requires moving beyond existing sentence-level techniques.</p>
<p>Keywords:</p>
<h3 id="656. Multi-Task Stance Detection with Sentiment and Stance Lexicons.">656. Multi-Task Stance Detection with Sentiment and Stance Lexicons.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1657">Paper Link</a>    Pages:6298-6304</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/32/1013.html">Yingjie Li</a> ; <a href="https://dblp.uni-trier.de/pid/69/6680.html">Cornelia Caragea</a></p>
<p>Abstract:
Stance detection aims to detect whether the opinion holder is in support of or against a given target. Recent works show improvements in stance detection by using either the attention mechanism or sentiment information. In this paper, we propose a multi-task framework that incorporates target-specific attention mechanism and at the same time takes sentiment classification as an auxiliary task. Moreover, we used a sentiment lexicon and constructed a stance lexicon to provide guidance for the attention layer. Experimental results show that the proposed model significantly outperforms state-of-the-art deep learning methods on the SemEval-2016 dataset.</p>
<p>Keywords:</p>
<h3 id="657. A Robust Self-Learning Framework for Cross-Lingual Text Classification.">657. A Robust Self-Learning Framework for Cross-Lingual Text Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1658">Paper Link</a>    Pages:6305-6309</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/25/7257-10.html">Xin Dong</a> ; <a href="https://dblp.uni-trier.de/pid/86/1747.html">Gerard de Melo</a></p>
<p>Abstract:
Based on massive amounts of data, recent pretrained contextual representation models have made significant strides in advancing a number of different English NLP tasks. However, for other languages, relevant training data may be lacking, while state-of-the-art deep learning methods are known to be data-hungry. In this paper, we present an elegantly simple robust self-learning framework to include unlabeled non-English samples in the fine-tuning process of pretrained multilingual representation models. We leverage a multilingual models own predictions on unlabeled non-English data in order to obtain additional information that can be used during further fine-tuning. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages.</p>
<p>Keywords:</p>
<h3 id="658. Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora.">658. Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1659">Paper Link</a>    Pages:6310-6315</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/16/6986.html">Canasai Kruengkrai</a></p>
<p>Abstract:
Flipping sentiment while preserving sentence meaning is challenging because parallel sentences with the same content but different sentiment polarities are not always available for model learning. We introduce a method for acquiring imperfectly aligned sentences from non-parallel corpora and propose a model that learns to minimize the sentiment and content losses in a fully end-to-end manner. Our model is simple and offers well-balanced results across two domains: Yelp restaurant and Amazon product reviews.</p>
<p>Keywords:</p>
<h3 id="659. Label Embedding using Hierarchical Structure of Labels for Twitter Classification.">659. Label Embedding using Hierarchical Structure of Labels for Twitter Classification.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1660">Paper Link</a>    Pages:6316-6321</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/19/2160.html">Taro Miyazaki</a> ; <a href="https://dblp.uni-trier.de/pid/215/6477.html">Kiminobu Makino</a> ; <a href="https://dblp.uni-trier.de/pid/215/6469.html">Yuka Takei</a> ; <a href="https://dblp.uni-trier.de/pid/136/7306.html">Hiroki Okamoto</a> ; <a href="https://dblp.uni-trier.de/pid/42/3385.html">Jun Goto</a></p>
<p>Abstract:
Twitter is used for various applications such as disaster monitoring and news material gathering. In these applications, each Tweet is classified into pre-defined classes. These classes have a semantic relationship with each other and can be classified into a hierarchical structure, which is regarded as important information. Label texts of pre-defined classes themselves also include important clues for classification. Therefore, we propose a method that can consider the hierarchical structure of labels and label texts themselves. We conducted evaluation over the Text REtrieval Conference (TREC) 2018 Incident Streams (IS) track dataset, and we found that our method outperformed the methods of the conference participants.</p>
<p>Keywords:</p>
<h3 id="660. Interpretable Word Embeddings via Informative Priors.">660. Interpretable Word Embeddings via Informative Priors.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1661">Paper Link</a>    Pages:6322-6328</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/7726.html">Miriam Hurtado Bodell</a> ; <a href="https://dblp.uni-trier.de/pid/86/4380.html">Martin Arvidsson</a> ; <a href="https://dblp.uni-trier.de/pid/119/9862.html">Mns Magnusson</a></p>
<p>Abstract:
Word embeddings have demonstrated strong performance on NLP tasks. However, lack of interpretability and the unsupervised nature of word embeddings have limited their use within computational social science and digital humanities. We propose the use of informative priors to create interpretable and domain-informed dimensions for probabilistic word embeddings. Experimental results show that sensible priors can capture latent semantic concepts better than or on-par with the current state of the art, while retaining the simplicity and generalizability of using priors.</p>
<p>Keywords:</p>
<h3 id="661. Adversarial Removal of Demographic Attributes Revisited.">661. Adversarial Removal of Demographic Attributes Revisited.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1662">Paper Link</a>    Pages:6329-6334</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/161/0059.html">Maria Barrett</a> ; <a href="https://dblp.uni-trier.de/pid/225/7708.html">Yova Kementchedjhieva</a> ; <a href="https://dblp.uni-trier.de/pid/223/4533.html">Yanai Elazar</a> ; <a href="https://dblp.uni-trier.de/pid/46/7536.html">Desmond Elliott</a> ; <a href="https://dblp.uni-trier.de/pid/30/2756.html">Anders Sgaard</a></p>
<p>Abstract:
Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a held-out subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural network also does not generalize to new samples. In other words, the biases detected in Elazar and Goldberg (2018) seem restricted to their particular data sample, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models.</p>
<p>Keywords:</p>
<h3 id="662. A deep-learning framework to detect sarcasm targets.">662. A deep-learning framework to detect sarcasm targets.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1663">Paper Link</a>    Pages:6335-6341</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/198/0968.html">Jasabanta Patro</a> ; <a href="https://dblp.uni-trier.de/pid/233/8180.html">Srijan Bansal</a> ; <a href="https://dblp.uni-trier.de/pid/m/AnimeshMukherjee.html">Animesh Mukherjee</a></p>
<p>Abstract:
In this paper we propose a deep learning framework for sarcasm target detection in predefined sarcastic texts. Identification of sarcasm targets can help in many core natural language processing tasks such as aspect based sentiment analysis, opinion mining etc. To begin with, we perform an empirical study of the socio-linguistic features and identify those that are statistically significant in indicating sarcasm targets (p-values in the range(0.05,0.001)). Finally, we present a deep-learning framework augmented with socio-linguistic features to detect sarcasm targets in sarcastic book-snippets and tweets.We achieve a huge improvement in the performance in terms of exact match and dice scores compared to the current state-of-the-art baseline.</p>
<p>Keywords:</p>
<h3 id="663. In Plain Sight: Media Bias Through the Lens of Factual Reporting.">663. In Plain Sight: Media Bias Through the Lens of Factual Reporting.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1664">Paper Link</a>    Pages:6342-6348</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/96/3057.html">Lisa Fan</a> ; <a href="https://dblp.uni-trier.de/pid/46/10196.html">Marshall White</a> ; <a href="https://dblp.uni-trier.de/pid/217/9648.html">Eva Sharma</a> ; <a href="https://dblp.uni-trier.de/pid/248/7531.html">Ruisi Su</a> ; <a href="https://dblp.uni-trier.de/pid/203/8260.html">Prafulla Kumar Choubey</a> ; <a href="https://dblp.uni-trier.de/pid/42/4811.html">Ruihong Huang</a> ; <a href="https://dblp.uni-trier.de/pid/49/3800-8.html">Lu Wang</a></p>
<p>Abstract:
The increasing prevalence of political bias in news media calls for greater public awareness of it, as well as robust methods for its detection. While prior work in NLP has primarily focused on the lexical bias captured by linguistic attributes such as word choice and syntax, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of informational bias: factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 news articles annotated with 1,727 bias spans and find evidence that informational bias appears in news articles more frequently than lexical bias. We further study our annotations to observe how informational bias surfaces in news articles by different media outlets. Lastly, a baseline model for informational bias prediction is presented by fine-tuning BERT on our labeled data, indicating the challenges of the task and future directions.</p>
<p>Keywords:</p>
<h3 id="664. Incorporating Label Dependencies in Multilabel Stance Detection.">664. Incorporating Label Dependencies in Multilabel Stance Detection.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1665">Paper Link</a>    Pages:6349-6353</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/77/4198.html">William Ferreira</a> ; <a href="https://dblp.uni-trier.de/pid/18/1071.html">Andreas Vlachos</a></p>
<p>Abstract:
Stance detection in social media is a well-studied task in a variety of domains. Nevertheless, previous work has mostly focused on multiclass versions of the problem, where the labels are mutually exclusive, and typically positive, negative or neutral. In this paper, we address versions of the task in which an utterance can have multiple labels, thus corresponding to multilabel classification. We propose a method that explicitly incorporates label dependencies in the training objective and compare it against a variety of baselines, as well as a reduction of multilabel to multiclass learning. In experiments with three datasets, we find that our proposed method improves upon all baselines on two out of three datasets. We also show that the reduction of multilabel to multiclass classification can be very competitive, especially in cases where the output consists of a small number of labels and one can enumerate over all label combinations.</p>
<p>Keywords:</p>
<h3 id="665. Investigating Sports Commentator Bias within a Large Corpus of American Football Broadcasts.">665. Investigating Sports Commentator Bias within a Large Corpus of American Football Broadcasts.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1666">Paper Link</a>    Pages:6354-6360</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/248/8361.html">Jack Merullo</a> ; <a href="https://dblp.uni-trier.de/pid/248/8067.html">Luke Yeh</a> ; <a href="https://dblp.uni-trier.de/pid/182/2581.html">Abram Handler</a> ; <a href="https://dblp.uni-trier.de/pid/126/8729.html">Alvin Grissom II</a> ; <a href="https://dblp.uni-trier.de/pid/10/1481.html">Brendan O&apos;Connor</a> ; <a href="https://dblp.uni-trier.de/pid/148/9178.html">Mohit Iyyer</a></p>
<p>Abstract:
Sports broadcasters inject drama into play-by-play commentary by building team and player narratives through subjective analyses and anecdotes. Prior studies based on small datasets and manual coding show that such theatrics evince commentator bias in sports broadcasts. To examine this phenomenon, we assemble FOOTBALL, which contains 1,455 broadcast transcripts from American football games across six decades that are automatically annotated with 250K player mentions and linked with racial metadata. We identify major confounding factors for researchers examining racial bias in FOOTBALL, and perform a computational analysis that supports conclusions from prior social science studies.</p>
<p>Keywords:</p>
<h3 id="666. Charge-Based Prison Term Prediction with Deep Gating Network.">666. Charge-Based Prison Term Prediction with Deep Gating Network.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1667">Paper Link</a>    Pages:6361-6366</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/61/8969.html">Huajie Chen</a> ; <a href="https://dblp.uni-trier.de/pid/c/DCai.html">Deng Cai</a> ; <a href="https://dblp.uni-trier.de/pid/76/2897.html">Wei Dai</a> ; <a href="https://dblp.uni-trier.de/pid/248/2693.html">Zehui Dai</a> ; <a href="https://dblp.uni-trier.de/pid/206/2445.html">Yadong Ding</a></p>
<p>Abstract:
Judgment prediction for legal cases has attracted much research efforts for its practice use, of which the ultimate goal is prison term prediction. While existing work merely predicts the total prison term, in reality a defendant is often charged with multiple crimes. In this paper, we argue that charge-based prison term prediction (CPTP) not only better fits realistic needs, but also makes the total prison term prediction more accurate and interpretable. We collect the first large-scale structured data for CPTP and evaluate several competitive baselines. Based on the observation that fine-grained feature selection is the key to achieving good performance, we propose the Deep Gating Network (DGN) for charge-specific feature selection and aggregation. Experiments show that DGN achieves the state-of-the-art performance.</p>
<p>Keywords:</p>
<h3 id="667. Restoring ancient text using deep learning: a case study on Greek epigraphy.">667. Restoring ancient text using deep learning: a case study on Greek epigraphy.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1668">Paper Link</a>    Pages:6367-6374</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/176/5056.html">Yannis M. Assael</a> ; <a href="https://dblp.uni-trier.de/pid/250/9741.html">Thea Sommerschield</a> ; <a href="https://dblp.uni-trier.de/pid/222/8808.html">Jonathan Prag</a></p>
<p>Abstract:
Ancient History relies on disciplines such as Epigraphy, the study of ancient inscribed texts, for evidence of the recorded past. However, these texts, inscriptions, are often damaged over the centuries, and illegible parts of the text must be restored by specialists, known as epigraphists. This work presents Pythia, the first ancient text restoration model that recovers missing characters from a damaged text input using deep neural networks. Its architecture is carefully designed to handle long-term context information, and deal efficiently with missing or corrupted character and word representations. To train it, we wrote a non-trivial pipeline to convert PHI, the largest digital corpus of ancient Greek inscriptions, to machine actionable text, which we call PHI-ML. On PHI-ML, Pythias predictions achieve a 30.1% character error rate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases the ground-truth sequence was among the Top-20 hypotheses of Pythia, which effectively demonstrates the impact of this assistive method on the field of digital epigraphy, and sets the state-of-the-art in ancient text restoration.</p>
<p>Keywords:</p>
<h3 id="668. Embedding Lexical Features via Tensor Decomposition for Small Sample Humor Recognition.">668. Embedding Lexical Features via Tensor Decomposition for Small Sample Humor Recognition.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1669">Paper Link</a>    Pages:6375-6380</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/145/6090.html">Zhenjie Zhao</a> ; <a href="https://dblp.uni-trier.de/pid/154/8163.html">Andrew Cattle</a> ; <a href="https://dblp.uni-trier.de/pid/48/9024.html">Evangelos E. Papalexakis</a> ; <a href="https://dblp.uni-trier.de/pid/06/4530.html">Xiaojuan Ma</a></p>
<p>Abstract:
We propose a novel tensor embedding method that can effectively extract lexical features for humor recognition. Specifically, we use word-word co-occurrence to encode the contextual content of documents, and then decompose the tensor to get corresponding vector representations. We show that this simple method can capture features of lexical humor effectively for continuous humor recognition. In particular, we achieve a distance of 0.887 on a global humor ranking task, comparable to the top performing systems from SemEval 2017 Task 6B (Potash et al., 2017) but without the need for any external training corpus. In addition, we further show that this approach is also beneficial for small sample humor recognition tasks through a semi-supervised label propagation procedure, which achieves about 0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day (Yang et al., 2015) humour classification datasets using only 10% of known labels.</p>
<p>Keywords:</p>
<h3 id="669. EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.">669. EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1670">Paper Link</a>    Pages:6381-6387</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/234/9068.html">Jason W. Wei</a> ; <a href="https://dblp.uni-trier.de/pid/135/5092.html">Kai Zou</a></p>
<p>Abstract:
We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.</p>
<p>Keywords:</p>
<h3 id="670. Neural News Recommendation with Multi-Head Self-Attention.">670. Neural News Recommendation with Multi-Head Self-Attention.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1671">Paper Link</a>    Pages:6388-6393</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/212/1864.html">Chuhan Wu</a> ; <a href="https://dblp.uni-trier.de/pid/136/7955.html">Fangzhao Wu</a> ; <a href="https://dblp.uni-trier.de/pid/242/4765.html">Suyu Ge</a> ; <a href="https://dblp.uni-trier.de/pid/130/7814.html">Tao Qi</a> ; <a href="https://dblp.uni-trier.de/pid/76/6824.html">Yongfeng Huang</a> ; <a href="https://dblp.uni-trier.de/pid/08/6809-1.html">Xing Xie</a></p>
<p>Abstract:
News recommendation can help users find interested news and alleviate information overload. Precisely modeling news and users is critical for news recommendation, and capturing the contexts of words and news is important to learn news and user representations. In this paper, we propose a neural news recommendation approach with multi-head self-attention (NRMS). The core of our approach is a news encoder and a user encoder. In the news encoder, we use multi-head self-attentions to learn news representations from news titles by modeling the interactions between words. In the user encoder, we learn representations of users from their browsed news and use multi-head self-attention to capture the relatedness between the news. Besides, we apply additive attention to learn more informative news and user representations by selecting important words and news. Experiments on a real-world dataset validate the effectiveness and efficiency of our approach.</p>
<p>Keywords:</p>
<h3 id="671. What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis.">671. What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1672">Paper Link</a>    Pages:6394-6400</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/32/5207.html">Xiaolei Huang</a> ; <a href="https://dblp.uni-trier.de/pid/00/4758.html">Jonathan May</a> ; <a href="https://dblp.uni-trier.de/pid/117/4036.html">Nanyun Peng</a></p>
<p>Abstract:
Building named entity recognition (NER) models for languages that do not have much training data is a challenging task. While recent work has shown promising results on cross-lingual transfer from high-resource languages, it is unclear what knowledge is transferred. In this paper, we first propose a simple and efficient neural architecture for cross-lingual NER. Experiments show that our model achieves competitive performance with the state-of-the-art. We further explore how transfer learning works for cross-lingual NER on two transferable factors: sequential order and multilingual embedding. Our results shed light on future research for improving cross-lingual NER.</p>
<p>Keywords:</p>
<h3 id="672. Telling the Whole Story: A Manually Annotated Chinese Dataset for the Analysis of Humor in Jokes.">672. Telling the Whole Story: A Manually Annotated Chinese Dataset for the Analysis of Humor in Jokes.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1673">Paper Link</a>    Pages:6401-6406</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/65.html">Dongyu Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/250/7078.html">Heting Zhang</a> ; <a href="https://dblp.uni-trier.de/pid/216/1410.html">Xikai Liu</a> ; <a href="https://dblp.uni-trier.de/pid/07/1644.html">Hongfei Lin</a> ; <a href="https://dblp.uni-trier.de/pid/62/3147.html">Feng Xia</a></p>
<p>Abstract:
Humor plays important role in human communication, which makes it important problem for natural language processing. Prior work on the analysis of humor focuses on whether text is humorous or not, or the degree of funniness, but this is insufficient to explain why it is funny. We therefore create a dataset on humor with 9,123 manually annotated jokes in Chinese. We propose a novel annotation scheme to give scenarios of how humor arises in text. Specifically, our annotations of linguistic humor not only contain the degree of funniness, like previous work, but they also contain key words that trigger humor as well as character relationship, scene, and humor categories. We report reasonable agreement between annota-tors. We also conduct an analysis and exploration of the dataset. To the best of our knowledge, we are the first to approach humor annotation for exploring the underlying mechanism of the use of humor, which may contribute to a significantly deeper analysis of humor. We also contribute with a scarce and valuable dataset, which we will release publicly.</p>
<p>Keywords:</p>
<h3 id="673. Generating Natural Anagrams: Towards Language Generation Under Hard Combinatorial Constraints.">673. Generating Natural Anagrams: Towards Language Generation Under Hard Combinatorial Constraints.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1674">Paper Link</a>    Pages:6407-6411</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/90/1078.html">Masaaki Nishino</a> ; <a href="https://dblp.uni-trier.de/pid/136/8649.html">Sho Takase</a> ; <a href="https://dblp.uni-trier.de/pid/68/6820.html">Tsutomu Hirao</a> ; <a href="https://dblp.uni-trier.de/pid/16/3520.html">Masaaki Nagata</a></p>
<p>Abstract:
An anagram is a sentence or a phrase that is made by permutating the characters of an input sentence or a phrase. For example, Trims cash is an anagram of Christmas. Existing automatic anagram generation methods can find possible combinations of words form an anagram. However, they do not pay much attention to the naturalness of the generated anagrams. In this paper, we show that simple depth-first search can yield natural anagrams when it is combined with modern neural language models. Human evaluation results show that the proposed method can generate significantly more natural anagrams than baseline methods.</p>
<p>Keywords:</p>
<h3 id="674. STANCY: Stance Classification Based on Consistency Cues.">674. STANCY: Stance Classification Based on Consistency Cues.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1675">Paper Link</a>    Pages:6412-6417</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/136/8735.html">Kashyap Popat</a> ; <a href="https://dblp.uni-trier.de/pid/37/11030.html">Subhabrata Mukherjee</a> ; <a href="https://dblp.uni-trier.de/pid/49/7109.html">Andrew Yates</a> ; <a href="https://dblp.uni-trier.de/pid/w/GerhardWeikum.html">Gerhard Weikum</a></p>
<p>Abstract:
Controversial claims are abundant in online media and discussion forums. A better understanding of such claims requires analyzing them from different perspectives. Stance classification is a necessary step for inferring these perspectives in terms of supporting or opposing the claim. In this work, we present a neural network model for stance classification leveraging BERT representations and augmenting them with a novel consistency constraint. Experiments on the Perspectrum dataset, consisting of claims and users perspectives from various debate websites, demonstrate the effectiveness of our approach over state-of-the-art baselines.</p>
<p>Keywords:</p>
<h3 id="675. Cross-lingual intent classification in a low resource industrial setting.">675. Cross-lingual intent classification in a low resource industrial setting.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1676">Paper Link</a>    Pages:6418-6423</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/186/7363.html">Talaat Khalil</a> ; <a href="https://dblp.uni-trier.de/pid/254/8277.html">Kornel Kielczewski</a> ; <a href="https://dblp.uni-trier.de/pid/254/8024.html">Georgios Christos Chouliaras</a> ; <a href="https://dblp.uni-trier.de/pid/250/2878.html">Amina Keldibek</a> ; <a href="https://dblp.uni-trier.de/pid/86/9229.html">Maarten Versteegh</a></p>
<p>Abstract:
This paper explores different approaches to multilingual intent classification in a low resource setting. Recent advances in multilingual text representations promise cross-lingual transfer for classifiers. We investigate the potential for this transfer in an applied industrial setting and compare to multilingual classification using machine translated text. Our results show that while the recently developed methods show promise, practical application calls for a combination of techniques for useful results.</p>
<p>Keywords:</p>
<h3 id="676. SoftRegex: Generating Regex from Natural Language Descriptions using Softened Regex Equivalence.">676. SoftRegex: Generating Regex from Natural Language Descriptions using Softened Regex Equivalence.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1677">Paper Link</a>    Pages:6424-6430</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/225/4093.html">Jun-U. Park</a> ; <a href="https://dblp.uni-trier.de/pid/71/9491.html">Sang-Ki Ko</a> ; <a href="https://dblp.uni-trier.de/pid/223/4918.html">Marco Cognetta</a> ; <a href="https://dblp.uni-trier.de/pid/h/YoSubHan.html">Yo-Sub Han</a></p>
<p>Abstract:
We continue the study of generating se-mantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model SemRegex produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ_Reg model for computing the simi-larity of two regular expressions using deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.</p>
<p>Keywords:</p>
<h3 id="677. Using Clinical Notes with Time Series Data for ICU Management.">677. Using Clinical Notes with Time Series Data for ICU Management.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1678">Paper Link</a>    Pages:6431-6436</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/224/0143.html">Swaraj Khadanga</a> ; <a href="https://dblp.uni-trier.de/pid/145/3946.html">Karan Aggarwal</a> ; <a href="https://dblp.uni-trier.de/pid/62/2078.html">Shafiq R. Joty</a> ; <a href="https://dblp.uni-trier.de/pid/s/JaideepSrivastava.html">Jaideep Srivastava</a></p>
<p>Abstract:
Monitoring patients in ICU is a challenging and high-cost task. Hence, predicting the condition of patients during their ICU stay can help provide better acute care and plan the hospitals resources. There has been continuous progress in machine learning research for ICU management, and most of this work has focused on using time series signals recorded by ICU instruments. In our work, we show that adding clinical notes as another modality improves the performance of the model for three benchmark tasks: in-hospital mortality prediction, modeling decompensation, and length of stay forecasting that play an important role in ICU management. While the time-series data is measured at regular intervals, doctor notes are charted at irregular times, making it challenging to model them together. We propose a method to model them jointly, achieving considerable improvement across benchmark tasks over baseline time-series model.</p>
<p>Keywords:</p>
<h3 id="678. Spelling-Aware Construction of Macaronic Texts for Teaching Foreign-Language Vocabulary.">678. Spelling-Aware Construction of Macaronic Texts for Teaching Foreign-Language Vocabulary.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1679">Paper Link</a>    Pages:6437-6442</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/76/4013.html">Adithya Renduchintala</a> ; <a href="https://dblp.uni-trier.de/pid/84/4538.html">Philipp Koehn</a> ; <a href="https://dblp.uni-trier.de/pid/37/3263.html">Jason Eisner</a></p>
<p>Abstract:
We present a machine foreign-language teacher that modifies text in a students native language (L1) by replacing some word tokens with glosses in a foreign language (L2), in such a way that the student can acquire L2 vocabulary simply by reading the resulting macaronic text. The machine teacher uses no supervised data from human students. Instead, to guide the machine teachers choice of which words to replace, we equip a cloze language model with a training procedure that can incrementally learn representations for novel words, and use this model as a proxy for the word guessing and learning ability of real human students. We use Mechanical Turk to evaluate two variants of the student model: (i) one that generates a representation for a novel word using only surrounding context and (ii) an extension that also uses the spelling of the novel word.</p>
<p>Keywords:</p>
<h3 id="679. Towards Machine Reading for Interventions from Humanitarian-Assistance Program Literature.">679. Towards Machine Reading for Interventions from Humanitarian-Assistance Program Literature.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1680">Paper Link</a>    Pages:6443-6447</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/69/5238.html">Bonan Min</a> ; <a href="https://dblp.uni-trier.de/pid/90/1053.html">Yee Seng Chan</a> ; <a href="https://dblp.uni-trier.de/pid/227/3166.html">Haoling Qiu</a> ; <a href="https://dblp.uni-trier.de/pid/37/11103.html">Joshua Fasching</a></p>
<p>Abstract:
Solving long-lasting problems such as food insecurity requires a comprehensive understanding of interventions applied by governments and international humanitarian assistance organizations, and their results and consequences. Towards achieving this grand goal, a crucial first step is to extract past interventions and when and where they have been applied, from hundreds of thousands of reports automatically. In this paper, we developed a corpus annotated with interventions to foster research, and developed an information extraction system for extracting interventions and their location and time from text. We demonstrate early, very encouraging results on extracting interventions.</p>
<p>Keywords:</p>
<h3 id="680. RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation.">680. RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1681">Paper Link</a>    Pages:6448-6454</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/249/2588.html">Tzuf Paz-Argaman</a> ; <a href="https://dblp.uni-trier.de/pid/21/3716.html">Reut Tsarfaty</a></p>
<p>Abstract:
Following navigation instructions in natural language (NL) requires a composition of language, action, and knowledge of the environment. Knowledge of the environment may be provided via visual sensors or as a symbolic world representation referred to as a map. Previous work on map-based NL navigation relied on small artificial worlds with a fixed set of entities known in advance. Here we introduce the Realistic Urban Navigation (RUN) task, aimed at interpreting NL navigation instructions based on a real, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of 2515 instructions aligned with actual routes over three regions of Manhattan. We then empirically study which aspects of a neural architecture are important for the RUN success, and empirically show that entity abstraction, attention over words and worlds, and a constantly updating world-state, significantly contribute to task accuracy.</p>
<p>Keywords:</p>
<h3 id="681. Context-Aware Conversation Thread Detection in Multi-Party Chat.">681. Context-Aware Conversation Thread Detection in Multi-Party Chat.</h3>
<p><a href="https://doi.org/10.18653/v1/D19-1682">Paper Link</a>    Pages:6455-6460</p>
<p>Authors:
<a href="https://dblp.uni-trier.de/pid/25/1150.html">Ming Tan</a> ; <a href="https://dblp.uni-trier.de/pid/161/3389.html">Dakuo Wang</a> ; <a href="https://dblp.uni-trier.de/pid/168/6305.html">Yupeng Gao</a> ; <a href="https://dblp.uni-trier.de/pid/50/8499-2.html">Haoyu Wang</a> ; <a href="https://dblp.uni-trier.de/pid/194/3158.html">Saloni Potdar</a> ; <a href="https://dblp.uni-trier.de/pid/139/1306.html">Xiaoxiao Guo</a> ; <a href="https://dblp.uni-trier.de/pid/28/9988.html">Shiyu Chang</a> ; <a href="https://dblp.uni-trier.de/pid/32/7445.html">Mo Yu</a></p>
<p>Abstract:
In multi-party chat, it is common for multiple conversations to occur concurrently, leading to intermingled conversation threads in chat logs. In this work, we propose a novel Context-Aware Thread Detection (CATD) model that automatically disentangles these conversation threads. We evaluate our model on four real-world datasets and demonstrate an overall im-provement in thread detection accuracy over state-of-the-art benchmarks.</p>
<p>Keywords:</p>
 

<div class="home">
<i title='' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title=''><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="https://github.com/huntercmd/ccf"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
